status,method,filepath,class_name,predict,confidence
survived,"def test_positional_param_removed():
    old_code = ""def func(a, b, c): pass""
    new_code = ""def func(a, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""Positional param 'c' was removed.""
    assert errors[0].param_name == ""c""
",tests/dev/test_check_function_signatures.py,,1,7
survived,"def test_optional_keyword_only_became_required():
    old_code = ""def func(*, a=1): pass""
    new_code = ""def func(*, a): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""Keyword-only param 'a' became required.""
    assert errors[0].param_name == ""a""
",tests/dev/test_check_function_signatures.py,,1,7
survived,"def test_new_required_keyword_only_param():
    old_code = ""def func(*, a): pass""
    new_code = ""def func(*, a, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""New required keyword-only param 'b' added.""
    assert errors[0].param_name == ""b""
",tests/dev/test_check_function_signatures.py,,1,6
survived,"def test_new_required_positional_param():
    old_code = ""def func(a): pass""
    new_code = ""def func(a, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""New required positional param 'b' added.""
    assert errors[0].param_name == ""b""
",tests/dev/test_check_function_signatures.py,,1,7
survived,"def get_file_content_at_revision(file_path: Path, revision: str) -> Optional[str]:
    try:
        return subprocess.check_output([""git"", ""show"", f""{revision}:{file_path}""], text=True)
    except subprocess.CalledProcessError as e:
        print(f""Warning: Failed to get file content at revision: {e}"", file=sys.stderr)
        return None
",dev/check_function_signatures.py,,1,7
survived,"def compare_signatures(base_branch: str = ""master"") -> list[Error]:
    errors: list[Error] = []
    for file_path in get_changed_python_files(base_branch):
        # Ignore non-Python files
        if not file_path.suffix == "".py"":
            continue

        # Ignore files not in the mlflow directory
        if file_path.parts[0] != ""mlflow"":
            continue

        # Ignore private modules
        if any(part.startswith(""_"") for part in file_path.parts):
            continue

        base_content = get_file_content_at_revision(file_path, base_branch)
        if base_content is None:
            # Find not found in the base branch, likely added in the current branch
            continue

        if not file_path.exists():
            # File not found, likely deleted in the current branch
            continue

        current_content = file_path.read_text()
        base_functions = parse_functions(base_content)
        current_functions = parse_functions(current_content)
        for func_name in set(base_functions.keys()) & set(current_functions.keys()):
            base_func = base_functions[func_name]
            current_func = current_functions[func_name]
            if param_errors := check_signature_compatibility(base_func, current_func):
                # Create individual errors for each problematic parameter
                for param_error in param_errors:
                    errors.append(
                        Error(
                            file_path=file_path,
                            line=param_error.lineno,
                            column=param_error.col_offset + 1,
                            lines=[
                                ""[Non-blocking]"",
                                param_error.message,
                                f""This breaks existing `{func_name}` calls."",
                                ""If this is not intended, please fix it."",
                            ],
                        )
                    )

    return errors
",dev/check_function_signatures.py,,1,7
survived,"def index():
    return render_template(""index.html"")
",triton_viz/visualizer/interface.py,,1,7
survived,"def test_move_matrix_min_count(array, window, min_count):
    """"""Test that matrix functions handle min_count correctly.""""""
    # Test correlation matrix
    result_corr = move_nancorrmatrix(array, window=window, min_count=min_count)

    # Test covariance matrix
    result_cov = move_nancovmatrix(array, window=window, min_count=min_count)

    # Check that results are NaN where we don't have enough observations
    n_obs = array.shape[-1]

    for t in range(n_obs):
        window_size = min(t + 1, window)

        if window_size < min_count:
            # Should be all NaN when we don't have enough observations
            assert np.all(np.isnan(result_corr[t])), (
                f""Expected NaN at position {t} for correlation""
            )
            assert np.all(np.isnan(result_cov[t])), (
                f""Expected NaN at position {t} for covariance""
            )
        else:
            # Check correlation matrix properties when we have enough data
            # Diagonal should be 1 for correlation (where not NaN)
            diag_corr = np.diag(result_corr[t])
            valid_diag = ~np.isnan(diag_corr)
            if np.any(valid_diag):
                assert_allclose(diag_corr[valid_diag], 1.0, rtol=1e-7)
",numbagg/test/test_moving.py,,1,7
survived,"    def test_dtype_preservation(self, func):
        """"""Test that dtypes are preserved.""""""
        # Set up appropriate data and window for rolling vs static
        is_rolling = func.__name__.startswith(""move_"")

        # Test float32
        data32 = np.random.randn(3, 10).astype(np.float32)
        if is_rolling:
            result32 = func(data32, window=5, min_count=3)
        else:
            result32 = func(data32)
        assert result32.dtype == np.float32

        # Test float64
        data64 = np.random.randn(3, 10).astype(np.float64)
        if is_rolling:
            result64 = func(data64, window=5, min_count=3)
        else:
            result64 = func(data64)
        assert result64.dtype == np.float64
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions,1,7
survived,"    def test_rolling_zero_variance_windows(self, move_func, expected_diag):
        """"""Test rolling windows with zero variance.""""""
        data = np.array([[1, 1, 1, 2, 3, 4], [2, 2, 2, 3, 4, 5]], dtype=np.float64)
        result = move_func(data, window=3, min_count=2)

        # First full window has constant values
        if move_func == move_nancorrmatrix:
            # Correlation undefined for zero variance
            assert np.isnan(result[2, 0, 1])
        else:
            # Covariance should be 0
            assert result[2, 0, 0] == 0.0
            assert result[2, 1, 1] == 0.0
            assert result[2, 0, 1] == 0.0

        # Later windows have variance
        assert not np.all(np.isnan(result[5]))
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions,1,7
survived,"    def test_correlation_matrix_properties(self):
        """"""Test mathematical properties of correlation matrices.""""""
        np.random.seed(123)
        data = np.random.randn(3, 30)

        result = move_exp_nancorrmatrix(data, alpha=0.4)

        for t in range(result.shape[0]):
            corr_matrix = result[t]
            if not np.any(np.isnan(corr_matrix)):
                # 1. Diagonal should be 1.0
                assert_allclose(np.diag(corr_matrix), 1.0, rtol=1e-12)

                # 2. Matrix should be symmetric
                assert_allclose(corr_matrix, corr_matrix.T, rtol=1e-12)

                # 3. All values should be in [-1, 1]
                assert np.all(corr_matrix >= -1.0)
                assert np.all(corr_matrix <= 1.0)

                # 4. Should be positive semi-definite
                eigenvals = np.linalg.eigvals(corr_matrix)
                assert np.all(eigenvals >= -1e-10), (
                    f""Correlation matrix not PSD at time {t}""
                )
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced,1,7
survived,"    def test_numerical_stability_near_singular(self):
        """"""Test numerical stability with nearly linearly dependent variables.""""""
        np.random.seed(456)
        n_obs = 100
        a1 = np.random.randn(n_obs)
        # Create a2 that's almost identical to a1 but with detectable noise
        a2 = a1 + 1e-8 * np.random.randn(
            n_obs
        )  # Slightly larger noise for detectability

        data = np.array([a1, a2])

        corr_result = move_exp_nancorrmatrix(data, alpha=0.3)
        cov_result = move_exp_nancovmatrix(data, alpha=0.3)

        # Should not produce NaN or inf values
        assert np.all(np.isfinite(corr_result[-1]))
        assert np.all(np.isfinite(cov_result[-1]))

        # Correlation should be very close to 1
        final_corr = corr_result[-1]
        assert_allclose(final_corr[0, 1], 1.0, rtol=1e-4)

        # With the added noise, correlation should be slightly less than 1.0
        # But we'll be more lenient since exponential weighting might make it exactly 1.0
        assert final_corr[0, 1] <= 1.0
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced,1,7
survived,"    def test_with_nans(self, func):
        """"""Test with NaN values.""""""
        data = np.array(
            [[1, 2, np.nan, 4], [2, 4, 6, np.nan], [np.nan, 1, 2, 3]], dtype=np.float64
        )
        alpha = 0.3
        result = func(data, alpha=alpha)

        # Check shape
        assert result.shape == (4, 3, 3)

        # Should handle NaN gracefully - check that we get some finite values
        assert np.any(np.isfinite(result))
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def test_constant_time_series(self):
        """"""Test behavior with constant time series (zero variance).""""""
        # Create constant data
        data = np.array([[5, 5, 5, 5], [3, 3, 3, 3]], dtype=np.float64)

        corr_result = move_exp_nancorrmatrix(data, alpha=0.5)
        cov_result = move_exp_nancovmatrix(data, alpha=0.5)

        # For constant data, covariance should be zero (off-diagonal)
        # and correlation should be undefined (NaN) for off-diagonal
        final_cov = cov_result[-1]
        final_corr = corr_result[-1]

        # Diagonal covariance should be zero for constant data
        assert_allclose(np.diag(final_cov), [0.0, 0.0], atol=1e-10)

        # Off-diagonal covariance should be zero
        assert_allclose(final_cov[0, 1], 0.0, atol=1e-10)
        assert_allclose(final_cov[1, 0], 0.0, atol=1e-10)

        # Correlation diagonal should be 1.0
        assert_allclose(np.diag(final_corr), [1.0, 1.0], rtol=1e-10)

        # Off-diagonal correlation should be NaN (0/0 case)
        assert np.isnan(final_corr[0, 1])
        assert np.isnan(final_corr[1, 0])
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced,1,7
survived,"    def test_scan_generate_parallel_config_migration(self, runner):
        """"""Test generate-parallel-config for model migration""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""migration.yaml""
            
            result = runner.invoke(
                scan_app,
                [""generate-parallel-config"",
                 ""--migration"", ""gpt-3.5:gpt-4"",
                 ""--max-files"", ""5"",
                 ""--output"", str(output_path)]
            )
            
            assert result.exit_code == 0
            assert ""Generated migration YAML"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def _normalize_model_name(self, model_name: str) -> str:
        """"""Normalize model name by stripping common prefixes if enabled""""""
        if not self.strip_prefix:
            return model_name.lower()
        
        normalized = model_name.lower()
        for prefix in self.model_prefixes:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):]
                break
        
        return normalized
",src/haconiwa/scan/scanner.py,ModelScanner,1,7
survived,"    def test_search_content(self, temp_model_dir):
        """"""Test content searching""""""
        scanner = ModelScanner(temp_model_dir)
        
        # Search for pattern in files
        results = scanner.search_content(""openai"", file_types=["".py""])
        assert results['total_matches'] > 0
        assert results['files_searched'] > 0
        assert len(results['matches']) > 0
        
        # Verify match details
        first_match = results['matches'][0]
        assert 'file' in first_match
        assert 'line_number' in first_match
        assert 'line' in first_match
        assert 'context' in first_match
",tests/test_scan/test_scanner.py,TestModelScanner,1,7
survived,"    def _is_model_directory(self, path: Path) -> bool:
        """"""Check if a directory likely contains model-related files""""""
        model_indicators = [
            'model', 'models', 'checkpoint', 'weights',
            'config.json', 'model.json', 'tokenizer',
            '.pt', '.pth', '.onnx', '.pb', '.h5'
        ]
        
        path_str = str(path).lower()
        return any(indicator in path_str for indicator in model_indicators)
",src/haconiwa/scan/scanner.py,ModelScanner,1,7
survived,"    def _get_action_prompt(self, action: str) -> str:
        """"""Get general action prompt""""""
        if action in self.task_templates:
            return self.task_templates[action]
        
        # Default prompt
        return f""Perform {action} on this file following best practices""
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator,1,7
survived,"    def test_scan_generate_parallel_config_project_wide(self, runner, temp_model_dir):
        """"""Test generate-parallel-config for project-wide changes""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""project-wide.yaml""
            
            # Change to temp_model_dir before running command
            import os
            original_cwd = os.getcwd()
            try:
                os.chdir(temp_model_dir)
                result = runner.invoke(
                    scan_app,
                    [""generate-parallel-config"",
                     ""--project-wide"", ""*.py"",
                     ""--action"", ""add_type_hints"",
                     ""--output"", str(output_path)]
                )
            finally:
                os.chdir(original_cwd)
            
            assert result.exit_code == 0
            assert ""Generated project-wide YAML"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def _compare_capabilities(self, model_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """"""Compare model capabilities""""""
        capabilities = {}
        
        capability_keywords = {
            'text_generation': ['generate', 'completion', 'text', 'language'],
            'code_generation': ['code', 'programming', 'syntax'],
            'translation': ['translate', 'multilingual', 'language'],
            'summarization': ['summary', 'summarize', 'abstract'],
            'classification': ['classify', 'classification', 'categorize'],
            'embedding': ['embed', 'embedding', 'vector'],
            'chat': ['chat', 'conversation', 'dialogue'],
            'reasoning': ['reason', 'logic', 'analytical'],
            'multimodal': ['multimodal', 'image', 'vision', 'audio']
        }
        
        for model, data in model_data.items():
            model_capabilities = set()
            
            # Check config for capabilities
            if data.get('config'):
                config_str = json.dumps(data['config']).lower()
                for capability, keywords in capability_keywords.items():
                    if any(keyword in config_str for keyword in keywords):
                        model_capabilities.add(capability)
            
            # Check file names and paths
            for file_info in data.get('files', []):
                file_str = file_info['path'].lower()
                for capability, keywords in capability_keywords.items():
                    if any(keyword in file_str for keyword in keywords):
                        model_capabilities.add(capability)
            
            capabilities[model] = list(model_capabilities)
        
        return capabilities
",src/haconiwa/scan/comparator.py,ModelComparator,1,6
survived,"    def test_generate_prompt_for_file_fallback(self):
        """"""Test prompt generation fallback for unknown categories""""""
        prompt = self.generator._generate_prompt_for_file(
            'src/unknown/file.py',
            'add_tests',
            None
        )
        
        assert prompt == 'Create unit tests with pytest covering edge cases'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,0,7
survived,"    def _format_tree(self, data: Any) -> str:
        """"""Format as tree structure""""""
        if not isinstance(data, dict):
            return str(data)
        
        lines = []
        
        def build_tree(node: Dict[str, Any], prefix: str = """", is_last: bool = True):
            """"""Recursively build tree representation""""""
            items = [(k, v) for k, v in node.items() if k != '__files__']
            files = node.get('__files__', [])
            
            # Add directories
            for i, (key, value) in enumerate(items):
                is_last_item = i == len(items) - 1 and not files
                
                connector = ""└── "" if is_last_item else ""├── ""
                lines.append(f""{prefix}{connector}{key}/"")
                
                if isinstance(value, dict):
                    extension = ""    "" if is_last_item else ""│   ""
                    build_tree(value, prefix + extension, is_last_item)
            
            # Add files
            for i, file_info in enumerate(files):
                is_last_file = i == len(files) - 1
                connector = ""└── "" if is_last_file else ""├── ""
                
                if isinstance(file_info, dict):
                    name = file_info.get('name', 'Unknown')
                    size = file_info.get('size', 0)
                    size_str = self._format_size(size)
                    lines.append(f""{prefix}{connector}{name} ({size_str})"")
                else:
                    lines.append(f""{prefix}{connector}{file_info}"")
        
        build_tree(data)
        return ""\n"".join(lines)
",src/haconiwa/scan/formatter.py,OutputFormatter,1,7
survived,"    def test_dynamic_concurrency_calculation(self):
        """"""Test dynamic max_concurrent calculation""""""
        scan_results = {
            'files': {f'file{i}.py': {} for i in range(20)}
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=20
        )
        
        # Should be min(5, max(1, 20//2)) = 5
        assert config['options']['max_concurrent'] == 5
        
        # Test with fewer files
        scan_results = {
            'files': {'file1.py': {}, 'file2.py': {}}
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=2
        )
        
        # Should be min(5, max(1, 2//2)) = 1
        assert config['options']['max_concurrent'] == 1
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,1,7
survived,"    def _format_json(self, data: Any) -> str:
        """"""Format as JSON""""""
        return json.dumps(data, indent=2, default=str)
",src/haconiwa/scan/formatter.py,OutputFormatter,1,7
survived,"def test_benchmark_matrix(benchmark, func, func_callable, shape):
    """"""
    Benchmark matrix functions on matrix-friendly shapes.
    """"""
    benchmark.group = f""{func}|{shape}""
    benchmark(func_callable)
",numbagg/test/test_benchmark.py,,1,7
survived,"    def test_rolling_comparison_with_pandas(self, move_func, window):
        """"""Compare rolling functions with pandas.""""""
        np.random.seed(42)
        n_vars = 4
        n_obs = 20
        # Moving functions expect (obs, vars) format
        data = np.random.randn(n_obs, n_vars)

        # NumBagg result
        numbagg_result = move_func(data, window=window, min_count=window)

        # Pandas result - data is already in (obs, vars) format that pandas expects
        df = pd.DataFrame(data)
        if move_func == move_nancorrmatrix:
            pandas_result = df.rolling(window, min_periods=window).corr()
        else:
            pandas_result = df.rolling(window, min_periods=window).cov()

        # Compare each window
        for t in range(window - 1, n_obs):
            # Extract pandas matrix for this timepoint
            pandas_matrix = pandas_result.loc[t].values
            # Compare
            assert_allclose(numbagg_result[t], pandas_matrix, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices,0,6
survived,"    def test_min_weight(self, func):
        """"""Test min_weight parameter.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 2], [2, 4], [3, 6], [4, 8]], dtype=np.float64)
        alpha = 0.1  # Low alpha means slow buildup of weight

        # High min_weight should produce more NaNs initially
        result_high = func(data, alpha=alpha, min_weight=0.8)
        result_low = func(data, alpha=alpha, min_weight=0.1)

        # Check that high min_weight produces more NaNs initially
        nan_count_high = np.sum(np.isnan(result_high[0]))
        nan_count_low = np.sum(np.isnan(result_low[0]))
        assert nan_count_high >= nan_count_low
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices,1,7
survived,"    def test_fixed_dimensional_conventions(self):
        """"""Test fixed dimensional conventions: (..., obs, vars) -> (..., obs, vars, vars).""""""
        np.random.seed(42)

        # Basic test: (obs, vars) -> (obs, vars, vars)
        data_moving = np.random.randn(100, 3)  # (obs, vars)
        corr_moving = move_nancorrmatrix(data_moving, window=10)
        cov_moving = move_nancovmatrix(data_moving, window=10)

        assert corr_moving.shape == (100, 3, 3)
        assert cov_moving.shape == (100, 3, 3)

        # Broadcasting test: (batch, obs, vars) -> (batch, obs, vars, vars)
        data_moving_3d = np.random.randn(2, 100, 3)  # (batch, obs, vars)
        corr_moving_3d = move_nancorrmatrix(data_moving_3d, window=10)

        assert corr_moving_3d.shape == (2, 100, 3, 3)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices,1,7
survived,"    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Context manager exit - ensure connections are closed.""""""
        self.close_all_connections()
",ocode_python/core/context_manager.py,ContextManager,1,7
survived,"def deploy_staticfiles(release: Release) -> bool:
    """"""Deploy static files to CDN.""""""
    print(""Deploying static files to cdn"")
    cc = f""public, max-age={int(datetime.timedelta(days=365).total_seconds())}""

    if not release.static_key:
        print(""No static files to deploy"")
        return True

    with tempfile.NamedTemporaryFile(suffix=os.path.basename(release.static_key)) as f:
        download_release_fileobj(release.static_key, f)
        f.flush()
        with DeploymentJob(f.name, ""ce-cdn.net"", version=release.version, cache_control=cc) as job:
            return job.run()
",bin/lib/builds_core.py,,1,7
survived,"    def test_run_with_uv_python_version(self, mock_run):
        """"""Test run_with_uv with Python version.""""""
        mock_run.return_value = Mock(returncode=0)

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(""server.py"", python_version=""3.11"")

        assert exc_info.value.code == 0

        cmd = mock_run.call_args[0][0]
        expected = [
            ""uv"",
            ""run"",
            ""--python"",
            ""3.11"",
            ""--with"",
            ""fastmcp"",
            ""fastmcp"",
            ""run"",
            ""server.py"",
        ]
        assert cmd == expected
",tests/cli/test_run_with_uv.py,TestRunWithUv,1,7
survived,"    def save_checkpoint(self, filename: str, data: List[T]) -> None:
        """"""Save data to a checkpoint file.
        
        Args:
            filename: Name of the checkpoint file
            data: List of model instances to save
        """"""
        if not self.enabled:
            return
            
        checkpoint_path = self.get_checkpoint_path(filename)
        with open(checkpoint_path, ""w"") as f:
            for item in data:
                f.write(item.model_dump_json() + ""\n"")
        logger.info(f""Saved checkpoint to {checkpoint_path} with {len(data)} items"")
",kura/v1/kura.py,CheckpointManager,1,7
survived,"async def reduce_clusters_from_base_clusters( 
    clusters: List[Cluster],
    *,
    model: BaseMetaClusterModel,
    checkpoint_manager: Optional[CheckpointManager] = None
) -> List[Cluster]:
    """"""Reduce clusters into a hierarchical structure.
    
    Iteratively combines similar clusters until the number of root clusters
    is less than or equal to the model's max_clusters setting.
    
    Args:
        clusters: List of initial clusters to reduce
        model: Meta-clustering model to use for reduction
        checkpoint_manager: Optional checkpoint manager for caching
        
    Returns:
        List of clusters with hierarchical structure
        
    Example:
        >>> meta_model = MetaClusterModel(max_clusters=5)
        >>> reduced = await reduce_clusters(
        ...     clusters=base_clusters,
        ...     model=meta_model,
        ...     checkpoint_manager=checkpoint_mgr
        ... )
    """"""
    logger.info(f""Starting cluster reduction from {len(clusters)} initial clusters using {type(model).__name__}"")
    
    # Try to load from checkpoint
    if checkpoint_manager:
        cached = checkpoint_manager.load_checkpoint(
            model.checkpoint_filename,
            Cluster
        )
        if cached:
            root_count = len([c for c in cached if c.parent_id is None])
            logger.info(f""Loaded {len(cached)} clusters from checkpoint ({root_count} root clusters)"")
            return cached
    
    # Start with all clusters as potential roots
    all_clusters = clusters.copy()
    root_clusters = clusters.copy()
    
    logger.info(f""Starting with {len(root_clusters)} clusters, target: {model.max_clusters}"")
    
    # Iteratively reduce until we have desired number of root clusters
    while len(root_clusters) > model.max_clusters:
        # Get updated clusters from meta-clustering
        new_current_level = await model.reduce_clusters(root_clusters)
        
        # Find new root clusters (those without parents)
        root_clusters = [c for c in new_current_level if c.parent_id is None]
        
        # Remove old clusters that now have parents
        old_cluster_ids = {c.id for c in new_current_level if c.parent_id}
        all_clusters = [c for c in all_clusters if c.id not in old_cluster_ids]
        
        # Add new clusters to the complete list
        all_clusters.extend(new_current_level)
        
        logger.info(f""Reduced to {len(root_clusters)} root clusters"")
    
    logger.info(f""Cluster reduction complete: {len(all_clusters)} total clusters, {len(root_clusters)} root clusters"")
    
    # Save to checkpoint
    if checkpoint_manager:
        checkpoint_manager.save_checkpoint(model.checkpoint_filename, all_clusters)
    
    return all_clusters
",kura/v1/kura.py,,1,7
survived,"    def __len__(self) -> int:
        """"""Number of steps in the pipeline.""""""
        return len(self.steps)
",bayesianbandits/pipelines/_learner.py,LearnerPipeline,1,8
survived,"    def test_transform_multiple_steps(self):
        """"""Test transformation with multiple steps.""""""
        steps = [
            (""double"", FunctionTransformer(lambda x: x * 2)),
            (""add_one"", FunctionTransformer(lambda x: x + 1)),
        ]
        X = np.array([[1], [2]])
        result = _transform_data(X, steps)
        expected = np.array([[3], [5]])  # (x * 2) + 1
        np.testing.assert_array_equal(result, expected)
",tests/test_agent_pipeline.py,TestTransformData,1,7
survived,"    def named_steps(self) -> Dict[str, Any]:
        """"""Access pipeline steps by name.""""""
        return dict(self.steps)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,7
survived,"    def __getitem__(self, ind: Union[int, str]) -> Any:
        """"""Get a step by index or name.""""""
        if isinstance(ind, str):
            return self.named_steps[ind]
        return self.steps[ind]
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,8
survived,"    def __init__(
        self, steps: List[Tuple[str, Any]], final_agent: Agent[TokenType]
    ) -> None:
        _validate_steps(
            steps
        ) if steps else None  # Allow empty steps for non-contextual
        self.steps = steps
        self._agent = final_agent
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,7
survived,"    def test_repr(self):
        """"""Test string representation.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        steps = []

        pipeline = NonContextualAgentPipeline(steps, agent)
        repr_str = repr(pipeline)

        assert ""NonContextualAgentPipeline"" in repr_str
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline,1,7
survived,"    def test_complex_pipeline(self):
        """"""Test complex pipeline with multiple transformers.""""""
        # Pre-fit transformers
        scaler = StandardScaler()
        pca = PCA(n_components=5)

        # Fit on dummy high-dimensional data
        dummy_data = np.random.randn(100, 20)
        scaler.fit(dummy_data)
        pca.fit(scaler.transform(dummy_data))

        pipeline = LearnerPipeline(
            steps=[(""scale"", scaler), (""pca"", pca)],
            learner=NormalRegressor(alpha=0.1, beta=1.0)
        )

        # High-dimensional data
        X = np.random.randn(50, 20)
        y = np.random.randn(50)

        # Should handle the full pipeline
        pipeline.partial_fit(X, y)

        # Test inference
        X_test = np.random.randn(10, 20)
        samples = pipeline.sample(X_test, size=1)
        assert samples.shape == (1, 10)  # (size, n_samples)

        predictions = pipeline.predict(X_test)
        assert predictions.shape == (10,)
",tests/test_learner_pipeline.py,TestLearnerPipelineIntegration,1,7
survived,"    def named_steps(self) -> Dict[str, Any]:
        """"""Access pipeline steps by name.""""""
        return dict(self.steps)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,1,7
survived,"    def test_complex_transformation_chain(self):
        """"""Test complex sequence of transformations.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, EpsilonGreedy(epsilon=0.1), random_seed=42)

        steps = [
            (""double"", FunctionTransformer(lambda x: x * 2)),
            (""add_one"", FunctionTransformer(lambda x: x + 1)),
            (""square"", FunctionTransformer(lambda x: x**2)),
        ]

        pipeline = ContextualAgentPipeline(steps, agent)

        X = np.array([[1.0], [2.0]])
        # Transform: x -> 2x -> 2x+1 -> (2x+1)^2
        # For x=1: 1 -> 2 -> 3 -> 9
        # For x=2: 2 -> 4 -> 5 -> 25

        result = pipeline.transform(X)
        expected = np.array([[9.0], [25.0]])
        np.testing.assert_array_equal(result, expected)

        # Test full pipeline
        actions = pipeline.pull(X)
        assert len(actions) == 2
",tests/test_agent_pipeline.py,TestTransformationFlow,1,7
survived,"    def transform(self, X):
        if not self.fitted:
            raise RuntimeError(""Transformer not fitted"")
        return X * 2
",tests/test_agent_pipeline.py,MockTransformer,1,7
survived,"    def arm(self, token: TokenType):
        """"""Get an arm by its action token.""""""
        return self._agent.arm(token)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,0,6
survived,"    def _generate_testing_strategy(self, feature_request: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """"""Generate testing strategy for the feature.""""""
        return {""unit_tests"": True, ""integration_tests"": True, ""coverage_target"": ""90%""}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"    def _generate_validation_criteria(self, requirements: str, analysis: Dict[str, Any]) -> str:
        """"""Generate validation criteria based on requirements and analysis.""""""
        return f""1. Implementation matches requirements: {requirements}\n2. Follows identified code patterns\n3. Maintains architectural consistency""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def _format_prp_codebase_analysis(self, analysis: Dict[str, Any]) -> str:
        """"""Format codebase analysis for PRP.""""""
        return json.dumps(analysis, indent=2)
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"    def _analyze_test_structure(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze test directory structure.""""""
        return {""pattern"": ""mirror"", ""location"": ""tests/""}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def _format_documentation_standards(self, doc_style: Dict[str, Any]) -> str:
        """"""Format documentation standards for context document.""""""
        return f""Format: {doc_style.get('format', 'markdown')}""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def generate_context_document(self, project_path: str, requirements: str, analysis: Dict[str, Any] = None) -> str:
        """"""
        Generate a comprehensive context document for AI coding assistants.
        
        Args:
            project_path (str): Path to the project being analyzed
            requirements (str): Feature requirements or task description
            analysis (Dict[str, Any]): Optional pre-computed codebase analysis
            
        Returns:
            str: Comprehensive context document
        """"""
        if analysis is None:
            analysis = self.analyze_codebase_patterns(project_path)
        
        context_doc = f""""""# Context Engineering Document

## Project Overview
**Path**: {project_path}
**Requirements**: {requirements}

## Architecture Patterns
{self._format_architecture_patterns(analysis.get('architecture_insights', {}))}

## Code Conventions
{self._format_code_conventions(analysis.get('code_patterns', {}), analysis.get('naming_conventions', {}))}

## Implementation Patterns
{self._format_implementation_patterns(analysis.get('code_patterns', {}))}

## Documentation Standards
{self._format_documentation_standards(analysis.get('documentation_style', {}))}

## Validation Criteria
{self._generate_validation_criteria(requirements, analysis)}

## Context Summary
This document provides comprehensive context for implementing: {requirements}

Key Insights:
- Project follows {analysis.get('architecture_insights', {}).get('primary_pattern', 'standard')} architecture
- Uses {analysis.get('naming_conventions', {}).get('style', 'conventional')} naming conventions
- Documentation style: {analysis.get('documentation_style', {}).get('format', 'standard')}

## Implementation Guidance
When implementing the requested feature:
1. Follow the established patterns identified above
2. Maintain consistency with existing code conventions
3. Use the documented validation criteria to verify success
4. Reference similar implementations in the codebase for guidance
""""""
        
        return context_doc
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"    def _find_artifact_path_index(index: ""SymbolIndex"", function_name: str) -> int | None:
        """"""
        Finds the index of the `artifact_path` argument in the function signature of `log_model`
        using the SymbolIndex.
        """"""
        if f := index.resolve(function_name):
            try:
                return f.all_args.index(""artifact_path"")
            except ValueError:
                return None
        return None",dev/clint/src/clint/rules/log_model_artifact_path.py,LogModelArtifactPath,1,6
survived,"    def _message(self) -> str:
        return f""DO NOT DISABLE: {self.rules}.""",dev/clint/src/clint/rules/do_not_disable.py,DoNotDisable,1,7
survived,"    def check(node: ast.Call, resolver: Resolver) -> bool:
        """"""
        Returns True if the call is threading.Thread() without a name parameter.
        """"""
        return (
            (resolved := resolver.resolve(node))
            and resolved == [""threading"", ""Thread""]
            and not any(keyword.arg == ""name"" for keyword in node.keywords)
        )",dev/clint/src/clint/rules/unnamed_thread.py,UnnamedThread,1,7
survived,"    def __init__(self, *, full_name: str, allowlist: list[str]) -> None:
        self.full_name = full_name
        self.allowlist = allowlist
",dev/clint/src/clint/rules/typing_extensions.py,TypingExtensions,1,7
survived,"    def _message(self) -> str:
        return (
            ""Found the MLflow Trace UI iframe in the notebook. ""
            ""The trace UI in cell outputs will not render correctly in previews or the website. ""
            ""Please run `mlflow.tracing.disable_notebook_display()` and rerun the cell ""
            ""to remove the iframe.""
        )",dev/clint/src/clint/rules/forbidden_trace_ui_in_notebook.py,ForbiddenTraceUIInNotebook,1,6
survived,"    def _is_bitor_none(ann: ast.AST) -> bool:
        """"""
        Returns True if `ann` looks like `... | None`.
        """"""
        return (
            isinstance(ann, ast.BinOp)
            and isinstance(ann.op, ast.BitOr)
            and (isinstance(ann.right, ast.Constant) and ann.right.value is None)
        )
",dev/clint/src/clint/rules/implicit_optional.py,ImplicitOptional,1,7
survived,"    def _message(self) -> str:
        args_str = "", "".join(f""`{arg}`"" for arg in sorted(self.unknown_args))
        return (
            f""Unknown arguments {args_str} passed to `{self.function_name}`. ""
            ""Check the function signature for valid parameter names.""
        )",dev/clint/src/clint/rules/unknown_mlflow_arguments.py,UnknownMlflowArguments,1,7
survived,"    def _message(self) -> str:
        return ""`artifact_path` parameter of `log_model` is deprecated. Use `name` instead.""
",dev/clint/src/clint/rules/log_model_artifact_path.py,LogModelArtifactPath,1,6
survived,"    def _message(self) -> str:
        return (
            ""Use `[sys.executable, '-m', 'mlflow', ...]` when running mlflow CLI in a subprocess.""
        )
",dev/clint/src/clint/rules/use_sys_executable.py,UseSysExecutable,1,6
survived,"    def check(node: ast.expr, resolver: Resolver) -> bool:
        """"""
        Returns True if the `@experimental` decorator from mlflow.utils.annotations is used
        incorrectly.
        """"""
        resolved = resolver.resolve(node)
        if not resolved:
            return False

        if resolved != [""mlflow"", ""utils"", ""annotations"", ""experimental""]:
            return False

        if not isinstance(node, ast.Call):
            return True

        version = next((k.value for k in node.keywords if k.arg == ""version""), None)
        if version is None:
            # No `version` argument, invalid usage
            return True

        if not isinstance(version, ast.Constant) or not isinstance(version.value, str):
            # `version` is not a string literal, invalid usage
            return True

        if not _is_valid_version(version.value):
            # `version` is not a valid semantic version, # invalid usage
            return True

        return False",dev/clint/src/clint/rules/invalid_experimental_decorator.py,InvalidExperimentalDecorator,1,7
survived,"    def check(node: ast.Call, index: ""SymbolIndex"") -> bool:
        """"""
        Returns True if the call looks like `mlflow.<flavor>.log_model(...)` and
        the `artifact_path` argument is specified.
        """"""
        parts = resolve_expr(node.func)
        if not parts or len(parts) != 3:
            return False

        first, second, third = parts
        if not (first == ""mlflow"" and third == ""log_model""):
            return False

        # TODO: Remove this once spark flavor supports logging models as logged model artifacts
        if second == ""spark"":
            return False

        function_name = f""{first}.{second}.log_model""
        artifact_path_idx = LogModelArtifactPath._find_artifact_path_index(index, function_name)
        if artifact_path_idx is None:
            return False

        if len(node.args) > artifact_path_idx:
            return True
        else:
            return any(kw.arg and kw.arg == ""artifact_path"" for kw in node.keywords)
",dev/clint/src/clint/rules/log_model_artifact_path.py,LogModelArtifactPath,1,7
survived,"    def test_webhook(
        self, webhook_id: str, event: Optional[WebhookEvent] = None
    ) -> WebhookTestResult:
        """"""
        Test a webhook by sending a test payload.

        Args:
            webhook_id: The ID of the webhook to test.
            event: Optional event type to test. If not specified, uses the first event from webhook.

        Returns:
            A :py:class:`mlflow.entities.webhook.WebhookTestResult` indicating success/failure and
            response details.
        """"""
        return self.store.test_webhook(webhook_id, event)",mlflow/tracking/_model_registry/client.py,ModelRegistryClient,1,7
survived,"def _send_webhook_request(
    url: str,
    payload: WebhookPayload,
    secret: Optional[str] = None,
) -> WebhookTestResult:
    """"""Send a webhook request to the specified URL.

    Args:
        url: The webhook URL to send the request to
        payload: The payload to send
        secret: Optional secret for HMAC signature

    Returns:
        WebhookTestResult indicating success/failure and response details
    """"""
    try:
        payload_bytes = json.dumps(payload).encode(""utf-8"")
        headers = {""Content-Type"": ""application/json""}

        # Add HMAC signature if secret is configured
        if secret:
            signature = _generate_hmac_signature(secret, payload_bytes)
            headers[WEBHOOK_SIGNATURE_HEADER] = signature

        response = requests.post(url, data=payload_bytes, headers=headers, timeout=30)

        return WebhookTestResult(
            success=response.status_code < 400,
            response_status=response.status_code,
            response_body=response.text[:1000] if response.text else None,  # Truncate response
        )
    except Exception as e:
        return WebhookTestResult(
            success=False,
            error_message=str(e)[:500],  # Truncate error message
        )
",mlflow/webhooks/dispatch.py,,1,7
survived,"    def test_new_OpArg(self):
        mod = self.compile(
        """"""
        from operator import OpArg

        @blue
        def create_blue_oparg(x: i32) -> OpArg:
            return OpArg('blue', i32, x)

        @blue
        def create_red_oparg() -> OpArg:
            return OpArg('red', i32, None)
        """""")

        # Test blue OpArg creation
        w_blue_oparg = mod.create_blue_oparg(42, unwrap=False)
        assert isinstance(w_blue_oparg, W_OpArg)
        assert w_blue_oparg.color == 'blue'
        assert w_blue_oparg.w_static_type is B.w_i32
        assert w_blue_oparg._w_val is not None

        # Test red OpArg creation
        w_red_oparg = mod.create_red_oparg(unwrap=False)
        assert isinstance(w_red_oparg, W_OpArg)
        assert w_red_oparg.color == 'red'
        assert w_red_oparg.w_static_type is B.w_i32
        assert w_red_oparg._w_val is None
",spy/tests/compiler/test_opimpl.py,TestOpImpl,1,7
survived,"    def test_definitions(self, mock_definitions):
        """"""Test the implementation of the definitions method""""""
        # Setup mock response
        mock_definitions.return_value = [
            (""GO:0005634"", ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated."", {}),
            (""GO:0005635"", ""The double lipid bilayer enclosing the nucleus and separating its contents from the rest of the cytoplasm."", {})
        ]
        
        # Test definitions retrieval
        definitions = list(self.oi.definitions([""GO:0005634"", ""GO:0005635""], include_metadata=True))
        
        # Check that we got two definitions back with expected content
        self.assertEqual(len(definitions), 2)
        
        # Check first definition
        self.assertEqual(definitions[0][0], ""GO:0005634"")
        self.assertEqual(definitions[0][1], ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated."")
        self.assertEqual(definitions[0][2], {}) # Empty metadata dict
        
        # Check second definition
        self.assertEqual(definitions[1][0], ""GO:0005635"")
        self.assertEqual(definitions[1][1], ""The double lipid bilayer enclosing the nucleus and separating its contents from the rest of the cytoplasm."")
        self.assertEqual(definitions[1][2], {}) # Empty metadata dict
        
        # Verify the mock was called correctly
        mock_definitions.assert_called_with([""GO:0005634"", ""GO:0005635""], include_metadata=True)
",tests/test_implementations/test_ols.py,TestOlsImplementation,1,7
survived,"def test_api_key_parameter_with_async_client():
    """"""Test that api_key parameter works with async clients.""""""
    from unittest.mock import patch, MagicMock

    # Mock the openai module
    with patch(""openai.AsyncOpenAI"") as mock_async_openai_class:
        mock_client = MagicMock()
        mock_async_openai_class.return_value = mock_client

        # Mock the from_openai import
        with patch(""instructor.from_openai"") as mock_from_openai:
            mock_instructor = MagicMock()
            mock_from_openai.return_value = mock_instructor

            # Test with async client
            from_provider(""openai/gpt-4"", async_client=True, api_key=""test-async-key"")

            # Verify AsyncOpenAI was called with the api_key
            mock_async_openai_class.assert_called_once()
            _, kwargs = mock_async_openai_class.call_args
            assert kwargs[""api_key""] == ""test-async-key""
",tests/test_auto_client.py,,1,7
survived,"def test_api_key_parameter_extraction():
    """"""Test that api_key parameter is correctly extracted from kwargs.""""""
    from unittest.mock import patch, MagicMock

    # Mock the openai module to avoid actual API calls
    with patch(""openai.OpenAI"") as mock_openai_class:
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client

        # Mock the from_openai import
        with patch(""instructor.from_openai"") as mock_from_openai:
            mock_instructor = MagicMock()
            mock_from_openai.return_value = mock_instructor

            # Test that api_key is passed to client constructor
            from_provider(""openai/gpt-4"", api_key=""test-key-123"")

            # Verify OpenAI was called with the api_key
            mock_openai_class.assert_called_once()
            _, kwargs = mock_openai_class.call_args
            assert kwargs[""api_key""] == ""test-key-123""
",tests/test_auto_client.py,,1,7
survived,"    def test_anticorrelation(self):
        # Test negative correlation
        data = np.array([[1, 2, 3, 4], [4, 3, 2, 1]], dtype=np.float64)
        result = nancorrmatrix(data)

        # Perfect negative correlation
        expected = np.array([[1.0, -1.0], [-1.0, 1.0]])
        assert_allclose(result, expected, rtol=1e-10)
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix,1,8
deleted,"def create_connection_pool() -> AsyncConnectionPool:
    """"""Create and return a PostgreSQL connection pool with configured settings.""""""
    conn_string = get_postgres_connection_string()
    
    # Create connection pool with settings from config
    pool = AsyncConnectionPool(
        conn_string,
        min_size=settings.POSTGRES_MIN_SIZE,
        max_size=settings.POSTGRES_POOL_SIZE,
        max_idle=settings.POSTGRES_MAX_IDLE,
    )
    
    logger.info(
        f""Created PostgreSQL connection pool: min_size={settings.POSTGRES_MIN_SIZE}, ""
        f""max_size={settings.POSTGRES_POOL_SIZE}, max_idle={settings.POSTGRES_MAX_IDLE}""
    )
    
    return pool
",src/memory/postgres.py,,1,7
survived,"        def __get__(self, obj: Any, objtype: Any = None) -> Any:
            warnings.warn(
                ""`jaxls.FactorGraph` has been renamed `jaxls.LeastSquaresProblem`"",
                DeprecationWarning,
                stacklevel=2,
            )

            class FactorGraph:
                @staticmethod
                def make(*args, **kwargs):
                    from ._core import FactorGraph

                    if ""factors"" in kwargs:
                        kwargs[""costs""] = kwargs.pop(""factors"")

                    warnings.warn(
                        ""`jaxls.FactorGraph` has been renamed `jaxls.FactorGraph`"",
                        DeprecationWarning,
                        stacklevel=2,
                    )

                    return FactorGraph(*args, **kwargs)

            return FactorGraph
",src/jaxls/__init__.py,_FactorGraphDescriptor,0,7
survived,"    def assert_called(
        self, hook: str | None = None, method: str | None = None, times: int = 1
    ) -> bool:
        """"""Assert that a hook was called a specific number of times.""""""
        calls = self.get_calls(hook=hook, method=method)
        actual_times = len(calls)
        assert actual_times == times, (
            f""Expected {hook!r} to be called {times} times""
            f""{f' for method {method!r}' if method else ''}, ""
            f""but was called {actual_times} times""
        )
        return True
",tests/server/middleware/test_middleware.py,RecordingMiddleware,1,7
survived,"    def assert_not_called(self, hook: str | None = None, method: str | None = None):
        """"""Assert that a hook was not called.""""""
        calls = self.get_calls(hook=hook, method=method)
        assert len(calls) == 0, f""Expected {hook!r} to not be called""
        return True
",tests/server/middleware/test_middleware.py,RecordingMiddleware,1,7
survived,"def _copy_dataset_as_is(
    dataset: DataSet,
    target_conn: AtomicConnection,
    target_exp_id: int,
) -> str:
    """"""
    Copy a dataset as-is (with raw data) to the target database.
    This is used as a fallback when NetCDF export fails.
    """"""
    try:
        with atomic(target_conn) as target_conn_atomic:
            _extract_single_dataset_into_db(dataset, target_conn_atomic, target_exp_id)
        log.info(f""Successfully copied dataset {dataset.run_id} as-is"")
        return ""copied_as_is""
    except Exception as e:
        log.error(f""Failed to copy dataset {dataset.run_id} as-is: {e}"")
        return f""failed: {str(e)}""",src/qcodes/dataset/database_extract_runs.py,,1,7
survived,"def _process_single_dataset(
    dataset: DataSet,
    source_conn: AtomicConnection,
    target_conn: AtomicConnection,
    export_path: Path,
    target_exp_id: int,
) -> str:
    """"""
    Process a single dataset: export to NetCDF and create metadata-only version
    or copy as-is if export fails.
    
    Returns:
        Status string indicating what was done with the dataset
    """"""
    run_id = dataset.run_id
    
    # Check if dataset is already in target database
    existing_run_id = get_runid_from_guid(target_conn, dataset.guid)
    if existing_run_id is not None:
        log.info(f""Dataset {run_id} (GUID: {dataset.guid}) already exists in target database"")
        return ""already_exists""
    
    # Check if dataset is completed
    if not dataset.completed:
        log.warning(f""Dataset {run_id} is not completed, copying as-is"")
        return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
    
    try:
        # Try to export to NetCDF
        log.info(f""Attempting to export dataset {run_id} to NetCDF"")
        netcdf_path = dataset.export(""netcdf"", path=export_path)
        
        if netcdf_path is None:
            log.warning(f""Failed to export dataset {run_id} to NetCDF, copying as-is"")
            return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
            
        # Load from NetCDF to create metadata-only dataset
        log.info(f""Loading dataset {run_id} from NetCDF to create metadata-only version"")
        netcdf_dataset = load_from_netcdf(netcdf_path)
        
        # Insert metadata-only version into target database
        with atomic(target_conn) as target_conn_atomic:
            _, _, target_table_name = _add_run_to_runs_table(
                netcdf_dataset, target_conn_atomic, target_exp_id
            )
            
            # Note: We deliberately don't populate the results table to keep only metadata
            log.info(f""Successfully created metadata-only version of dataset {run_id}"")
        
        return ""exported""
        
    except Exception as e:
        log.warning(f""Failed to export dataset {run_id} to NetCDF: {e}, copying as-is"")
        return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
",src/qcodes/dataset/database_extract_runs.py,,1,7
survived,"    async def register(self, *_, **__):
        self.calls += 1
        if self.calls < 3:
            raise biotech_agent.AdkClientError(""boom"")
",tests/test_register_mesh_backoff.py,StubClient,0,6
survived,"    def __init__(self):
        self.calls = 0
        self.node_id = ""X""
",tests/test_register_mesh_backoff.py,StubClient,1,6
survived,"            def _make_check(
                p: re.Pattern[str], r: GuardrailRule
            ) -> Callable[[str], Awaitable[str]]:
                if r.action is GuardrailAction.REDACT:

                    async def _check(text: str) -> str:
                        return p.sub(""[REDACTED]"", text)

                else:  # DENY or FLAG -> raise error on match

                    async def _check(text: str) -> str:
                        if p.search(text):
                            raise ValueError(f""Policy violation: {r.name}"")
                        return text

                return _check
",src/meta_agent/policy.py,PolicyChecker,1,7
survived,"    async def plugin(text: str) -> str:
        return text.upper()
",tests/test_policy_checker.py,,1,7
survived,"def test_replay_since_and_count(tmp_path: Path) -> None:
    path = tmp_path / ""audit.db""
    with logging.Ledger(str(path), broadcast=False) as led:
        led.log(messaging.Envelope(""a"", ""b"", {""x"": 1}, 0.0))
        led.log(messaging.Envelope(""b"", ""c"", {""y"": 2}, 1.0))

    with patch.object(cli.config.CFG, ""ledger_path"", str(path)):
        with patch.object(cli.time, ""sleep"", return_value=None):
            res = CliRunner().invoke(cli.main, [""replay"", ""--since"", ""0.5"", ""--count"", ""1""])

    lines = [ln.strip() for ln in res.output.splitlines() if ln.strip()]
    assert len(lines) == 1
    assert ""b -> c"" in lines[0]
",tests/test_demo_cli.py,,1,7
survived,"  def test_civic(self):

    dbc_file = ""honda_civic_touring_2016_can_generated""
    defs = CANDefine(dbc_file)

    assert defs.dv[399] == defs.dv['STEER_STATUS']
    assert defs.dv[399] == {'STEER_STATUS':
                            {7: 'PERMANENT_FAULT',
                             6: 'TMP_FAULT',
                             5: 'FAULT_1',
                             4: 'NO_TORQUE_ALERT_2',
                             3: 'LOW_SPEED_LOCKOUT',
                             2: 'NO_TORQUE_ALERT_1',
                             0: 'NORMAL'}
                            }
",opendbc/can/tests/test_define.py,TestCANDefine,1,6
survived,"    async def run_manager(manager: DummyManager, *args: object, **kwargs: object) -> None:
        await manager.run()
",test/windows/test_shutdown.py,,0,6
survived,"    def __init__(self) -> None:
        self.shutdown = asyncio.Event()
",test/windows/test_shutdown.py,DummyManager,1,7
survived,"    async def factory() -> qm.QueueManager:
        return cast(qm.QueueManager, dummy)
",test/windows/test_shutdown.py,,1,6
survived,"        async def send_json(self, data: dict[str, object]) -> None:
            messages.append(data)
",tests/test_api_server.py,DummyWS,0,7
survived,"async def test_self_improver_agent_apply(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)
    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text(patch)
    bus = messaging.A2ABus(config.Settings(bus_port=0))
    agent = SelfImproverAgent(bus, DummyLedger(), str(repo_dir), str(patch_file), allowed=[""metric.txt""])
    await agent.run_cycle()
    assert (repo_dir / ""metric.txt"").read_text().strip() == ""2""
    REGISTRY._names_to_collectors.clear()
    REGISTRY._collector_to_names.clear()
",tests/test_self_improver.py,,1,6
survived,"    async def handle(self, _env: object) -> None:  # pragma: no cover - no messaging
        return None
",src/agents/self_improver_agent.py,SelfImproverAgent,0,7
survived,"    def start_merkle_task(self, *_a, **_kw) -> None:
        pass
",tests/test_self_improver.py,DummyLedger,0,7
survived,"    async def run() -> None:
        async with bus, ledger:
            await chaos.run_cycle()
            await asyncio.sleep(0)
",tests/test_safety_agent.py,,1,6
survived,"            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(save_dir, filenames_prefix + name + count_value + ""."" + self.image_extension)
",webscout/Provider/TTI/magicstudio.py,MagicStudioImager,1,6
survived,"    def save(
        self,
        response: List[str],
        name: str = None,
        dir: str = os.getcwd(),
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire images! 💾

        Args:
            response (List[str]): Your image URLs to save
            name (str, optional): Custom name (default: uses prompt)
            dir (str, optional): Where to save (default: current directory)
            filenames_prefix (str, optional): Add prefix to filenames

        Returns:
            List[str]: Where your images were saved
        """"""
        assert isinstance(response, list), f""Response gotta be a list, not {type(response)} 🤔""
        name = self.prompt if name is None else name

        filenames = []
        count = 0

        for img_url in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            try:
                img_response = requests.get(img_url, stream=True, timeout=self.timeout)
                img_response.raise_for_status()

                with open(absolute_path_to_file, ""wb"") as fh:
                    for chunk in img_response.iter_content(chunk_size=8192):
                        fh.write(chunk)

            except requests.exceptions.RequestException as e:
                raise

        return filenames",webscout/Provider/TTI/artbit.py,ArtbitImager,1,7
survived,"    def save(
        self,
        response: List[bytes],
        name: Optional[str] = None,
        dir: Optional[Union[str, Path]] = None,
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire generated images! 💾

        Examples:
            >>> provider = FastFluxImager()
            >>> images = provider.generate(""Cool art"")
            >>> # Save with default settings
            >>> paths = provider.save(images)
            >>> # Save with custom name and directory
            >>> paths = provider.save(
            ...     images,
            ...     name=""my_art"",
            ...     dir=""my_images"",
            ...     filenames_prefix=""test_""
            ... )

        Args:
            response (List[bytes]): Your generated images
            name (Optional[str]): Custom name for your images
            dir (Optional[Union[str, Path]]): Where to save the images (default: current directory)
            filenames_prefix (str): Prefix for your image files

        Returns:
            List[str]: Paths to your saved images
        """"""
        save_dir = dir if dir else os.getcwd()
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        name = self.prompt if name is None else name
        
        # Clean up name for filename use
        safe_name = """".join(c if c.isalnum() or c in ""_-"" else ""_"" for c in name)
        safe_name = safe_name[:50]  # Truncate if too long
        
        filenames = []

        for i, image in enumerate(response):
            filename = f""{filenames_prefix}{safe_name}_{i}.{self.image_extension}""
            filepath = os.path.join(save_dir, filename)
            
            with open(filepath, ""wb"") as f:
                f.write(image)
            
            filenames.append(filename)

        return filenames
",webscout/Provider/TTI/fastflux.py,FastFluxImager,1,7
survived,"    def to_string(value) -> str:
        if isinstance(value, str):
            return value
        elif isinstance(value, dict):
            if ""text"" in value:
                return value.get(""text"", """")
            return """"
        elif isinstance(value, list):
            return """".join([to_string(v) for v in value])
        return str(value)
",webscout/Provider/TTI/utils.py,,1,6
survived,"    def test_matrix_grad(self):
        klong = KlongInterpreter()
        klong('A::˙[2 2]:^!4')
        klong('B::[2 2]:^!4')
        r = klong('(A ∇ {+/(+/ (A*B)) })')
        self.assertTrue(np.allclose(r, klong('B'), atol=1e-3))
",tests/test_autograd.py,TestAutograd,1,7
survived,"    def test_array_grad_torch(self):
        klong = KlongInterpreter()
        klong('x::˙!5')
        klong('loss::{+/x*x}')
        r = klong('x ∇ loss')

        x = torch.arange(5, dtype=torch.float64, requires_grad=True)
        loss = (x * x).sum()
        loss.backward()
        self.assertTrue(np.allclose(r, x.grad.numpy(), atol=1e-3))
",tests/test_autograd.py,TestAutograd,1,7
survived,"def _setup_simulations() -> None:
    api._simulations.clear()
    api._simulations[""a""] = api.ResultsResponse(
        id=""a"",
        forecast=[api.ForecastPoint(year=1, capability=0.1)],
        population=None,
    )
    api._simulations[""b""] = api.ResultsResponse(
        id=""b"",
        forecast=[api.ForecastPoint(year=1, capability=0.9)],
        population=None,
    )
",tests/test_insight_endpoint.py,,1,6
survived,"    def __post_init__(self) -> None:
        if self.auth_token:
            self.headers[""Authorization""] = f""Bearer {self.auth_token}""
        self.headers.setdefault(""Content-Type"", ""application/json"")
",src/meta_agent/services/telemetry_client.py,EndpointConfig,1,7
survived,"    def __init__(
        self,
        endpoints: Dict[str, EndpointConfig],
        *,
        rate_limit: int = 5,
        timeout: int = 10,
    ) -> None:
        if not endpoints:
            raise ValueError(""At least one endpoint must be configured"")
        self.endpoints = endpoints
        self.timeout = timeout
        self._sem = asyncio.Semaphore(rate_limit)
        self._session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=None)
        )
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        await self._session.close()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"async def telemetry_client():
    with patch(""aiohttp.ClientSession"") as mock_session:
        response = AsyncMock()
        response.status = 200
        response.json = AsyncMock(return_value={""ok"": True})
        cm = AsyncMock()
        cm.__aenter__.return_value = response
        mock_session.return_value.post.return_value = cm
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        try:
            yield client
        finally:
            await client.close()
",tests/unit/test_telemetry_client.py,,1,7
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,8
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def generate(
        self,
        spec: Mapping[str, Any],
        *,
        diagram_type: str = ""flowchart"",
        direction: str | None = None,
        node_styles: Mapping[str, str] | None = None,
    ) -> str:
        """"""Return a Mermaid diagram describing the agent.

        Parameters
        ----------
        spec:
            Mapping describing the agent (e.g. :class:`SpecSchema` dict).
        diagram_type:
            Mermaid diagram type such as ``flowchart`` or ``graph``.
        direction:
            Layout direction (``TB`` top-bottom, ``LR`` left-right, etc.).
        node_styles:
            Optional mapping of node identifiers to Mermaid style strings.

        Returns
        -------
        str
            Mermaid diagram definition.
        """"""
        if not isinstance(spec, Mapping):
            raise DiagramGenerationError(""spec must be a mapping"")

        direction = direction or self.default_direction

        lines: list[str] = [f""{diagram_type} {direction}""]

        inputs = spec.get(""inputs"") or {}
        outputs = spec.get(""outputs"") or {}
        task_desc = spec.get(""task_description"", ""Agent"")

        agent_id = ""AGENT""
        lines.append(f""    {agent_id}[{task_desc}]"")

        for name in inputs:
            node_id = f""IN_{name}"".replace("" "", ""_"")
            lines.append(f""    {node_id}[{name}]"")
            lines.append(f""    {node_id} --> {agent_id}"")

        for name in outputs:
            node_id = f""OUT_{name}"".replace("" "", ""_"")
            lines.append(f""    {agent_id} --> {node_id}"")
            lines.append(f""    {node_id}[{name}]"")

        if node_styles:
            for node, style in node_styles.items():
                lines.append(f""    style {node} {style}"")

        return ""\n"".join(lines) + ""\n""",src/meta_agent/ux/diagram_generator.py,DiagramGenerator,1,7
survived,"    def __init__(self, default_direction: str = ""TB"") -> None:
        self.default_direction = default_direction
",src/meta_agent/ux/diagram_generator.py,DiagramGenerator,1,8
survived,"    def warning(self, message: str, *, level: int = 1) -> None:
        """"""Output a warning message.""""""
        self._echo(message, fg=""yellow"", level=level)
",src/meta_agent/ux/cli_output.py,CLIOutput,1,6
survived,"    def _echo(
        self,
        message: str,
        *,
        fg: str | None = None,
        bold: bool = False,
        err: bool = False,
        level: int = 1,
    ) -> None:
        if self.verbosity >= level:
            click.secho(message, fg=fg, bold=bold, err=err)
",src/meta_agent/ux/cli_output.py,CLIOutput,1,7
survived,"    def form(self, fields: Sequence[str]) -> dict[str, str]:
        """"""Prompt for multiple fields and return a mapping of answers.""""""
        responses: dict[str, str] = {}
        for field in fields:
            responses[field] = self.ask(f""{field}:"")
        return responses",src/meta_agent/ux/interactive.py,Interactive,1,7
survived,"    def ask(self, prompt: str) -> str:
        """"""Return a response to the given prompt.""""""
        return input(f""{prompt.strip()} "")
",src/meta_agent/ux/interactive.py,Interactive,0,7
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestAgentRegistryFunctions.AAgent,1,6
survived,"    def test_forecast_structure(self):
        data = asyncio.run(self.agent._forecast())
        payload = json.loads(data)
        self.assertEqual(payload[""agent""], self.agent.NAME)
        self.assertEqual(len(payload[""payload""]), 48)
        self.assertIsInstance(payload[""payload""], list)
",tests/test_energy_agent_behavior.py,TestEnergyAgentBehavior,1,7
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestVersionOverride.AgentOld,1,6
survived,"    def test_run_cycle_negative_delta_g_posts_job(self) -> None:
        class LowFin(demo.AgentFin):
            def latent_work(self, bundle):
                return 0.0

        class CaptureOrch(demo.Orchestrator):
            def __init__(self) -> None:
                self.called = False

            def post_alpha_job(self, bundle_id: int, delta_g: float) -> None:
                self.called = True

        orch = CaptureOrch()
        demo.run_cycle(
            orch,
            LowFin(),
            demo.AgentRes(),
            demo.AgentEne(),
            demo.AgentGdl(),
            DummyModel(),
        )
        self.assertTrue(orch.called)
",tests/test_alpha_agi_business_3_v1.py,TestAlphaAgiBusiness3Demo,0,6
survived,"    def commit(self, weight_update: dict[str, object]) -> None:  # type: ignore[override]
        self.committed = True
        super().commit(weight_update)
",tests/test_alpha_agi_business_3_v1.py,DummyModel,1,7
survived,"    def __enrich_model__(cls) -> type[EnrichModel]:
        """"""
        Convert this SQLAlchemy model to an EnrichModel representation.

        This method introspects the SQLAlchemy model and creates a corresponding
        EnrichModel with fields and relationships based on the SQLAlchemy metadata.

        Returns:
            A dynamically created EnrichModel class
        """"""
        if not issubclass(cls, DeclarativeBase):
            raise TypeError(f""{cls.__name__} must inherit from SQLAlchemy DeclarativeBase"")

        # Get SQLAlchemy mapper
        mapper = inspect(cls)

        # Build field definitions for the EnrichModel
        field_definitions: dict[str, Any] = {}

        # Process columns
        for column_prop in mapper.column_attrs:
            column = column_prop.columns[0]
            field_name = column_prop.key

            # Skip fields marked with exclude in info
            if column.info.get(""exclude"", False):
                continue

            # Get Python type from SQLAlchemy column type
            python_type = _sqlalchemy_type_to_python(column.type)

            # Handle nullable columns
            if column.nullable:
                python_type = python_type | None

            # Get description from column info
            description = column.info.get(""description"", f""{field_name} field"")

            # Create Pydantic Field
            if column.default is not None or column.server_default is not None:
                # Has default value
                field_definitions[field_name] = (python_type, Field(description=description))
            else:
                # Required field
                field_definitions[field_name] = (python_type, Field(description=description))

        # Process relationships
        for rel_prop in mapper.relationships:
            field_name = rel_prop.key
            rel_info = rel_prop.info

            # Skip relationships marked with exclude
            if rel_info.get(""exclude"", False):
                continue

            # Get description
            description = rel_info.get(
                ""description"", f""Relationship to {rel_prop.mapper.class_.__name__}EnrichModel""
            )

            # Determine relationship type
            if rel_prop.uselist:
                # One-to-many or many-to-many relationship
                target_class_name = rel_prop.mapper.class_.__name__
                # Map to EnrichModel version of the class
                enrich_target_name = f""{target_class_name}EnrichModel""
                rel_type = list[enrich_target_name]  # Using string forward reference
            else:
                # One-to-one or many-to-one relationship
                target_class_name = rel_prop.mapper.class_.__name__
                # Map to EnrichModel version of the class
                enrich_target_name = f""{target_class_name}EnrichModel""
                rel_type = enrich_target_name

            # Create Relationship field
            field_definitions[field_name] = (rel_type, Relationship(description=description))

        # Get model documentation
        model_doc = cls.__doc__ or f""{cls.__name__} entity""

        # Create the EnrichModel class dynamically
        enrich_model_class = create_model(
            f""{cls.__name__}EnrichModel"",
            __base__=EnrichModel,
            __doc__=model_doc,
            **field_definitions,
        )

        # Store reference to original SQLAlchemy model
        # Use setattr to ensure it's properly set on the class
        enrich_model_class._sqlalchemy_model = cls

        return enrich_model_class
",src/enrichmcp/sqlalchemy/mixin.py,EnrichSQLAlchemyMixin,1,6
survived,"async def get_user(user_id: int, ctx: EnrichContext) -> UserEnrichModel | None:
    """"""Get a specific user by ID.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        user = await session.get(User, user_id)
        if not user:
            return None

        return UserEnrichModel(
            id=user.id,
            username=user.username,
            email=user.email,
            full_name=user.full_name,
            is_active=user.is_active,
            created_at=user.created_at,
        )
",examples/sqlalchemy_shop/app.py,,1,7
survived,"    def test_async_attrs_compatibility(self):
        """"""Test that the mixin works with AsyncAttrs.""""""

        class Base(DeclarativeBase):
            pass

        class AsyncUser(Base, AsyncAttrs, EnrichSQLAlchemyMixin):
            """"""Async user model.""""""

            __tablename__ = ""async_users""

            id: Mapped[int] = mapped_column(primary_key=True)
            username: Mapped[str] = mapped_column()

        # Should work without issues
        AsyncUserEnrichModel = AsyncUser.__enrich_model__()
        assert issubclass(AsyncUserEnrichModel, EnrichModel)
        assert ""id"" in AsyncUserEnrichModel.model_fields
        assert ""username"" in AsyncUserEnrichModel.model_fields
",tests/test_sqlalchemy_integration.py,TestEdgeCases,1,7
survived,"    def test_one_to_many_relationship(self):
        """"""Test one-to-many relationship conversion.""""""

        class Base(DeclarativeBase):
            pass

        class User(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""users""

            id: Mapped[int] = mapped_column(primary_key=True)
            username: Mapped[str] = mapped_column()
            orders: Mapped[list[""Order""]] = relationship(
                back_populates=""user"", info={""description"": ""User's orders""}
            )

        class Order(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""orders""

            id: Mapped[int] = mapped_column(primary_key=True)
            user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))
            user: Mapped[User] = relationship(
                back_populates=""orders"", info={""description"": ""Order's user""}
            )

        # Convert to EnrichModel
        UserEnrichModel = User.__enrich_model__()
        fields = UserEnrichModel.model_fields

        # Check that orders field exists and is a Relationship
        assert ""orders"" in fields
        assert isinstance(fields[""orders""].default, Relationship)
        assert fields[""orders""].default.description == ""User's orders""

        # Check the type annotation (should be list[""OrderEnrichModel""])
        # The annotation will be a string forward reference
        assert ""list"" in str(fields[""orders""].annotation)
        assert ""OrderEnrichModel"" in str(fields[""orders""].annotation)
",tests/test_sqlalchemy_integration.py,TestRelationships,1,6
survived,"    def log(self, event: str, **payload):
        with self.path.open(""a"", encoding=""utf-8"") as fp:
            json.dump({""ts"": _utcnow_ms(), ""event"": event, **payload}, fp, ensure_ascii=False)
            fp.write(""\n"")
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,LineageTracer,1,7
survived,"    def __init__(self, ledger_path: str | pathlib.Path):
        self.path = pathlib.Path(ledger_path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,LineageTracer,1,7
survived,"            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] ← %s"", cls_name, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Stub,1,6
survived,"    def __call__(self, prompt:str, **kw):
        return self.run(prompt, **kw)
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,Agent,1,7
survived,"    def __call__(self, prompt:str, **kw):
        return self.run(prompt, **kw)
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,Agent,1,7
survived,"    def __init__(self, hidden: int, act_dim: int):
        super().__init__(); self.r = nn.Linear(hidden+act_dim, 1); self.h = nn.Linear(hidden+act_dim, hidden)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Dyn,1,6
survived,"def emit_helm(dir_:Path=Path(""helm_chart"")):
    dir_.mkdir(exist_ok=True)
    (dir_/""values.yaml"").write_text(HELM_VALUES)
    (dir_/""Chart.yaml"").write_text(""apiVersion: v2\nname: alpha-asi-demo\nversion: 0.1.0\n"")
    print(""Helm chart →"",dir_)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,,1,7
survived,"            def handle(self,msg):
                if ""ask_plan"" in msg:
                    try:
                        plan=self._safe_call(msg[""ask_plan""])
                        self.emit(""planning_agent"",{""llm_plan"":plan})
                    except Exception as e:
                        LOG.warning(""LLMPlanner error: %s"",e)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,LLMPlanner,1,7
survived,"def _str_tkn(text: str) -> int:
    # naïve token estimate ≈‑ 1 token / 4 chars in English
    return max(1, math.ceil(len(text)/4))
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,,1,7
survived,"    def chat(self, msgs: List[Dict[str,str]], **kw) -> str:
        merged = dict(temperature=self.temperature, max_tokens=self.max_tokens, **kw)
        attempts = 0
        while True:
            GLOBAL_LIMITER.acquire(_str_tkn(json.dumps(msgs)))
            try:
                if self._backend == ""openai"":
                    rsp = self._client.chat.completions.create(model=self._model, messages=msgs, stream=False, **merged)
                    return rsp.choices[0].message.content
                if self._backend == ""anthropic"":
                    rsp = self._client.messages.create(model=self._model, messages=msgs, **merged)
                    return rsp.content[0].text
                if self._backend == ""gemini"":
                    return self._client.generate_content(msgs[-1][""content""], **merged).text
                if self._backend in (""mistral"",""llama""):
                    prompt = """".join(f""<{m['role']}> {m['content']}"" for m in msgs)+""\n<assistant> ""
                    out = self._client(prompt, max_tokens=self.max_tokens, temperature=self.temperature, stop=[""</assistant>""])
                    return out[""choices""][0][""text""].strip()
            except Exception as e:
                attempts += 1
                wait = min(60, 2**attempts)
                LOGGER.warning(""LM error %s; retry in %.1fs"", e, wait)
                time.sleep(wait)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,LMClient,1,7
survived,"    def update(self, **kw):
        for k, v in kw.items():
            if hasattr(self, k):
                setattr(self, k, v)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Config,1,7
survived,"    def score(self, metrics: Dict[str,float]) -> float:
        return (
            self.latency * (1/ (1+metrics.get(""latency"",0))) +
            self.cost    * (1/ (1+metrics.get(""cost"",0))) +
            self.carbon  * (1/ (1+metrics.get(""carbon"",0))) +
            self.risk    * (1- metrics.get(""risk"",0))
        )
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,ObjectiveWeights,1,7
survived,"    def __init__(self, name: str):
        self.name = name
        A2ABus.subscribe(name, self._on)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Agent,1,7
survived,"    def run(self, code: str, func_name: str, *args, **kw):
        loc: Dict[str,Any] = {}
        with self:
            exec(code, {}, loc)
        if func_name not in loc:
            raise AttributeError(f""{func_name} not found"")
        return loc[func_name](*args, **kw)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,SafeExec,0,7
survived,"    def chat(self, msgs: List[Dict[str,str]], **kw) -> str:
        merged = dict(temperature=self.temperature, max_tokens=self.max_tokens, **kw)
        attempts = 0
        while True:
            GLOBAL_LIMITER.acquire(_str_tkn(json.dumps(msgs)))
            try:
                if self._backend == ""openai"":
                    rsp = self._client.chat.completions.create(model=self._model, messages=msgs, stream=False, **merged)
                    return rsp.choices[0].message.content
                if self._backend == ""anthropic"":
                    rsp = self._client.messages.create(model=self._model, messages=msgs, **merged)
                    return rsp.content[0].text
                if self._backend == ""gemini"":
                    return self._client.generate_content(msgs[-1][""content""], **merged).text
                if self._backend in (""mistral"",""llama""):
                    prompt = """".join(f""<{m['role']}> {m['content']}"" for m in msgs)+""\n<assistant> ""
                    out = self._client(prompt, max_tokens=self.max_tokens, temperature=self.temperature, stop=[""</assistant>""])
                    return out[""choices""][0][""text""].strip()
            except Exception as e:
                attempts += 1
                wait = min(60, 2**attempts)
                LOGGER.warning(""LM error %s; retry in %.1fs"", e, wait)
                time.sleep(wait)
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,LMClient,1,7
survived,"    def remember(self, obs, reward):
        self.buffer.append((obs,reward))
        if len(self.buffer)>CFG.buffer_limit:
            self.buffer.pop(0)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Learner,1,7
survived,"    def __init__(self, name):
        super().__init__(""127.0.0.1"", 0)
        self.name = name
",tests/test_multi_contributor.py,FakeComm,1,7
survived,"        async def __call__(self, prompt: str) -> str:
            self.prompt = prompt
            return ""online""
",tests/test_alpha_agi_business_3_v1.py,DummyAgent,1,6
survived,"def test_list_ids_empty_db(temp_db_path):
    command = ListIdsCommand(
        db_path=temp_db_path,
    )

    results = list_ids(command)

    assert len(results) == 0",src/mcp_server_pocket_pick/tests/functionality/test_list_ids.py,,1,7
survived,"def test_list_ids_with_tags(populated_db):
    command = ListIdsCommand(
        tags=[""programming""],
        limit=10,
        db_path=populated_db,
    )

    results = list_ids(command)

    assert len(results) == 4
    for expected in [""python-1"", ""sql-1"", ""testing-1"", ""regex-1""]:
        assert expected in results
",src/mcp_server_pocket_pick/tests/functionality/test_list_ids.py,,1,7
survived,"    def test_list_agents_flag(self):
        args = self._parse([""--list-agents""])
        self.assertTrue(args.list_agents)
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest,1,6
survived,"    def test_venv_python_windows(self):
        with mock.patch.object(os, 'name', 'nt'):
            path = PureWindowsPath('C:/v')
            self.assertEqual(
                quickstart._venv_python(path),
                path / 'Scripts' / 'python.exe'
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest,1,8
survived,"    def test_policy(self):
        if minimuzero is None:
            self.skipTest(""muZero demo deps missing"")
        agent = minimuzero.MiniMu()
        obs = agent.reset()
        policy = agent.policy(obs)
        self.assertEqual(len(policy), agent.action_dim)
        self.assertAlmostEqual(policy.sum(), 1.0, places=3)
",alpha_factory_v1/tests/test_muzero_demo.py,MiniMuTest,1,7
survived,"def main(base_dir: str = DEFAULT_DIR) -> int:
    failures = []
    for entry in os.listdir(base_dir):
        path = os.path.join(base_dir, entry)
        if os.path.isdir(path):
            if entry.startswith('.') or entry.startswith('__'):
                continue
            readme = os.path.join(path, ""README.md"")
            if not os.path.isfile(readme):
                failures.append(f""Missing README.md in {entry}"")
    if failures:
        for msg in failures:
            print(f""ERROR: {msg}"", file=sys.stderr)
        return 1
    print(""All demo directories contain README.md"")
    return 0
",alpha_factory_v1/demos/validate_demos.py,,1,7
survived,"    def do_POST(self):
        length = int(self.headers.get(""Content-Length"", 0))
        type(self).received_body = self.rfile.read(length)
        type(self).received_path = self.path
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b""ok"")
",alpha_factory_v1/tests/test_marketplace_client.py,_Handler,1,6
survived,"    def __init__(self):
        self.messages = []
",alpha_factory_v1/tests/test_ping_agent.py,DummyOrchestrator,1,6
survived,"    def test_fallback_when_package_missing(self):
        original = im.distribution
        def fake_distribution(name):
            raise im.PackageNotFoundError
        im.distribution = fake_distribution
        try:
            mod = importlib.import_module(""requests"")
            from alpha_factory_v1 import requests as shim
            self.assertIs(mod.get, shim.get)
            self.assertIs(mod.post, shim.post)
        finally:
            im.distribution = original
            sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest,1,7
survived,"    def test_alpha_factory_chart(self):
        chart = HELM_DIR / ""alpha-factory"" / ""Chart.yaml""
        values = HELM_DIR / ""alpha-factory"" / ""values.yaml""
        self.check_chart_file(chart)
        self.assertTrue(values.is_file(), ""values.yaml missing for alpha-factory"")
",alpha_factory_v1/tests/test_helm_charts.py,HelmChartTests,1,6
survived,"    def test_short_readme_fails(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            demo_dir = Path(tmpdir) / ""demo""
            demo_dir.mkdir()
            (demo_dir / ""README.md"").write_text(""short\n"")
            ret = validate_demos.main(str(tmpdir))
            self.assertEqual(ret, 1)
",alpha_factory_v1/tests/test_validate_demos.py,TestValidateDemos,1,6
survived,"    def evaluate(
        self,
        path: Path,
        timeout: int = 60,
        output_format: str = ""text"",
    ) -> str:
        """"""Run tests at ``path`` and return a formatted report.""""""
        self.logger.info(""Starting evaluation for %s"", path)
        result: CollectionResult = self.result_collector.execute_and_collect(
            path, timeout=timeout
        )
        return self.reporter.generate_report(result, output_format=output_format)",src/meta_agent/evaluation/harness.py,EvaluationHarness,1,7
survived,"def test_simulate_export_formats() -> None:
    runner = CliRunner()
    with patch.object(cli.orchestrator, ""Orchestrator""):
        with patch.object(cli, ""asyncio""):
            res_json = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--pop-size"",
                    ""1"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""json"",
                ],
            )
            res_csv = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--pop-size"",
                    ""1"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""csv"",
                ],
            )
    assert res_json.output.startswith(""["")
    assert ""year,capability,affected"" in res_csv.output
",tests/test_cli_runner_ext.py,,1,7
survived,"def safe_current_wb_run_step() -> int | None:
    try:
        import wandb

        wandb_run = wandb.run
        if wandb_run is None:
            return None
    except ImportError:
        return None
    else:
        try:
            return int(wandb_run.step)
        except Exception:
            return None
",weave/trace/weave_client.py,,1,7
survived,"async def main() -> None:
    """"""Poll live APIs and emit events via data_feeds.""""""
    logging.basicConfig(level=logging.INFO, format=""%(asctime)s %(message)s"")
    async for evt in stream_macro_events(live=True):
        log.info(""event=%s"", json.dumps(evt))
",alpha_factory_v1/demos/macro_sentinel/collector/collector.py,,1,7
survived,"def randN(n):
    global seed
    seed = (seed * 1664525 + 1013904223) % 2147483647
    return seed % n
",tests/rosetta/transpiler/Python/conways-game-of-life.py,,1,6
survived,"def someCondition():
    return False",tests/rosetta/transpiler/Python/conditional-structures-7.py,,0,6
survived,"def step(l):
    y = 0
    while y < l.h:
        x = 0
        while x < l.w:
            setCell(l.b, x, y, nextState(l.a, x, y))
            x = x + 1
        y = y + 1
    tmp = l.a
    l = dataclasses.replace(l, a=l.b)
    l = dataclasses.replace(l, b=tmp)
",tests/rosetta/transpiler/Python/conways-game-of-life.py,,1,6
survived,"def main():
    inputs = [""0.9054054"", ""0.518518"", ""0.75""]
    for s in inputs:
        r = parseRational(s)
        print(s + "" = "" + str(r[""num""]) + ""/"" + str(r[""den""]))
",tests/rosetta/transpiler/Python/convert-decimal-number-to-rational.py,,1,6
survived,"def fetch_():
    return """"
",tests/rosetta/transpiler/Python/conditional-structures-7.py,,1,6
survived,"def shuffle(xs):
    arr = xs
    i = len(arr) - 1
    while i > 0:
        j = _now() % (i + 1)
        tmp = arr[i]
        arr[i] = arr[j]
        arr[j] = tmp
        i = i - 1
    return arr
",tests/rosetta/transpiler/Python/concurrent-computing-1.py,,1,6
survived,"    async def get_next_item(self):
        prompt = (
            f""Compose a four line Sanskrit poem in the {self.config.meter} meter. ""
            ""Use IAST transliteration only.""
        )
        user_msg = {""role"": ""user"", ""content"": prompt}
        return (tuple([frozenset(user_msg.items())]), None, None)
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv,1,7
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Remove stale entries from the in-memory cache.""""""
        now = datetime.now()
        with self.lock:
            keys_to_delete = [
                k for k, v in self.cache.items() if now - v.time > stale_after
            ]
            for key in keys_to_delete:
                del self.cache[key]",src/cachier/cores/memory.py,_MemoryCore,1,7
survived,"def test_planning_agent_no_openai_sdk() -> None:
    """"""Agent should run even when openai.agents is missing.""""""
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils import config, messaging
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.agents import planning_agent

    class DummyLedger:
        def __init__(self, *_a, **_kw) -> None:
            pass

        def log(self, _env) -> None:  # type: ignore[override]
            pass

        def start_merkle_task(self, *_a, **_kw) -> None:
            pass

        async def stop_merkle_task(self) -> None:
            pass

        def close(self) -> None:
            pass

    settings = config.Settings(bus_port=0, openai_api_key=""k"")
    bus = messaging.A2ABus(settings)
    agent = planning_agent.PlanningAgent(bus, DummyLedger())

    assert agent.oai_ctx is None
    asyncio.run(agent.run_cycle())",tests/test_agents.py,,1,7
survived,"def test_auto_device_from_config(monkeypatch, tmp_path, non_network: None) -> None:
    """""" ""device: auto"" should resolve to cuda when available.""""""
    cfg = tmp_path / ""config.yaml""
    cfg.write_text(""general:\n  device: auto\n"")

    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")

    module = ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    if module in sys.modules:
        del sys.modules[module]
    mod = importlib.import_module(module)

    import torch

    expected = ""cuda"" if torch.cuda.is_available() else ""cpu""
    assert mod.CFG.device == expected",tests/test_world_model_config.py,,1,7
survived,"    def __init__(self, *args, **kwargs):
        pass
",stubs/openai_agents/__init__.py,AgentRuntime,1,6
survived,"    def register(self, *args, **kwargs):
        pass
",stubs/openai_agents/__init__.py,AgentRuntime,0,7
survived,"    def decorator(func):
        return func
",stubs/openai_agents/__init__.py,,1,6
survived,"    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)
",tests/test_selfheal_entrypoint_offline.py,,1,6
survived,"    def launch(self, *a, **k):
        pass
",tests/test_selfheal_entrypoint_offline.py,DummyBlocks,0,7
survived,"    def test_no_log_skips_directory(self) -> None:
        with tempfile.TemporaryDirectory() as home:
            env = os.environ.copy()
            env[""HOME""] = home
            env.pop(""ALPHA_CONVERSION_LEDGER"", None)
            result = subprocess.run(
                [sys.executable, STUB, ""--alpha"", ""skip"", ""--no-log""],
                capture_output=True,
                text=True,
                env=env,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            default_dir = Path(home) / "".aiga""
            self.assertFalse(default_dir.exists())
",tests/test_alpha_conversion_stub.py,TestAlphaConversionStub,1,7
survived,"def test_load_documents(tmp_path):
    (tmp_path / ""doc1.txt"").write_text(""This is the first document. Hello world!"")
    (tmp_path / ""doc2.txt"").write_text(""Second document: world is big and bright."")

    corpus = run_hlda.load_documents(str(tmp_path))
    assert corpus == [
        [""first"", ""document"", ""hello"", ""world""],
        [""second"", ""document"", ""world"", ""big"", ""bright""],
    ]
",tests/test_run_hlda_utils.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_left_join.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/right_join.py,Order,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/cross_join_triple.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/outer_join.py,Order,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_having.py,Person,1,7
survived,"        def set_draw_color(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyFPDF,0,7
survived,"        def add_paragraph(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyDocxDocument,0,7
survived,"def stub_dependencies(monkeypatch):
    fpdf_mod = types.ModuleType('fpdf')
    class DummyFPDF:
        def add_page(self):
            pass
        def add_font(self, *args, **kwargs):
            pass
        def set_font(self, *args, **kwargs):
            pass
        def set_margins(self, *args, **kwargs):
            pass
        def multi_cell(self, *args, **kwargs):
            pass
        def set_draw_color(self, *args, **kwargs):
            pass
        def line(self, *args, **kwargs):
            pass
        def output(self, *args, **kwargs):
            pass
        y = 0
    fpdf_mod.FPDF = DummyFPDF
    monkeypatch.setitem(sys.modules, 'fpdf', fpdf_mod)

    docx_mod = types.ModuleType('docx')
    class DummyDocxDocument:
        def add_heading(self, *args, **kwargs):
            pass
        def add_paragraph(self, *args, **kwargs):
            pass
        def save(self, *args, **kwargs):
            pass
    docx_mod.Document = DummyDocxDocument
    monkeypatch.setitem(sys.modules, 'docx', docx_mod)

    import KindleClippings
    monkeypatch.setattr(KindleClippings, 'args', types.SimpleNamespace(format='txt'), raising=False)",tests/conftest.py,,1,6
survived,"def test_remove_question_and_ampersand():
    assert remove_chars('Where & When?') == 'Where and When'
    assert remove_chars('Q? & A?') == 'Q and A'
    assert remove_chars('This & That & Those') == 'This and That and Those'
    assert remove_chars('??What?') == 'What'
    assert remove_chars(' weird??? &?? ') == 'weird and'
",tests/test_remove_chars.py,,1,7
survived,"    async def _fake_comment(_: float) -> str:
        return ""ok""
",tests/test_alpha_agi_business_3_v1.py,,0,6
survived,"def _load_results() -> None:
    for f in _results_dir.glob(""*.json""):
        try:
            data = json.loads(f.read_text())
            res = ResultsResponse(**data)
        except Exception:
            continue
        _simulations[res.id] = res
",src/interface/api_server.py,,1,7
survived,"            def __init__(self, *a, **kw):
                self.name = kw.get(""name"", ""agent"")
",tests/test_macro_adk_integration.py,_Agent,1,7
survived,"def test_governance_bridge_port_arg() -> None:
    """"""Verify the CLI accepts the --port option.""""""
    result = subprocess.run(
        [""governance-bridge"", ""--port"", ""1234"", ""--help""],
        capture_output=True,
        text=True,
        check=True,
    )
    assert result.returncode == 0",tests/test_governance_bridge_cli.py,,1,7
survived,"    def visit_If(self, node: ast.If) -> None:
        test = self.convert_expr(node.test)
        self.emit(f""if {test} {{"")
        self.indent += 1
        for stmt in node.body:
            self.visit(stmt)
        self.indent -= 1
        if node.orelse:
            self.emit(""} else {"")
            self.indent += 1
            for stmt in node.orelse:
                self.visit(stmt)
            self.indent -= 1
            self.emit(""}"")
        else:
            self.emit(""}"")
",tools/any2mochi/py/py2mochi.py,Converter,1,7
survived,"    def visit_Expr(self, node: ast.Expr) -> None:
        if (
            isinstance(node.value, ast.Call)
            and isinstance(node.value.func, ast.Name)
            and node.value.func.id == ""print""
        ):
            args = "", "".join(self.convert_expr(a) for a in node.value.args)
            self.emit(f""print({args})"")
        else:
            self.emit(self.convert_expr(node.value))
",tools/any2mochi/py/py2mochi.py,Converter,1,7
survived,"    def visit_Module(self, node: ast.Module) -> None:
        # first collect dataclass information
        for stmt in node.body:
            if isinstance(stmt, ast.ClassDef):
                dec_names = [
                    getattr(d, ""id"", None) or getattr(d, ""attr"", None)
                    for d in stmt.decorator_list
                ]
                if ""dataclass"" not in dec_names:
                    continue
                fields: list[tuple[str, str]] = []
                methods: list[ast.FunctionDef] = []
                for sub in stmt.body:
                    if isinstance(sub, ast.AnnAssign) and isinstance(
                        sub.target, ast.Name
                    ):
                        fields.append(
                            (sub.target.id, self.convert_type(sub.annotation))
                        )
                    if isinstance(sub, ast.FunctionDef):
                        methods.append(sub)
                base = stmt.bases[0].id if stmt.bases else None
                self.dataclasses.add(stmt.name)
                if base:
                    self.unions.setdefault(base, []).append((stmt.name, fields))
                else:
                    self.structs[stmt.name] = (fields, methods)

        # emit structs
        for name, (fields, methods) in self.structs.items():
            self.emit(f""type {name} {{"")
            self.indent += 1
            for n, t in fields:
                self.emit(f""{n}: {t}"")
            for m in methods:
                args = [
                    f""{a.arg}: {self.convert_type(a.annotation)}"" for a in m.args.args[1:]
                ]
                ret = self.convert_type(m.returns)
                self.emit(f""fun {m.name}({', '.join(args)}): {ret} {{"")
                self.indent += 1
                for st in m.body:
                    self.visit(st)
                self.indent -= 1
                self.emit(""}"")
            self.indent -= 1
            self.emit(""}"")

        # emit unions
        for base, variants in self.unions.items():
            self.emit(f""type {base} ="")
            self.indent += 1
            for i, (name, fields) in enumerate(variants):
                field_str = "", "".join(f""{n}: {t}"" for n, t in fields)
                if field_str:
                    self.emit(
                        f""{name}({field_str})"" + ("" |"" if i < len(variants) - 1 else """")
                    )
                else:
                    self.emit(f""{name} {{}}"" + ("" |"" if i < len(variants) - 1 else """"))
            self.indent -= 1

        # now handle remaining statements
        for stmt in node.body:
            if (
                isinstance(stmt, ast.If)
                and isinstance(stmt.test, ast.Compare)
                and isinstance(stmt.test.left, ast.Name)
                and stmt.test.left.id == ""__name__""
            ):
                continue
            if isinstance(stmt, ast.FunctionDef) and stmt.name in {""_get"", ""_fetch"", ""_sort_key"", ""_load"", ""_save""}:
                continue
            if isinstance(stmt, ast.FunctionDef) and stmt.name == ""main"":
                for sub in stmt.body:
                    if isinstance(sub, ast.Global):
                        continue
                    if (
                        isinstance(sub, ast.Expr)
                        and isinstance(sub.value, ast.Call)
                        and isinstance(sub.value.func, ast.Name)
                        and sub.value.func.id.startswith(""test_"")
                    ):
                        continue
                    self.visit(sub)
                continue
            if isinstance(stmt, ast.FunctionDef):
                self.visit(stmt)
                continue
            if isinstance(stmt, ast.ClassDef):
                continue
            if isinstance(stmt, ast.Assign):
                self.visit(stmt)
",tools/any2mochi/py/py2mochi.py,Converter,0,7
survived,"    def visit_Return(self, node: ast.Return) -> None:
        if node.value is not None:
            if isinstance(node.value, ast.Lambda) and self.current_callable:
                args, ret = self.current_callable
                self.emit(""return "" + self.convert_lambda(node.value, args, ret))
            else:
                self.emit(""return "" + self.convert_expr(node.value))
        else:
            self.emit(""return"")
",tools/any2mochi/py/py2mochi.py,Converter,1,7
deleted,"    def to_dict(self) -> dict[str, int]:
        return {
            ""count"": self.count,
            ""input_chars"": self.input_chars,
            ""output_chars"": self.output_chars,
        }
",src/serena/analytics.py,ToolStatsEntry,1,7
survived,"    def Dataframe(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyBlocks,0,7
survived,"        def __init__(self, *a, **k) -> None:
            self.memory = DummyMemory()
",tests/test_agent_experience_entrypoint.py,DummyAgent,1,7
survived,"    def click(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyButton,0,7
survived,"def test_experience_launcher_env_vars(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    script = Path(""alpha_factory_v1/demos/era_of_experience/run_experience_demo.sh"")
    config = script.parent / ""config.env""
    docker_log = tmp_path / ""docker.log""
    curl_log = tmp_path / ""curl.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""STREAM_RATE_HZ=$STREAM_RATE_HZ"" >> ""$DOCKER_LOG""\n'
        'echo ""PORT=$PORT"" >> ""$DOCKER_LOG""\n'
        'echo ""$@"" >> ""$DOCKER_LOG""\n'
        'if [ ""$1"" = ""info"" ]; then echo ""{}""; fi\n'
        'if [ ""$1"" = ""version"" ]; then echo ""24.0.0""; fi\n'
        ""exit 0\n""
    )
    docker_stub.chmod(0o755)

    curl_stub = bin_dir / ""curl""
    curl_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""$@"" >> ""$CURL_LOG""\n'
        'out=""""\n'
        ""for ((i=1;i<=$#;i++)); do\n""
        '  if [ ""${!i}"" = ""-o"" ]; then\n'
        ""    j=$((i+1))\n""
        ""    out=${!j}\n""
        ""  fi\n""
        ""done\n""
        'if [ -n ""$out"" ]; then echo sample > ""$out""; fi\n'
        'echo ""OK""\n'
    )
    curl_stub.chmod(0o755)

    env = os.environ.copy()
    env.update(
        {
            ""PATH"": f""{bin_dir}:{env['PATH']}"",
            ""SKIP_ENV_CHECK"": ""1"",
            ""SAMPLE_DATA_DIR"": str(tmp_path / ""samples""),
            ""STREAM_RATE_HZ"": ""7"",
            ""DOCKER_LOG"": str(docker_log),
            ""CURL_LOG"": str(curl_log),
        }
    )
    env.pop(""OPENAI_API_KEY"", None)

    if config.exists():
        config.unlink()
    try:
        result = subprocess.run(
            [f""./{script.name}"", ""--port"", ""9999""],
            cwd=script.parent,
            env=env,
            capture_output=True,
            text=True,
        )
        created = config.exists()
    finally:
        if config.exists():
            config.unlink()

    assert result.returncode == 0, result.stderr
    assert docker_log.exists()
    log = docker_log.read_text()
    assert ""STREAM_RATE_HZ=7"" in log
    assert ""PORT=9999"" in log
    assert created",tests/test_experience_launcher.py,,0,7
survived,"def test_plain_table_handles_no_rows() -> None:
    assert cli._plain_table([""h1"", ""h2""], []) == ""h1 | h2""",tests/test_demo_cli.py,,1,7
survived,"            async def ingest_loop() -> None:
                async for evt in demo.experience_stream():
                    await queue.put(evt)
                    break
",tests/test_era_experience.py,TestEraOfExperience,1,6
survived,"        async def run_tasks() -> None:
            queue: asyncio.Queue[dict[str, Any]] = asyncio.Queue()

            async def ingest_loop() -> None:
                async for evt in demo.experience_stream():
                    await queue.put(evt)
                    break

            async def step_once() -> None:
                evt = await queue.get()
                self.assertIsInstance(evt, dict)

            await asyncio.gather(ingest_loop(), step_once())
",tests/test_era_experience.py,TestEraOfExperience,1,6
survived,"    def _safe_open(*_: Any, **__: Any) -> None:
        raise PermissionError(""File operations are not permitted"")
",backend/tools/analysis_tools.py,,1,6
survived,"def init_config(env_file: str = "".env"") -> None:
    """"""Load environment variables and refresh :data:`CFG`.""""""

    _load_dotenv(env_file)
    _prefetch_vault()
    global CFG
    CFG = Settings()
",src/utils/config.py,,1,7
survived,"    def __delitem__(self, key: Any) -> None:
        if self.__dict__.get(""_frozen"", False):
            raise TypeError(""Cannot modify attributes after call start"")
        super().__delitem__(key)
",weave/trace/weave_client.py,AttributesDict,1,7
survived,"def get_macros(context: t.Optional[LSPContext], file_uri: t.Optional[URI]) -> t.Set[str]:
    """"""Return a set of all macros with the ``@`` prefix.""""""
    names = set(macro.get_registry())
    try:
        if context is not None:
            names.update(context.context._macros)
    except Exception:
        pass
    return names
",sqlmesh/lsp/completions.py,,1,7
survived,"    def calculate_doc_likelihood(self, node_weights, level_word_counts):

        # calculate the weight for a new path at a given level
        new_topic_weights = np.zeros(self.num_levels)
        for level in range(1, self.num_levels):  # skip the root

            word_counts = level_word_counts[level]
            total_tokens = 0

            for w in word_counts:
                count = word_counts[w]
                for i in range(count):  # why ?????????
                    new_topic_weights[level] += log((self.eta + i) / (self.eta_sum + total_tokens))
                    total_tokens += 1

        self.calculate_word_likelihood(node_weights, self.root_node, 0.0, level_word_counts, new_topic_weights, 0)
",src/hlda/sampler.py,HierarchicalLDA,0,6
survived,"    def print_node(self, node, indent, n_words, with_weights):
        out = '    ' * indent
        out += 'topic=%d level=%d (documents=%d): ' % (node.node_id, node.level, node.customers)
        out += node.get_top_words(n_words, with_weights)
        print(out)
        for child in node.children:
            self.print_node(child, indent+1, n_words, with_weights)
",src/hlda/sampler.py,HierarchicalLDA,1,7
survived,"def main():
    parser = argparse.ArgumentParser(
        description=""Run hierarchical LDA on a directory of text documents""
    )
    parser.add_argument(
        ""--data-dir"", required=True, help=""Directory containing text files""
    )
    parser.add_argument(""--iterations"", type=int, default=100, help=""Number of Gibbs samples"")
    parser.add_argument(
        ""--display-topics"", type=int, default=50, help=""Report topics every N iterations""
    )
    parser.add_argument(
        ""--n-words"", type=int, default=5, help=""Number of words to display per topic""
    )
    parser.add_argument(
        ""--num-levels"", type=int, default=3, help=""Depth of the topic hierarchy""
    )
    parser.add_argument(""--alpha"", type=float, default=10.0, help=""Alpha hyperparameter"")
    parser.add_argument(""--gamma"", type=float, default=1.0, help=""Gamma hyperparameter"")
    parser.add_argument(""--eta"", type=float, default=0.1, help=""Eta hyperparameter"")
    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")

    args = parser.parse_args()
    run_demo(args)
",scripts/run_hlda.py,,1,7
survived,"def test_notebook_conversion(tmp_path):
    """"""With the notebook flag enabled code cells are converted.""""""
    nb = tmp_path / ""t.ipynb""
    _write_notebook(nb)
    result = _fstringify_file(str(nb), State(process_notebooks=True))
    assert result and result.n_changes == 1
    with open(nb) as fh:
        data = json.load(fh)
    assert ""f'{1}'"" in """".join(data[""cells""][0][""source""])",test/integration/test_api.py,,1,7
survived,"def set_language():
    lang = request.form.get(""language"")
    if lang in app.config[""BABEL_SUPPORTED_LOCALES""]:
        session[""lang""] = lang
    return redirect(request.referrer or url_for(""index""))
",app.py,,1,7
survived,"def test_attention_paged_decode_matches_full_prefill():
    B = Axis(""batch"", 2)
    Pos = Axis(""position"", 4)
    Embed = Axis(""embed"", 16)

    cfg = AttentionConfig(Embed=Embed, num_heads=2, num_kv_heads=2, rope=None, attn_backend=AttentionBackend.VANILLA)
    attn_key, x_key = jrandom.split(jrandom.PRNGKey(0))
    attn = Attention.init(cfg, key=attn_key)

    x = hax.random.normal(x_key, (B, Pos, Embed)) * 0.2
    full_out = attn(x, AttentionMask.causal(), key=jrandom.PRNGKey(1))

    cache = _build_page_cache(cfg, B, Pos)
    pos_ids = hax.arange(Pos, dtype=jnp.int32)
    decode_out, _ = _jit_paged_decode(attn, x, pos_ids, cache)

    assert_trees_all_close(full_out.array, decode_out.array, atol=1e-4, rtol=1e-4)
",tests/test_attention.py,,1,6
survived,"    def __init__(self):
        super().__init__(
            id=""3fd9c73d-4370-4925-a1ff-1b86b99fabfa"",
            description=(
                ""Edit images using BlackForest Labs' Flux Kontext models. Provide a prompt ""
                ""and optional reference image to generate a modified image.""
            ),
            categories={BlockCategory.AI, BlockCategory.MULTIMEDIA},
            input_schema=AIImageEditorBlock.Input,
            output_schema=AIImageEditorBlock.Output,
            test_input={
                ""prompt"": ""Add a hat to the cat"",
                ""input_image"": ""https://example.com/cat.png"",
                ""aspect_ratio"": AspectRatio.MATCH_INPUT_IMAGE,
                ""seed"": None,
                ""model"": FluxKontextModelName.PRO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""image_url"", ""https://replicate.com/output/edited-image.png""),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, input_image, aspect_ratio, seed: ""https://replicate.com/output/edited-image.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )
",autogpt_platform/backend/backend/blocks/flux_kontext.py,AIImageEditorBlock,1,7
survived,"def _alpaca_order(symbol: str, qty: int, side: str, key: str, secret: str) -> Dict[str, Any]:
    """"""Send an order to Alpaca Markets and return the JSON response.""""""

    hdrs = {""APCA-API-KEY-ID"": key, ""APCA-API-SECRET-KEY"": secret}
    data = {
        ""symbol"": symbol,
        ""qty"": qty,
        ""side"": side.lower(),
        ""type"": ""market"",
        ""time_in_force"": ""gtc"",
    }
    r = requests.post(f""{ALPACA_BASE}/orders"", json=data, headers=hdrs, timeout=4)
    r.raise_for_status()
    return r.json()
",alpha_factory_v1/backend/trade_broker.py,,1,7
survived,"    def test_broadcast_success(self) -> None:
        led = self._ledger()
        env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
        led.log(env)
        root = led.compute_merkle_root()
        captured, DummyClient, DummyTx, DummyInstr, DummyPk = self._dummy_classes()
        with (
            mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
            mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
            mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
        ):
            asyncio.run(led.broadcast_merkle_root())
        self.assertEqual(captured[""url""], ""http://rpc.test"")
        self.assertEqual(captured[""root""], root)
",tests/test_merkle_broadcast.py,TestMerkleBroadcast,1,7
survived,"    def path_raw_retro_route():
        args = request.args
        currtime = int(args[""currtime""])
        data = rs.path_raw_retro(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")
",pygs/graphserver/ext/routeserver/routeserver.py,,1,6
survived,"    def path_route():
        args = request.args
        data = rs.path(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)
",pygs/graphserver/ext/routeserver/routeserver.py,,1,7
survived,"def _get_model() -> SentenceTransformer:
    if SentenceTransformer is None:
        raise ImportError(""sentence-transformers missing"")
    global _MODEL
    if _MODEL is None:
        _MODEL = SentenceTransformer(""all-MiniLM-L6-v2"")
    return _MODEL
",src/evaluators/novelty.py,,1,7
survived,"def test_cli_inline_requires_filename(capsys):
    """"""cli() should exit with an error when --inline is passed without a filename.""""""
    with pytest.raises(SystemExit) as exc:
        cli(inline_args=[""--inline""])
    captured = capsys.readouterr()
    assert captured.err.strip() == ""Error: Inline mode requires a filename""
    assert exc.value.code != 0
",tests/test_json_repair.py,,1,8
survived,"def test_valid_token(monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(monkeypatch)
    resp = client.get(""/agents"", headers={""Authorization"": ""Bearer secret""})
    assert resp.status_code == 200
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,,1,7
survived,"def test_missing_token(monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(monkeypatch)
    resp = client.get(""/agents"")
    assert resp.status_code == 403
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,,1,7
survived,"def test_empty_wheelhouse_fallback(tmp_path, monkeypatch, capsys):
    """"""Ensure empty wheelhouse is ignored and network install is used.""""""
    _no_missing(monkeypatch)
    empty = tmp_path / ""wheels""
    empty.mkdir()
    monkeypatch.setattr(check_env, ""has_network"", lambda: True)

    monkeypatch.setattr(subprocess, ""run"", lambda *a, **k: subprocess.CompletedProcess([], 0, """", """"))
    rc = check_env.main([""--auto-install"", ""--wheelhouse"", str(empty)])
    out = capsys.readouterr().out.lower()
    assert rc == 0
    assert ""falling back to network"" in out",tests/test_check_env_wheelhouse.py,,1,6
survived,"def _no_missing(monkeypatch):
    monkeypatch.setattr(check_env, ""REQUIRED"", [])
    monkeypatch.setattr(check_env, ""OPTIONAL"", [])
    monkeypatch.setattr(check_env, ""warn_missing_core"", lambda: [])
",tests/test_check_env_wheelhouse.py,,0,6
survived,"def test_refinement_rejected_patch(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    logs = tmp_path / ""logs""
    logs.mkdir()
    (logs / ""log.json"").write_text(
        ""\n"".join([
            '{""hash"":""h0"",""ts"":0}',
            '{""hash"":""h1"",""ts"":1}',
            '{""hash"":""h2"",""ts"":5}'
        ]),
        encoding=""utf-8"",
    )

    reg = StakeRegistry()
    reg.set_stake(""meta"", 1.0)

    with patch.object(harness, ""vote_and_merge"", return_value=False):
        agent = MetaRefinementAgent(repo, logs, reg)
        merged = agent.refine()

    assert not merged
    assert (repo / ""metric.txt"").read_text().strip() == ""1""",tests/test_meta_refinement_agent.py,,1,7
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
",src/meta_agent/template_creator.py,TemplateCreator,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q22.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Supplier,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q16.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q15.py,Supplier,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q13.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q12.py,Order,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q12.py,Lineitem,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto9,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto8,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto10,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto6,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto8,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto8,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto6,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto1,1,7
survived,"def test_Q29_finds_the_actress_voicing_the_Queen_in_Shrek_2():
    assert result == [
        Auto1(
            voiced_char=""Queen"",
            voicing_actress=""Angela Aniston"",
            voiced_animation=""Shrek 2"",
        )
    ]
",tests/dataset/job/compiler/py/q29.py,,0,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q2.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto6,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto4,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto5,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto6,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto4,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto14,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto6,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto2,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q17.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto10,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto13,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q5.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto11,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto5,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto8,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto4,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto8,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto1,1,7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q16.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto3,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto9,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto7,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q45.py,_Group,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q26.py,CustomerDemo,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q67.py,Reason,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q6.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,Item,1,6
survived,"def _q0():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = Auto2(item=ss.item)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(item=g.key[""item""], revenue=sum([x.price for x in g])) for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q65.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,Customer,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto2,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q98.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto3,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q73.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q32.py,CatalogSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Auto2,1,6
survived,"def test_TPCDS_Q54_simplified():
    assert result == [
        Auto1(segment=1, num_customers=1, segment_base=50),
        Auto1(segment=0, num_customers=1, segment_base=0),
    ]
",tests/dataset/tpc-ds/compiler/py/q54.py,,0,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q38.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,Auto2,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q30.py,_Group,1,8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,SalesYear1,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q1.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,CatalogSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,CustomerDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,CustomerAddres,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,CustomerDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,Auto1,1,7
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q71.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Store,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,StoreSale,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q75.py,_Group,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q91.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Store,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q56.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Store,1,7
survived,"def test_TPCDS_Q17_stats():
    assert result == [
        Auto1(
            i_item_id=""I1"",
            i_item_desc=""Item 1"",
            s_state=""CA"",
            store_sales_quantitycount=1,
            store_sales_quantityave=10.0,
            store_sales_quantitystdev=0.0,
            store_sales_quantitycov=0.0,
            store_returns_quantitycount=1,
            store_returns_quantityave=2.0,
            store_returns_quantitystdev=0.0,
            store_returns_quantitycov=0.0,
            catalog_sales_quantitycount=1,
            catalog_sales_quantityave=5.0,
            catalog_sales_quantitystdev=0.0,
            catalog_sales_quantitycov=0.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q17.py,,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q35.py,,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q26.py,_Group,1,7
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q2.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Result,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q20.py,_Group,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q2.py,_Group,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Warehouse,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,StoreSale,1,7
survived,"def _q0():
    _src = item
    _rows = _query(
        _src,
        [
            {""items"": inventory, ""on"": lambda i, inv: i.i_item_sk == inv.inv_item_sk},
            {""items"": date_dim, ""on"": lambda i, inv, d: inv.inv_date_sk == d.d_date_sk},
            {
                ""items"": catalog_sales,
                ""on"": lambda i, inv, d, cs: cs.cs_item_sk == i.i_item_sk,
            },
        ],
        {
            ""select"": lambda i, inv, d, cs: (i, inv, d, cs),
            ""where"": lambda i, inv, d, cs: (
                (
                    (
                        (i.i_current_price >= 20 and i.i_current_price <= 50)
                        and i.i_manufact_id >= 800
                    )
                    and i.i_manufact_id <= 803
                )
                and inv.inv_quantity_on_hand >= 100
            )
            and inv.inv_quantity_on_hand <= 500,
        },
    )
    _groups = _group_by(
        _rows,
        lambda i, inv, d, cs: Auto2(
            id=i.i_item_id, desc=i.i_item_desc, price=i.i_current_price
        ),
    )
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: _sort_key(g.key[""id""]))
    return [
        Auto1(
            i_item_id=g.key[""id""],
            i_item_desc=g.key[""desc""],
            i_current_price=g.key[""price""],
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q37.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q92.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,CustomerDemographic,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,WebSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,HouseholdDemographic,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q24.py,_Group,1,8
survived,"def test_TPCDS_Q37_simplified():
    assert result == [Auto1(i_item_id=""I1"", i_item_desc=""Item1"", i_current_price=30.0)]
",tests/dataset/tpc-ds/compiler/py/q37.py,,1,6
survived,"def _q8():
    _src = web_sales
    _rows = _query(
        _src,
        [{""items"": date_dim, ""on"": lambda ws, d: d.d_date_sk == ws.ws_sold_date_sk}],
        {""select"": lambda ws, d: (ws, d)},
    )
    _groups = _group_by(_rows, lambda ws, d: ws.ws_web_page_sk)
    _items9 = _groups
    return [
        Auto6(
            wp_web_page_sk=g.key,
            sales=_sum([x[0].ws_ext_sales_price for x in g]),
            profit=_sum([x[0].ws_net_profit for x in g]),
        )
        for g in _items9
    ]
",tests/dataset/tpc-ds/compiler/py/q77.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CustomerAddres,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,CustomerAddres,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,Auto2,1,7
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q93.py,,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q54.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,Customer,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,WebSale,1,7
survived,"def test_TPCDS_Q72_simplified():
    assert result == [
        Auto1(
            i_item_desc=""ItemA"",
            w_warehouse_name=""Main"",
            d_week_seq=10,
            no_promo=1,
            promo=0,
            total_cnt=1,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q72.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Auto1,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q63.py,_Group,1,7
survived,"def test_TPCDS_Q99_buckets():
    assert grouped == [
        Auto1(
            warehouse=""Warehouse1"",
            sm_type=""EXP"",
            cc_name=""CC1"",
            d30=1,
            d60=1,
            d90=1,
            d120=1,
            dmore=1,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q99.py,,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q71.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CallCenter,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogSale,1,7
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q29.py,,1,7
survived,"def _q0():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = Auto1(customer_sk=ss.ss_customer_sk, item_sk=ss.ss_item_sk)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(customer_sk=g.key[""customer_sk""], item_sk=g.key[""item_sk""])
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q97.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q47.py,Auto1,1,6
survived,"def test_TPCDS_Q46_simplified():
    assert result == [
        Auto1(
            c_last_name=""Doe"",
            c_first_name=""John"",
            ca_city=""Seattle"",
            bought_city=""Portland"",
            ss_ticket_number=1,
            amt=5.0,
            profit=20.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q46.py,,1,6
survived,"def test_TPCDS_Q3_result():
    assert result == [
        Auto1(d_year=1998, brand_id=2, brand=""Brand2"", sum_agg=20.0),
        Auto1(d_year=1998, brand_id=1, brand=""Brand1"", sum_agg=10.0),
    ]
",tests/dataset/tpc-ds/compiler/py/q3.py,,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q35.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q49.py,Auto1,1,6
survived,"def _q0():
    _src = date_dim
    _rows = _query(
        _src,
        [
            {
                ""items"": store_sales,
                ""on"": lambda d, ss: ss.ss_sold_date_sk == d.d_date_sk,
            },
            {
                ""items"": item,
                ""on"": lambda d, ss, i: ss.ss_item_sk == i.i_item_sk
                and i.i_manager_id == 10,
            },
            {
                ""items"": customer,
                ""on"": lambda d, ss, i, c: ss.ss_customer_sk == c.c_customer_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda d, ss, i, c, ca: c.c_current_addr_sk == ca.ca_address_sk,
            },
            {
                ""items"": store,
                ""on"": lambda d, ss, i, c, ca, s: ss.ss_store_sk == s.s_store_sk
                and ca.ca_zip[0:5] != s.s_zip[0:5],
            },
        ],
        {
            ""select"": lambda d, ss, i, c, ca, s: (d, ss, i, c, ca, s),
            ""where"": lambda d, ss, i, c, ca, s: d.d_moy == 11 and d.d_year == 1999,
        },
    )
    _groups = _group_by(
        _rows,
        lambda d, ss, i, c, ca, s: Auto2(
            brand=i.i_brand,
            brand_id=i.i_brand_id,
            man_id=i.i_manufact_id,
            man=i.i_manufact,
        ),
    )
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: _sort_key([g.key[""brand""]]))
    return [
        Auto1(
            i_brand=g.key[""brand""],
            i_brand_id=g.key[""brand_id""],
            i_manufact_id=g.key[""man_id""],
            i_manufact=g.key[""man""],
            ext_price=_sum([x[1].ss_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q19.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CustomerAddres,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,HouseholdDemographic,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Inventory,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,DateDim,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q43.py,_Group,1,7
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": item,
                ""on"": lambda ss, i: ss.item == i.i_item_sk and i.i_manager_id == 1,
            },
            {
                ""items"": date_dim,
                ""on"": lambda ss, i, d: (
                    ss.sold_date == d.d_date_sk and d.d_year == 2001
                )
                and d.d_moy == 11,
            },
        ],
        {""select"": lambda ss, i, d: (ss, i, d)},
    )
    _groups = _group_by(
        _rows,
        lambda ss, i, d: Auto2(year=d.d_year, brand_id=i.i_brand_id, brand=i.i_brand),
    )
    _items1 = _groups
    return [
        Auto1(
            d_year=g.key[""year""],
            brand_id=g.key[""brand_id""],
            ext_price=sum([x[0].price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q52.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,DateDim,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q75.py,_Group,1,7
survived,"def test_TPCDS_Q48_simplified():
    assert result == 35
",tests/dataset/tpc-ds/compiler/py/q48.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Customer,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Auto2,1,7
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q77.py,,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q17.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q83.py,SrItem,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q3.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto4,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,S,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,StoreSale,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q56.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,Auto1,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q13.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,DateDim,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,DateDim,1,7
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q55.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,DateDim,1,6
survived,"def _q0():
    _groups = {}
    _order = []
    for r in records:
        _k = Auto3(
            d_year=r.d_year, i_category_id=r.i_category_id, i_category=r.i_category
        )
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            d_year=g.key[""d_year""],
            i_category_id=g.key[""i_category_id""],
            i_category=g.key[""i_category""],
            sum_ss_ext_sales_price=sum([x.price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q42.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,CustomerAddres,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Customer,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q19.py,_Group,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q34.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q80.py,StoreSale,1,7
survived,"def test_TPCDS_Q2_result():
    assert result == [Auto1(d_week_seq1=1, sun_ratio=0.5, mon_ratio=0.5)]
",tests/dataset/tpc-ds/compiler/py/q2.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Auto1,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q15.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,HouseholdDemographic,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,WebSale,1,7
survived,"def test_TPCDS_Q40_simplified():
    assert result == [
        Auto1(w_state=""CA"", i_item_id=""I1"", sales_before=100.0, sales_after=0.0)
    ]
",tests/dataset/tpc-ds/compiler/py/q40.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,StoreSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,DateDim,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,DateDim,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q91.py,,1,6
survived,"def _q0():
    _src = customer_address
    _rows = _query(
        _src,
        [
            {
                ""items"": customer,
                ""on"": lambda a, c: a.ca_address_sk == c.c_current_addr_sk,
            },
            {
                ""items"": store_sales,
                ""on"": lambda a, c, s: c.c_customer_sk == s.ss_customer_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda a, c, s, d: s.ss_sold_date_sk == d.d_date_sk,
            },
            {""items"": item, ""on"": lambda a, c, s, d, i: s.ss_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda a, c, s, d, i: (a, c, s, d, i),
            ""where"": lambda a, c, s, d, i: d.d_month_seq == target_month_seq
            and i.i_current_price
            > 1.2
            * (
                sum([j.i_current_price for j in item if j.i_category == i.i_category])
                / len([j.i_current_price for j in item if j.i_category == i.i_category])
                if [j.i_current_price for j in item if j.i_category == i.i_category]
                else 0
            ),
        },
    )
    _groups = _group_by(_rows, lambda a, c, s, d, i: a.ca_state)
    _items1 = _groups
    _items1 = [g for g in _items1 if len(g) >= 10]
    _items1 = sorted(_items1, key=lambda g: _sort_key([len(g), g.key]))
    _items1 = _items1[: max(100, 0)]
    return [Auto1(state=g.key, cnt=len(g)) for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q6.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,DateDim,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q25.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,CatalogSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,Auto2,1,6
survived,"def _q0():
    _src = customer
    _rows = _query(
        _src,
        [
            {
                ""items"": customer_address,
                ""on"": lambda c, ca: c.c_current_addr_sk == ca.ca_address_sk,
            },
            {
                ""items"": customer_demographics,
                ""on"": lambda c, ca, cd: c.c_current_cdemo_sk == cd.cd_demo_sk,
            },
        ],
        {
            ""select"": lambda c, ca, cd: (c, ca, cd),
            ""where"": lambda c, ca, cd: c.c_customer_sk in purchased,
        },
    )
    _groups = _group_by(
        _rows,
        lambda c, ca, cd: Auto2(
            state=ca.ca_state,
            gender=cd.cd_gender,
            marital=cd.cd_marital_status,
            dep=cd.cd_dep_count,
            emp=cd.cd_dep_employed_count,
            col=cd.cd_dep_college_count,
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            ca_state=g.key[""state""],
            cd_gender=g.key[""gender""],
            cd_marital_status=g.key[""marital""],
            cd_dep_count=g.key[""dep""],
            cd_dep_employed_count=g.key[""emp""],
            cd_dep_college_count=g.key[""col""],
            cnt=len(g),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q35.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Auto3,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,StoreSale,1,7
survived,"    def __call__(cls, *args, **kwargs):
        # Create the instance without invoking ``__init__`` so we can inject
        # the base initialization beforehand.
        obj = cls.__new__(cls, *args, **kwargs)
        if isinstance(obj, cls):
            # ``_base_init`` sets attributes that should exist on all modules
            # even when a subclass forgets to call ``super().__init__``.
            Module._base_init(obj)
            cls.__init__(obj, *args, **kwargs)

            # Guarantee existence of critical attributes if ``__init__`` didn't
            # create them.
            if not hasattr(obj, ""callbacks""):
                obj.callbacks = []
            if not hasattr(obj, ""history""):
                obj.history = []
        return obj
",dspy/primitives/program.py,ProgramMeta,1,7
survived,"    def __init__(self) -> None:
        self._data: dict[str, any] = {}
",custom_components/gree/config_flow.py,ConfigFlow,1,7
survived,"    def __init__(self):
        self.completions = _ChatCompletions()
",openai/__init__.py,_Chat,1,6
survived,"def internet_available() -> bool:
    try:
        socket.create_connection((""api.openai.com"", 443), timeout=1).close()
        return True
    except OSError:
        return False
",tests/integration/test_llm_service_integration.py,,1,7
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,,1,6
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        if not errors:
            self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,6
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        if not errors:
            self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator,1,8
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,,0,6
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,,1,7
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,6
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,,1,8
survived,"def apply_unicode_escape_map(code: str, mapping: Dict[str, str]) -> str:
    """"""Replace characters in ``code`` with their escape sequences.""""""
    if not mapping:
        return code
    pattern = ""["" + """".join(re.escape(c) for c in mapping) + ""]""
    return re.sub(pattern, lambda m: mapping[m.group(0)], code)
",src/flynt/utils/utils.py,,1,7
deleted,"    def __init__(
        self,
        system_message: str,
        user_input: str,
        thinking_instructions: str | None = None,
    ) -> None:
        self.system_message = system_message
        self.user_input = user_input
        self.thinking_instructions = thinking_instructions
        self._messages: List[ChatMessage] = []
        self._state = ""start""
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,ChatFormatter,1,7
deleted,"def get_chat_formatter(
    *,
    strategy: ChatStrategy,
    system_message: str,
    user_input: str,
    thinking_instructions: str | None = None,
) -> ChatFormatter:
    if strategy == ChatStrategy.final_only:
        return FinalOnlyFormatter(system_message, user_input)
    if strategy == ChatStrategy.final_and_intermediate:
        return FinalAndIntermediateFormatter(
            system_message, user_input, thinking_instructions
        )
    if strategy == ChatStrategy.final_and_intermediate_r1_compatible:
        return R1Formatter(system_message, user_input)

    raise ValueError(f""Unsupported strategy {strategy}"")",libs/core/kiln_ai/adapters/chat/chat_formatter.py,,1,7
survived,"def test_notebook_runs(tmp_path: Path) -> None:
    nb_path = Path(""alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_colab.ipynb"")
    assert nb_path.exists(), nb_path
    nb = nbformat.read(nb_path, as_version=4)

    skip = {2, 4, 8, 15, 17, 19}
    for idx in skip:
        nb.cells[idx].source = ""print('skipped')""

    mod = tmp_path / ""mod.ipynb""
    nbformat.write(nb, mod)

    os.environ[""NO_LLM""] = ""1""
    os.environ.setdefault(""ALPHA_ASI_SILENT"", ""1"")

    pm.execute_notebook(str(mod), str(tmp_path / ""out.ipynb""), kernel_name=""python3"")",tests/test_world_model_notebook_exec.py,,0,7
survived,"    def __init__(self, status_code: int, text: str):
        self.status_code = status_code
        self.text = text
",alpha_factory_v1/requests.py,Response,1,7
survived,"    def dispatch_request(self, request: Request) -> Response:
        try:
            return self.run(request=request)
        except HTTPException as e:
            return Response(text=e.reason, status=e.status_code)
",src/graphql_server/webob/views.py,GraphQLView,1,6
survived,"    def __init__(
        self,
        schema: GraphQLSchema,
        graphiql: Optional[bool] = None,
        graphql_ide: Optional[GraphQL_IDE] = ""graphiql"",
        allow_queries_via_get: bool = True,
        multipart_uploads_enabled: bool = False,
    ) -> None:
        self.schema = schema
        self.allow_queries_via_get = allow_queries_via_get
        self.multipart_uploads_enabled = multipart_uploads_enabled

        if graphiql is not None:
            warnings.warn(
                ""The `graphiql` argument is deprecated in favor of `graphql_ide`"",
                DeprecationWarning,
                stacklevel=2,
            )
            self.graphql_ide = ""graphiql"" if graphiql else None
        else:
            self.graphql_ide = graphql_ide
",src/graphql_server/webob/views.py,GraphQLView,1,8
survived,"    def test_runtime_list_agents(self):
        runtime = MagicMock()
        with patch.object(bridge, ""AgentRuntime"", return_value=runtime) as rt_cls, \
                patch.object(bridge.requests, ""get"", return_value=DummyResponse([""a""])) as get:
            agent = bridge.InspectorAgent()
            rt = bridge.AgentRuntime(api_key=None)
            rt.register(agent)

            rt_cls.assert_called_once_with(api_key=None)
            runtime.register.assert_called_once_with(agent)

            result = asyncio.run(bridge.list_agents())
        get.assert_called_once_with(""http://localhost:7860/agents"", timeout=5)
        self.assertIsInstance(result, list)
",tests/test_inspector_bridge.py,TestInspectorAgent,1,6
survived,"    async def stop(self) -> None:
        """"""Stop servers and scheduler.""""""
        await self.scheduler.stop()
        if self._rest_task:
            self._rest_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._rest_task
        if self._grpc_server:
            self._grpc_server.stop(0)
",alpha_factory_v1/backend/orchestrator.py,Orchestrator,1,7
survived,"    async def start(self) -> None:
        """"""Start heartbeat and regression checks.""""""
        await self.manager.start()
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler,1,6
survived,"    def test_run_with_data_dir(self) -> None:
        data_dir = Path(
            ""alpha_factory_v1/demos/macro_sentinel/offline_samples""
        ).as_posix()
        result = subprocess.run(
            [
                sys.executable,
                ""alpha_factory_v1/demos/era_of_experience/alpha_report.py"",
                ""--data-dir"",
                data_dir,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Alpha signals"", result.stdout)
",tests/test_alpha_report_cli.py,TestAlphaReportCLI,0,6
survived,"    def _log_slice(self, count: int = 5) -> str:
        rows = self.ledger.tail(count)
        lines = []
        for r in rows:
            lines.append(f""{r.get('sender')}->{r.get('recipient')}: {r.get('payload')}"")
        return ""\n"".join(lines)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/llm_mutator.py,LLMMutator,1,6
survived,"def test_query_speed_and_histogram(tmp_path) -> None:
    arch = SolutionArchive(tmp_path / ""sol.duckdb"")
    for i in range(10000):
        arch.add(""sec"", ""app"", float(i % 100), {""i"": i})
    start = time.perf_counter()
    res = arch.query(sector=""sec"")
    duration = time.perf_counter() - start
    assert len(res) == 10000
    assert duration < 0.2
    hist = arch.diversity_histogram()
    assert hist[(""sec"", ""app"")] == 10000",tests/test_solution_archive.py,,0,7
survived,"    def _load_logs(self) -> List[Mapping[str, object]]:
        records: List[Mapping[str, object]] = []
        for file in sorted(self.log_dir.glob(""*.json"")):
            for line in file.read_text(encoding=""utf-8"").splitlines():
                if not line.strip():
                    continue
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        return records
",src/agents/meta_refinement_agent.py,MetaRefinementAgent,1,7
survived,"def fix_file(path: str):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    in_code = False
    changed = False

    for i, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith('```') or stripped.startswith('~~~'):
            in_code = not in_code
            continue
        if not in_code and line.startswith('# '):
            title = line[2:].rstrip('\n')
            new_title = title_case(title)
            if new_title != title:
                lines[i] = '# ' + new_title + '\n'
                changed = True
            break

    if changed:
        with open(path, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        print(f'Updated {path}')
",scripts/fix_titlecase.py,,1,7
survived,"def append(lst, item):
    return lst + [item]
",tests/human/python/append_builtin.py,,1,7
survived,"            def patched_curl_async_init(session_self, *args, **kwargs):
                if self._proxies and 'proxies' not in kwargs:
                    kwargs['proxies'] = self._proxies
                self._original_curl_async_session_init(session_self, *args, **kwargs)
",webscout/Provider/TTI/base.py,_GlobalProxyManager,1,6
survived,"    def get_help(self, ctx: click.Context) -> str:  # pragma: no cover - CLI
        help_text = super().get_help(ctx)
        return f""{DISCLAIMER}\n\n{help_text}""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,DisclaimerGroup,1,7
survived,"    def setUp(self) -> None:
        self._backup = AGENT_REGISTRY.copy()
        AGENT_REGISTRY.clear()
",tests/test_demo_registration.py,TestRegisterDemoAgents,1,7
survived,"def test_init_creates_manager(monkeypatch):
    fake_cls = MagicMock()
    fake_instance = MagicMock()
    fake_cls.return_value = fake_instance
    monkeypatch.setattr(exec_mod, ""SandboxManager"", fake_cls)
    module = ExecutionModule()
    assert module.sandbox_manager is fake_instance
",tests/unit/test_execution_module.py,,1,7
survived,"def test_run_tests_propagates_error(monkeypatch, tmp_path):
    fake_manager = MagicMock()
    fake_manager.run_code_in_sandbox.side_effect = exec_mod.SandboxExecutionError(
        ""boom""
    )
    module = ExecutionModule(fake_manager)
    with pytest.raises(exec_mod.SandboxExecutionError):
        module.run_tests(tmp_path)",tests/unit/test_execution_module.py,,1,7
survived,"    def tearDown(self):
        self.fabric.close()
        os.environ.pop(""VECTOR_STORE_USE_SQLITE"", None)
",tests/test_memory_fabric_sqlite.py,TestMemoryFabricSQLiteClose,1,7
survived,"def _truthy(val: bool | str | None) -> bool:
    """"""Return ``True`` when *val* represents an affirmative value.""""""
    if isinstance(val, bool):
        return val
    if isinstance(val, str):
        return val.strip().lower() in {""1"", ""true"", ""yes"", ""y""}
    return False
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,,1,7
survived,"async def _run_broadcast():
    hub = TraceHub()
    q = await hub.subscribe()
    with mock.patch(""alpha_factory_v1.backend.trace_ws.asyncio.create_task"", asyncio.ensure_future):
        await hub.broadcast({""label"": ""hi"", ""type"": ""tool_call""})
        await asyncio.sleep(0)
    payload = await asyncio.wait_for(q.get(), timeout=1)
    await hub.unsubscribe(q)
    return json.loads(payload.decode())
",tests/test_trace_hub.py,,1,7
survived,"def _sync_fn(x):
    return x + 1
",tests/test_agent_runner_utils.py,,1,6
survived,"def best_alpha(signals: Dict[str, str]) -> str:
    """"""Select the most actionable alpha message.""""""
    yc = signals.get(""yield_curve"", """")
    sc = signals.get(""supply_chain"", """")

    # simple heuristics
    if ""bottleneck"" in sc.lower():
        return sc
    if ""long bonds"" in yc.lower():
        return yc
    return yc if yc else sc
",alpha_factory_v1/demos/era_of_experience/alpha_report.py,,1,6
survived,"async def discover(num: int = 1) -> List[Dict[str, str]]:
    return discover_alpha(num=num)
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,,0,6
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = CrossIndustryAgent()
    runtime.register(agent)
    try:
        from alpha_factory_v1.backend.adk_bridge import auto_register, maybe_launch

        auto_register([agent])
        maybe_launch()
    except Exception as exc:  # pragma: no cover - ADK optional
        print(f""ADK bridge unavailable: {exc}"")

    print(""Registered CrossIndustryAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,,1,7
survived,"    def test_top_n(self):
        data = [
            {""alpha"": ""low"", ""score"": 1},
            {""alpha"": ""mid"", ""score"": 3},
            {""alpha"": ""high"", ""score"": 5},
        ]
        tmp = Path(""/tmp/opps3.json"")
        tmp.write_text(json.dumps(data), encoding=""utf-8"")
        self.temp_files.append(tmp)
        os.environ[""ALPHA_OPPS_FILE""] = str(tmp)
        os.environ[""ALPHA_TOP_N""] = ""2""
        self.env_vars[""ALPHA_OPPS_FILE""] = str(tmp)
        self.env_vars[""ALPHA_TOP_N""] = ""2""
        agent = biz.AlphaOpportunityAgent()
        self.assertEqual(agent._top_n, 2)
        self.assertEqual(agent._opportunities[0][""alpha""], ""high"")
        self.assertEqual(agent._opportunities[1][""alpha""], ""mid"")
",tests/test_alpha_opportunity_env.py,TestAlphaOpportunityEnv,1,7
survived,"def _start_server(directory: Path):
    handler = partial(http.server.SimpleHTTPRequestHandler, directory=str(directory))
    server = http.server.ThreadingHTTPServer((""localhost"", 0), handler)
    thread = threading.Thread(target=server.serve_forever, daemon=True)
    thread.start()
    return server, thread
",tests/test_pwa_offline.py,,1,7
survived,"def test_csp_meta_tag() -> None:
    index_file = BROWSER / ""dist/index.html""
    html = index_file.read_text()
    match = re.search(r'<meta[^>]*http-equiv=[""\']Content-Security-Policy[""\'][^>]*>', html)
    assert match, ""Content Security Policy meta tag missing""
    tag = match.group(0)
    content = re.search(r'content=""([^""]+)""', tag)
    assert content, ""content attribute missing""
    policy = content.group(1)
    assert ""script-src 'self' 'wasm-unsafe-eval'"" in policy, ""CSP missing script-src 'self' 'wasm-unsafe-eval'""",tests/test_integrity.py,,1,7
survived,"def test_agent_macro_entrypoint_custom_base_url(monkeypatch: pytest.MonkeyPatch) -> None:
    stub = ModuleType(""openai_agents"")
    captured = {}

    class DummyOpenAI:
        def __init__(self, *a, **kw) -> None:
            captured[""base_url""] = kw.get(""base_url"")

    stub.Agent = object
    stub.OpenAIAgent = DummyOpenAI
    stub.Tool = lambda *_a, **_k: (lambda f: f)
    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    monkeypatch.setenv(""OPENAI_API_KEY"", """")
    monkeypatch.setenv(""OLLAMA_BASE_URL"", ""http://example.com/v1"")

    mod_path = ""alpha_factory_v1.demos.macro_sentinel.agent_macro_entrypoint""
    sys.modules.pop(mod_path, None)

    with patch(f""{mod_path}._check_ollama""):
        importlib.import_module(mod_path)

    assert captured[""base_url""] == ""http://example.com/v1""",tests/test_macro_agent_base_url.py,,1,7
survived,"        def start_merkle_task(self, *_a, **_kw) -> None:
            pass
",tests/test_orchestrator.py,DummyLedger,0,7
survived,"    def _generate_ip_pool(self, count: int = 20) -> List[str]:
        """"""Generate a pool of random IP addresses.""""""
        return [self.random_crypto_ip() for _ in range(count)]
",webscout/litagent/agent.py,LitAgent,1,6
survived,"            def norm(mat, axis=1, keepdims=True):
                if axis == 1:
                    norms = [sqrt(sum(x * x for x in row)) for row in mat]
                    return [[n] for n in norms] if keepdims else norms
                raise NotImplementedError
",alpha_factory_v1/backend/memory_vector.py,_SimpleNP.linalg,1,6
survived,"        def array(self, obj, dtype=None):  # noqa: D401 - mimic numpy API
            if obj and isinstance(obj[0], (list, tuple)):
                return [list(map(float, row)) for row in obj]
            return [float(x) for x in obj]
",alpha_factory_v1/backend/memory_vector.py,_SimpleNP,1,6
survived,"    def test_version_flag(self):
        args = _parse_with(['--version'])
        self.assertTrue(args.version)
",alpha_factory_v1/tests/test_cli.py,CliParseTest,1,7
survived,"def test_postgres_merkle_root(tmp_path) -> None:
    params = {
        ""host"": os.getenv(""PGHOST"", ""localhost""),
        ""port"": int(os.getenv(""PGPORT"", ""5432"")),
        ""user"": os.getenv(""PGUSER"", ""postgres""),
        ""password"": os.getenv(""PGPASSWORD"", """"),
        ""dbname"": os.getenv(""PGDATABASE"", ""postgres""),
    }
    try:
        conn = psycopg2.connect(**params)
    except Exception:
        pytest.skip(""postgres unavailable"")
    with conn:
        with conn.cursor() as cur:
            cur.execute(""DROP TABLE IF EXISTS messages"")
    conn.close()

    ledger = Ledger(tmp_path / ""ignore.db"", db=""postgres"", broadcast=False)
    env = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
    ledger.log(env)
    assert ledger.compute_merkle_root() == _expected_root([env])
    ledger.close()",tests/test_ledger_backends.py,,0,7
survived,"async def file_rename_on_frame(frame_id: int, src: str, dst: str, timeout: int = 60):
    """"""Rename a file or directory on the frame via agent.""""""
    payload = {
        ""type"": ""cmd"",
        ""name"": ""file_rename"",
        ""args"": {""src"": src, ""dst"": dst},
    }
    fut, _ = queue_command(frame_id, payload)
    return await asyncio.wait_for(fut, timeout=timeout)
",backend/app/ws/agent_ws.py,,1,7
survived,"async def rename_path(
    db: Session,
    redis: Redis,
    frame: Frame,
    src: str,
    dst: str,
    *,
    timeout: int = 120,
) -> None:
    """"""Rename a file or directory on the device.""""""

    if await _use_agent(frame, redis):
        from app.ws.agent_ws import file_rename_on_frame

        try:
            await log(db, redis, frame.id, ""stdout"", f""> mv {src} {dst} (agent)"")
            await file_rename_on_frame(frame.id, src, dst, timeout)
            return
        except Exception as e:  # noqa: BLE001
            await log(db, redis, frame.id, ""stderr"", f""Agent rename error ({e})"")
            raise

    ssh = await get_ssh_connection(db, redis, frame)
    try:
        cmd = f""mv {shlex.quote(src)} {shlex.quote(dst)}""
        await exec_command(db, redis, frame, ssh, cmd)
    finally:
        await remove_ssh_connection(db, redis, ssh, frame)",backend/app/utils/remote_exec.py,,1,7
survived,"async def delete_path(
    db: Session,
    redis: Redis,
    frame: Frame,
    remote_path: str,
    *,
    timeout: int = 120,
) -> None:
    """"""Delete a file or directory on the device.""""""

    if await _use_agent(frame, redis):
        from app.ws.agent_ws import file_delete_on_frame

        try:
            await log(db, redis, frame.id, ""stdout"", f""> rm -rf {remote_path} (agent)"")
            await file_delete_on_frame(frame.id, remote_path, timeout)
            return
        except Exception as e:  # noqa: BLE001
            await log(db, redis, frame.id, ""stderr"", f""Agent delete error ({e})"")
            raise

    ssh = await get_ssh_connection(db, redis, frame)
    try:
        cmd = f""rm -rf {shlex.quote(remote_path)}""
        await exec_command(db, redis, frame, ssh, cmd)
    finally:
        await remove_ssh_connection(db, redis, ssh, frame)
",backend/app/utils/remote_exec.py,,1,7
survived,"    def set_stake(self, agent_id: str, amount: float) -> None:
        """"""Register ``agent_id`` with ``amount`` tokens.""""""
        self.stakes[agent_id] = float(amount)
",src/governance/stake_registry.py,StakeRegistry,1,7
survived,"    def total(self) -> float:
        """"""Return total stake across all agents.""""""
        return float(sum(self.stakes.values()))
",src/governance/stake_registry.py,StakeRegistry,1,8
survived,"    def add(self, entry: ArchiveEntry) -> None:
        with Session(self.engine) as session:
            session.merge(_ArchiveRow(**dataclasses.asdict(entry)))
            session.commit()
",src/archive/db.py,ArchiveDB,1,6
survived,"    def add(self, meta: dict[str, Any], score: float) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(""INSERT INTO agents(meta, score) VALUES (?, ?)"", (json.dumps(meta), score))
",src/archive/__init__.py,Archive,1,6
survived,"    def all(self) -> List[Agent]:
        with sqlite3.connect(self.path) as cx:
            rows = list(cx.execute(""SELECT id, meta, score FROM agents ORDER BY id""))
        return [Agent(id=r[0], meta=json.loads(r[1]), score=float(r[2])) for r in rows]
",src/archive/__init__.py,Archive,1,7
survived,"def softmax(arr: np.ndarray) -> np.ndarray:
    exp = np.exp(arr - np.max(arr))
    return exp / exp.sum()
",tests/test_selector.py,,1,8
survived,"def test_compute_fitness_table1() -> None:
    results_path = FIXTURE_DIR / ""table1_results.json""
    with results_path.open() as fh:
        results = json.load(fh)

    metrics = compute_fitness(results)

    expected = json.loads((FIXTURE_DIR / ""table1_metrics.json"").read_text())
    assert metrics == expected",tests/test_fitness.py,,1,7
survived,"def test_metrics_curl() -> None:
    port = _free_port()
    proc = _start_server(port)
    url = f""http://127.0.0.1:{port}""
    try:
        _wait_ready(url)
        out = subprocess.check_output([""curl"", ""-sf"", f""{url}/metrics""])
        text = out.decode()
        assert ""api_requests_total"" in text
        assert ""api_request_seconds"" in text
    finally:
        proc.terminate()
        proc.wait(timeout=5)
",tests/test_metrics.py,,1,7
survived,"def pytest_configure(config):
    config.addinivalue_line(
        ""markers"", ""asyncio: mark test as running in an event loop""
    )
",pytest_asyncio.py,,1,8
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,,1,7
survived,"def pytest_pyfunc_call(pyfuncitem):
    test_func = pyfuncitem.obj
    if inspect.iscoroutinefunction(test_func):
        asyncio.run(test_func(**pyfuncitem.funcargs))
        return True
    return None",pytest_asyncio.py,,1,6
survived,"def pytest_configure(config):
    config.addinivalue_line(
        ""markers"", ""asyncio: mark test as running in an event loop""
    )
",pytest_asyncio.py,,1,8
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,,1,7
survived,"    def exception(self, message: str):
        exc = sys.exc_info()
        formatted = f""{message}\n"" + """".join(traceback.format_exception(*exc))
        self.error(formatted)
",webscout/litlogger/logger.py,Logger,1,7
survived,"    def __init__(self, host: str, port: int, level: LogLevel = LogLevel.DEBUG):
        super().__init__(level)
        self.host = host
        self.port = port
",webscout/litlogger/handlers.py,TCPHandler,1,7
survived,"    def __init__(self, level: LogLevel = LogLevel.DEBUG):
        self.level = level
",webscout/litlogger/handlers.py,Handler,1,7
survived,"    def _rotate(self):
        if self.backups <= 0:
            self._file.close()
            self.path.unlink(missing_ok=True)
            self._open()
            return
        self._file.close()
        for i in range(self.backups, 0, -1):
            src = self.path.with_suffix(f"".{i}"") if i == 1 else self.path.with_suffix(f"".{i-1}"")
            dst = self.path.with_suffix(f"".{i}"")
            if src.exists():
                if dst.exists():
                    dst.unlink()
                src.rename(dst)
        self._open()
",webscout/litlogger/handlers.py,FileHandler,1,7
survived,"    def _open(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self._file = open(self.path, ""a"", encoding=""utf-8"")
",webscout/litlogger/handlers.py,FileHandler,1,7
survived,"    def emit(self, message: str, level: LogLevel):
        if level < self.level:
            return
        if self.use_https:
            conn = http.client.HTTPSConnection(self.host, self.port, timeout=5)
        else:
            conn = http.client.HTTPConnection(self.host, self.port, timeout=5)
        try:
            conn.request(""POST"", ""/"", body=message.encode(), headers={""Content-Type"": ""text/plain""})
            conn.getresponse().read()
        finally:
            conn.close()
",webscout/litlogger/handlers.py,NetworkHandler,1,7
survived,"def test_dump_builtin_config(tmp_path):
    path = tmp_path / ""out.json""
    dump_builtin_config(path)
    loaded = deserialize_config(path)
    assert [m.model_dump(mode=""json"") for m in loaded] == [
        m.model_dump(mode=""json"") for m in built_in_models
    ]
",libs/core/kiln_ai/adapters/test_remote_config.py,,1,7
survived,"def deserialize_config(path: str | Path) -> List[KilnModel]:
    raw = json.loads(Path(path).read_text())
    model_data = raw.get(""model_list"", raw if isinstance(raw, list) else [])
    return [KilnModel.model_validate(item) for item in model_data]
",libs/core/kiln_ai/adapters/remote_config.py,,1,7
survived,"def load_from_url(url: str) -> List[KilnModel]:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    data = response.json()
    if isinstance(data, list):
        model_data = data
    else:
        model_data = data.get(""model_list"", [])
    return [KilnModel.model_validate(item) for item in model_data]
",libs/core/kiln_ai/adapters/remote_config.py,,1,7
survived,"async def test_load_remote_models_failure(monkeypatch):
    original = built_in_models.copy()

    def fake_fetch(url):
        raise RuntimeError(""fail"")

    monkeypatch.setattr(""kiln_ai.adapters.remote_config.load_from_url"", fake_fetch)

    load_remote_models(""http://example.com/models.json"")
    await asyncio.sleep(0.01)
    assert built_in_models == original",libs/core/kiln_ai/adapters/test_remote_config.py,,1,7
survived,"        def __init__(self, *_: object, **__: object) -> None:
            pass
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,OpenAIAgent,0,7
survived,"        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator
",tests/test_aiga_agents_import.py,TestAigaAgentsImport,1,6
survived,"        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)
",tests/test_aiga_agents_import.py,TestAigaAgentsImport,1,6
survived,"    def findAll(self, name=None, attrs=None, recursive=True):
        result = []
        for child in self.children:
            if (name is None or child.name == name) and self._attr_match(child, attrs):
                result.append(child)
            if recursive:
                result.extend(child.findAll(name, attrs, recursive))
        return result
",tests/conftest.py,_Node,1,6
survived,"    def findParent(self):
        return self.parent
",tests/conftest.py,_Node,1,6
survived,"def test_build_and_get_result_similar():
    scraper = AutoScraper()
    result = scraper.build(html=HTML, wanted_list=[""Banana""])
    assert result == [""Banana""]
    similar = scraper.get_result_similar(html=HTML, contain_sibling_leaves=True)
    assert similar == [""Banana"", ""Apple"", ""Orange""]",tests/unit/test_build.py,,1,6
survived,"def _format_results(res: List[forecast.ForecastPoint]) -> str:
    rows = [(r.year, f""{r.capability:.2f}"", "","".join(s.name for s in r.affected)) for r in res]
    return _pretty_table([""year"", ""capability"", ""affected""], rows)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,,1,7
survived,"def test_labels_allowed_characters():

    runner = ScenarioRunner(uri=GMT_DIR, uri_type='folder', filename='tests/data/usage_scenarios/labels_stress_allowed.yml', skip_unsafe=False, skip_system_checks=True, dev_cache_build=True, dev_no_sleeps=True, dev_no_metrics=True, dev_no_phase_stats=True)
    with Tests.RunUntilManager(runner) as context:
        context.run_until('setup_services')
        labels = get_labels()

        assert labels.get('TESTALLOWED') == 'alpha-num123_', Tests.assertion_info('TESTALLOWED label', labels)
        assert labels.get('test.label') == 'example.com', Tests.assertion_info('test.label label', labels)
        assert labels.get('OTHER_LABEL') == 'http://localhost:8080', Tests.assertion_info('OTHER_LABEL label', labels)
",tests/test_usage_scenario.py,,1,6
survived,"    def test_web_server_start_and_stop(self):
        klong = self.klong
        port = self._free_port()

        klong('.py(""klongpy.web"")')
        klong('index::{x;""hello""}')
        klong('get:::{}')
        klong('get,""/"",index')
        klong('post:::{}')
        handle = klong(f'h::.web({port};get;post)')
        self.handle = handle

        async def fetch():
            async with aiohttp.ClientSession() as session:
                async with session.get(f""http://localhost:{port}/"") as resp:
                    return await resp.text()

        response = asyncio.run_coroutine_threadsafe(fetch(), self.ioloop).result()
        self.assertEqual(response, ""hello"")

        asyncio.run_coroutine_threadsafe(handle.shutdown(), self.ioloop).result()
",tests/test_sys_fn_web.py,TestSysFnWeb,1,6
survived,"def create_repl(debug: bool = False):
    io_loop, io_thread, io_stop = setup_async_loop(debug=debug)
    klong_loop, klong_thread, klong_stop = setup_async_loop(debug=debug)

    append_pkg_resource_path_KLONGPATH()

    klong = KlongInterpreter()
    shutdown_event = CallbackEvent()
    klong['.system'] = {'ioloop': io_loop, 'klongloop': klong_loop, 'closeEvent': shutdown_event}

    return klong, (io_loop, io_thread, io_stop, klong_loop, klong_thread, klong_stop)
",klongpy/repl.py,,1,7
survived,"        async def fetch():
            async with aiohttp.ClientSession() as session:
                async with session.get(f""http://localhost:{port}/"") as resp:
                    return await resp.text()
",tests/test_sys_fn_web.py,TestSysFnWeb,1,6
survived,"    def test_runtime_port_env(self) -> None:
        """"""--runtime-port propagates AGENTS_RUNTIME_PORT.""""""
        from unittest.mock import patch

        mod = __import__(
            'alpha_factory_v1.demos.alpha_agi_business_v1.run_business_v1_local',
            fromlist=['main']
        )

        captured = {}

        def fake_start_bridge(host: str, runtime_port: int) -> None:  # type: ignore
            captured['env'] = os.getenv('AGENTS_RUNTIME_PORT')
            captured['port'] = runtime_port

        with patch.object(mod, '_start_bridge', fake_start_bridge), \
             patch.object(mod, 'check_env'):  # type: ignore
            with patch('alpha_factory_v1.demos.alpha_agi_business_v1.alpha_agi_business_v1.main'):
                mod.main(['--bridge', '--runtime-port', '7001'])

        self.assertEqual(captured['port'], 7001)
        self.assertEqual(captured['env'], '7001')
",tests/test_alpha_business_v1_script.py,TestAlphaBusinessV1Script,1,7
survived,"def test_Q2_finds_earliest_title_for_German_companies_with_character_keyword():
    assert result == ""Der Film""
",tests/dataset/job/compiler/py/q2.py,,1,6
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q3.py,,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q2.py,,1,6
survived,"def capability_agents(capability: str):
    """"""Return list of agent names exposing *capability*.""""""
    with _REGISTRY_LOCK:
        return CAPABILITY_GRAPH.get(capability, []).copy()
",alpha_factory_v1/backend/agents/registry.py,,1,7
survived,"    async def run_cycle(self) -> None:
        await asyncio.sleep(999)
",tests/test_insight_orchestrator_restart.py,FreezeAgent,1,6
survived,"        async def run() -> bool:
            await orch.bus.start()
            orch.ledger.start_merkle_task(3600)
            runner.start(orch.bus, orch.ledger)
            monitor = asyncio.create_task(orch._monitor())
            await asyncio.sleep(3)
            active = runner.task is not None and not runner.task.done()
            monitor.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await monitor
            if runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
            await orch.bus.stop()
            await orch.ledger.stop_merkle_task()
            orch.ledger.close()
            return active
",tests/test_insight_orchestrator_restart.py,TestInsightOrchestratorRestart,1,6
survived,"    def setUp(self):
        self.klong = KlongInterpreter()
",tests/test_eval_monad_list.py,TestEvalMonadList,1,6
survived,"    def __init__(self) -> None:
        self.logged: list[messaging.Envelope] = []
",tests/test_safety_guardian_fuzz.py,DummyLedger,1,7
survived,"    def _check_matrix_grad(self, name: str):
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x):
            return b.sum(b.matmul(x, x))

        g = b.grad(f)
        x = b.array([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        grad = g(x)
        if hasattr(grad, ""detach""):
            grad = grad.detach().cpu().numpy()
        np.testing.assert_allclose(np.array(grad), np.array([[7.0, 11.0], [9.0, 13.0]]))
",tests/test_autograd.py,TestAutograd,1,6
survived,"def test_simulation_benchmark(tmp_path: Path, benchmark: Any) -> None:
    os.environ[""SIM_RESULTS_DIR""] = str(tmp_path)
    from src.interface import api_server

    api = importlib.reload(api_server)
    cfg = api.SimRequest(horizon=1, pop_size=2, generations=1)

    def run() -> None:
        asyncio.run(api._background_run(""bench"", cfg))

    result = benchmark(run)
    p95 = quantiles(result.stats[""data""], n=20)[18] if result.stats[""data""] else 0.0
    data = {""p95"": p95, ""tokens"": _token_usage()}
    bench_dir = Path(__file__).parent / ""benchmarks""
    bench_dir.mkdir(exist_ok=True)
    (bench_dir / ""latest.json"").write_text(json.dumps(data))",tests/test_benchmark.py,,0,7
survived,"    def fake_run(
        cmd: list[str],
        repo_dir: str,
        *,
        image: str | None = None,
        mounts: dict[str, str] | None = None,
    ) -> tuple[int, str]:
        if ""pytest"" in cmd:
            res = subprocess.run(
                [""pytest"", ""-q"", ""--color=no""],
                cwd=repo_dir,
                capture_output=True,
                text=True,
            )
            return res.returncode, res.stdout + res.stderr
        if ""patch"" in cmd:
            ok, out = diff_utils.apply_diff(patch, repo_dir=repo_dir)
            return (0 if ok else 1), out
        return 0, """"
",tests/test_self_healer_sandbox.py,,1,6
survived,"def check_directory(directory: Path) -> int:
    bundle = directory / ""insight.bundle.js""
    html = directory / ""index.html""
    if not bundle.exists():
        print(f""{directory}: insight.bundle.js missing"")
        return 1
    if not html.exists():
        print(f""{directory}: index.html missing"")
        return 1
    text = html.read_text()
    match = re.search(r""<script[^>]*src=['\""]insight.bundle.js['\""][^>]*>"", text)
    if not match:
        print(f""{directory}: script tag for insight.bundle.js missing"")
        return 1
    tag = match.group(0)
    sri = re.search(r""integrity=['\""]([^'\""]+)['\""]"", tag)
    if not sri:
        print(f""{directory}: integrity attribute missing"")
        return 1
    expected = compute_hash(bundle)
    if sri.group(1) != expected:
        print(f""{directory}: hash mismatch: expected {expected}, found {sri.group(1)}"")
        return 1
    return 0
",scripts/verify_insight_bundle_hash.py,,1,7
survived,"def test_safety_agent_halts_on_nan(monkeypatch):
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    mod = _reload_module(monkeypatch)
    mod.A2ABus._subs = {}
    safety = mod.BasicSafetyAgent()
    msgs: list[dict] = []
    mod.A2ABus.subscribe(""orch"", lambda m: msgs.append(m))
    safety.handle({""loss"": np.nan})
    assert {""cmd"": ""stop""} in msgs
",tests/test_world_model_safety.py,,1,7
survived,"def _subdir_url() -> str:
    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"")
    repo_path = repo_path.removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    return f""https://{org}.github.io/{repo}/alpha_factory_v1/demos/""
",scripts/open_subdir_gallery.py,,1,6
survived,"def _pareto_ranks(pop: Sequence[Any]) -> list[int]:
    metrics = [_metrics(p) for p in pop]
    ranks = [1 for _ in pop]
    for i, a in enumerate(metrics):
        for j, b in enumerate(metrics):
            if i == j:
                continue
            if all(bk <= ak for bk, ak in zip(b, a)) and any(bk < ak for bk, ak in zip(b, a)):
                ranks[i] += 1
    return ranks
",src/simulation/selector.py,,1,6
survived,"def test_uses_subdirs_true(monkeypatch, tmp_path):
    monkeypatch.setenv('NOTES_EXPORT_USE_SUBDIRS', 'true')
    tracker = utils.NotesExportTracker(root_directory=str(tmp_path))
    assert tracker._uses_subdirs() is True
",tests/test_tracker.py,,1,7
survived,"def test_manual_build_missing_tsc(tmp_path: Path) -> None:
    work = tmp_path / ""browser""
    shutil.copytree(BROWSER_DIR, work)
    # provide required .env
    (work / "".env"").write_text((BROWSER_DIR / "".env.sample"").read_text())
    (work / ""build"" / ""__init__.py"").touch()

    # scrub placeholder text to avoid asset download
    for sub in (""wasm"", ""wasm_llm""):
        d = work / sub
        if d.exists():
            for p in d.rglob(""*""):
                if p.is_file():
                    data = p.read_bytes().replace(b""placeholder"", b"""")
                    p.write_bytes(data)
    bundle = work / ""lib"" / ""bundle.esm.min.js""
    bundle.write_text(bundle.read_text().replace(""Placeholder"", """"))

    # isolate PATH with node only
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()
    node = shutil.which(""node"")
    assert node
    os.symlink(node, bin_dir / ""node"")

    env = os.environ.copy()
    env[""PATH""] = str(bin_dir)

    result = subprocess.run(
        [sys.executable, ""manual_build.py""],
        cwd=work,
        capture_output=True,
        text=True,
        env=env,
    )
    assert result.returncode != 0
    assert ""TypeScript compiler not found"" in result.stderr",tests/test_manual_build_missing_tsc.py,,0,7
survived,"        async def _cycle() -> None:
            t0 = time.time()
            span_cm = tracer.start_as_current_span(self.name) if tracer else contextlib.nullcontext()
            with span_cm:
                try:
                    await asyncio.wait_for(maybe_await(self.inst.run_cycle), timeout=self._max_cycle_sec)
                except asyncio.TimeoutError:
                    MET_ERR.labels(self.name).inc()
                    log.error(""%s run_cycle exceeded %ss budget – skipped"", self.name, self._max_cycle_sec)
                except Exception as exc:  # noqa: BLE001
                    MET_ERR.labels(self.name).inc()
                    log.exception(""%s.run_cycle crashed: %s"", self.name, exc)
                finally:
                    dur_ms = (time.time() - t0) * 1_000
                    MET_LAT.labels(self.name).observe(dur_ms)
                    self.last_beat = time.time()
                    self._publish(""agent.cycle"", {""agent"": self.name, ""latency_ms"": dur_ms, ""ts"": utc_now()})
",alpha_factory_v1/backend/agent_runner.py,AgentRunner,1,7
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""dest"", type=Path, nargs=""?"", default=Path(""models""), help=""Target directory"")
    parser.add_argument(""--model"", default=""124M"", help=""GPT-2 model size"")
    args = parser.parse_args()

    try:
        download_model(args.dest, args.model)
    except Exception as exc:
        sys.exit(str(exc))
",scripts/download_gpt2_small.py,,1,7
survived,"async def get_orders_for_user(user_id: int, ctx: EnrichContext) -> list[Order]:
    return await list_orders(user_id=user_id, ctx=ctx)
",examples/shop_api_gateway/app.py,,0,7
survived,"async def get_order_products(order_id: int, ctx: EnrichContext) -> list[Product]:
    client = await _client(ctx)
    resp = await client.get(f""/orders/{order_id}"")
    resp.raise_for_status()
    data = resp.json()
    products = []
    for pid in data.get(""product_ids"", []):
        r = await client.get(f""/products/{pid}"")
        r.raise_for_status()
        products.append(Product(**r.json()))
    return products
",examples/shop_api_gateway/app.py,,1,6
survived,"async def get_user(user_id: int, ctx: EnrichContext) -> User:
    client = await _client(ctx)
    resp = await client.get(f""/users/{user_id}"")
    resp.raise_for_status()
    return User(**resp.json())
",examples/shop_api_gateway/app.py,,1,7
survived,"def padLeft(s, w):
    res = """"
    n = w - len(s)
    while n > 0:
        res = res + "" ""
        n = n - 1
    return res + s
",tests/rosetta/transpiler/Python/box-the-compass.py,,1,6
survived,"def isPrime(n):
    if n < 2:
        return False
    if n % 2 == 0:
        return n == 2
    if n % 3 == 0:
        return n == 3
    d = 5
    while d * d <= n:
        if n % d == 0:
            return False
        d = d + 2
        if n % d == 0:
            return False
        d = d + 4
    return True
",tests/rosetta/transpiler/Python/blum-integer.py,,1,7
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/box-the-compass.py,,1,7
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/bulls-and-cows.py,,1,7
survived,"def isPrime(n):
    if n < 2:
        return False
    if n % 2 == 0:
        return n == 2
    if n % 3 == 0:
        return n == 3
    d = 5
    while d * d <= n:
        if n % d == 0:
            return False
        d = d + 2
        if n % d == 0:
            return False
        d = d + 4
    return True
",tests/rosetta/transpiler/Python/brazilian-numbers.py,,1,7
survived,"def zeroptr(ref):
    ref[0] = 0
",tests/rosetta/transpiler/Python/call-a-function-11.py,,1,6
survived,"def shiftRune(r, k):
    if r >= ""a"" and r <= ""z"":
        return chr(((ord(r) - 97 + k) % 26) + 97)
    if r >= ""A"" and r <= ""Z"":
        return chr(((ord(r) - 65 + k) % 26) + 65)
    return r
",tests/rosetta/transpiler/Python/caesar-cipher-2.py,,1,7
survived,"def link(graphdb_filename, osmdb_filename, gtfsdb_filename):
    """"""Link OSM vertices to GTFS vertices.""""""
    gtfsdb = GTFSDatabase(gtfsdb_filename)
    osmdb = OSMDB(osmdb_filename)
    gdb = GraphDatabase(graphdb_filename)

    n_stops = gtfsdb.count_stops()
    c = gdb.get_cursor()
    for i, (stop_id, _name, stop_lat, stop_lon) in enumerate(gtfsdb.stops()):
        click.echo(f""{i}/{n_stops}"")

        nd_id, nd_lat, nd_lon, nd_dist = osmdb.nearest_node(stop_lat, stop_lon)
        station_vertex_id = f""sta-{stop_id}""
        osm_vertex_id = f""osm-{nd_id}""

        click.echo(f""{station_vertex_id} {osm_vertex_id}"")

        gdb.add_edge(station_vertex_id, osm_vertex_id, Link(), c)
        gdb.add_edge(osm_vertex_id, station_vertex_id, Link(), c)

    gdb.commit()
",pygs/graphserver/cli.py,,1,7
survived,"def gtfs(gtfs_filename, gtfsdb_filename, tables, verbose):
    """"""Compile a GTFS zip file into a GTFS database.""""""
    if not tables:
        tables = None
    gtfsdb = GTFSDatabase(gtfsdb_filename, overwrite=True)
    gtfsdb.load_gtfs(gtfs_filename, tables, reporter=sys.stdout, verbose=verbose)
",pygs/graphserver/cli.py,,1,7
survived,"def _mutate(g: float) -> float:
    return g + random.uniform(-3.0, 3.0)
",experiments/ablate_selector.py,,1,6
survived,"        def register_agent(self, _agent):
            pass
",tests/test_adk_gateway_startup.py,_Router,0,7
survived,"def __dir__() -> list[str]:
    return sorted(list(globals().keys()) + __all__)",alpha_factory_v1/demos/__init__.py,,1,7
survived,"        def rewrite_fn(ag: List[int]) -> List[int]:
            """"""Rewrite agents using the Anthropic model.""""""
            return cast(List[int], anthropic_rewrite(ag, model=model))
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/run_demo.py,,1,6
survived,"def problem_response(exc: HTTPException) -> JSONResponse:
    """"""Return an RFC 7807 compliant response for ``exc``.""""""

    try:
        title = HTTPStatus(exc.status_code).phrase
    except Exception:  # pragma: no cover - unknown status code
        title = str(exc.status_code)

    detail = (
        exc.detail if isinstance(exc.detail, str) else str(exc.detail) if exc.detail else """"
    )

    body: dict[str, Any] = {""type"": ""about:blank"", ""title"": title, ""status"": exc.status_code}
    if detail:
        body[""detail""] = detail

    return JSONResponse(status_code=exc.status_code, content=body)
",src/interface/problem_json.py,,1,7
survived,"def pad2(x):
    s = str(x)
    if len(s) < 2:
        s = "" "" + s
    sys.exit(s)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,,0,7
survived,"def randInt(s, n):
    next = (s * 1664525 + 1013904223) % 2147483647
    sys.exit([next, next % n])
",tests/rosetta/transpiler/Python/evolutionary-algorithm.py,,0,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fasta-format.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    nums = [""3"", ""5"", ""17"", ""257"", ""65537"", ""4294967297"", ""18446744073709551617"", ""340282366920938463463374607431768211457""]
    print(""First 8 Fermat numbers:"")
    for n in nums:
        print(n)
    factors = [""3"", ""5"", ""17"", ""257"", ""65537"", ""641 6700417"", ""274177 67280421310721"", ""59649589127497217 5704689200685129054721""]
    print(""\nFactors:"")
    i = 0
    while i < len(nums):
        print(""F"" + str(i) + "" = "" + factors[i])
        i = i + 1
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fermat-numbers.py,,1,7
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    b = 2
    while b <= 16:
        start = 4 * b
        stop = 6 * b
        print(""Base "" + str(b) + "": "" + str(start) + ""th to "" + str(stop) + ""th esthetic numbers:"")
        n = 1
        c = 0
        line = """"
        while c < stop:
            if isEsthetic(n, b):
                c = c + 1
                if c >= start:
                    if len(line) > 0:
                        line = line + "" ""
                    line = line + toBase(n, b)
            n = n + 1
        print(line)
        print("""")
        b = b + 1
    listEsths(1000, 1010, 9999, 9898, 16, True)
    listEsths(100000000, 101010101, 130000000, 123456789, 9, True)
    listEsths(100000000000, 101010101010, 130000000000, 123456789898, 7, False)
    listEsths(100000000000000, 101010101010101, 130000000000000, 123456789898989, 5, False)
    listEsths(100000000000000000, 101010101010101010, 130000000000000000, 123456789898989898, 4, False)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/esthetic-numbers.py,,1,7
survived,"def runRules(rules, s):
    changed = True
    while changed:
        changed = False
        i = 0
        while i < len(rules):
            r = rules[i]
            pat = r.get(""pat"")
            rep = r.get(""rep"")
            term = r.get(""term"")
            idx = indexOfSub(s, pat)
            if idx >= 0:
                s = """".join(s[:idx]) + rep + """".join(s[idx + len(pat):])
                changed = True
                if term:
                    sys.exit(s)
                break
            i = i + 1
    sys.exit(s)
",tests/rosetta/transpiler/Python/execute-a-markov-algorithm.py,,0,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/euler-method.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    seq = fib(10)
    for v in seq:
        print(str(v))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fibonacci-sequence-4.py,,1,6
survived,"def gen(init, n):
    b = init
    res = []
    sum = 0
    for x in b:
        res = res + [x]
        sum = sum + x
    while len(res) < n:
        next = sum
        res = res + [next]
        sum = sum + next - b[0]
        b = b[1:len(b)] + [next]
    sys.exit(res)
",tests/rosetta/transpiler/Python/fibonacci-n-step-number-sequences.py,,0,8
survived,"def endsWith(s, suf):
    if len(s) < len(suf):
        sys.exit(False)
    sys.exit(s[len(s) - len(suf):len(s)] == suf)
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,,0,9
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/exponentiation-order.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/evaluate-binomial-coefficients.py,,1,6
survived,"def test_devicon_directory_match():
    file = MockFile('Documents', is_directory=True)
    assert devicons.devicon(file) == ''
",tests/test_devicons.py,,1,7
survived,"def test_devicon_py_file():
    file = MockFile('example.py')
    assert devicons.devicon(file) == ''
",tests/test_devicons.py,,1,6
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an informational or error event.""""""
        self.events.append(
            TelemetryCollector.Event(
                category=category,
                severity=severity,
                message=message,
            )
        )
        log = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log = self.logger.error
        elif severity is self.Severity.WARNING:
            log = self.logger.warning
        log(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,8
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an informational or error event.""""""
        self.events.append(
            TelemetryCollector.Event(
                category=category,
                severity=severity,
                message=message,
            )
        )
        log = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log = self.logger.error
        elif severity is self.Severity.WARNING:
            log = self.logger.warning
        log(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,8
survived,"    def resources(self) -> RayResources:
        return RayResources(cpu=1)
",marin/rl/envs/math_env.py,MathEnvConfig,1,6
survived,"    def build(self, inference: InferenceEndpoint, rollout_sink: RolloutSink) -> AbstractMarinEnv:
        """"""Instantiate the environment.

        The *rollout_sink* should be called with :class:`~marin.rl.types.RolloutGroup` batches.
        """"""
",marin/rl/config.py,AbstractEnvConfig,1,6
survived,"    async def shutdown(self) -> None:
        """"""Optional: release resources before shutdown.""""""

        logger.debug(""%s closed"", self.__class__.__name__)
",marin/rl/env.py,AbstractMarinEnv,0,6
survived,"def test_chat_echo_env(openai_mock):  # type: ignore[valid-type]
    # Prepare mock response
    openai_mock.chat.completions.create.response = {
        ""choices"": [
            {
                ""index"": 0,
                ""finish_reason"": ""stop"",
                ""message"": {""content"": ""Hello! How can I help?"", ""role"": ""assistant""},
            }
        ]
    }

    # Collect rollouts emitted by the env
    collected: deque[RolloutGroup] = deque()

    def sink(groups: list[RolloutGroup]):
        collected.extend(groups)

    env = ChatEchoEnv(
        inference=InferenceEndpoint(""https://api.openai.com/v1""),
        rollout_sink=sink,  # type: ignore[arg-type]
        prompt=""Hello!"",
        max_iters=1,  # run exactly once then stop
        api_key=""sk-fake"",
    )

    asyncio.run(env.run())

    # Ensure sink received one rollout group with expected content
    assert len(collected) == 1
    rg = collected.pop()
    assert rg.rollouts[0].turns[0].message == ""Hello! How can I help?""
    # Mock should have been hit exactly once
    assert openai_mock.chat.completions.create.route.call_count == 1",tests/rl/test_openai_env.py,,1,7
survived,"def _groups_to_table(groups: list[RolloutGroup]) -> pa.Table:
    rows = []
    for g in groups:
        for r in g.rollouts:
            row = _rollout_to_pyobj(r)
            row[""id""] = g.id
            row[""source""] = g.source
            row[""created""] = g.created
            row[""group_metadata_json""] = json.dumps(g.metadata, separators=("","", "":""))
            rows.append(row)

    return pa.Table.from_pylist(rows)
",marin/rl/parquet_store.py,,1,6
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = WorkflowAgent()
    runtime.register(agent)
    print(""Registered WorkflowAgent with runtime"")

    if ADK_AVAILABLE:
        auto_register([agent])
        maybe_launch()
        print(""WorkflowAgent exposed via ADK gateway"")

    runtime.run()
",alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py,,1,7
survived,"    def test_detect_supply_chain_alpha(self) -> None:
        msg = alpha_detection.detect_supply_chain_alpha()
        self.assertIsInstance(msg, str)
        self.assertTrue(""USD"" in msg or ""offline data missing"" in msg)
",tests/test_alpha_detection.py,TestAlphaDetection,1,6
survived,"def pulse_source(
    token: str,
    start_date: str,
    end_date: Optional[str] = None,
    metrics: Optional[Iterable[str]] = None,
    topsites: Optional[bool] = None,
    ip_version: Optional[str] = None,
) -> Iterable[dlt.sources.DltResource]:
    """"""Create resources for Internet Society Pulse metrics.

    Args:
        token: Bearer token for the API.
        start_date: First date of the data range (YYYY-MM-DD).
        end_date: Last date of the data range.
        metrics: Subset of metrics to fetch. Defaults to all available metrics.
        topsites: Optional flag used by some endpoints.
        ip_version: IP version parameter used by some endpoints.
    """"""
    if metrics is None:
        metrics = GLOBAL_METRICS.keys()

    headers = {""Authorization"": f""Bearer {token}""}

    resources: List[EndpointResource] = []
    for name in metrics:
        path = GLOBAL_METRICS.get(name)
        if not path:
            continue

        endpoint: Dict[str, Any] = {
            ""path"": path,
            ""params"": {
                ""start_date"": ""{incremental.start_value}"",
            },
            ""incremental"": {
                ""cursor_path"": ""date"",
                ""start_param"": ""start_date"",
                ""end_param"": ""end_date"",
                ""initial_value"": start_date,
                ""end_value"": end_date,
                ""range_start"": ""closed"",
                ""range_end"": ""closed"",
            },
            ""paginator"": ""single_page"",
        }

        if end_date is not None:
            endpoint[""params""][""end_date""] = end_date
        if topsites is not None and name in {""http"", ""https""}:
            endpoint[""params""][""topsites""] = topsites
        if ip_version is not None and name in {""roa"", ""rov"", ""tls"", ""tls13""}:
            endpoint[""params""][""ip_version""] = ip_version

        resources.append({""name"": name, ""endpoint"": endpoint})

    config: RESTAPIConfig = {
        ""client"": {
            ""base_url"": ""https://pulse.internetsociety.org/api/"",
            ""headers"": headers,
        },
        ""resource_defaults"": {
            ""write_disposition"": ""merge"",
            ""primary_key"": ""date"",
        },
        ""resources"": resources,
    }

    yield from rest_api_resources(config)",ingestr/src/pulse/__init__.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/binary-digits.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/biorhythms.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/benfords-law.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/binary-strings.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-7.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-8.py,,1,6
survived,"def test_queue_never_exceeds_cap() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""navigator.sendBeacon=()=>false;""
            ""Object.defineProperty(navigator,'onLine',{get:()=>false,configurable:true});""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        for _ in range(105):
            page.click(""text=Share"")
            page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert (
            page.evaluate(
                ""JSON.parse(localStorage.getItem('telemetryQueue')).length""
            )
            == 100
        )
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,,1,7
survived,"def test_main_no_streamlit(monkeypatch) -> None:
    mod_name = ""src.interface.lineage_dashboard""
    monkeypatch.setitem(sys.modules, ""streamlit"", None)
    monkeypatch.setitem(sys.modules, ""streamlit_autorefresh"", None)
    mod = importlib.reload(importlib.import_module(mod_name))
    mod.main([])",tests/test_lineage_dashboard.py,,1,6
survived,"def test_refinement_proposes_cycle_adjustment(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    logs = tmp_path / ""logs""
    logs.mkdir()
    (logs / ""log.json"").write_text(
        ""\n"".join(['{""agent"":""demo"",""latency_ms"":6000,""ts"":0}', '{""agent"":""demo"",""latency_ms"":7000,""ts"":1}']),
        encoding=""utf-8"",
    )

    reg = StakeRegistry()
    reg.set_stake(""meta"", 1.0)

    with patch.object(harness, ""vote_and_merge"") as vote:
        agent = MetaRefinementAgent(repo, logs, reg)
        agent.refine()

    called_diff = vote.call_args.args[1]
    assert ""increase cycle"" in called_diff",tests/test_meta_refinement_agent.py,,1,6
survived,"def main() -> None:
    try:
        orchestrator.Orchestrator().run_forever()
    except KeyboardInterrupt:
        pass
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,,1,6
survived,"def create_app() -> FastAPI:
    app = FastAPI()
    for tool_cls in load_tools():
        router = tool_cls.get_router()
        app.include_router(router, prefix=tool_cls.endpoint_path, tags=[tool_cls.slug])
    return app",servers/server_clear_thought/app.py,,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        raise NotImplementedError",servers/server_clear_thought/core/base_tool.py,BaseTool,0,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        steps = [
            f""Practice {payload['skill']} at level {lvl}""
            for lvl in range(payload[""current_level""], payload[""target_level""] + 1)
        ]
        measures = [""Take breaks"", ""Monitor progress""]
        return {
            ""scaffold_steps"": steps,
            ""safety_measures"": measures,
            ""review_intervals"": ""weekly"",
        }",servers/server_clear_thought/tools/safe_struggle_designer.py,SafeStruggleDesigner,1,7
survived,"def test_analogical_mapper():
    client = get_client()
    resp = client.post(
        ""/analogical-mapper/execute"",
        json={""problem"": ""p""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""analogies"", ""suggested_prompts""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"def test_meme_disabled() -> None:
    rng = random.Random(0)
    op = SelfRewriteOperator(steps=3, rng=rng, templates=[""meme""], reuse_rate=0.0)
    result = op(""improve quick test"")
    assert result != ""meme""
",tests/test_meme_reuse.py,,1,6
survived,"    def test_ensure_offline_creates_placeholder_rows(self) -> None:
        """"""_ensure_offline should write one row when downloads fail.""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp = Path(tmpdir)
            with patch.object(data_feeds, ""DATA_DIR"", tmp), \
                 patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds.urlopen"", side_effect=Exception):
                data_feeds._ensure_offline()
                for name in data_feeds.OFFLINE_URLS:
                    with open(tmp / name, newline="""") as f:
                        rows = list(csv.DictReader(f))
                    self.assertEqual(len(rows), 1)
",tests/test_macro_sentinel.py,TestMacroSentinel,0,6
survived,"def _prefetch_vault() -> None:
    """"""Populate environment secrets from HashiCorp Vault if configured.""""""
    if ""VAULT_ADDR"" in os.environ:
        try:  # pragma: no cover - optional dependency
            import importlib

            hvac = importlib.import_module(""hvac"")

            addr = os.environ[""VAULT_ADDR""]
            token = os.getenv(""VAULT_TOKEN"")
            secret_path = os.getenv(""OPENAI_API_KEY_PATH"", ""OPENAI_API_KEY"")
            client = hvac.Client(url=addr, token=token)
            data = client.secrets.kv.read_secret_version(path=secret_path)
            value = data[""data""][""data""].get(""OPENAI_API_KEY"")
            if value:
                os.environ[""OPENAI_API_KEY""] = value
        except Exception as exc:  # noqa: BLE001
            _log.warning(""Vault lookup failed: %s"", exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/config.py,,1,6
survived,"async def test_attach_runner(monkeypatch):
    # Fake runner class
    class FakeRunner:
        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()

    client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
    send_mock = AsyncMock(return_value={""ok"": True})
    monkeypatch.setattr(client, ""send"", send_mock)

    client.attach_runner(FakeRunner, ""trace"")
    res = await FakeRunner().run(None)
    assert hasattr(res, ""span_graph"")
    send_mock.assert_awaited_once_with(""trace"", {""span"": 1})
    await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"async def test_send_success():
    with patch(""aiohttp.ClientSession"") as mock_session:
        response = AsyncMock()
        response.status = 200
        response.json = AsyncMock(return_value={""ok"": True})
        cm = AsyncMock()
        cm.__aenter__.return_value = response
        mock_session.return_value.post.return_value = cm
        mock_session.return_value.close = AsyncMock()
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        result = await client.send(""trace"", {""data"": 1})
        assert result == {""ok"": True}
        await client.close()
",tests/unit/test_telemetry_client.py,,1,7
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def __post_init__(self) -> None:
        if self.auth_token:
            self.headers[""Authorization""] = f""Bearer {self.auth_token}""
        self.headers.setdefault(""Content-Type"", ""application/json"")
",src/meta_agent/services/telemetry_client.py,EndpointConfig,1,6
survived,"def test_cost_cap_threshold_events(caplog):
    t = TelemetryCollector(cost_cap=0.02)
    with caplog.at_level(logging.INFO):
        t.add_usage(1000, 0, model=""o3"")
        assert len(t.events) == 0
        t.add_usage(500, 0, model=""o3"")
        assert len(t.events) == 1
        assert t.events[0].severity == TelemetryCollector.Severity.WARNING
        t.add_usage(300, 0, model=""o3"")
        assert len(t.events) == 2
        assert t.events[1].severity == TelemetryCollector.Severity.ERROR
        with pytest.raises(RuntimeError):
            t.add_usage(200, 0, model=""o3"")
        assert len(t.events) == 3
        assert t.events[-1].severity == TelemetryCollector.Severity.CRITICAL
",tests/unit/test_telemetry_collector.py,,1,7
survived,"def test_collector_with_db(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    collector = TelemetryCollector(db=db, include_sensitive=False)
    collector.start_timer()
    collector.stop_timer()
    line = collector.summary_line()
    assert ""<redacted>"" in line
    assert db.fetch_all()
    db.close()",tests/unit/test_telemetry_db.py,,1,6
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """"""
            CREATE TABLE IF NOT EXISTS telemetry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                tokens INTEGER,
                cost REAL,
                latency REAL,
                guardrail_hits INTEGER
            )
            """"""
        )
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an informational or error event.""""""
        self.events.append(
            TelemetryCollector.Event(
                category=category,
                severity=severity,
                message=message,
            )
        )
        log = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log = self.logger.error
        elif severity is self.Severity.WARNING:
            log = self.logger.warning
        log(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"async def test_send_retry_success():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp1 = AsyncMock()
        resp1.status = 500
        resp1.text = AsyncMock(return_value=""bad"")
        cm1 = AsyncMock()
        cm1.__aenter__.return_value = resp1

        resp2 = AsyncMock()
        resp2.status = 200
        resp2.json = AsyncMock(return_value={""ok"": True})
        cm2 = AsyncMock()
        cm2.__aenter__.return_value = resp2

        mock_session.return_value.post.side_effect = [cm1, cm2]
        mock_session.return_value.close = AsyncMock()

        client = TelemetryAPIClient(
            {""trace"": EndpointConfig(""http://example.com"")}, retries=1, backoff=0
        )
        result = await client.send(""trace"", {""d"": 1})
        assert result == {""ok"": True}
        assert mock_session.return_value.post.call_count == 2
        await client.close()
",tests/unit/test_telemetry_client.py,,1,7
survived,"async def test_send_retry_failure():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        mock_session.return_value.close = AsyncMock()

        client = TelemetryAPIClient(
            {""trace"": EndpointConfig(""http://example.com"")}, retries=1, backoff=0
        )
        with pytest.raises(Exception):
            await client.send(""trace"", {""d"": 1})
        assert mock_session.return_value.post.call_count == 2
        await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"    def __init__(self, *args, **kwargs):
        self.request_info = kwargs.get(""request_info"")
        self.history = kwargs.get(""history"")
        self.status = kwargs.get(""status"")
        self.message = kwargs.get(""message"")
        self.headers = kwargs.get(""headers"")
        super().__init__(self.message)
",src/aiohttp/__init__.py,ClientResponseError,1,6
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def close(self) -> None:
        self.conn.close()",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def close(self) -> None:
        self.conn.close()",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def test_purge_old(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(1, 0.01, 0.1, 0)
    # update timestamp to old date
    old_ts = ""2000-01-01T00:00:00""
    db.conn.execute(""UPDATE telemetry SET timestamp=?"", (old_ts,))
    db.conn.commit()
    db.purge_old()
    assert db.fetch_all() == []
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"    async def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 2:
            raise ValueError(""boom"")
        return ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_retry.py,,1,6
survived,"def test_has_network_with_proxy(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Ensure proxy variables are consulted for connectivity.""""""

    attempts: list[tuple[str, int]] = []

    class _Sock:
        def __enter__(self) -> ""_Sock"":
            return self

        def __exit__(self, *exc: object) -> None:
            pass

    def _connect(addr: tuple[str, int], timeout: float = 1.0) -> _Sock:
        attempts.append(addr)
        if addr[0] == ""proxy.local"":
            return _Sock()
        raise OSError

    monkeypatch.setenv(""HTTP_PROXY"", ""http://proxy.local:8080"")
    monkeypatch.setenv(""HTTPS_PROXY"", ""http://proxy.local:8080"")
    monkeypatch.setattr(check_env.socket, ""create_connection"", _connect)  # type: ignore[attr-defined]
    assert check_env.has_network() is True
    assert attempts[0] == (""proxy.local"", 8080)
",tests/test_check_env_network.py,,1,7
survived,"def test_has_network_head_fallback(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Use urllib as fallback when socket connections fail.""""""

    def _connect(_addr: tuple[str, int], timeout: float = 1.0) -> None:
        raise OSError

    called: list[str] = []

    class _Resp:
        def __enter__(self) -> ""_Resp"":
            return self

        def __exit__(self, *exc: object) -> None:
            pass

    def _urlopen(req: object, timeout: float = 1.0) -> _Resp:
        called.append(getattr(req, ""full_url"", """"))
        return _Resp()

    monkeypatch.setenv(""HTTP_PROXY"", ""http://proxy.local:3128"")
    monkeypatch.setenv(""HTTPS_PROXY"", ""http://proxy.local:3128"")
    monkeypatch.setattr(check_env.socket, ""create_connection"", _connect)  # type: ignore[attr-defined]
    monkeypatch.setattr(urllib.request, ""urlopen"", _urlopen)
    assert check_env.has_network() is True
    assert called and called[0].startswith(""https://"")",tests/test_check_env_network.py,,1,7
survived,"def print_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]):
    print(format_prompt_summary(prompt_messages))
",backend/utils.py,,1,6
deleted,"    def repl(match: re.Match[str]) -> str:
        char = match.group(0)
        escapes = mapping.get(char)
        if escapes:
            return escapes.pop(0)
        return char
",src/flynt/utils/utils.py,,1,6
survived,"    async def _chain(first, it):
        yield first
        async for x in it:
            yield x
",webscout/AIutel.py,,1,6
survived,"async def _decode_byte_stream_async(
    byte_iterator: Iterable[bytes],
    encoding: EncodingType = 'utf-8',
    errors: str = 'replace',
    buffer_size: int = 8192
) -> AsyncGenerator[str, None]:
    """"""Asynchronous version of :func:`_decode_byte_stream`.""""""
    try:
        decoder = codecs.getincrementaldecoder(encoding)(errors=errors)
    except LookupError:
        decoder = codecs.getincrementaldecoder('utf-8')(errors=errors)

    buffer = bytearray(buffer_size)
    buffer_view = memoryview(buffer)

    async for chunk_bytes in byte_iterator:
        if not chunk_bytes:
            continue
        try:
            if len(chunk_bytes) <= buffer_size:
                buffer[:len(chunk_bytes)] = chunk_bytes
                text = decoder.decode(buffer_view[:len(chunk_bytes)], final=False)
            else:
                text = decoder.decode(chunk_bytes, final=False)
            if text:
                yield text
        except UnicodeDecodeError:
            yield f""[Encoding Error: Could not decode bytes with {encoding}]\n""

    try:
        final_text = decoder.decode(b'', final=True)
        if final_text:
            yield final_text
    except UnicodeDecodeError:
        yield f""[Encoding Error: Could not decode final bytes with {encoding}]\n""
",webscout/AIutel.py,,1,7
survived,"    def start_merkle_task(self, interval: int = 3600) -> None:
        if self._task is None:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:  # pragma: no cover - no loop in sync context
                _log.warning(""Merkle task requires a running event loop"")
                return
            self._task = loop.create_task(self._loop(interval))
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"    async def start(self) -> None:
        logger.info(
            ""A2ABus.start() called: port=%s broker=%s"",
            self.settings.bus_port,
            self.settings.broker_url or ""disabled"",
        )
        self._handshake_peers.clear()
        self._handshake_failures.clear()
        self._handshake_nonces.clear()
        if self.settings.broker_url and AIOKafkaProducer:
            self._producer = AIOKafkaProducer(bootstrap_servers=self.settings.broker_url)
            await self._producer.start()

        if not self.settings.bus_port or grpc is None:
            return
        server = grpc.aio.server()
        method = grpc.unary_unary_rpc_method_handler(
            self._handle_rpc,
            request_deserializer=lambda b: b,
            response_serializer=lambda b: b,
        )
        service = grpc.method_handlers_generic_handler(""bus.Bus"", {""Send"": method})
        server.add_generic_rpc_handlers((service,))
        if self.settings.bus_cert and self.settings.bus_key:
            key = Path(self.settings.bus_key).read_bytes()
            crt = Path(self.settings.bus_cert).read_bytes()
            creds = grpc.ssl_server_credentials(((key, crt),))
            server.add_secure_port(f""[::]:{self.settings.bus_port}"", creds)
        elif self.settings.allow_insecure:
            server.add_insecure_port(f""[::]:{self.settings.bus_port}"")
        else:
            raise RuntimeError(""AGI_INSIGHT_BUS_CERT and AGI_INSIGHT_BUS_KEY are required"")
        await server.start()
        self._server = server
",alpha_factory_v1/common/utils/messaging.py,A2ABus,1,7
survived,"    def unsubscribe(self, topic: str, handler: Callable[[EnvelopeLike], Awaitable[None] | None]) -> None:
        """"""Remove a previously subscribed handler.""""""
        handlers = self._subs.get(topic)
        if not handlers:
            return
        with contextlib.suppress(ValueError):
            handlers.remove(handler)
        if not handlers:
            self._subs.pop(topic, None)
",alpha_factory_v1/common/utils/messaging.py,A2ABus,1,7
survived,"    def close(self) -> None:
        if self.conn:
            self.conn.close()
            self.conn = None
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"def init_config(env_file: str = "".env"") -> None:
    """"""Load environment variables and refresh :data:`CFG`.""""""

    _load_dotenv(env_file)
    _prefetch_vault()
    global CFG
    CFG = Settings()
",alpha_factory_v1/common/utils/config.py,,1,7
survived,"async def test_api_server_start_stop(monkeypatch):
    events = []

    async def fake_start_servers(*a, **k):
        events.append(""start"")

        async def sleeper():
            await asyncio.sleep(0)
        task = asyncio.create_task(sleeper())
        server = SimpleNamespace(stop=lambda code=0: events.append(""stop""))
        return task, server

    monkeypatch.setattr(
        ""alpha_factory_v1.backend.services.api_server_service.start_servers"",
        fake_start_servers,
    )

    srv = APIServer({}, 1, object(), 0, 0, ""INFO"", True)
    await srv.start()
    assert events == [""start""]
    await srv.stop()
    assert events[-1] == ""stop""",tests/test_api_server_service.py,,1,7
survived,"    async def stop(self) -> None:  # pragma: no cover - close handled by EventBus
        return None
",alpha_factory_v1/backend/services/kafka_service.py,KafkaService,0,7
survived,"def update() -> bool:
    lines = WORKFLOW.read_text().splitlines()
    changed = False
    for i, line in enumerate(lines):
        m = PATTERN.match(line)
        if not m:
            continue
        prefix, action, current, comment = m.groups()
        if action.startswith(""./""):
            continue
        latest = fetch_latest(action)
        if not latest:
            continue
        tag, sha = latest
        new_comment = f"" # {sha}""
        if current == tag and comment == new_comment:
            continue
        lines[i] = f""{prefix}{action}@{tag}{new_comment}""
        changed = True
    if changed:
        WORKFLOW.write_text(""\n"".join(lines) + ""\n"")
    return changed
",tools/update_actions.py,,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q7.py,_Group,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q8.py,_Group,1,8
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q10.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q14.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Auto2,1,7
survived,"        def normal(self, mu: float, sigma: float, size: int):
            return [self._rand.gauss(mu, sigma) for _ in range(size)]
",src/meta_agent/embedding_models.py,_RandomNormal,1,7
survived,"    def test_env_overrides_default_ledger(self) -> None:
        default = Path(STUB).with_name(""cross_alpha_log.json"")
        if default.exists():
            default.unlink()
        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""env_log.json""
            env = os.environ.copy()
            env[""CROSS_ALPHA_LEDGER""] = str(ledger)
            result = subprocess.run(
                [
                    sys.executable,
                    STUB,
                    ""-n"",
                    ""1"",
                    ""--seed"",
                    ""3"",
                    ""--model"",
                    ""gpt-4o-mini"",
                ],
                capture_output=True,
                text=True,
                env=env,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            self.assertFalse(default.exists(), ""default ledger should not be used"")
            self.assertTrue(ledger.exists())
            data = json.loads(ledger.read_text())
            self.assertIsInstance(data, list)
            self.assertEqual(len(data), 1)
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub,1,7
survived,"    def test_register_and_get(self):
        class DummyAgent(AgentBase):
            NAME = ""dummy_test""
            CAPABILITIES = [""foo""]

            async def step(self):
                return None

        meta = AgentMetadata(
            name=DummyAgent.NAME,
            cls=DummyAgent,
            version=""0.1"",
            capabilities=DummyAgent.CAPABILITIES,
            compliance_tags=[],
        )
        register_agent(meta)

        self.assertIn(DummyAgent.NAME, list_agents())
        self.assertEqual(capability_agents(""foo""), [DummyAgent.NAME])
        agent = get_agent(DummyAgent.NAME)
        self.assertIsInstance(agent, DummyAgent)
",tests/test_agents_registry.py,TestAgentRegistryFunctions,1,7
survived,"    def test_condition_true(self):
        from alpha_factory_v1.backend.agents import register, _agent_base
        Base = _agent_base()

        @register
        class OkAgent(Base):
            NAME = ""ok""

            async def step(self):
                return None

        self.assertIn(""ok"", AGENT_REGISTRY)
",tests/test_agents_registry.py,TestRegisterDecorator,1,7
survived,"    async def run_cycle(self) -> None:  # pragma: no cover - default wrapper
        """"""Single orchestrator cycle – runs :meth:`step` once.""""""
        await self.step()
",alpha_factory_v1/backend/agents/base.py,AgentBase,0,6
survived,"    def test_battery_optim_no_pulp(self):
        with patch.object(energy_agent, ""pulp"", None):
            res = energy_agent._battery_optim([1, 2], [3, 4])
        self.assertEqual(res, {""schedule"": []})
",tests/test_energy_utils.py,TestEnergyUtils,1,7
survived,"def test_simulate_export_json() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--pop-size"",
                    ""1"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""json"",
                ],
            )
    assert res.exit_code == 0
    assert res.output.startswith(""["")
",tests/test_cli.py,,0,6
survived,"def test_formulate_query():
    mgr = ToolResearchManager(web_search_tool=DummyTool(), enabled=True)
    q = mgr.formulate_query(""foo"", ""does bar"")
    assert ""foo"" in q and ""bar"" in q
",tests/unit/test_tool_research_manager.py,,1,7
survived,"def test_search_caching():
    tool = DummyTool()
    mgr = ToolResearchManager(web_search_tool=tool, enabled=True)
    r1 = mgr.research(""foo"", ""bar"")
    r2 = mgr.research(""foo"", ""bar"")
    assert r1 == [""result line 1"", ""result line 2""]
    assert r2 == r1
    assert len(tool.calls) == 1
",tests/unit/test_tool_research_manager.py,,1,8
survived,"    def __init__(
        self,
        web_search_tool: Optional[Callable[[str], str]] = None,
        enabled: bool = True,
        max_results: int = 3,
    ) -> None:
        self.web_search_tool = web_search_tool or WebSearchTool()
        self.enabled = enabled
        self.max_results = max_results
        self.cache: Dict[str, List[str]] = {}
",src/meta_agent/research_manager.py,ToolResearchManager,1,7
survived,"    async def run(self, initial_prompt: Any) -> Any:
        result = initial_prompt
        for step in self.steps:
            user_id = step.user_id or self.default_user_id
            session_id = step.session_id or self.default_session_id
            llm = step.llm or self.default_llm
            sdk_context = step.sdk_context or self.sdk_context

            if step.mode == StepMode.SEQUENTIAL:
                result = await self._execute_runner(step.runner, result, user_id, session_id, llm, sdk_context)

            elif step.mode == StepMode.PARALLEL:
                runners: Iterable[Any] = step.runner if isinstance(step.runner, Iterable) else [step.runner]
                results = await asyncio.gather(
                    *[self._execute_runner(r, result, user_id, session_id, llm, sdk_context) for r in runners]
                )
                result = results

            elif step.mode == StepMode.CONDITIONAL:
                if step.condition is None or step.condition(result):
                    result = await self._execute_runner(step.runner, result, user_id, session_id, llm, sdk_context)

            elif step.mode == StepMode.LOOP:
                iterations = 0
                max_iter = step.max_iterations or 1
                while True:
                    result = await self._execute_runner(step.runner, result, user_id, session_id, llm, sdk_context)
                    iterations += 1
                    if step.condition and step.condition(result):
                        break
                    if iterations >= max_iter:
                        break
            else:
                raise ValueError(f""Unsupported step mode {step.mode}"")
        return result",swarmzero/workflow.py,Workflow,1,7
survived,"    async def second(prompt, **kwargs):
        return ""ran""
",tests/test_workflow.py,,1,7
survived,"def test_offline_no_wheelhouse(monkeypatch: pytest.MonkeyPatch, capsys: pytest.CaptureFixture[str]) -> None:
    """"""Fail fast when offline without a wheelhouse.""""""
    _no_missing(monkeypatch)
    monkeypatch.setattr(check_env, ""has_network"", lambda: False)
    monkeypatch.delenv(""WHEELHOUSE"", raising=False)
    rc = check_env.main([""--auto-install""])
    out = capsys.readouterr().out
    assert rc == 1
    assert ""--wheelhouse <dir>"" in out
",tests/test_check_env_network.py,,1,7
survived,"def test_scatter_add():
    B, S, V = Axis(""batch"", 2), Axis(""seq"", 3), Axis(""vocab"", 5)
    x = hax.zeros((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    ones = hax.ones((B, S))
    y = x.at[{V: idx}].add(ones)
    ref = jnp.zeros((2, 3, 5)).at[jnp.arange(2)[:, None], jnp.arange(3)[None, :], idx.array].add(1.0)
    assert jnp.array_equal(y.array, ref)
",tests/test_scatter_gather.py,,1,7
survived,"def test_noncontig_selectors():
    B, X, Z, Y = Axis(""batch"", 2), Axis(""x"", 4), Axis(""z"", 6), Axis(""y"", 5)
    a = hax.arange((B, X, Z, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a[""x"", ix, ""y"", iy]
    assert out.axes == (B, Z)
    ref = a.array[jnp.arange(2), ix.array, :, iy.array]
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,,1,7
survived,"    async def init_async(self) -> None:  # pragma: no cover - optional hook
        """"""Launch background tasks once the event loop is running.""""""
        return None
",alpha_factory_v1/backend/agents/base.py,AgentBase,0,7
survived,"    def append_token(self, token_id: int) -> None:
        self.token_ids.append(token_id)",src/levanter/inference/sequence.py,Sequence,1,6
survived,"    def postprocess(self, seqs: List[Sequence], token_ids: List[int]) -> None:
        for seq, token_id in zip(seqs, token_ids):
            seq.append_token(int(token_id))
            if (
                (not seq.sampling_params.ignore_eos and token_id == self.eos)
                or seq.num_completion_tokens >= seq.sampling_params.max_tokens
            ):
                seq.status = SequenceStatus.FINISHED
",src/levanter/inference/scheduler.py,Scheduler,1,7
survived,"def listconflicts(goal_list):
    """"""
    list all possible start lists that will have at least
    one linear conflict.

    Possible goal tile configurations

    g g g g
    g g g x
    g g x g
    g x g g
    x g g g
    g g x x
    g x g x
    g x x g
    x g g x
    x g x g
    x x g g

    """"""

    all_tiles = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

    non_goal_tiles = []

    for t in all_tiles:
        if t not in goal_list:
            non_goal_tiles.append(t)

    combinations = lcmap()

    # g g g g

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in tile_list2:
            tile_list3 = tile_list2[:]
            tile_list3.remove(j)
            for k in tile_list3:
                tile_list4 = tile_list3[:]
                tile_list4.remove(k)
                for l in tile_list4:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # g g g x

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in tile_list2:
            tile_list3 = tile_list2[:]
            tile_list3.remove(j)
            for k in tile_list3:
                for l in non_goal_tiles:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # g g x g

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in tile_list2:
            tile_list3 = tile_list2[:]
            tile_list3.remove(j)
            for k in non_goal_tiles:
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd
    # g x g g

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in non_goal_tiles:
            for k in tile_list2:
                tile_list3 = tile_list2[:]
                tile_list3.remove(k)
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # x g g g

    for i in non_goal_tiles:
        for j in goal_list:
            tile_list2 = goal_list[:]
            tile_list2.remove(j)
            for k in tile_list2:
                tile_list3 = tile_list2[:]
                tile_list3.remove(k)
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # g g x x

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in tile_list2:
            tile_list3 = tile_list2[:]
            tile_list3.remove(j)
            for k in non_goal_tiles:
                tile_list4 = non_goal_tiles[:]
                tile_list4.remove(k)
                for l in tile_list4:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # g x g x

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in non_goal_tiles:
            tile_list3 = non_goal_tiles[:]
            tile_list3.remove(j)
            for k in tile_list2:
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # g x x g

    for i in goal_list:
        tile_list2 = goal_list[:]
        tile_list2.remove(i)
        for j in non_goal_tiles:
            tile_list3 = non_goal_tiles[:]
            tile_list3.remove(j)
            for k in tile_list2:
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # x g g x

    for i in non_goal_tiles:
        tile_list2 = non_goal_tiles[:]
        tile_list2.remove(i)
        for j in goal_list:
            tile_list3 = goal_list[:]
            tile_list3.remove(j)
            for k in tile_list3:
                for l in tile_list2:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # x g x g

    for i in non_goal_tiles:
        tile_list2 = non_goal_tiles[:]
        tile_list2.remove(i)
        for j in goal_list:
            tile_list3 = goal_list[:]
            tile_list3.remove(j)
            for k in tile_list3:
                for l in tile_list2:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    # x x g g

    for i in non_goal_tiles:
        tile_list2 = non_goal_tiles[:]
        tile_list2.remove(i)
        for j in tile_list2:
            for k in goal_list:
                tile_list3 = goal_list[:]
                tile_list3.remove(k)
                for l in tile_list3:
                    start_list = (i, j, k, l)
                    conflictadd = linear_conflicts(start_list,goal_list)
                    if conflictadd > 0:
                        combinations[start_list]=conflictadd

    return combinations
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,,0,7
survived,"def visualize_shardings(tree) -> None:
    """"""Print the sharding for each array-like leaf in ``tree``.

    Both :class:`NamedArray` and regular JAX arrays are supported. NamedArrays
    will show the mapping from logical axis names to physical axes. Plain arrays
    will fall back to :func:`jax.debug.visualize_sharding`.
    """"""

    import haliax.tree_util as htu

    def _show(x):
        if isinstance(x, NamedArray):
            arr = x.array
            axes = x.axes
        else:
            arr = x
            axes = None

        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass

        jax.debug.inspect_array_sharding(arr, callback=cb)
        return x

    htu.tree_map(_show, tree, is_leaf=is_jax_or_hax_array_like)",src/haliax/debug.py,,1,7
survived,"def test_visualize_shardings_runs(capsys):
    mesh = jax.sharding.Mesh(
        np.array(jax.devices()).reshape(-1, 1, 1),
        (ResourceAxis.DATA, ResourceAxis.MODEL, ResourceAxis.REPLICA),
    )
    with axis_mapping(resource_map), mesh:
        arr = hax.ones((Dim1, Dim2, Dim3))
        visualize_shardings(arr)

    out = capsys.readouterr().out
    assert ""dim1"" in out and ""dim2"" in out and ""dim3"" in out
",tests/test_visualize_sharding.py,,1,7
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        return await self.tools.list_agents()
",alpha_factory_v1/demos/alpha_asi_world_model/openai_agents_bridge.py,InspectorAgent,1,7
survived,"    def test_summary_offline(self) -> None:
        text = summarise_with_agent(0.8, agents=10, rounds=100, delta=0.9, stake=1.0)
        self.assertIsInstance(text, str)
        self.assertIn(""mean cooperation"", text)
",tests/test_governance_sim.py,TestGovernanceSim,1,7
survived,"    def test_notebook_valid(self) -> None:
        nb_path = Path(""alpha_factory_v1/demos/alpha_agi_business_v1/colab_alpha_agi_business_v1_demo.ipynb"")
        self.assertTrue(nb_path.exists(), ""Notebook missing"")
        data = json.loads(nb_path.read_text(encoding=""utf-8""))
        self.assertIn(""cells"", data)
        self.assertIn(""nbformat"", data)
        self.assertGreaterEqual(data.get(""nbformat"", 0), 4)
",tests/test_business_notebook.py,TestBusinessNotebook,1,7
survived,"    def test_list_option(self) -> None:
        result = subprocess.run([sys.executable, STUB, '--list'], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        data = json.loads(result.stdout)
        self.assertIsInstance(data, list)
        self.assertGreaterEqual(len(data), 5)
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub,1,7
survived,"async def best_alpha() -> dict:
    return {
        ""architecture"": EVOLVER.best_architecture,
        ""fitness"": EVOLVER.best_fitness,
    }
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,,1,6
survived,"    async def step(self) -> None:
        """"""Post a short market insight using OpenAI Agents if available.""""""
        insight = ""LLM unavailable""
        try:
            from openai_agents import OpenAIAgent

            agent = OpenAIAgent(
                model=os.getenv(""MODEL_NAME"", ""gpt-4o-mini""),
                api_key=os.getenv(""OPENAI_API_KEY""),
                base_url=None
                if os.getenv(""OPENAI_API_KEY"")
                else ""http://ollama:11434/v1"",
            )
            insight = await agent(""One sentence on today's market outlook"")
        except Exception as exc:  # noqa: BLE001
            logging.getLogger(__name__).warning(""LLM fallback: %s"", exc)
        await self.publish(""alpha.insight"", {""insight"": insight})
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,LLMCommentAgent,1,7
survived,"def _rest_positions() -> Any:
    """"""Return positions via the REST fallback.""""""
    return requests.get(f""{BASE}/api/finance/positions"", timeout=3).json()
",alpha_factory_v1/demos/finance_alpha/agent_control.py,,1,6
survived,"async def reset_endpoint(background_tasks: BackgroundTasks):
    background_tasks.add_task(service.reset)
    return {""msg"": ""reset scheduled""}
",alpha_factory_v1/demos/aiga_meta_evolution/agent_aiga_entrypoint.py,,1,7
survived,"def triple(x: int) -> int:
    return x * 3
",tests/machine/x/python/pure_fold.py,,1,7
survived,"def add(a: int, b: int) -> int:
    return a + b
",tests/machine/x/python/partial_application.py,,1,9
survived,"    def get_state(self):
        """"""Return position and velocity as Python lists for easy C++ access.""""""
        return self.position.cpu().tolist(), self.velocity.cpu().tolist()",pytorch_solver.py,PytorchSolver,1,7
survived,"    def generate_document(
        self,
        word_dists,
        n_topics,
        vocab_size,
        document_length,
    ):

        # sample topic proportions with uniform dirichlet parameter alpha
        # of length n_topics
        theta = np.random.mtrand.dirichlet([self.alpha] * n_topics)

        # for every word in the vocab for this document
        d = np.zeros(vocab_size)
        for n in range(document_length):

            # sample a new topic index
            k = np.random.multinomial(1, theta).argmax()

            # sample a new word from the word distribution of topic k
            w = np.random.multinomial(1, word_dists[k, :]).argmax()

            # increase the occurrence of word w in document d
            d[w] += 1

        return d
",examples/synthetic_data.py,HldaDataGenerator,1,7
survived,"def sync(
    neo4j_session: neo4j.Session,
    boto3_session: boto3.session.Session,
    regions: List[str],
    current_aws_account_id: str,
    update_tag: int,
    common_job_parameters: Dict,
) -> None:
    for region in regions:
        logger.info(
            f""Syncing ACM certificates for region {region} in account {current_aws_account_id}.""
        )
        certs = get_acm_certificates(boto3_session, region)
        transformed = transform_acm_certificates(certs, region)
        load_acm_certificates(
            neo4j_session,
            transformed,
            region,
            current_aws_account_id,
            update_tag,
        )

    cleanup_acm_certificates(neo4j_session, common_job_parameters)

    merge_module_sync_metadata(
        neo4j_session,
        group_type=""AWSAccount"",
        group_id=current_aws_account_id,
        synced_type=""ACMCertificate"",
        update_tag=update_tag,
        stat_handler=stat_handler,
    )",cartography/intel/aws/acm.py,,1,7
survived,"    async def start_consumer(self) -> None:
        if self._queues is None or self._consumer_task is not None:
            return
        self._consumer_task = asyncio.create_task(self._drain_loop())
",alpha_factory_v1/backend/agent_runner.py,EventBus,1,7
survived,"            def body(page_idx, state):
                page_indices, page_owners = state
                free_page_idx = hax.argmin(page_owners, ""page"")
                page_owners = page_owners.at[""page"", free_page_idx].set(seq_id)
                page_indices = page_indices.at[""seq"", seq_id, ""page"", page_idx].set(free_page_idx)
                return page_indices, page_owners
",src/levanter/layers/page_table.py,PageTable,1,6
survived,"    def current_num_seqs(self) -> int:
        return hax.sum(self.seq_lens >= 0).scalar()
",src/levanter/layers/page_table.py,PageTable,1,7
survived,"    def __post_init__(self):
        assert isinstance(self.num_seqs, jnp.ndarray), ""num_seqs must be a JAX ndarray""",src/levanter/layers/page_table.py,PageBatchInfo,1,7
survived,"    def __init__(self, adapters: Dict[str, ModelAdapter], default_model: str) -> None:
        if not adapters:
            raise ValueError(""At least one model adapter must be provided"")
        if default_model not in adapters:
            raise ValueError(""Default model must exist in adapters"")
        self.adapters = adapters
        self.default_model = default_model
        self.input_guardrails: List[Callable[[str], Awaitable[None]]] = []
        self.output_guardrails: List[Callable[[str], Awaitable[None]]] = []
",src/meta_agent/services/guardrail_router.py,GuardrailModelRouter,1,8
survived,"    def __init__(self, name: str | None = None, *_, **__):
        self.name = name or ""StubAgent""
",src/meta_agent/agents/guardrail_designer_agent.py,AgentBase,1,7
survived,"    def _check_stop_grad(self, name: str):
        """"""Verify gradients ignore values detached with ``stop``.""""""
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x):
            return b.sum(b.mul(b.stop(x), x))

        g = b.grad(f)
        x = b.array([2.0, 3.0], requires_grad=True)
        grad = to_numpy(g(x))
        np.testing.assert_allclose(np.array(grad), np.array([2.0, 3.0]))
",tests/test_autograd.py,TestAutograd,1,6
survived,"    def test_mixed_args_grad_torch(self):
        self._check_mixed_args_grad(""torch"")
",tests/test_autograd.py,TestAutograd,1,6
survived,"    def test_vector_elemwise_grad_torch(self):
        self._check_vector_elemwise_grad(""torch"")
",tests/test_autograd.py,TestAutograd,1,6
survived,"def sha384(path: Path) -> str:
    digest = hashlib.sha384(path.read_bytes()).digest()
    return ""sha384-"" + base64.b64encode(digest).decode()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,6
survived,"        def __init__(self, *a: object, port: int = 5001, **_k: object) -> None:
            captured[""port""] = port
",tests/test_alpha_opportunity_stub.py,DummyRuntime,1,6
deleted,"    def __init__(self):
        self.carFingerprint = ""stub""
        self.brand = ""toyota""
        self.lateralTuning = SimpleNamespace()
        self.lateralTuning.torque = SimpleNamespace(friction=0.0, latAccelFactor=1.0)
        self.lateralTuning.which = lambda: ""torque""
",selfdrive/locationd/test/test_torqued.py,CPStub,1,7
survived,"def test_session_id_deterministic() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""navigator.sendBeacon=(...a)=>{window.beacon=a;return true;}""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.click(""text=Share"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        first = page.evaluate(""window.beacon[1]"")
        page.reload()
        page.evaluate(
            ""navigator.sendBeacon=(...a)=>{window.beacon=a;return true;}""
        )
        page.wait_for_selector(""#controls"")
        page.click(""text=Share"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        second = page.evaluate(""window.beacon[1]"")
        import json

        assert json.loads(first)[""session""] == json.loads(second)[""session""]
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,,1,6
survived,"    def __init__(self, bus: messaging.A2ABus, ledger: _Ledger) -> None:
        super().__init__(""dummy"", bus, ledger)
        self.count = 0
",tests/test_agent_runner.py,DummyBaseAgent,1,7
survived,"    def fake_run(cmd, cwd):
        return 1, ""patch failed""
",tests/test_patcher_core_additional.py,,1,6
survived,"    def scan_via(self, fn: Callable[..., tuple[CarryT, OutputT_co]]) -> Callable[[CarryT], tuple[CarryT, OutputT_co]]:
        ...
",src/haliax/nn/scan.py,BlockFoldable,1,6
survived,"    def agents_status(*_a: object, **_kw: object) -> None:
        raise click.ClickException(""Insight demo not installed"")
",alpha_factory_v1/core/interface/cli.py,,0,6
survived,"    def tearDown(self) -> None:
        llm._cache_mem = self.orig_cache
        llm._CACHE_SIZE = self.orig_size
        llm._DB = self.orig_db
",tests/test_llm_cache.py,TestLLMCacheLRU,1,7
survived,"def test_dgm_import(tmp_path: Path) -> None:
    log_dir = Path(""tests/fixtures/dgm_logs"")
    db_path = tmp_path / ""archive.db""
    count = dgm_import.import_logs(log_dir, db_path=db_path)
    assert count == 80

    db = ArchiveDB(db_path)
    history = list(db.history(""h79""))
    assert len(history) == 80",tests/test_dgm_import.py,,1,7
survived,"def _run(rate):
    random.seed(123)
    arch = InMemoryArchive()
    asyncio.run(arch.accept(Candidate(0.0, fitness=0.0, novelty=1.0)))
    asyncio.run(
        evolve(
            _mutate,
            _evaluate,
            arch,
            max_cost=0.1,
            backtrack_rate=rate,
        )
    )
    return [c.genome for c in arch.all()]
",tests/test_backtrack_boost.py,,1,6
survived,"def _diversity(values):
    if len(values) < 2:
        return 0.0
    d = 0.0
    c = 0
    for i in range(len(values)):
        for j in range(i + 1, len(values)):
            d += abs(values[i] - values[j])
            c += 1
    return d / c
",tests/test_backtrack_boost.py,,1,6
survived,"    def __init__(self, db_path: str | Path, window: int = 10) -> None:
        self.db = ArchiveDB(db_path)
        self.window = window
        self.history: deque[float] = deque(maxlen=window)
        self._dataset = self.db.get_state(""dataset"", self.MINI)
        self._log = logging.getLogger(__name__)
        self._log.info(""current dataset: %s"", self._dataset)
",src/eval/fitness.py,CurriculumSwitcher,1,7
survived,"    def dataset(self) -> str:
        """"""Return the active dataset name.""""""

        return self._dataset
",src/eval/fitness.py,CurriculumSwitcher,1,8
survived,"            async def run() -> None:
                async with messaging.A2ABus(cfg) as bus:
                    env = types.SimpleNamespace(sender=""a"", recipient=""b"", payload={}, ts=0.0)

                    async def _send() -> None:
                        bus.publish(""b"", env)
                        await asyncio.sleep(0)

                    await _send()
",tests/test_message_bus.py,TestMessageBus,1,6
survived,"    def test_index_row_wildcard(self):
        """"""Select entire first row using wildcard""""""
        self.assert_eval_cmp('[[1 2 3] [4 5 6]]:@[0 []]', '[1 2 3]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior,1,7
survived,"    def test_reshape_wildcard_front(self):
        """"""Wildcard as the first dimension""""""
        self.assert_eval_cmp('[[] 2]:^[1 2 3 4 5 6]', '[[1 2] [3 4] [5 6]]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior,1,6
survived,"    def rebuild(self) -> None:
        """"""Rebuild the index from all registered templates.""""""
        self._index = []
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                version = version_info[""version""]
                path = self.registry.templates_dir / version_info[""path""]
                metadata_path = path.parent / METADATA_FILE_NAME
                try:
                    content = path.read_text(encoding=""utf-8"")
                except OSError:  # pragma: no cover - file missing
                    continue
                checksum = sha256(content.encode(""utf-8"")).hexdigest()
                try:
                    with open(metadata_path, ""r"", encoding=""utf-8"") as f:
                        metadata = json.load(f)
                except (OSError, json.JSONDecodeError):
                    metadata = {}
                self._index.append(
                    {
                        ""slug"": slug,
                        ""version"": version,
                        ""path"": str(path.relative_to(self.registry.templates_dir)),
                        ""checksum"": checksum,
                        ""metadata"": metadata,
                        ""content"": content,
                    }
                )
        self.save()
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_index.py,,1,6
survived,"    def needs_rebuild(self) -> bool:
        """"""Return True if stored checksums differ from source files.""""""
        if not self.index_path.exists():
            return True
        if not self._index:
            self.load()
        for item in self._index:
            template_path = self.registry.templates_dir / item[""path""]
            try:
                content = template_path.read_text(encoding=""utf-8"")
            except OSError:  # file removed
                return True
            checksum = sha256(content.encode(""utf-8"")).hexdigest()
            if checksum != item.get(""checksum""):
                return True
        # check for new templates not in index
        seen = {(i[""slug""], i[""version""]) for i in self._index}
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                if (slug, version_info[""version""]) not in seen:
                    return True
        return False
",src/meta_agent/template_index.py,TemplateIndex,1,8
survived,"def server() -> Iterator[str]:
    port = _free_port()
    config = uvicorn.Config(evolution_worker.app, host=""127.0.0.1"", port=port, log_level=""warning"")
    server = uvicorn.Server(config)
    thread = threading.Thread(target=server.run, daemon=True)
    thread.start()
    for _ in range(50):
        if server.started:
            break
        time.sleep(0.1)
    yield f""http://127.0.0.1:{port}""
    server.should_exit = True
    thread.join(timeout=5)
",tests/test_evolution_worker.py,,1,6
survived,"async def _prepare() -> None:
    STORAGE_PATH.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evolution_worker.py,,1,7
survived,"    def best_architecture(self) -> str:
        return self.best_genome.to_json() if self.best_genome else """"",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"    def _save(self):
        data = {
            ""gen"": self.gen,
            ""pop"": [g.to_json() for g in self.population],
            ""hist"": self.history,
            ""arc"": [a.tolist() for a in self._archive[-256:]],
            ""seed"": self.rng.random(),
            ""sha"": self.population_sha(),
            ""best_fitness"": self._best_fitness,
            ""best_genome"": self.best_genome.to_json() if self.best_genome else None,
            ""ts"": datetime.now(timezone.utc).isoformat()
        }
        p = CHKPT_DIR / f""gen_{self.gen:04d}.json.tmp""
        p.write_text(json.dumps(data)); p.replace(p.with_suffix(""""))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"def _wrap_mcp(agent: str, payload: Any) -> Dict[str, Any]:
    return {
        ""mcp_version"": ""0.1"",
        ""agent"": agent,
        ""ts"": _now_iso(),
        ""digest"": _sha256(json.dumps(payload, separators=("","", "":""))),
        ""payload"": payload,
    }
",alpha_factory_v1/backend/agents/drug_design_agent.py,,1,7
survived,"def banner(msg: str, color: str = 'GREEN') -> None:
    color_code = COLORS.get(color.upper(), '')
    reset = COLORS['RESET']
    print(f""{color_code}{msg}{reset}"")
",alpha_factory_v1/scripts/preflight.py,,1,7
survived,"def test_rate_limiter_throttles(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""API_RATE_LIMIT"", ""1"")
    from src.interface import api_server as api

    api = importlib.reload(api)

    limiter = api.SimpleRateLimiter(api.app, limit=1, window=0.1)

    resp1 = asyncio.run(limiter.dispatch(_make_request(""3.3.3.3""), _call_next))
    assert resp1.status_code == 200
    resp2 = asyncio.run(limiter.dispatch(_make_request(""3.3.3.3""), _call_next))
    assert resp2.status_code == 429
    time.sleep(0.11)
    resp3 = asyncio.run(limiter.dispatch(_make_request(""3.3.3.3""), _call_next))
    assert resp3.status_code == 200",tests/test_rate_limiter_eviction.py,,1,7
survived,"def test_safari_pyodide_fallback() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    try:
        with sync_playwright() as p:
            browser = p.webkit.launch()
            context = browser.new_context(
                user_agent=(
                    ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ""
                    ""AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15""
                )
            )
            page = context.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_selector(""#toast.show"")
            assert ""Pyodide unavailable; using JS only"" in page.inner_text(""#toast"")
            assert page.evaluate(""typeof d3 !== 'undefined'"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_safari_pyodide.py,,0,7
survived,"def test_bundle_size_under_limit() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    app_js = browser_dir / ""dist"" / ""app.js""
    data = app_js.read_bytes()
    compressed = gzip.compress(data)
    assert len(compressed) <= 6 * 1024 * 1024",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_bundle_size.py,,1,7
survived,"    def test_offline_fallback_base_url(self) -> None:
        """"""OpenAI bridge should use OLLAMA_BASE_URL when api key is empty.""""""

        def fake_openai_agent(*_a, **kwargs):
            return types.SimpleNamespace(base_url=kwargs.get(""base_url""))

        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = object
        stub.OpenAIAgent = fake_openai_agent

        env_stub = types.ModuleType(""curriculum_env"")
        env_stub.CurriculumEnv = object

        evo_stub = types.ModuleType(""meta_evolver"")
        evo_stub.MetaEvolver = object

        with patch.dict(
            sys.modules,
            {
                ""openai_agents"": stub,
                ""alpha_factory_v1.demos.aiga_meta_evolution.curriculum_env"": env_stub,
                ""alpha_factory_v1.demos.aiga_meta_evolution.meta_evolver"": evo_stub,
            },
        ), patch.dict(
            os.environ,
            {""OPENAI_API_KEY"": """", ""OLLAMA_BASE_URL"": ""http://example.com""},
            clear=False,
        ):
            mod = importlib.reload(
                importlib.import_module(
                    ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge""
                )
            )

            self.assertEqual(mod.LLM.base_url, ""http://example.com"")
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime,1,7
survived,"def download_hf_gpt2(dest: Path | str = ""models/gpt2"", attempts: int = 3) -> None:
    dest_dir = Path(dest)
    base = _base_url()
    last_exc: Exception | None = None
    for name in _FILES:
        url = f""{base}/{name}""
        target = dest_dir / name
        if target.exists():
            print(f""{target} already exists, skipping"")
            continue
        for i in range(1, attempts + 1):
            try:
                print(f""Downloading {url} to {target} (attempt {i})"")
                _download(url, target)
                _verify(target)
                break
            except Exception as exc:  # noqa: PERF203
                last_exc = exc
                if i < attempts:
                    print(f""Attempt {i} failed: {exc}, retrying..."")
                else:
                    print(f""ERROR: could not download {url}: {exc}"")
                    if target.exists():
                        try:
                            target.unlink()
                        except Exception:
                            pass
    if last_exc:
        raise last_exc
",scripts/download_hf_gpt2.py,,1,6
survived,"def _verify(dest: Path) -> None:
    expected = CHECKSUMS.get(dest.name)
    if expected:
        digest = hashlib.sha256(dest.read_bytes()).hexdigest()
        if digest != expected:
            raise RuntimeError(f""Checksum mismatch for {dest.name}"")
",scripts/download_hf_gpt2.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/gamma-function.py,,1,6
survived,"def test_ppm(h, f):
    """"""Verify if the image is a PPM (portable pixmap).""""""
    if len(h) >= 3 and \
        h[0] == ord(b'P') and h[1] in b'36' and h[2] in b' \t\n\r':
        return 'ppm'
",metaflow/_vendor/imghdr/__init__.py,,1,7
survived,"def set_trace_provider(provider: TraceProvider) -> None:
    """"""Set the global trace provider used by tracing utilities.""""""
    global GLOBAL_TRACE_PROVIDER
    GLOBAL_TRACE_PROVIDER = provider",src/agents/tracing/setup.py,,1,7
survived,"    def __init__(self, rng: random.Random | None = None) -> None:
        self.rng = rng or random.Random()
        self.synonyms = {""improve"": ""enhance"", ""quick"": ""fast"", ""test"": ""trial""}
",src/simulation/mats_ops.py,PromptRewrite,1,7
survived,"    def test_gaussian_param_bounds_and_diversity(self) -> None:
        rng = random.Random(123)
        pop = [mats.Individual([rng.uniform(-0.05, 0.05) for _ in range(2)]) for _ in range(10)]
        base_div = _diversity(pop)
        op = GaussianParam(std=0.3, rng=rng)
        mutated = [mats.Individual(op(ind.genome)) for ind in pop]
        after_div = _diversity(mutated)
        for ind in mutated:
            for gene in ind.genome:
                self.assertGreaterEqual(gene, -1.0)
                self.assertLessEqual(gene, 1.0)
        self.assertGreater(after_div, base_div * 1.3)
",tests/test_mats_ops.py,TestMatsOps,1,7
survived,"    def __call__(self, code: str) -> str:
        suffix = ""# patched""
        if not code.endswith(""\n""):
            code += ""\n""
        return code + suffix + ""\n""",src/simulation/mats_ops.py,CodePatch,1,7
survived,"    def fake_post(url: str, json=None, timeout=None):
        called[""url""] = url
        called[""json""] = json
        return DummyResp()
",tests/test_llm_client_offline.py,,1,6
survived,"    def test_valid_signature(self) -> None:
        exit_code = self._run_main(self.wheel_path)
        self.assertEqual(exit_code, 0)
",tests/test_verify_wheel_sig.py,VerifyWheelSigTests,1,7
survived,"        def __call__(self, *args, **kwargs):  # noqa: D401
            raise ModuleNotFoundError(
                ""The OpenAI Agents SDK is required for this operation. ""
                ""Please install it with:  pip install openai-agents""
            )
",alpha_factory_v1/backend/__init__.py,_MissingSDK,1,7
survived,"def load_sectors(path: str | os.PathLike[str], *, energy: float = 1.0, entropy: float = 1.0) -> list[Sector]:
    """"""Load sector definitions from a JSON file.

    The file may contain a list of strings representing sector names or a list
    of objects with ``name`` and optional ``energy``, ``entropy`` and ``growth``
    fields. The ``energy`` and ``entropy`` arguments provide defaults when these
    values are omitted.
    """"""
    with open(path, ""r"", encoding=""utf-8"") as f:
        data = json.load(f)

    sectors: list[Sector] = []
    for entry in data:
        if isinstance(entry, str):
            sectors.append(Sector(entry, energy, entropy))
        elif isinstance(entry, dict):
            sectors.append(
                Sector(
                    entry.get(""name"", """"),
                    float(entry.get(""energy"", energy)),
                    float(entry.get(""entropy"", entropy)),
                    float(entry.get(""growth"", 0.05)),
                    bool(entry.get(""disrupted"", False)),
                )
            )
        else:
            raise ValueError(f""Invalid sector entry: {entry!r}"")
    return sectors",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/sector.py,,1,7
survived,"def test_load_sectors_names(tmp_path: Path) -> None:
    path = tmp_path / ""s.json""
    path.write_text(json.dumps([""a"", ""b""]))
    secs = sector.load_sectors(path)
    assert [s.name for s in secs] == [""a"", ""b""]
",tests/test_sector_loader.py,,1,7
survived,"def cytomat_rack_29mm_16(name: str):
  return _cytomat_rack(name=name, site_height=29, num_sites=16, model=""cytomat_rack_29mm_16"")
",pylabrobot/storage/cytomat/racks.py,,1,6
survived,"    def create_pretty_table(header, *columns) -> str:
      col_widths = [
        max(len(str(item)) for item in [header[i]] + list(columns[i])) for i in range(len(header))
      ]

      def format_row(row, border=""|"") -> str:
        return (
          f""{border} ""
          + "" | "".join(f""{str(row[i]).ljust(col_widths[i])}"" for i in range(len(row)))
          + f"" {border}""
        )

      def separator_line(cross: str = ""+"", line: str = ""-"") -> str:
        return cross + cross.join(line * (width + 2) for width in col_widths) + cross

      table = []
      table.append(separator_line())  # Top border
      table.append(format_row(header))
      table.append(separator_line())  # Header separator
      for row in zip(*columns):
        table.append(format_row(row))
      table.append(separator_line())  # Bottom border
      return ""\n"".join(table)
",pylabrobot/storage/incubator.py,Incubator,1,7
survived,"  async def stop_shaking(self):
    print(""Stopping shaking"")",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,1,6
survived,"def cytomat_rack_28mm_17(name: str):
  return _cytomat_rack(name=name, site_height=28, num_sites=17, model=""cytomat_rack_28mm_17"")
",pylabrobot/storage/cytomat/racks.py,,1,6
survived,"  async def start_shaking(self, frequency: float):
    """"""Start shaking the incubator at the given frequency in Hz.""""""
    pass
",pylabrobot/storage/backend.py,IncubatorBackend,0,7
survived,"  async def get_temperature(self) -> float:
    raise NotImplementedError(""Temperature query not implemented yet"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend,0,7
survived,"  def test_serialization(self):
    i = Incubator(
      name=""test_tc"",
      size_x=10,
      size_y=10,
      size_z=10,
      backend=IncubatorChatterboxBackend(),
      loading_tray_location=Coordinate(0, 0, 0),
      racks=[],
    )

    serialized = i.serialize()
    deserialized = Incubator.deserialize(serialized)
    self.assertEqual(i, deserialized)",pylabrobot/storage/incubator_tests.py,IncubatorTests,1,7
survived,"  async def fetch_plate_to_loading_tray(self, plate: Plate):
    print(f""Fetching plate {plate} to loading tray"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,0,7
survived,"  async def wait_for_transfer_station(self, occupied: bool = False):
    # send the command, but don't wait when we are in chatting mode.
    _ = await self.get_overview_register()
",pylabrobot/storage/cytomat/cytomat.py,CytomatChatterbox,0,7
survived,"  async def wait_for_task_completion(self, timeout=60) -> OverviewRegisterState:
    """"""
    Wait for the cytomat to finish the current task. This is done by checking the overview register
    until the busy bit is not set. If the cytomat is busy for too long, a TimeoutError is raised.
    If the error bit is set in the overview register, the error register is read and the corresponding
    error is raised.
    """"""
    start = time.time()
    while True:
      overview_register = await self.get_overview_register()
      if not overview_register.busy_bit_set:
        # only check for errors once the cytomat is done, so that the user has the chance to
        # handle the error and proceed if desired.
        if overview_register.error_register_set:
          error_register = await self.get_error_register()
          await self.reset_error_register()
          raise error_register_map[error_register]
        return overview_register
      await asyncio.sleep(1)
      if time.time() - start > timeout:
        raise TimeoutError(""Cytomat did not complete task in time"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,8
survived,"  async def setup(self) -> Serial:
    """"""
    1. Open serial port (9600 8E1, RTS/CTS) via the Serial wrapper.
    2. Send >200 ms break, wait 150 ms, flush buffers.
    3. Handshake: CR → wait for CC<CR><LF>
    4. Activate handling: ST 1801 → expect OK<CR><LF>
    5. Poll ready-flag: RD 1915 → wait for ""1""<CR><LF>
    """"""
    try:
      await self.io.setup()
    except serial.SerialException as e:
      raise RuntimeError(f""Could not open {self.io.port}: {e}"")

    await self.io.send_break(duration=0.2)  # >100 ms required
    await asyncio.sleep(0.15)
    await self.io.reset_input_buffer()
    await self.io.reset_output_buffer()

    await self.io.write(b""CR\r"")
    deadline = time.time() + self.init_timeout
    while time.time() < deadline:
      resp = await self.io.readline()  # reads through LF
      if resp.strip() == b""CC"":
        break
    else:
      await self.io.stop()
      raise TimeoutError(f""No CC response from PLC within {self.init_timeout} seconds"")

    await self.io.write(b""ST 1801\r"")
    resp = await self.io.readline()
    if resp.strip() != b""OK"":
      await self.io.stop()
      raise RuntimeError(f""Unexpected reply to ST 1801: {resp!r}"")

    deadline = time.time() + self.start_timeout
    while time.time() < deadline:
      await self.io.write(b""RD 1915\r"")
      flag = await self.io.readline()
      if flag.strip() == b""1"":
        return self.io
      await asyncio.sleep(self.poll_interval)

    await self.io.stop()
    raise TimeoutError(f""PLC did not signal ready within {self.start_timeout} seconds"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend,1,8
survived,"  def from_resp(self, resp) -> ""OverviewRegisterState"":
    binary_value = hex_to_binary(resp)
    return OverviewRegisterState(
      **{member.name.lower(): binary_value[member.value] == ""1"" for member in OverviewRegister}
    )
",pylabrobot/storage/cytomat/schemas.py,OverviewRegisterState,1,7
survived,"def test_dependency_graph(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    creator = TemplateCreator(reg)
    creator.create(_meta(""a""), ""A"")
    creator.create(_meta(""b""), ""{% extends 'a' %}B"")
    creator.create(_meta(""c""), ""{% include 'b' %}C"")

    mixer = TemplateMixer(reg)
    graph = mixer.dependency_graph(""c"")
    assert graph[""c""] == [""b""]
    assert graph[""b""] == [""a""]
    assert graph[""a""] == []",tests/test_template_mixer.py,,1,7
survived,"    def render(
        self,
        slug: str,
        *,
        version: str = ""latest"",
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """"""Render a template and all of its dependencies.""""""
        name = slug if version == ""latest"" else f""{slug}@{version}""
        template = self.env.get_template(name)
        return template.render(context or {})
",src/meta_agent/template_mixer.py,TemplateMixer,1,7
survived,"def test_export_import_and_rating(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    manager = TemplateSharingManager(reg)
    reg.register(_meta(""greet""), ""hello"", version=""0.1.0"")

    exported = manager.export_template(""greet"")
    assert exported[""content""] == ""hello""

    reg2 = TemplateRegistry(base_dir=tmp_path / ""other"")
    manager2 = TemplateSharingManager(reg2)
    manager2.import_template(exported)
    assert reg2.load_template(""greet"") == ""hello""

    manager.add_rating(""greet"", 5)
    manager.add_rating(""greet"", 3)
    count, avg = manager.get_rating(""greet"")
    assert count == 2 and avg == 4.0

    top = manager.showcase()
    assert top and top[0][0] == ""greet""
",tests/test_template_sharing.py,,1,7
survived,"def test_docs_available() -> None:
    port = _free_port()
    env = os.environ.copy()
    env[""PYTHONPATH""] = str(REPO_ROOT)
    cmd = [
        sys.executable,
        ""-m"",
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface.api_server"",
        ""--host"",
        ""127.0.0.1"",
        ""--port"",
        str(port),
    ]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
    url = f""http://127.0.0.1:{port}""
    try:
        for _ in range(50):
            try:
                r = httpx.get(url + ""/docs"")
                if r.status_code == 200:
                    break
            except Exception:
                pass
            time.sleep(0.1)
        else:
            raise AssertionError(""server failed to start"")
        assert r.status_code == 200
    finally:
        proc.terminate()
        try:
            proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            proc.kill()",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_subprocess.py,,0,7
survived,"def _start_server(port: int, env: dict[str, str] | None = None) -> subprocess.Popen[bytes]:
    cmd = [
        sys.executable,
        ""-m"",
        ""src.interface.api_server"",
        ""--host"",
        ""127.0.0.1"",
        ""--port"",
        str(port),
    ]
    return subprocess.Popen(cmd, env=env or os.environ.copy())
",tests/test_api_server_subprocess.py,,1,6
survived,"def test_rate_limit_exceeded() -> None:
    port = _free_port()
    env = os.environ.copy()
    env[""API_RATE_LIMIT""] = ""3""
    proc = _start_server(port, env)
    url = f""http://127.0.0.1:{port}""
    headers = {""Authorization"": ""Bearer test-token""}
    try:
        _wait_running(url, headers)
        assert httpx.get(f""{url}/runs"", headers=headers).status_code == 200
        assert httpx.get(f""{url}/runs"", headers=headers).status_code == 200
        r = httpx.get(f""{url}/runs"", headers=headers)
        assert r.status_code == 429
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_api_server_subprocess.py,,1,7
survived,"def test_agents_status_outputs_names() -> None:
    runner = CliRunner()
    from unittest.mock import patch

    with patch.object(cli.orchestrator, ""Orchestrator"") as orch_cls:  # type: ignore[attr-defined]
        orch = orch_cls.return_value
        runner_obj = type(
            ""Runner"",
            (),
            {""agent"": type(""Agent"", (), {""name"": ""AgentZ""})()},
        )()
        orch.runners = {""AgentZ"": runner_obj}
        result = runner.invoke(cli.main, [""agents-status""])
    assert result.exit_code == 0
    assert ""AgentZ"" in result.output",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,,1,7
survived,"def percent_conditional(line):
    return ""%s\n"" % line if not line.endswith('\\') or line.endswith('\\\\') else ""%s"" % line[:-1]
",test/integration/samples_in/issue192.py,,1,6
survived,"def unique_inverse(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]:
    """"""Shortcut for :func:`unique` that also returns inverse indices.""""""

    values, inverse = typing.cast(
        tuple[NamedArray, NamedArray],
        unique(
            array,
            Unique,
            return_inverse=True,
            axis=axis,
            fill_value=fill_value,
        ),
    )
    return values, inverse
",src/haliax/ops.py,,1,7
survived,"def test_page_cache_extend_simple():
    Seq = Axis(""seq"", 2)
    Page = Axis(""page"", 2)
    MaxPage = Axis(""max_page"", 2)
    Slot = Axis(""slot"", 2)
    KVH = Axis(""kv_head"", 1)
    HD = Axis(""head_dim"", 1)

    cache = PageCache.init(Seq, Page, Slot, KVH, HD, MaxPage, dtype=jnp.float32)

    Tok = Axis(""tok"", 2)
    new_k = hax.arange(Tok).broadcast_axis((KVH, HD)).rearrange((Tok, KVH, HD)) + 1
    new_v = hax.arange(Tok).broadcast_axis((KVH, HD)).rearrange((Tok, KVH, HD)) + 101

    cu = jnp.array([0, 1, 2], dtype=jnp.int32)
    pages = jnp.array([0, 1], dtype=jnp.int32)

    jit_extend = eqx.filter_jit(PageCache.extend)
    cache = jit_extend(cache, new_k, new_v, cu, pages, 2)

    assert jnp.all(cache.kv_lens.array == jnp.array([1, 1], dtype=jnp.int32))
    assert jnp.all(cache.page_indices.array == jnp.array([[0, -1], [1, -1]], dtype=jnp.int32))
    assert cache.kv_pages.array[0, 0, 0, 0] == 1
    assert cache.kv_pages.array[0, 0, 1, 0] == 101
    assert cache.kv_pages.array[1, 0, 0, 0] == 2
    assert cache.kv_pages.array[1, 0, 1, 0] == 102
",tests/test_page_cache.py,,1,7
survived,"    def _generate_basic_docs(self, spec: ToolSpecification) -> str:
        """"""Return minimal documentation for a generated tool.""""""
        lines = [
            f""# {spec.name}"",
            """",
            spec.purpose,
            """",
            ""## Inputs"",
        ]
        for p in spec.input_parameters:
            req = ""(Required)"" if p.required else ""(Optional)""
            lines.append(f""- {p.name}: {p.description or 'No description'} {req}"")
        lines.append("""")
        lines.append(""## Output"")
        lines.append(str(spec.output_format))
        return ""\n"".join(lines)
",src/meta_agent/agents/tool_designer_agent.py,ToolDesignerAgent,1,7
survived,"    def test_discover_alpha_invalid_num(self) -> None:
        with self.assertRaises(ValueError):
            stub.discover_alpha(num=0, ledger=None, model=""gpt-4o-mini"")
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha,1,7
survived,"  def test_single_bit(self):
    self.assertEqual(getbits(0b100000000, 8, 8), 1)
",test/unit/test_helpers.py,TestGetBits,1,7
survived,"def _sin(x):
    y = _mod(x + PI, 2.0 * PI) - PI
    y2 = y * y
    y3 = y2 * y
    y5 = y3 * y2
    y7 = y5 * y2
    return y - y3 / 6.0 + y5 / 120.0 - y7 / 5040.0
",tests/rosetta/transpiler/Python/fractal-tree.py,,1,6
survived,"def say(n):
    t = """"
    if n < 0:
        t = ""negative ""
        n = -n
    if n < 20:
        return t + small[n]
    else:
        if n < 100:
            t = tens[n // 10]
            s = n % 10
            if s > 0:
                t = t + ""-"" + small[s]
            return t
        else:
            if n < 1000:
                t = small[n // 100] + "" hundred""
                s = n % 100
                if s > 0:
                    t = t + "" "" + say(s)
                return t
    sx = """"
    i = 0
    nn = n
    while nn > 0:
        p = nn % 1000
        nn = nn // 1000
        if p > 0:
            ix = say(p) + illions[i]
            if sx != """":
                ix = ix + "" "" + sx
            sx = ix
        i = i + 1
    return t + sx
",tests/rosetta/transpiler/Python/four-is-magic.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm.py,,1,6
survived,"def fa(a, b, c0):
    r1 = ha(a, c0)
    r2 = ha(r1.s, b)
    return SumCarry(s=r2.s, c=r1.c or r2.c)
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fractran.py,,1,6
survived,"def parseInt(str):
    i = 0
    neg = False
    if len(str) > 0 and str[0:1] == ""-"":
        neg = True
        i = 1
    n = 0
    digits = {""0"": 0, ""1"": 1, ""2"": 2, ""3"": 3, ""4"": 4, ""5"": 5, ""6"": 6, ""7"": 7, ""8"": 8, ""9"": 9}
    while i < len(str):
        n = n * 10 + digits[str[i:i + 1]]
        i = i + 1
    if neg:
        n = -n
    return n
",tests/rosetta/transpiler/Python/gui-component-interaction.py,,1,6
survived,"def repLeap(year):
    a = int(((year + 1) % 4))
    b = int(((year + 1) % 100))
    c = int(((year + 1) % 400))
    return a == 0 and (b != 0 or c == 0)
",tests/rosetta/transpiler/Python/french-republican-calendar.py,,1,6
survived,"def pad(s, w):
    t = s
    while len(t) < w:
        t = "" "" + t
    return t
",tests/rosetta/transpiler/Python/floyds-triangle.py,,1,6
survived,"def totalLength():
    tot = 0
    i = 0
    while i < len(words):
        tot = tot + len(words[i])
        if i < len(words) - 1:
            tot = tot + 1
        i = i + 1
    return tot
",tests/rosetta/transpiler/Python/four-is-the-number-of-letters-in-the-....py,,1,7
survived,"def greToDay(d, m, y):
    yy = y
    mm = m
    if mm < 3:
        yy = yy - 1
        mm = mm + 12
    return (yy * 36525 // 100) - (yy // 100) + (yy // 400) + 306 * (mm + 1) // 10 + d - 654842
",tests/rosetta/transpiler/Python/french-republican-calendar.py,,1,7
survived,"def rand10000():
    return _now() % 10000
",tests/rosetta/transpiler/Python/gui-component-interaction.py,,1,6
survived,"def mul(a, b):
    return newFps(_lambda1)
",tests/rosetta/transpiler/Python/formal-power-series.py,,0,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/general-fizzbuzz.py,,1,6
survived,"def repToDay(d, m, y):
    dd = d
    mm = m
    if mm == 13:
        mm = mm - 1
        dd = dd + 30
    if repLeap(y):
        dd = dd - 1
    return 365 * y + (y + 1) // 4 - (y + 1) // 100 + (y + 1) // 400 + 30 * mm + dd - 395
",tests/rosetta/transpiler/Python/french-republican-calendar.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    program = [[17, 91], [78, 85], [19, 51], [23, 38], [29, 33], [77, 29], [95, 23], [77, 19], [1, 17], [11, 13], [13, 11], [15, 14], [15, 2], [55, 1]]
    n = 2
    primes = 0
    count = 0
    limit = 1000000
    two = 2
    line = """"
    while primes < 20 and count < limit:
        res = step(n, program)
        n = res.n
        if not res.ok:
            break
        m = n
        pow = 0
        while m % two == 0:
            m = m // two
            pow = pow + 1
        if m == 1 and pow > 1:
            line = line + str(pow) + "" ""
            primes = primes + 1
        count = count + 1
    if len(line) > 0:
        print(line[0:len(line) - 1])
    else:
        print("""")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fractran.py,,1,6
survived,"def test_legacy_fmtspec_extra_aggressive(state: State):
    s_in = """"""d = '%i' % var""""""
    s_expected = """"""d = f'{var}'""""""

    state.aggressive = 2
    out, count = code_editor.fstringify_code_by_line(s_in, state)
    assert out == s_expected
",test/test_edits.py,,0,6
survived,"    async def get_latest(_: None = Depends(verify_token)) -> ResultsResponse:
        """"""Return the most recently completed simulation.""""""
        if _latest_id is None:
            raise HTTPException(status_code=404)
        result = _simulations.get(_latest_id)
        if result is None:
            raise HTTPException(status_code=404)
        return result
",src/interface/api_server.py,,1,7
survived,"    def test_env_override(self):
        data = [{""alpha"": ""env test""}]
        tmp = Path(""/tmp/opps.json"")
        tmp.write_text(json.dumps(data), encoding=""utf-8"")
        os.environ[""ALPHA_OPPS_FILE""] = str(tmp)
        try:
            agent = biz.AlphaOpportunityAgent()
            self.assertEqual(agent._opportunities, data)
        finally:
            del os.environ[""ALPHA_OPPS_FILE""]
            tmp.unlink()
",tests/test_alpha_opportunity_env.py,TestAlphaOpportunityEnv,1,6
survived,"    def test_reset_batch_matches_vector_env(self):
        env_fn = lambda: ce.CurriculumEnv(genome=ce.EnvGenome(max_steps=10), size=6)
        vec = gym.vector.SyncVectorEnv([env_fn for _ in range(3)])
        obs_vec, _ = vec.reset()
        env = env_fn()
        obs, infos = env.reset_batch(3)
        self.assertEqual(obs.shape, obs_vec.shape)
        self.assertEqual(len(infos), 3)
",alpha_factory_v1/tests/test_aiga_meta_evolution.py,CurriculumEnvTest,1,7
survived,"        def close(self) -> None:
            self.closed = True
",tests/test_alpha_agi_business_3_v1.py,DummyADK,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/if_then_else_nested.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/sum_builtin.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/pure_fold.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/two-sum.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/outer_join.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/nested_function.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/for_map_collection.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/append_builtin.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/test_block.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/let_and_print.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_membership.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/string_contains.py,,1,7
survived,"def ingest() -> pd.DataFrame:
    """"""Ingest headline metrics from the PostHog query API.""""""
    import requests
    from dagster import get_dagster_logger

    logger = get_dagster_logger()

    api_key = os.getenv(""POSTHOG_API_KEY"")
    project_id = os.getenv(""POSTHOG_PROJECT_ID"")
    if not api_key or not project_id:
        raise RuntimeError(
            ""POSTHOG_API_KEY and POSTHOG_PROJECT_ID env vars must be set""
        )

    host = os.getenv(""POSTHOG_HOST"", ""https://app.posthog.com"")
    url = f""{host}/api/projects/{project_id}/query""
    headers = {
        ""Authorization"": f""Bearer {api_key}"",
        ""Content-Type"": ""application/json"",
    }

    query = """"""
        SELECT
            count() AS events_yesterday,
            count(DISTINCT person_id) AS users_yesterday
        FROM events
        WHERE timestamp >= toStartOfDay(now() - INTERVAL 1 day)
          AND timestamp < toStartOfDay(now())
    """"""

    try:
        response = requests.post(
            url, headers=headers, json={""query"": query}, timeout=10
        )
        response.raise_for_status()
    except requests.RequestException as ex:
        logger.error(f""Failed to fetch PostHog data from {url}: {ex}"")
        return pd.DataFrame(columns=[""metric_timestamp"", ""metric_name"", ""metric_value""])

    data = response.json()

    rows = []
    ts = pd.Timestamp.utcnow().floor(""s"")
    results = data.get(""results"") or data.get(""data"")
    if results:
        first = results[0]
        if isinstance(first, dict):
            events = first.get(""events_yesterday"")
            users = first.get(""users_yesterday"")
        elif isinstance(first, list):
            columns = data.get(""columns"", [])
            try:
                events = first[columns.index(""events_yesterday"")]
            except (ValueError, IndexError):
                events = first[0]
            try:
                users = first[columns.index(""users_yesterday"")]
            except (ValueError, IndexError):
                users = first[1] if len(first) > 1 else None
        else:
            events = users = None

        if events is not None:
            rows.append(
                {
                    ""metric_timestamp"": ts,
                    ""metric_name"": ""posthog.events_yesterday"",
                    ""metric_value"": float(events),
                }
            )
        if users is not None:
            rows.append(
                {
                    ""metric_timestamp"": ts,
                    ""metric_name"": ""posthog.users_yesterday"",
                    ""metric_value"": float(users),
                }
            )

    df = pd.DataFrame(rows)
    df = df.dropna()
    df = df[[""metric_timestamp"", ""metric_name"", ""metric_value""]]
    return df",metrics/examples/posthog/posthog.py,,1,7
survived,"def _start_demo_server(port: int, env: dict[str, str] | None = None) -> subprocess.Popen[bytes]:
    cmd = [
        sys.executable,
        ""-m"",
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface.api_server"",
        ""--host"",
        ""127.0.0.1"",
        ""--port"",
        str(port),
    ]
    return subprocess.Popen(cmd, env=env or os.environ.copy())
",tests/test_api_server_subprocess.py,,1,6
survived,"def inc(x: int) -> int:
    return x + k
",tests/human/x/python/pure_global_fold.py,,0,7
survived,"def sum_tree(t: Tree) -> int:
    if isinstance(t, Leaf):
        return 0
    elif isinstance(t, Node):
        return sum_tree(t.left) + t.value + sum_tree(t.right)
    else:
        raise TypeError(""Unknown node"")
",tests/human/x/python/tree_sum.py,,1,7
survived,"    def test_invalid_env_fallback(self) -> None:
        env = {""PORT"": ""foo"", ""CYCLE"": ""bar"", ""METRICS_PORT"": ""baz"", ""A2A_PORT"": ""qux""}
        with patch.dict(os.environ, env, clear=True):
            args = edge_runner.parse_args([])
        self.assertEqual(args.port, 8000)
        self.assertIsNone(args.cycle)
        self.assertIsNone(args.metrics_port)
        self.assertIsNone(args.a2a_port)
",tests/test_edge_runner_parse.py,TestParseArgs,1,7
survived,"        def generate_text(self, prompt: str) -> str:
            calls.append(prompt)
            return ""sum""
",tests/test_adk_agent.py,StubADK,1,6
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:
        self.published.append((topic, env))
",tests/test_adk_agent.py,DummyBus,1,7
survived,"    async def run() -> None:
        async with orch.bus:
            runner.start(orch.bus, orch.ledger)
            monitor = asyncio.create_task(orch._monitor())
            for _ in range(6):
                await orig_sleep(0)
            monitor.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await monitor
            if runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
",tests/test_orchestrator_backoff.py,,1,6
survived,"    async def skill_test(self, payload: dict) -> dict:
        return {""ok"": True}
",tests/test_orchestrator_rest.py,DummyAgent,1,7
survived,"    async def skill_test(self, payload: dict) -> dict:
        """"""Respond to skill test pings.""""""
        return {""pong"": True}
",alpha_factory_v1/backend/agents/ping_agent.py,PingAgent,1,7
survived,"    def __init__(self, inst: SimpleAgent) -> None:
        self.inst = inst
        self.next_ts = 0
",tests/test_skill_test_route.py,Runner,1,7
survived,"def test_insight_cli_full_simulation(tmp_path: Path) -> None:
    """"""Run the Insight CLI simulate command end-to-end.""""""
    ledger = tmp_path / ""audit.db""
    env = os.environ.copy()
    env[""AGI_INSIGHT_LEDGER_PATH""] = str(ledger)

    cmd = [
        sys.executable,
        ""-m"",
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface.cli"",
        ""simulate"",
        ""--horizon"",
        ""1"",
        ""--pop-size"",
        ""1"",
        ""--generations"",
        ""1"",
        ""--offline"",
        ""--no-broadcast"",
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, env=env)
    assert result.returncode == 0, result.stderr
    assert ""year"" in result.stdout.lower()
    assert ledger.exists()",tests/test_insight_cli_e2e.py,,1,7
survived,"def test_xdg_music_dir(monkeypatch):
    monkeypatch.setenv(""XDG_MUSIC_DIR"", ""/home/user/Music"")
    monkeypatch.setenv(""XDG_UNKNOWN_DIR"", ""/home/user/Unknown"")

    devicons = reload_devicons()

    assert devicons.dir_node_exact_matches.get(""Music"") == """"
    assert ""Unknown"" not in devicons.dir_node_exact_matches",tests/test_xdg.py,,1,7
survived,"def insert(
    parent_hash: str,
    child_hash: str,
    metrics: Mapping[str, float],
    *,
    db_path: str | Path = _DEFAULT_DB,
) -> str:
    """"""Insert ``child_hash`` with ``parent_hash`` and return updated Merkle root.""""""
    path = Path(db_path)
    _ensure(path)
    record = {
        ""parent"": parent_hash,
        ""child"": child_hash,
        ""metrics"": dict(metrics),
    }
    h = hashlib.sha256(json.dumps(record, sort_keys=True).encode()).hexdigest()
    with sqlite3.connect(path) as cx:
        cx.execute(
            ""INSERT INTO entries(parent, child, metrics, hash, ts) VALUES(?,?,?,?,?)"",
            (parent_hash, child_hash, json.dumps(record[""metrics""]), h, time.time()),
        )
    return _update_root(path)
",src/archive/archive.py,,1,7
survived,"    def visit_Continue(self, node):
        self.emit(""continue"")
",tools/any2mochi/py_simple.py,Conv,1,7
survived,"    def visit_If(self, node):
        self.emit(f""if {self.expr(node.test)} {{"")
        self.indent += 1
        for s in node.body:
            self.visit(s)
        self.indent -= 1
        if node.orelse:
            self.emit(""} else {"")
            self.indent += 1
            for s in node.orelse:
                self.visit(s)
            self.indent -= 1
        self.emit(""}"")
",tools/any2mochi/py_simple.py,Conv,1,7
survived,"    def __init__(self, broker: str | None, dev_mode: bool) -> None:
        self._queues: Dict[str, asyncio.Queue] | None = None
        self._producer: KafkaProducer | None = None  # type: ignore
        if broker and ""KafkaProducer"" in globals():
            self._producer = KafkaProducer(
                bootstrap_servers=broker.split("",""),
                value_serializer=lambda v: json.dumps(v).encode(),
                linger_ms=50,
            )
            atexit.register(self._close)
        else:
            if broker and not dev_mode:
                log.warning(""Kafka unavailable → falling back to in-proc bus"")
            self._queues = {}
",alpha_factory_v1/backend/agent_manager.py,EventBus,1,7
survived,"    async def _skill_test(request: Request, name: str) -> Any:
        payload = await request.json()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""skill_test""):
            raise HTTPException(501, ""Agent does not support skill_test"")
        return await inst.skill_test(payload)  # type: ignore[func-returns-value]
",alpha_factory_v1/backend/api_server.py,,1,6
survived,"    async def _update_model(request: Request, name: str, file: Optional[bytes] = upload_param) -> Dict[str, str]:
        if ""FastAPI"" not in globals() and file is None:
            file = await request.body()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""load_weights""):
            raise HTTPException(501, ""Agent does not support model updates"")
        import io
        import stat
        import tempfile
        import zipfile

        with tempfile.TemporaryDirectory() as td:
            with zipfile.ZipFile(io.BytesIO(file)) as zf:
                base = Path(td).resolve()
                total = 0
                for info in zf.infolist():
                    if stat.S_ISLNK(info.external_attr >> 16):
                        raise HTTPException(400, ""Symlinks not allowed"")
                    if info.is_dir():
                        continue
                    total += info.file_size
                    if total > model_max_bytes:
                        raise HTTPException(400, ""Archive too large"")
                    dest = (base / info.filename).resolve()
                    if not str(dest).startswith(str(base)):
                        raise HTTPException(400, ""Unsafe path in archive"")
                    zf.extractall(td)
            inst.load_weights(td)  # type: ignore[attr-defined]
        return {""status"": ""ok""}
",alpha_factory_v1/backend/api_server.py,,1,7
survived,"async def start_servers(
    runners: Dict[str, AgentRunner],
    model_max_bytes: int,
    mem: Any,
    rest_port: int,
    grpc_port: int,
    loglevel: str,
    ssl_disable: bool,
) -> tuple[Optional[asyncio.Task], Optional[""grpc.aio.Server""]]:
    """"""Convenience helper to launch REST and gRPC services.""""""

    app = build_rest(runners, model_max_bytes, mem)
    rest_task = await start_rest(app, rest_port, loglevel)
    grpc_server = await serve_grpc(runners, grpc_port, ssl_disable)
    return rest_task, grpc_server
",alpha_factory_v1/backend/api_server.py,,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join_sort.py,Order,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/outer_join.py,Order,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/left_join_multi.py,Item,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by.py,Person,1,6
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/join_multi.py,Customer,1,6
survived,"def _prepare_input(vectorizer):
    def _transform(X):
        if hasattr(X, ""toarray""):
            arr = X.toarray()
        else:
            arr = np.asarray(X)
        corpus = []
        for row in arr:
            doc = []
            for idx, count in enumerate(row):
                doc.extend([idx] * int(count))
            corpus.append(doc)
        vocab = list(vectorizer.get_feature_names_out())
        return corpus, vocab

    return _transform
",tests/test_sklearn_wrapper.py,,1,6
survived,"def test_error_when_user_declines_creation(tmp_path, monkeypatch, mocker):
    monkeypatch.setenv(""HOME"", str(tmp_path))
    mocker.patch(""builtins.input"", return_value=""n"")

    with pytest.raises(FileNotFoundError):
        CredentialsProvider(""default"")",tests/dhapi/port/test_credentials_provider.py,,1,7
survived,"def test_execute_in_sandbox_exception() -> None:
    agent = _make_agent()
    out, err = agent.execute_in_sandbox(""1/0"")
    assert out == """"
    assert ""ZeroDivisionError"" in err",tests/test_codegen_agent.py,,1,8
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",tests/test_bus_ssl_gen.py,,1,7
survived,"def test_run_forever_shutdown() -> None:
    settings = config.Settings(bus_port=0)
    with mock.patch.object(orchestrator.Orchestrator, ""_init_agents"", lambda self: []):
        orch = orchestrator.Orchestrator(settings)

    async def run() -> None:
        with mock.patch.object(orch.bus, ""stop"", mock.AsyncMock()) as bus_stop, \
             mock.patch.object(orch.ledger, ""stop_merkle_task"", mock.AsyncMock()) as merkle_stop:
            task = asyncio.create_task(orch.run_forever())
            await asyncio.sleep(0.05)
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
            bus_stop.assert_awaited_once()
            merkle_stop.assert_awaited_once()

    asyncio.run(run())",tests/test_orchestrator.py,,1,7
survived,"async def get_group_members(client: GraphServiceClient, group_id: str) -> List[str]:
    """"""Get member user IDs for a given group.""""""
    members: List[str] = []
    request_builder = client.groups.by_group_id(group_id).members
    page = await request_builder.get()
    while page:
        if page.value:
            for obj in page.value:
                if isinstance(obj, DirectoryObject):
                    # Filter to user objects based on odata_type
                    if getattr(obj, ""odata_type"", """") == ""#microsoft.graph.user"":
                        members.append(obj.id)
        if not page.odata_next_link:
            break
        page = await request_builder.with_url(page.odata_next_link).get()
    return members
",cartography/intel/entra/groups.py,,1,7
survived,"async def test_sync_entra_groups(mock_get_members, mock_get_groups, neo4j_session):
    """"""Ensure groups and relationships load""""""
    # Load users first for membership relationships
    load_users(neo4j_session, transform_users(MOCK_ENTRA_USERS), TEST_TENANT_ID, TEST_UPDATE_TAG)

    await sync_entra_groups(
        neo4j_session,
        TEST_TENANT_ID,
        TEST_CLIENT_ID,
        TEST_CLIENT_SECRET,
        TEST_UPDATE_TAG,
        {""UPDATE_TAG"": TEST_UPDATE_TAG, ""TENANT_ID"": TEST_TENANT_ID},
    )

    expected_nodes = {
        (""11111111-1111-1111-1111-111111111111"", ""Security Team""),
        (""22222222-2222-2222-2222-222222222222"", ""Developers""),
    }
    assert check_nodes(neo4j_session, ""EntraGroup"", [""id"", ""display_name""]) == expected_nodes

    expected_rels = {
        (""11111111-1111-1111-1111-111111111111"", TEST_TENANT_ID),
        (""22222222-2222-2222-2222-222222222222"", TEST_TENANT_ID),
    }
    assert (
        check_rels(
            neo4j_session,
            ""EntraGroup"",
            ""id"",
            ""EntraTenant"",
            ""id"",
            ""RESOURCE"",
            rel_direction_right=False,
        )
        == expected_rels
    )

    expected_membership = {
        (""ae4ac864-4433-4ba6-96a6-20f8cffdadcb"", ""11111111-1111-1111-1111-111111111111""),
        (""11dca63b-cb03-4e53-bb75-fa8060285550"", ""11111111-1111-1111-1111-111111111111""),
    }
    assert (
        check_rels(
            neo4j_session,
            ""EntraUser"",
            ""id"",
            ""EntraGroup"",
            ""id"",
            ""MEMBER_OF"",
        )
        == expected_membership
    )
",tests/integration/cartography/intel/entra/test_groups.py,,1,7
survived,"async def get_entra_groups(client: GraphServiceClient) -> List[Group]:
    """"""Get all groups from Microsoft Graph API with pagination.""""""
    all_groups: List[Group] = []

    request_configuration = client.groups.GroupsRequestBuilderGetRequestConfiguration(
        query_parameters=client.groups.GroupsRequestBuilderGetQueryParameters(top=999)
    )
    page = await client.groups.get(request_configuration=request_configuration)
    while page:
        if page.value:
            all_groups.extend(page.value)
        if not page.odata_next_link:
            break
        page = await client.groups.with_url(page.odata_next_link).get()

    return all_groups
",cartography/intel/entra/groups.py,,1,7
survived,"    def single_for_current_platform(self) -> RuntimeDependency:
        deps = self.for_current_platform()
        if len(deps) != 1:
            raise RuntimeError(
                f""Expected exactly one runtime dependency for {PlatformUtils.get_platform_id().value}, found {len(deps)}""
            )
        return deps[0]
",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection,1,7
survived,"    def _visit_string_node(self) -> None:
        if self.in_fmt_value:
            self.string_in_string = True
",src/flynt/utils/utils.py,StringInStringVisitor,1,6
survived,"def test_bundle_generator_custom_metadata(tmp_path: Path) -> None:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""print('agent')"",
        metadata_fields={""meta_agent_version"": ""1.2.3"", ""extra"": ""field""},
        custom_metadata={""tag"": ""example""},
    )

    with open(tmp_path / ""bundle.json"", encoding=""utf-8"") as f:
        data = json.load(f)

    assert data[""meta_agent_version""] == ""1.2.3""
    assert data[""extra""] == ""field""
    assert data[""custom""][""tag""] == ""example""",tests/test_bundle_generator.py,,1,7
survived,"    def push(self, remote: str = ""origin"", branch: str = ""main"") -> None:
        self._run(""push"", remote, f""HEAD:{branch}"")
        # If the remote is a local path, set its HEAD to the pushed branch so
        # commands like ``git log`` default to the new branch.
        remote_url = self._run(""remote"", ""get-url"", remote).stdout.strip()
        remote_path = Path(remote_url)
        if remote_path.exists():
            subprocess.run(
                [
                    ""git"",
                    ""-C"",
                    str(remote_path),
                    ""symbolic-ref"",
                    ""HEAD"",
                    f""refs/heads/{branch}"",
                ],
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )",src/meta_agent/git_utils.py,GitManager,1,6
survived,"    def commit_all(self, message: str = ""Initial commit"") -> str:
        """"""Add all files and create a commit.

        Returns the commit SHA.
        """"""
        env = os.environ.copy()
        # Deterministic commit timestamp
        env.setdefault(""GIT_AUTHOR_DATE"", ""1970-01-01T00:00:00+0000"")
        env.setdefault(""GIT_COMMITTER_DATE"", ""1970-01-01T00:00:00+0000"")
        self._run(""add"", ""-A"", env=env)
        self._run(""commit"", ""-m"", message, env=env)
        result = self._run(""rev-parse"", ""HEAD"", env=env)
        return result.stdout.strip()
",src/meta_agent/git_utils.py,GitManager,1,7
survived,"def test_git_manager_push(tmp_path: Path) -> None:
    remote = tmp_path / ""remote.git""
    subprocess.run([""git"", ""init"", ""--bare"", str(remote)], check=True)

    repo = tmp_path / ""repo""
    gm = GitManager(repo)
    gm.init()
    (repo / ""bar.txt"").write_text(""bar"")
    gm.commit_all(""first"")
    gm.add_remote(""origin"", str(remote))
    gm.push(""origin"", ""main"")

    log = subprocess.check_output(
        [""git"", ""-C"", str(remote), ""log"", ""--oneline""], text=True
    )
    assert ""first"" in log",tests/test_git_utils.py,,1,7
survived,"    def add_remote(self, name: str, url: str) -> None:
        self._run(""remote"", ""add"", name, url)
",src/meta_agent/git_utils.py,GitManager,1,6
survived,"    def git_available() -> bool:
        """"""Return True if the ``git`` executable can be found.""""""
        return shutil.which(""git"") is not None
",src/meta_agent/git_utils.py,GitManager,1,7
survived,"        async def run() -> None:
            await orch.bus.start()
            assert orch.bus._server is not None
            try:
                creds = grpc.ssl_channel_credentials(root_certificates=ca)
                async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                    stub = ch.unary_unary(""/bus.Bus/Send"")
                    payload = {
                        ""sender"": ""a"",
                        ""recipient"": ""b"",
                        ""payload"": {},
                        ""ts"": 0.0,
                        ""token"": ""bad"",
                    }
                    with pytest.raises(grpc.aio.AioRpcError):
                        await stub(json.dumps(payload).encode())
            finally:
                await orch.bus.stop()
                await orch.ledger.stop_merkle_task()
                orch.ledger.close()
",tests/test_orchestrator_bus_tls_env.py,,1,7
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,,1,6
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,,1,8
survived,"    def __init__(self) -> None:
        self.completions = _ChatCompletions()
",src/meta_agent/services/openai_stub.py,_Chat,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/100-doors-2.py,,1,6
survived,"def pow(c, d):
    di = toInt(d)
    prod = c
    i = 1
    while i < di:
        prod = mul(prod, c)
        i = i + 1
    return prod
",tests/rosetta/transpiler/Python/church-numerals-1.py,,1,6
survived,"def verti(r1, r2, c):
    r = r1
    while r <= r2:
        n[r][c] = ""x""
        r = r + 1
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,1,6
survived,"def main():
    programText = ""Datasize: 1 Strings: 2\n"" + ""\""count is: \""\n"" + ""\""\\n\""\n"" + ""    0 push  1\n"" + ""    5 store [0]\n"" + ""   10 fetch [0]\n"" + ""   15 push  10\n"" + ""   20 lt\n"" + ""   21 jz     (43) 65\n"" + ""   26 push  0\n"" + ""   31 prts\n"" + ""   32 fetch [0]\n"" + ""   37 prti\n"" + ""   38 push  1\n"" + ""   43 prts\n"" + ""   44 fetch [0]\n"" + ""   49 push  1\n"" + ""   54 add\n"" + ""   55 store [0]\n"" + ""   60 jmp    (-51) 10\n"" + ""   65 halt\n""
    prog = parseProgram(programText)
    runVM(prog)
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,,1,7
survived,"def printNumeral():
    i = 0
    while i < 15:
        line = """"
        j = 0
        while j < 11:
            line = line + n[i][j] + "" ""
            j = j + 1
        print(line)
        i = i + 1
    print("""")
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,1,6
survived,"def toStr(x):
    s = """"
    def fCounter(f):
        global s
        s = s + ""|""
        return f
    x(fCounter)(id)
    return s
",tests/rosetta/transpiler/Python/church-numerals-2.py,,0,7
survived,"def printMat(m):
    i = 0
    while i < len(m):
        line = """"
        j = 0
        while j < len(m[i]):
            line = line + str(m[i][j])
            if j < len(m[i]) - 1:
                line = line + "" ""
            j = j + 1
        print(line)
        i = i + 1
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,,1,6
survived,"def unpackSym(m):
    n = m[""order""]
    ele = m[""ele""]
    mat = []
    idx = 0
    r = 0
    while r < n:
        row = []
        c = 0
        while c <= r:
            row = row + [ele[idx]]
            idx = idx + 1
            c = c + 1
        while c < n:
            row = row + [0.0]
            c = c + 1
        mat = mat + [row]
        r = r + 1
    r = 0
    while r < n:
        c = r + 1
        while c < n:
            mat[r][c] = mat[c][r]
            c = c + 1
        r = r + 1
    return mat
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,,1,7
survived,"def test_format_time_ago_outputs():
    now = datetime.now(timezone.utc)
    assert format_time_ago((now - timedelta(days=2)).isoformat()) == ""2 days ago""
    assert format_time_ago((now - timedelta(hours=5)).isoformat()) == ""5 hours ago""
    thirty_min_ago = (now - timedelta(minutes=30)).isoformat()
    assert format_time_ago(thirty_min_ago) == ""30 minutes ago""
    assert format_time_ago(now.isoformat()) == ""just now""
",tests/test_dashboard.py,,1,7
survived,"    def __str__(self) -> str:
        return self.message
",src/meta_agent/ux/error_handler.py,UXError,1,7
survived,"        def copy(self, text):
            copied[""text""] = text
",tests/ux/test_user_feedback.py,Dummy,1,6
survived,"    def metadata(self) -> BundleMetadata:
        if self._metadata is None:
            self.refresh_metadata()
        assert self._metadata is not None
        return self._metadata
",src/meta_agent/bundle.py,Bundle,1,7
survived,"        def __init__(self, sender: str = """", recipient: str = """", payload: dict | None = None, ts: float = 0.0) -> None:
            self.sender = sender
            self.recipient = recipient
            self.payload = payload or {}
            self.ts = ts
",tests/test_adapters.py,Envelope,1,7
survived,"    async def fake_call_tool(self, name: str, args: dict[str, object]) -> object:
        calls[""call""] = (name, args)
        return {""done"": True}
",tests/test_adapters.py,,1,6
survived,"def test_adk_generate_text_calls_library(monkeypatch) -> None:
    """"""Ensure ADKAdapter.generate_text delegates to the ADK client.""""""
    try:
        mod = importlib.import_module(""adk"")
    except Exception:
        mod = importlib.import_module(""google.adk"")

    calls: dict[str, str] = {}

    def fake_generate(self, prompt: str) -> str:
        calls[""prompt""] = prompt
        return ""resp""

    monkeypatch.setattr(mod.Client, ""generate"", fake_generate, raising=False)
    adapter = ADKAdapter()
    result = adapter.generate_text(""hello"")
    assert result == ""resp""
    assert calls == {""prompt"": ""hello""}
",tests/test_adapters.py,,1,7
survived,"    def __init__(self, *args, **kwargs):
        pass
",openai_agents/__init__.py,OpenAIAgent,0,8
survived,"def _reset_islands() -> None:
    mats.ISLANDS.clear()
    mats.ISLAND_SEEDS.clear()
",tests/test_mats.py,,1,6
survived,"    def __init__(self, examples: Iterable[str] | None = None, *, seed: int | None = None) -> None:
        self.examples = list(examples) if examples is not None else load_examples()
        self.rng = random.Random(seed)
",src/evaluators/feasibility_critic.py,FeasibilityCritic,1,7
survived,"    async def list_resource(
        ctx: EnrichContext, page: int = 1, page_size: int = 20
    ) -> PageResult[enrich_model]:  # type: ignore[name-defined]
        session_factory = ctx.request_context.lifespan_context[session_key]
        async with session_factory() as session:  # type: AsyncSession
            total = await session.scalar(select(func.count()).select_from(sa_model))
            result = await session.execute(
                select(sa_model).offset((page - 1) * page_size).limit(page_size)
            )
            items = [_sa_to_enrich(obj, enrich_model) for obj in result.scalars().all()]
            has_next = page * page_size < int(total or 0)
            return PageResult.create(
                items=items,
                page=page,
                page_size=page_size,
                total_items=int(total or 0),
                has_next=has_next,
            )
",src/enrichmcp/sqlalchemy/auto.py,,1,7
survived,"def test_ragged_paged_attention_incremental_single_seq():
    rng = jr.PRNGKey(2)
    seq_lens = [47]
    k_lens = [5]
    q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs = _build_incremental_case(rng, seq_lens, k_lens)

    ragged = default_ragged_paged_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs, sm_scale=SM_SCALE)
    ref = _reference_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, k_lens)

    assert ragged.axes == ref.axes
    assert_trees_all_close(ragged.array, ref.array, atol=1e-3, rtol=1e-3)
",tests/test_paged_attention.py,,1,7
survived,"def health() -> dict[str, str]:
    return {""status"": ""ok""}",backend/main.py,,1,8
survived,"def test_adk_auto_register_disabled(monkeypatch):
    if importlib.util.find_spec(""google_adk""):
        import google_adk as gadk
    else:  # pragma: no cover
        from google import adk as gadk

    class DummyRouter:
        def __init__(self):
            self.app = types.SimpleNamespace(middleware=lambda *_a, **_k: lambda f: f)
        def register_agent(self, agent):
            raise AssertionError(""should not register"")

    monkeypatch.setattr(gadk, ""Router"", DummyRouter)
    monkeypatch.delenv(""ALPHA_FACTORY_ENABLE_ADK"", raising=False)

    import importlib as _imp
    bridge = _imp.reload(_imp.import_module(""alpha_factory_v1.backend.adk_bridge""))
    bridge.auto_register([object()])  # no error",tests/test_external_integrations.py,,1,7
survived,"def test_get_tables_trailing_semicolon():
    dialect = ""bigquery""

    query = ""SELECT * FROM table1;""
    expected = {""tables"": [""table1""]}
    assert get_tables(query, dialect) == expected

    query = ""SELECT * FROM table1;;""
    expected = {""tables"": [""table1""]}
    assert get_tables(query, dialect) == expected
",pythonsrc/parser/main_test.py,,1,7
survived,"  def supports_active_cooling(self) -> bool:
    """"""Whether this backend can actively cool below ambient temperature.""""""
    raise NotImplementedError
",pylabrobot/temperature_controlling/backend.py,TemperatureControllerBackend,1,6
survived,"        def query_text(self, _):
            return {""embedding"": [0.0]}
",no-ocr-api/tests/test_ingest_search.py,FakeColPali,1,6
survived,"        async def send_and_wait(self, topic: str, data: bytes) -> None:
            return None
",tests/test_bus_large_payloads_property.py,Prod,0,7
survived,"def test_publish_invalid_payload_errors(bad: object) -> None:  # type: ignore[misc]
    """"""Non-JSON payloads should raise ``TypeError`` during publish.""""""

    class Prod:
        def __init__(self, bootstrap_servers: str) -> None:
            pass

        async def start(self) -> None:
            return None

        async def send_and_wait(self, topic: str, data: bytes) -> None:
            return None

        async def stop(self) -> None:
            return None

    cfg = config.Settings(bus_port=0, broker_url=""k:1"")
    with mock.patch.object(messaging, ""AIOKafkaProducer"", Prod):

        async def run() -> None:
            async with messaging.A2ABus(cfg) as bus:
                env = types.SimpleNamespace(sender=""s"", recipient=""x"", payload={""bad"": bad}, ts=0.0)
                with pytest.raises(TypeError):
                    bus.publish(""x"", env)
                    await asyncio.sleep(0)

        asyncio.run(run())",tests/test_bus_large_payloads_property.py,,0,6
survived,"def test_check_gzip_call_present() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    text = (browser_dir / ""manual_build.py"").read_text()
    assert ""def check_gzip_size"" in text
    pattern = r""write_text\(bundle\).*\n\s*check_gzip_size\(dist_dir / \""insight.bundle.js\""\)""
    assert re.search(pattern, text)
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_manual_build_size_limit.py,,1,7
survived,"def test_request_patch_respects_model_env(monkeypatch: pytest.MonkeyPatch) -> None:
    diff = ""--- a/z\n+++ b/z\n@@\n-old\n+new\n""
    openai_stub = types.ModuleType(""openai"")
    create_mock = Mock(return_value={""choices"": [{""message"": {""content"": diff}}]})
    openai_stub.ChatCompletion = types.SimpleNamespace(create=create_mock)
    monkeypatch.setitem(sys.modules, ""openai"", openai_stub)
    monkeypatch.setenv(""OPENAI_API_KEY"", ""x"")
    monkeypatch.setenv(""OPENAI_MODEL"", ""test-model"")
    monkeypatch.setenv(""USE_LOCAL_LLM"", ""false"")
    client = _reload_client(monkeypatch, diff)
    client.request_patch([{""role"": ""user"", ""content"": ""fix""}])
    assert create_mock.call_args.kwargs.get(""model"") == ""test-model""
",tests/test_llm_client_offline.py,,1,7
survived,"def scenario_2012_dl() -> replay.Scenario:
    return replay.load_scenario(""2012_dl"")
",tests/conftest.py,,1,6
survived,"def _load_model(config: SampleLmConfig, Vocab: Axis, *, key) -> LmHeadModel:
    """"""Load a model either from a checkpoint or HF repo.""""""

    if config.checkpoint_path is None and config.hf_checkpoint is None:
        raise ValueError(""Must specify either checkpoint_path or hf_checkpoint"")
    if config.checkpoint_path is not None and config.hf_checkpoint is not None:
        raise ValueError(""Specify only one of checkpoint_path or hf_checkpoint"")

    mp = config.trainer.mp

    if config.checkpoint_path is not None:
        with use_cpu_device():
            model = eqx.filter_eval_shape(config.model.build, Vocab, key=key)
            model = load_checkpoint(model, config.checkpoint_path, subpath=""model"")
        return model
    else:
        assert hasattr(config.model, ""hf_checkpoint_converter""), ""model config lacks HF loader""
        converter: HFCheckpointConverter = config.model.hf_checkpoint_converter()
        converter = converter.replaced(reference_checkpoint=config.hf_checkpoint, tokenizer=load_tokenizer(config.tokenizer))
        model = converter.load_pretrained(config.model.model_type, ref=config.hf_checkpoint, dtype=mp.compute_dtype)
        return model
",src/levanter/main/sample_lm.py,,1,7
survived,"def get(
    url: str,
    *,
    params: dict | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    """"""Perform a simple HTTP GET request.""""""
    return _call(""GET"", url, params=params, headers=headers, timeout=timeout)
",alpha_factory_v1/af_requests.py,,1,7
survived,"def test_register_and_load(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    meta = _meta()
    reg.register(meta, ""hello {{name}}"", version=""0.1.0"")

    templates = reg.list_templates()
    assert len(templates) == 1
    info = templates[0]
    assert info[""slug""] == ""greet""
    assert info[""current_version""] == ""0.1.0""
    assert info[""versions""][0][""version""] == ""0.1.0""

    content = reg.load_template(""greet"")
    assert content == ""hello {{name}}""
",tests/test_template_registry.py,,1,7
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_search.py,TemplateSearchEngine,1,7
survived,"    async def run() -> None:
        bus.publish(""x"", env)
        await asyncio.sleep(0)  # allow handler task to run
",tests/test_bus_fuzz.py,,1,6
survived,"def test_bus_handles_arbitrary_envelopes(env: messaging.Envelope | types.SimpleNamespace) -> None:
    """"""Publishing arbitrary envelopes should not raise exceptions.""""""

    bus = messaging.A2ABus(config.Settings(bus_port=0))
    received: list[object] = []

    async def handler(e: object) -> None:
        received.append(e)

    bus.subscribe(""x"", handler)

    async def run() -> None:
        bus.publish(""x"", env)
        await asyncio.sleep(0)  # allow handler task to run

    asyncio.run(run())
    assert received",tests/test_bus_fuzz.py,,1,7
survived,"    def count_disclaimers_in_markdown(md_path: Path) -> int:
        content = md_path.read_text(encoding=""utf-8"", errors=""ignore"")
        return """".join(content.split()).count(disclaimer_normalized)
",scripts/verify_disclaimer_snippet.py,,1,6
survived,"def test_single_disclaimer_passes(tmp_path: Path) -> None:
    repo = _create_repo(tmp_path, SNIPPET_TEXT)
    missing, duplicates = verify_disclaimer_snippet.check_repo(repo)
    assert missing == []
    assert duplicates == []
",tests/test_verify_disclaimer_snippet.py,,1,7
survived,"    def count_disclaimers_in_notebook(nb_path: Path) -> int:
        try:
            data = json.loads(nb_path.read_text(encoding=""utf-8""))
        except Exception:
            return 0
        text = """"
        for cell in data.get(""cells"", []):
            if cell.get(""cell_type"") == ""markdown"":
                src = cell.get(""source"", """")
                if isinstance(src, list):
                    text += """".join(src)
                else:
                    text += str(src)
        return """".join(text.split()).count(disclaimer_normalized)
",scripts/verify_disclaimer_snippet.py,,1,6
survived,"def check_gzip_size(path: Path, max_bytes: int = 2 * 1024 * 1024) -> None:
    """"""Exit if gzip-compressed ``path`` exceeds ``max_bytes``.""""""
    compressed = gzip.compress(path.read_bytes())
    if len(compressed) > max_bytes:
        sys.exit(f""gzip size {len(compressed)} bytes exceeds limit"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build/common.py,,1,6
survived,"    def __init__(self, fm, *, buffer_max: int = MAX_BUF, logger: Optional[Callable[[str], None]] = None):
        self.fm = fm
        self.buffer: List[Triplet] = []
        self.buffer_max = buffer_max
        self.temperature = 0.5
        self._baseline = 0.0  # moving baseline for REINFORCE
        self.log = logger or (lambda m: LOG.info(m))
        self._rng = random.Random(RNG_SEED)
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine,1,7
survived,"    def to_json(self) -> str:
        state = {""T"": self.temperature, ""baseline"": self._baseline, ""buffer"": [t.__dict__ for t in self.buffer[-128:]]}
        return json.dumps(state, separators=("","", "":""))
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine,1,7
survived,"def _apply_limits() -> None:
    """"""CPU & memory rlimits inside subprocess.""""""
    try:
        resource.setrlimit(resource.RLIMIT_AS, (MEM_MB << 20, MEM_MB << 20))
        resource.setrlimit(resource.RLIMIT_CPU, (SOFT_T, SOFT_T))
    except Exception:
        pass  # non‑POSIX platforms
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,,1,7
survived,"    def start(self, bus: messaging.A2ABus, ledger: Ledger) -> None:
        self.task = asyncio.create_task(self.loop(bus, ledger))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,AgentRunner,1,7
survived,"    def test_merkle_task(self) -> None:
        tmp = tempfile.TemporaryDirectory()
        led = orchestrator.Ledger(os.path.join(tmp.name, ""l.db""))
        env = messaging.Envelope(""a"", ""b"", {}, 0.0)
        led.log(env)
        asyncio.run(led.broadcast_merkle_root())
        led.start_merkle_task(0.1)
        asyncio.run(asyncio.sleep(0.2))
        asyncio.run(led.stop_merkle_task())
        tmp.cleanup()
",tests/test_insight_orchestrator_features.py,TestLedger,1,6
survived,"            def abort(self, *_a, **_kw):
                raise RuntimeError(""denied"")
",tests/test_insight_orchestrator_features.py,TestMessaging.Ctx,1,6
survived,"    def tearDown(self) -> None:
        asyncio.run(self.bus.stop())
",tests/test_insight_orchestrator_features.py,TestMessaging,1,6
survived,"def forecast_disruptions(
    sectors: Iterable[Sector],
    horizon: int,
    curve: str = ""logistic"",
    *,
    pop_size: int = 6,
    generations: int = 1,
) -> List[TrajectoryPoint]:
    """"""Simulate sector trajectories and disruption events.""""""

    secs = list(sectors)
    results: List[TrajectoryPoint] = []
    for year in range(1, horizon + 1):
        t = year / horizon
        cap = capability_growth(t, curve)
        affected: List[Sector] = []
        for sec in secs:
            if not sec.disrupted:
                sec.energy *= 1.0 + sec.growth
                if thermodynamic_trigger(sec, cap):
                    sec.disrupted = True
                    sec.energy += _innovation_gain(pop_size, generations)
                    affected.append(sec)
        snapshot = [Sector(s.name, s.energy, s.entropy, s.growth, s.disrupted) for s in secs]
        results.append(TrajectoryPoint(year, cap, snapshot))
    return results
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/forecast.py,,1,7
survived,"def _simulate(horizon: int, curve: str, pop_size: int, generations: int) -> list[Any]:
    """"""Run the disruption forecast and return the trajectory.""""""

    secs = [sector.Sector(f""s{i:02d}"") for i in range(pop_size)]
    return forecast.forecast_disruptions(
        secs,
        horizon,
        curve,
        pop_size=pop_size,
        generations=generations,
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/web_app.py,,0,6
survived,"def restrict_wizard():
    if current_user.is_authenticated:
        return
    if not session.get(""wizard_access""):
        return redirect(""/"")
",app/blueprints/wizard/routes.py,,1,6
survived,"def test_dtype_category_annotation_and_check():
    def baz(x: Float[""b""]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(baz, include_extras=True)[""x""])[1]
    assert str(spec.dtype) == ""float""

    B = Axis(""b"", 1)
    arr = NamedArray(jnp.ones((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float[""b""])  # type: ignore
    assert not arr.matches_axes(Int[""b""])  # type: ignore",tests/test_dtype_typing.py,,1,7
survived,"    def raise_for_status(self):
        if self.status_code >= 400:
            raise RuntimeError(""http error"")
",tests/test_openai_bridge_integration.py,DummyResponse,1,6
survived,"def run_demo(args):
    corpus = load_documents(args.data_dir)
    vocab, index = build_vocab(corpus)
    int_corpus = convert_corpus(corpus, index)

    hlda = HierarchicalLDA(
        int_corpus,
        vocab,
        alpha=args.alpha,
        gamma=args.gamma,
        eta=args.eta,
        num_levels=args.num_levels,
        seed=args.seed,
    )

    hlda.estimate(
        args.iterations,
        display_topics=args.display_topics,
        n_words=args.n_words,
        with_weights=False,
    )

    print(""\nFinal topic hierarchy:"")
    hlda.print_nodes(args.n_words, with_weights=False)

    return hlda
",scripts/bbc_demo.py,,1,7
survived,"def test_repeated_event_score_decreases() -> None:
    """"""Repeated events yield lower scores.""""""
    _reset_cache()
    first = cr.reward({}, None, {""event"": 1})
    second = cr.reward({}, None, {""event"": 1})
    assert first == 1.0
    assert 0.0 < second <= 1.0 and second < first",tests/test_curiosity_reward.py,,1,7
survived,"def test_generate_records_telemetry(tmp_path, sample_json_file, monkeypatch):
    runner = CliRunner()
    monkeypatch.setenv(""TMPDIR"", str(tmp_path))
    result = runner.invoke(
        cli, [""--no-sensitive-logs"", ""generate"", ""--spec-file"", str(sample_json_file)]
    )
    assert result.exit_code == 0
    assert ""<redacted>"" in result.output
    db_path = Path(tmp_path) / ""meta_agent_telemetry.db""
    db = TelemetryDB(db_path)
    records = db.fetch_all()
    db.close()
    assert len(records) == 1",tests/integration/test_telemetry_integration.py,,1,7
survived,"def valid_spec_dict():
    return {
        ""task_description"": ""Test agent for CLI"",
        ""inputs"": {""data"": ""string""},
        ""outputs"": {""status"": ""string""},
        ""constraints"": [""Must run quickly""],
        ""technical_requirements"": [""Python 3.10+""],
        ""metadata"": {""test_id"": ""cli-001""},
    }
",tests/integration/test_telemetry_integration.py,,1,7
survived,"def valid_spec_dict():
    return {
        ""task_description"": ""Test agent for CLI"",
        ""inputs"": {""data"": ""string""},
        ""outputs"": {""status"": ""string""},
        ""constraints"": [""Must run quickly""],
        ""technical_requirements"": [""Python 3.10+""],
        ""metadata"": {""test_id"": ""cli-001""},
    }
",tests/integration/test_telemetry_integration.py,,1,7
survived,"def test_router_init_requires_valid_default():
    with pytest.raises(ValueError):
        GuardrailModelRouter({""a"": MockAdapter()}, default_model=""b"")
",tests/test_guardrail_router.py,,1,7
survived,"async def test_build_regex_guardrails_multiple_rules():
    cfg = GuardrailConfig(
        rules=[
            GuardrailRule(name=""a"", pattern=""foo""),
            GuardrailRule(name=""b"", pattern=""bar""),
        ]
    )
    guards = build_regex_guardrails(cfg)
    assert len(guards) == 2

    await guards[0](""nothing here"")
    await guards[1](""nothing here"")

    with pytest.raises(ValueError):
        await guards[0](""foo"")
    with pytest.raises(ValueError):
        await guards[1](""bar"")",tests/test_guardrail_generator.py,,1,8
survived,"async def test_output_guardrail_exception_propagates():
    adapter = MockAdapter()
    router = GuardrailModelRouter({""a"": adapter}, default_model=""a"")

    async def bad_guard(_output: str):
        raise RuntimeError(""bad"")

    router.add_output_guardrail(bad_guard)

    with pytest.raises(RuntimeError):
        await router.invoke(""x"")",tests/test_guardrail_router.py,,1,7
survived,"def markdown_processor():
    def render_markdown(text):
        return _renderer.render(text)
    return dict(render_markdown=render_markdown)
",app/utils/contextProcessor/markdown.py,,1,6
survived,"def test_restart_crashed_agent(monkeypatch: mock.Mock) -> None:
    events: list[str | None] = []

    class DummyLedger:
        def __init__(self, *_a, **_kw) -> None:
            pass

        def log(self, env) -> None:  # type: ignore[override]
            events.append(env.payload.get(""event""))

        def start_merkle_task(self, *_a, **_kw) -> None:
            pass

        async def stop_merkle_task(self) -> None:
            pass

        def close(self) -> None:
            pass

    settings = config.Settings(bus_port=0)
    monkeypatch.setattr(orchestrator, ""Ledger"", DummyLedger)
    monkeypatch.setattr(
        orchestrator.Orchestrator, ""_init_agents"", lambda self: [BoomAgent(self.bus, self.ledger)]
    )

    async def loop_no_catch(self: orchestrator.AgentRunner, bus, ledger) -> None:
        await self.agent.run_cycle()

    async def restart_no_error(self: orchestrator.AgentRunner, bus, ledger) -> None:
        if self.task:
            self.task.cancel()
            with contextlib.suppress(Exception):
                await self.task
        self.agent = self.cls(bus, ledger)
        self.start(bus, ledger)
        self.last_beat = orchestrator.time.time()

    monkeypatch.setattr(orchestrator.AgentRunner, ""loop"", loop_no_catch)
    monkeypatch.setattr(orchestrator.AgentRunner, ""restart"", restart_no_error)

    orch = orchestrator.Orchestrator(settings)
    runner = orch.runners[""boom""]
    start_beat = runner.last_beat

    async def run() -> None:
        await orch.bus.start()
        runner.start(orch.bus, orch.ledger)
        orig_sleep = asyncio.sleep
        with mock.patch.object(
            orchestrator.asyncio,
            ""sleep"",
            new=lambda _t: orig_sleep(0.05),
        ):
            monitor = asyncio.create_task(orch._monitor())
            await orig_sleep(0.2)
            monitor.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await monitor
        if runner.task:
            runner.task.cancel()
            with contextlib.suppress(asyncio.CancelledError, BaseException):
                await runner.task
        await orch.bus.stop()

    asyncio.run(run())

    assert ""restart"" in events
    assert runner.last_beat > start_beat",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,,0,7
survived,"        def start_merkle_task(self, *_a, **_kw) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,DummyLedger,0,7
survived,"        def log(self, env) -> None:  # type: ignore[override]
            events.append(env.payload.get(""event""))
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,DummyLedger,1,6
survived,"    async def loop_no_catch(self: orchestrator.AgentRunner, bus, ledger) -> None:
        await self.agent.run_cycle()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,,0,7
survived,"        def close(self) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,DummyLedger,0,7
survived,"    async def handle(self, _env) -> None:  # pragma: no cover - test helper
        pass
",tests/test_agents.py,FreezeAgent,0,6
survived,"    async def run_cycle(self) -> None:
        await asyncio.sleep(999)
",tests/test_agents.py,FreezeAgent,0,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/machine/x/python/q3.py,,1,6
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Region,1,7
survived,"def test_bincount():
    X = Axis(""X"", 6)
    x = hax.named([0, 1, 1, 2, 3, 1], (X,))
    B = Axis(""B"", 5)

    out = hax.bincount(x, B)
    expected = jnp.bincount(x.array, length=B.size)
    assert out.axes == (B,)
    assert jnp.all(out.array == expected)

    w = hax.arange((X,), dtype=jnp.float32)
    out_w = hax.bincount(x, B, weights=w)
    expected_w = jnp.bincount(x.array, weights=w.array, length=B.size)
    assert jnp.allclose(out_w.array, expected_w)",tests/test_ops.py,,1,7
survived,"def test_cli_transfer_test_invokes(monkeypatch) -> None:
    called = {}

    def fake_run(models, top_n):
        called[""models""] = models
        called[""top_n""] = top_n

    monkeypatch.setattr(tt, ""run_transfer_test"", fake_run)

    res = CliRunner().invoke(cli.main, [""transfer-test"", ""--models"", ""x,y"", ""--top-n"", ""2""])
    assert res.exit_code == 0
    assert called == {""models"": [""x"", ""y""], ""top_n"": 2}",tests/test_transfer_test.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""19""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(19)",benchmarks/poly_mini/task_019.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""12""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(12)",benchmarks/poly_mini/task_012.py,,1,6
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""1""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(1)",benchmarks/poly_mini/task_001.py,,1,6
survived,"def _run_container() -> str:
    cmd = [
        ""docker"",
        ""run"",
        ""--rm"",
        ""-v"",
        f""{ROOT.parent}:/work"",
        ""-w"",
        ""/work"",
        IMAGE,
        ""python"",
        ""benchmarks/run_benchmarks.py"",
    ]
    result = subprocess.run(cmd, check=True, capture_output=True, text=True)
    return result.stdout
",benchmarks/docker_runner.py,,1,6
survived,"def run() -> None:
    n = 20
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_020.py,,1,7
survived,"def run() -> None:
    n = 17
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_017.py,,1,7
survived,"    async def lineage(_: None = Depends(verify_token)) -> list[LineageNode]:
        """"""Return archive lineage information.""""""
        path = Path(os.getenv(""ARCHIVE_PATH"", ""archive.db""))
        arch = Archive(path)
        nodes: list[LineageNode] = []
        for a in arch.all():
            nodes.append(
                LineageNode(
                    id=a.id,
                    parent=a.meta.get(""parent""),
                    diff=a.meta.get(""diff"") or a.meta.get(""patch""),
                    pass_rate=a.score,
                )
            )
        return nodes
",src/interface/api_server.py,,1,7
survived,"def get_default_tools() -> List[Any]:
    """"""Return the hardened default tool-chain.

    The selection is recalculated each time to honour environment variables
    that may change at runtime.  The returned list is safe to mutate.
    """"""
    base: List[Any] = [
        FileSearchTool(max_num_results=5),
        WebSearchTool(),
        run_pytest,
    ]

    # Remote tools (ComputerTool runs in OpenAI's sandbox) need an API key.
    if SDK_AVAILABLE and os.getenv(""OPENAI_API_KEY""):
        base.append(ComputerTool())

    # PythonTool executes *locally* – only enable if user opts in explicitly.
    if SDK_AVAILABLE and os.getenv(""ALPHAFAC_ALLOW_LOCAL_CODE"") == ""1"":
        base.append(PythonTool())

    return base
",alpha_factory_v1/backend/agent_factory.py,,1,7
survived,"    async def _optimise_async(self, sequence: str) -> Dict[str, Any]:
        """"""Minimal GC-content optimisation when heavy libs are absent.""""""
        gc_orig = sequence.count(""G"") + sequence.count(""C"")
        gc_new_seq = sequence.replace(""A"", ""G"")
        gc_new = gc_new_seq.count(""G"") + gc_new_seq.count(""C"")
        delta = (gc_new - gc_orig) / len(sequence)
        return {
            ""optimised_sequence"": gc_new_seq,
            ""delta_stability"": round(delta, 4),
        }
",alpha_factory_v1/backend/agents/biotech_agent.py,BiotechAgent,1,7
survived,"                    def clear(self):
                        self.nodes.clear()
                        self.edges.clear()
",alpha_factory_v1/backend/memory_graph.py,GraphMemory._Stub,1,7
survived,"    def complete(self, prompt: str) -> str:  # noqa: D401
        return self.resp
",alpha_factory_v1/tests/test_planner_agent.py,DummyModel,1,6
survived,"    def to_json(self) -> str:
        return json.dumps(asdict(self), separators=("","", "":""))
",alpha_factory_v1/backend/portfolio.py,Fill,1,7
survived,"    def portfolio_value(self) -> float:
        """"""Current cash + mark-to-market value of the position.""""""
        return self.cash + self.position * self.price
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv,1,7
survived,"    def propose(self)->MiniWorld:
        size=random.randint(5,9)
        obstacles={(random.randint(1,size-2),random.randint(1,size-2)) for _ in range(random.randint(0,size))}
        env=MiniWorld(size,list(obstacles),(size-1,size-1))
        self.pool.append(env)
        return env
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,POETGenerator,1,7
survived,"    def remember(self, obs, reward):
        self.buffer.append((obs,reward))
        if len(self.buffer)>CFG.buffer_limit:
            self.buffer.pop(0)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Learner,1,7
survived,"    def publish(cls, topic: str, msg: dict):
        with cls._lock:
            for cb in list(cls._subs.get(topic, [])):
                try:
                    cb(msg)
                except Exception as exc:  # pragma: no cover
                    LOG.error(""[A2A] handler error on %s: %s"", topic, exc)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,A2ABus,1,7
survived,"def _main():
    p=argparse.ArgumentParser(prog=""alpha_asi_world_model_demo"")
    p.add_argument(""--demo"",action=""store_true"")
    p.add_argument(""--emit-docker"",action=""store_true"")
    p.add_argument(""--emit-helm"",action=""store_true"")
    p.add_argument(""--emit-notebook"",action=""store_true"")
    p.add_argument(""--host"",default=""127.0.0.1"")
    p.add_argument(""--port"",type=int,default=7860)
    args=p.parse_args()
    if args.emit_docker: emit_docker()
    elif args.emit_helm: emit_helm()
    elif args.emit_notebook: emit_notebook()
    elif args.demo:
        uvicorn.run(""alpha_asi_world_model_demo:app"",host=args.host,port=args.port,log_level=""info"")
    else: p.print_help()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,6
survived,"    def reset(self):
        self.agent = (0,0)
        return self._obs()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MiniWorld,1,7
survived,"def mcts_policy(net: MuZeroTiny, obs: np.ndarray, simulations: int = 16) -> int:
    """"""Very small UCB‑based MCTS on top of MuZeroTiny.""""""
    act_dim = 4
    with torch.no_grad():
        h, v0, p0 = net.initial(torch.tensor(obs, device=CFG.device, dtype=torch.float32))
    N = np.zeros(act_dim); W = np.zeros(act_dim)
    P = p0.exp().cpu().numpy()
    for _ in range(simulations):
        a = np.argmax(P * (np.sqrt(N.sum()+1e-8)/(1+N)))
        a_one = F.one_hot(torch.tensor(a), num_classes=act_dim).float().to(CFG.device)
        h2, r, v, p = net.recurrent(h, a_one)
        q = (r+v).item()
        N[a] += 1; W[a] += q
    best = int(np.argmax(W / (N+1e-8)))
    return best
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,6
survived,"    def __init__(self, input_dim: int, hidden: int):
        super().__init__(); self.l = nn.Linear(input_dim, hidden)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Repr,1,7
survived,"            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] ← %s"", cls_name, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Stub,0,6
survived,"async def list_agents(): return list(AGENTS.keys())
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,7
survived,"    def __init__(self, env: MiniWorld):
        self.net = MuZeroTiny(env.size**2, 4).to(CFG.device)
        self.opt = optim.Adam(self.net.parameters(), CFG.lr)
        self.buffer : List[Tuple[np.ndarray,float]]=[]
        self.step_count=0
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Learner,1,8
survived,"            def _safe_call(self,prompt:str,timeout:int=15)->str:
                with concurrent.futures.ThreadPoolExecutor() as ex:
                    fut=ex.submit(lambda:openai.ChatCompletion.create(
                        model=""gpt-4o-mini"",
                        messages=[{""role"":""user"",""content"":prompt}],
                        timeout=timeout))
                    return fut.result().choices[0].message.content
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,LLMPlanner,1,6
survived,"    def test_free_energy(self):
        logp = [math.log(0.7), math.log(0.3)]
        fe = gibbs.free_energy(logp, temperature=1.0, task_cost=1.0)
        probs = [0.7, 0.3]
        entropy = -sum(p * math.log(p) for p in probs)
        expected = 1.0 - entropy
        self.assertAlmostEqual(fe, expected, places=6)
",alpha_factory_v1/tests/test_gibbs.py,TestGibbs,1,7
survived,"    async def _live() -> str:  # noqa: D401
        return ""OK""
",alpha_factory_v1/demos/era_of_experience/agent_experience_entrypoint.py,,1,6
survived,"def discover_domain():
    """"""Attempt to discover the AD domain using realm""""""
    try:
        output = run_cmd(""realm discover 2>/dev/null | awk '/realm.name/ {print $2; exit}'"")
        return output if output else None
    except SystemExit:
        return None
",adconnection_app.py,,1,6
survived,"def run_cmd(cmd):
    """"""Run a shell command and return output and exit code.""""""
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return result.stdout.strip(), result.returncode
",adconnection_gui.py,,1,7
survived,"    def root(self):
        return {""ok"": True}
",tests/test_core/test_decorators/test_guard.py,GuardController,1,7
survived,"    def decorator(obj):
        existing = list(getattr(obj, ""__guards__"", []))
        existing.extend(guards)
        setattr(obj, ""__guards__"", existing)
        return obj
",nest/core/guards.py,,1,6
survived,"        def __call__(self, _prompt: str) -> str:
            return Path(self.patch_file).read_text() if self.patch_file else """"
",tests/test_patcher_core_cli.py,StubAgent,1,7
survived,"def test_scan_via():
    class Module(eqx.Module):
        w: hax.NamedArray

        def with_output(self, x):
            out = x + self.w
            return out, 2 * self.w

        @staticmethod
        def init(named):
            return Module(w=named)

    Block = hax.Axis(""block"", 4)
    E = hax.Axis(""E"", 6)

    named = hax.random.uniform(jax.random.PRNGKey(0), (Block, E))
    m = Stacked.init(Block, Module)(named=named)

    x = hax.random.uniform(jax.random.PRNGKey(1), (E,))
    carry, outs = m.scan_via(Module.with_output)(x)

    expected_carry = x + hax.sum(named, Block)
    expected_outs = 2 * named

    assert hax.all(hax.isclose(carry, expected_carry))
    assert hax.all(hax.isclose(outs, expected_outs))",tests/test_scan.py,,1,7
survived,"    def TellSecret(self):
        return self.secret
",tests/rosetta/transpiler/Python/call-an-object-method.py,Box,1,6
survived,"        def __init__(self, *args, **kwargs):
            pass
",stubs/google_adk/__init__.py,Agent,0,7
survived,"def _tool(*_a, **_kw):
    def _decorator(func):
        return func

    return _decorator
",tests/test_openai_bridge_integration.py,,1,7
survived,"    def run(self) -> None:
        pass
",tests/test_openai_bridge_integration.py,_AgentRuntime,0,7
survived,"def sum3(a: int, b: int, c: int) -> int:
    return a + b + c
",tests/human/py/fun_three_args.py,,1,7
survived,"    async def get_latest(_: None = Depends(verify_token)) -> ResultsResponse | JSONResponse:
        try:
            if _latest_id is None:
                raise HTTPException(status_code=404)
            result = _simulations.get(_latest_id)
            if result is None:
                raise HTTPException(status_code=404)
            return result
        except HTTPException as exc:
            return problem_response(exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def readiness() -> str:
        """"""Check orchestrator background task.""""""

        task = getattr(app_f.state, ""task"", None)
        if task and not task.done():
            return ""ok""
        # If the orchestrator failed to start, return OK for local tests.
        return ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,0,6
survived,"    async def insight(req: InsightRequest, _: None = Depends(verify_token)) -> InsightResponse | JSONResponse:
        """"""Return aggregated forecast data across runs.""""""

        try:
            ids = req.ids or list(_simulations.keys())
            forecasts = [_simulations[i].forecast for i in ids if i in _simulations]
            if not forecasts:
                raise HTTPException(status_code=404)

            year_map: dict[int, list[float]] = {}
            for fc in forecasts:
                for point in fc:
                    year_map.setdefault(point.year, []).append(point.capability)
            agg = [
                InsightPoint(year=year, capability=sum(vals) / len(vals))
                for year, vals in sorted(year_map.items())
            ]
            return InsightResponse(forecast=agg)
        except HTTPException as exc:
            return problem_response(exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"def run_jax_simulation(model, importer, ts, atol, rtol):
    p = jnp.array([importer.sbml.getParameter(pid).getValue() for pid in model.parameter_ids])
    ts_jnp = jnp.asarray(ts, dtype=float)
    zeros = jnp.zeros_like(ts_jnp)
    solver = diffrax.Kvaerno5()
    controller = diffrax.PIDController(rtol=rtol / 1e4, atol=atol / 1e4)
    x, stats = model.simulate_condition(
        p,
        ts_jnp,
        jnp.array([]),
        zeros,
        jnp.zeros_like(ts_jnp, dtype=int),
        jnp.zeros_like(ts_jnp, dtype=int),
        jnp.zeros((ts_jnp.shape[0], 0)),
        jnp.zeros((ts_jnp.shape[0], 0)),
        solver,
        controller,
        diffrax.DirectAdjoint(),
        diffrax.steady_state_event(),
        2 ** 8,
        ret=amici.jax.ReturnValue.x,
    )
    tcl = model._tcl(x[0], p)
    y = jax.vmap(lambda t, xs: model._y(t, xs, p, tcl, jnp.zeros(len(model.observable_ids))))(
        ts_jnp, stats[""x""]
    )
    w = jax.vmap(lambda t, xs: model._w(t, xs, p, tcl))(ts_jnp, stats[""x""])

    class RData(dict):
        __getattr__ = dict.__getitem__

    return RData(
        ts=np.asarray(ts_jnp).copy(),
        y=np.asarray(y).copy(),
        w=np.asarray(w).copy(),
        status=amici.AMICI_SUCCESS,
    )
",tests/testSBMLSuiteJax.py,,1,7
survived,"    def getParameterById(self, pid: str):
        par = self.importer.sbml.getParameter(pid)
        return par.getValue() if par else np.nan
",tests/testSBMLSuiteJax.py,DummyModel,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_multi_join_sort.py,_Group,1,7
survived,"def parse_and_validate_diff(
    diff_text: str,
    repo_dir: str,
    allowed_paths: list[str] | None = None,
) -> str | None:
    """"""Verify the LLM's output is a valid unified diff and meets safety criteria.

    Diffs that exceed ``MAX_DIFF_LINES`` or ``MAX_DIFF_BYTES`` are rejected to
    avoid accidentally applying huge patches.
    """"""
    if not diff_text:
        return None

    lines = diff_text.splitlines()
    if len(lines) > MAX_DIFF_LINES or len(diff_text.encode(""utf-8"")) > MAX_DIFF_BYTES:
        logger.warning(
            ""Diff too large: %s lines, %s bytes"",
            len(lines),
            len(diff_text.encode(""utf-8"")),
        )
        return None

    # Basic unified diff check: should contain lines starting with '+++ ' and '--- '
    if ""+++"" not in diff_text or ""---"" not in diff_text:
        return None  # Not a diff format
    repo_root = Path(repo_dir).resolve()
    allowed = allowed_paths if allowed_paths is not None else ALLOWED_PATHS
    allowed_dirs = (
        [repo_root.joinpath(p).resolve() for p in allowed]
        if allowed
        else [repo_root]
    )

    for line in diff_text.splitlines():
        if line.startswith(""+++ "") or line.startswith(""--- ""):
            m = re.match(r""^[+-]{3} [ab]/(.+)$"", line)
            if m:
                file_path = m.group(1)
                target = (repo_root / file_path).resolve()
                if not target.is_relative_to(repo_root):
                    logger.warning(""Diff outside repository: %s"", file_path)
                    return None
                if not any(target.is_relative_to(d) for d in allowed_dirs):
                    logger.warning(""Diff touches disallowed path: %s"", file_path)
                    return None
    # (Additional checks: e.g., diff length, certain forbidden content can be added here.)
    return diff_text
",alpha_factory_v1/demos/self_healing_repo/agent_core/diff_utils.py,,1,7
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    req_txt = repo_root / ""alpha_factory_v1"" / ""demos"" / ""meta_agentic_tree_search_v0"" / ""requirements.txt""
    lock_file = repo_root / ""alpha_factory_v1"" / ""demos"" / ""meta_agentic_tree_search_v0"" / ""requirements.lock""

    with tempfile.TemporaryDirectory() as tmpdir:
        out_path = Path(tmpdir) / ""requirements.lock""
        pip_compile = shutil.which(""pip-compile"")
        if pip_compile:
            cmd = [pip_compile]
        else:
            cmd = [sys.executable, ""-m"", ""piptools"", ""compile""]
        wheelhouse = os.getenv(""WHEELHOUSE"")
        cmd += [""--quiet""]
        if wheelhouse:
            cmd += [""--no-index"", ""--find-links"", wheelhouse]
        cmd += [""--generate-hashes"", str(req_txt), ""-o"", str(out_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        sys.stdout.write(result.stdout)
        sys.stderr.write(result.stderr)
        if result.returncode != 0:
            return result.returncode
        if not lock_file.exists() or out_path.read_bytes() != lock_file.read_bytes():
            extra = """"
            if wheelhouse:
                extra = f""--no-index --find-links {wheelhouse} ""
            msg = (
                ""alpha_factory_v1/demos/meta_agentic_tree_search_v0/requirements.lock is outdated. Run 'pip-compile ""
                f""{extra}--quiet --generate-hashes alpha_factory_v1/demos/meta_agentic_tree_search_v0/requirements.txt -o ""
                ""alpha_factory_v1/demos/meta_agentic_tree_search_v0/requirements.lock'\n""
            )
            sys.stderr.write(msg)
            return 1
    return 0
",scripts/verify_mats_requirements_lock.py,,1,7
survived,"        def pct(p):
            return data[int(n*p/100)] if n else 0
",alpha_factory_v1/demos/macro_sentinel/simulation_core.py,MonteCarloSimulator,1,7
survived,"        def reset(self, *, seed=None):
            return [0.0]*4, {}
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_StubEnv,1,6
survived,"def main() -> int:
    missing = []
    for pkg in REQUIRED:
        if importlib.util.find_spec(pkg) is None:
            missing.append(pkg)
    if missing:
        print('Missing packages:', ', '.join(missing))
        print('Install with: pip install -r requirements.txt')
        return 1
    print('Environment OK')
    return 0
",check_env.py,,1,7
survived,"            def _runner() -> None:
                nonlocal result
                result = asyncio.run(_run())
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/meta_rewrite.py,,1,6
survived,"        def __call__(self, *_a: object, **_k: object) -> str:
            return diff
",tests/test_llm_client_offline.py,DummyAgent,0,7
survived,"def test_llm_offline_pipeline() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")

        out = page.evaluate(""window.llmChat('hello')"")
        assert not out.startswith('[offline]')
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,,0,6
survived,"def banner(msg: str, color: str = """") -> None:
    """"""Print *msg* in *color* using ANSI codes.""""""
    colors = {
        ""RED"": ""\033[91m"",
        ""GREEN"": ""\033[92m"",
        ""YELLOW"": ""\033[93m"",
        ""RESET"": ""\033[0m"",
    }
    code = colors.get(color.upper(), """")
    reset = colors[""RESET""]
    print(f""{code}{msg}{reset}"")
",scripts/setup_wizard.py,,1,7
survived,"def test_simulator_init_fast(tmp_path: Path) -> None:
    js_out = tmp_path / ""sim.js""
    subprocess.run([
        ""tsc"",
        ""--target"",
        ""es2020"",
        ""--module"",
        ""es2020"",
        SIM_TS,
        ""--outFile"",
        js_out,
    ], check=True)

    script = tmp_path / ""run.mjs""
    script.write_text(
        f""import {{ Simulator }} from '{js_out.resolve().as_posix()}';\n""
        ""const start = performance.now();\n""
        ""const it = Simulator.run({popSize:1,generations:1});\n""
        ""await it.next();\n""
        ""console.log(performance.now()-start);\n"",
        encoding=""utf-8"",
    )
    res = subprocess.run([""node"", script], capture_output=True, text=True, check=True)
    elapsed = float(res.stdout.strip())
    assert elapsed < 70",tests/test_simulator_init.py,,0,7
survived,"def test_manager_starts_and_stops_bus_consumer(monkeypatch: pytest.MonkeyPatch) -> None:
    started = False
    stopped = False

    class DummyBus:
        def __init__(self, *_a: object, **_k: object) -> None:
            pass

        async def start_consumer(self) -> None:
            nonlocal started
            started = True

        async def stop_consumer(self) -> None:
            nonlocal stopped
            stopped = True

        def publish(self, *_a: object, **_kw: object) -> None:
            pass

    async def dummy_run_cycle() -> None:
        return None

    class DummyAgent:
        NAME = ""dummy""
        CYCLE_SECONDS = 0.0
        run_cycle = dummy_run_cycle

    def list_agents(_detail: bool = False) -> list[str]:
        return [""dummy""]

    def get_agent(name: str) -> DummyAgent:
        assert name == ""dummy""
        return DummyAgent()

    def start_background_tasks() -> None:
        pass

    monkeypatch.setattr(""alpha_factory_v1.backend.agent_manager.EventBus"", DummyBus)
    monkeypatch.setattr(""backend.agents.list_agents"", list_agents)
    monkeypatch.setattr(""backend.agents.get_agent"", get_agent)
    monkeypatch.setattr(""backend.agents.start_background_tasks"", start_background_tasks)
    monkeypatch.setattr(""alpha_factory_v1.backend.agent_runner.get_agent"", get_agent)

    mgr = AgentManager({""dummy""}, True, None, 60, 30)

    async def _run() -> None:
        await mgr.start()
        await mgr.stop()

    asyncio.run(_run())

    assert started
    assert stopped",tests/test_agent_manager_consumer.py,,1,7
survived,"        def __init__(self, *_a: object, **_k: object) -> None:
            pass
",tests/test_agent_manager_consumer.py,DummyBus,0,7
survived,"def main(argv: list[str] | None = None) -> None:
    """"""Launch the α‑AGI Insight API server.""""""

    if FastAPI is None or uvicorn is None:
        raise SystemExit(""FastAPI is required to run the α‑AGI Insight API."")

    parser = argparse.ArgumentParser(description=""Run the α‑AGI Insight API"")
    parser.add_argument(""--host"", default=""0.0.0.0"", help=""Bind host"")
    parser.add_argument(""--port"", type=int, default=8000, help=""Bind port"")
    args = parser.parse_args(argv)

    uvicorn.run(app, host=args.host, port=args.port)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,8
survived,"def show_results() -> None:
    """"""Display the last ledger entries.""""""
    path = Path(config.Settings().ledger_path)
    if not path.exists():
        click.echo(""No results found"")
        return
    for line in path.read_text(encoding=""utf-8"").splitlines()[-10:]:
        click.echo(line)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,,1,7
survived,"async def _update_listener(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """"""Handle options update.""""""
    await hass.config_entries.async_reload(entry.entry_id)",custom_components/gree/__init__.py,,1,7
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=""Remove outdated LMDB records"")
    parser.add_argument(""path"", type=Path, help=""Path to LMDB directory"")
    parser.add_argument(
        ""keep"",
        help=""Retention period, e.g. 14d or 24h. Use 'all' to delete every record"",
    )
    args = parser.parse_args()

    rotate_lmdb(args.path, args.keep)
",scripts/rotate_lmdb.py,,1,7
survived,"def rotate_lmdb(path: Path, keep: str) -> None:
    """"""Remove records older than the specified duration.""""""
    env = lmdb.open(str(path), writemap=True, readahead=False, meminit=False)
    if keep == ""all"":
        with env.begin(write=True) as txn:
            cursor = txn.cursor()
            for key, _ in cursor:
                txn.delete(key)
        env.close()
        return

    delta = _parse_duration(keep)
    threshold = datetime.now() - delta

    with env.begin(write=True) as txn:
        cursor = txn.cursor()
        for key, value in list(cursor):
            try:
                record = orjson.loads(value)
            except orjson.JSONDecodeError:
                continue
            if _should_delete(record, threshold):
                txn.delete(key)
    env.close()
",scripts/rotate_lmdb.py,,1,7
survived,"def _should_delete(record: dict[str, Any], threshold: datetime) -> bool:
    """"""Check if the record is older than the threshold.""""""
    timestamp = record.get(""updated_at"") or record.get(""created_at"")
    if not timestamp:
        return False
    try:
        ts = datetime.fromisoformat(timestamp)
    except ValueError:
        return False
    return ts < threshold
",scripts/rotate_lmdb.py,,1,7
survived,"def test_launch_dashboard_missing_gradio(monkeypatch):
    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""gradio"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)
    sys.modules.pop(MODULE, None)
    mod = importlib.import_module(MODULE)
    mod = importlib.reload(mod)

    with pytest.raises(RuntimeError, match=""gradio is required""):
        mod.launch_dashboard()",tests/test_agent_muzero_entrypoint.py,,1,7
survived,"def test_time_dependent_discontinuity_equilibration(tmp_path):
    """"""Time dependent discontinuities are handled during equilibration.""""""

    from amici.antimony_import import antimony2sbml
    from amici.sbml_import import SbmlImporter
    from amici.jax.petab import DEFAULT_CONTROLLER_SETTINGS

    ant_model = """"""
    model time_disc_eq
        x' = piecewise(1, time - sin(time) - 1 < 0, -x)
        x = 0
    end
    """"""

    sbml = antimony2sbml(ant_model)
    importer = SbmlImporter(sbml, from_file=False)
    importer.sbml2jax(""time_disc_eq"", output_dir=tmp_path)

    module = amici._module_from_path(""time_disc_eq"", tmp_path / ""__init__.py"")
    model = module.Model()

    p = jnp.array([1.0])
    x0_full = model._x0(0.0, p)
    tcl = model._tcl(x0_full, p)
    x0 = model._x_solver(x0_full)

    assert len(model._root_cond_fns(p)) > 0
    assert model._known_discs(p).size == 0

    xs, _ = model._eq(
        p,
        tcl,
        x0,
        diffrax.Tsit5(),
        diffrax.PIDController(**DEFAULT_CONTROLLER_SETTINGS),
        diffrax.steady_state_event(
            rtol=1e-8, atol=1e-8, norm=lambda y: jnp.linalg.norm(y)
        ),
        1000,
    )

    assert_allclose(xs[0], 0.0, atol=1e-2)",python/tests/test_jax.py,,1,7
survived,"    async def ws_progress(websocket: WebSocket) -> None:
        """"""Stream year-by-year progress updates to the client.""""""
        await websocket.accept()
        _progress_ws.add(websocket)
        try:
            while True:
                await websocket.receive_text()
        except Exception:
            pass
        finally:
            _progress_ws.discard(websocket)
",src/interface/api_server.py,,1,7
survived,"    async def ws_progress(ws: WebSocket, sim_id: str) -> None:
        await ws.accept()
        idx = 0
        try:
            while True:
                items = _progress.get(sim_id, [])
                while idx < len(items):
                    await ws.send_text(items[idx])
                    idx += 1
                if sim_id in _simulations and idx >= len(items):
                    break
                await asyncio.sleep(0.1)
        finally:
            await ws.close()
",src/interface/api_server.py,,1,7
survived,"def test_replay_missing(tmp_path) -> None:
    with patch.object(cli.config, ""Settings"") as settings:
        settings.return_value.ledger_path = tmp_path / ""led.txt""
        out = CliRunner().invoke(cli.main, [""replay""])
        assert ""No ledger"" in out.output
",tests/test_cli.py,,1,7
survived,"    def readlines(self, *args):
        return self.inputs.split(""\n"")
",scripts/utils/lcb_runner.py,MockStdinWithBuffer,1,6
survived,"def make_function(code: str) -> str:
    try:
        import_stmts = []
        all_other_stmts = []
        astree = ast.parse(code)
        for stmt in astree.body:
            if isinstance(stmt, (ast.Import, ast.ImportFrom)):
                import_stmts.append(stmt)
            else:
                all_other_stmts.append(stmt)

        function_ast = ast.FunctionDef(
            name=""wrapped_function"",
            args=ast.arguments(
                posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]
            ),
            body=all_other_stmts,
            decorator_list=[],
            lineno=-1,
        )
        main_code = (
            import_string
            + ""\n""
            + ast.unparse(import_stmts)  # type: ignore
            + ""\n""
            + ast.unparse(function_ast)  # type: ignore
        )
        return main_code
    except Exception as e:
        return code
",scripts/utils/lcb_runner.py,,1,6
survived,"    def readline(self, *args):
        return self._stringio.readline(*args)
",scripts/utils/lcb_runner.py,MockStdinWithBuffer,1,7
survived,"    async def serve(self) -> None:
        """"""Run the scheduler until quotas are exhausted or queue is empty.""""""
        self.start_time = time.time()
        await self.app.serve()
        # wait for running tasks to finish
        if self.running:
            await asyncio.gather(*self.running, return_exceptions=True)
",src/scheduler.py,SelfImprovementScheduler,1,7
survived,"def edit(path: str | Path, start: int, end: Optional[int], new_code: str) -> None:
    """"""Replace lines ``start:end`` in ``path`` with ``new_code``.""""""
    p = _safe_path(path)
    lines = p.read_text(encoding=""utf-8"", errors=""replace"").splitlines()
    new_lines = new_code.splitlines()
    if end is None:
        end = start
    lines[start:end] = new_lines
    p.write_text(""\n"".join(lines), encoding=""utf-8"")
",src/self_edit/tools.py,,1,7
survived,"    def __init__(self) -> None:
        super().__init__(name=""file_tools"")
",src/self_edit/tools.py,FileToolsADK,1,7
survived,"def test_replace_property(data: str) -> None:
    path = REPO_ROOT / ""tmp_self_edit_prop.txt""
    try:
        path.write_bytes(data.encode())
        count = replace(path, ""a"", ""b"")
        expected, exp_count = re.subn(""a"", ""b"", data, flags=re.MULTILINE)
        assert count == exp_count
        assert path.read_bytes().decode() == expected
    finally:
        if path.exists():
            path.unlink()
",tests/test_self_edit_tools.py,,1,7
survived,"    def view_task(self, *, path: str, start: int = 0, end: Optional[int] = None) -> dict[str, str]:
        return {""text"": view(path, start, end)}
",src/self_edit/tools.py,FileToolsADK,1,6
survived,"    def __init__(self) -> None:
        self._items: list[Candidate] = []
",src/evolve.py,InMemoryArchive,1,7
survived,"    def tearDown(self) -> None:
        agents._WHEEL_PUBKEY = self.orig_pub
        agents._WHEEL_SIGS = self.orig_sigs
",tests/test_verify_wheel.py,VerifyWheelTests,1,7
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    req_txt = repo_root / ""alpha_factory_v1"" / ""requirements.txt""
    lock_file = repo_root / ""alpha_factory_v1"" / ""requirements.lock""

    with tempfile.TemporaryDirectory() as tmpdir:
        out_path = Path(tmpdir) / ""requirements.lock""
        pip_compile = shutil.which(""pip-compile"")
        if pip_compile:
            cmd = [pip_compile]
        else:
            cmd = [sys.executable, ""-m"", ""piptools"", ""compile""]
        cmd += [""--quiet"", ""--generate-hashes"", str(req_txt), ""-o"", str(out_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        sys.stdout.write(result.stdout)
        sys.stderr.write(result.stderr)
        if result.returncode != 0:
            return result.returncode
        if out_path.read_bytes() != lock_file.read_bytes():
            sys.stderr.write(
                ""alpha_factory_v1/requirements.lock is outdated. Run 'pip-compile --quiet --generate-hashes alpha_factory_v1/requirements.txt'\n""
            )
            return 1
    return 0
",scripts/verify_alpha_requirements_lock.py,,1,7
survived,"        def ipfs_handler(route):
            route.fulfill(
                status=200,
                content_type=""application/json"",
                body='{""gen"":0,""pop"":[{""logic"":0,""feasible"":0,""front"":false,""strategy"":""base""}],""rngState"":0}',
            )
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py,,1,6
survived,"def test_cache_cleanup_on_activate() -> None:
    repo = Path(__file__).resolve().parents[1]
    dist = repo / ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist""

    server, thread = _start_server(dist)
    host, port = server.server_address
    url = f""http://{host}:{port}""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            context = browser.new_context()
            context.add_init_script(""caches.open('legacy-cache')"")
            page = context.new_page()
            page.goto(url + ""/index.html"")
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""navigator.serviceWorker.controller !== null"")
            names = page.evaluate(""caches.keys()"")
            assert ""legacy-cache"" not in names
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
    finally:
        server.shutdown()
        thread.join()",tests/test_pwa_offline.py,,0,6
survived,"    def parse_kwargs():
        param = dict()
        for k, v in args.kwargs:
            if v.isdigit():
                param[k] = int(v)
            elif v == 'True' or v == 'true':
                param[k] = True
            elif v == 'False' or v == 'false':
                param[k] = False
            elif isfloat(v):
                param[k] = float(v)
            else:
                param[k] = v
        return param
",label_studio_ml/examples/timeseries_segmenter/_wsgi.py,,1,6
survived,"        def __init__(self, program_id: Any, data: bytes, keys: list[Any]):
            self.data = data
",tests/test_ledger.py,DummyInstr,1,6
survived,"def test_broadcast_merkle_root_uses_async_client() -> None:
    tmp = tempfile.TemporaryDirectory()
    ledger = Ledger(os.path.join(tmp.name, ""l.db""), rpc_url=""http://rpc.test"", broadcast=True)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    root = ledger.compute_merkle_root()

    calls: list[Any] = []

    class DummyClient:
        def __init__(self, url: str) -> None:
            calls.append((""url"", url))

        async def send_transaction(self, tx: Any, *args: Any) -> None:
            calls.append((""sent"", tx.instructions[0].data.decode()))

        async def close(self) -> None:
            pass

    class DummyTx:
        def __init__(self) -> None:
            self.instructions: list[Any] = []

        def add(self, instr: Any) -> ""DummyTx"":
            self.instructions.append(instr)
            return self

    class DummyInstr:
        def __init__(self, program_id: Any, data: bytes, keys: list[Any]):
            self.data = data

    class DummyPk:
        def __init__(self, val: str) -> None:
            pass

    # ensure mock module hierarchy exists for patching
    with (
        mock.patch.dict(
            sys.modules,
            {
                ""solana"": ModuleType(""solana""),
                ""solana.rpc"": ModuleType(""solana.rpc""),
                ""solana.rpc.async_api"": ModuleType(""solana.rpc.async_api""),
            },
        ),
        mock.patch(""solana.rpc.async_api.AsyncClient"", DummyClient, create=True),
        mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
        mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
        mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
        mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
    ):
        asyncio.run(ledger.broadcast_merkle_root())

    assert (""url"", ""http://rpc.test"") in calls
    assert (""sent"", root) in calls
    tmp.cleanup()",tests/test_ledger.py,,1,6
survived,"def summarise_shard(shard_path: str, test_dataset: str, training_dataset: str, attr_key: str) -> dict:
    """"""Return basic overlap statistics for a single attribute shard.""""""

    ids_seen: set[str] = set()
    overlap_ids: set[str] = set()
    with fsspec.open(shard_path, ""rt"", compression=""infer"") as f:
        for line in f:
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            doc_id = rec.get(""id"")
            if doc_id is None:
                continue
            ids_seen.add(doc_id)
            attrs = rec.get(""attributes"", {})
            if attrs.get(attr_key):
                overlap_ids.add(doc_id)

    return {
        ""shard_path"": shard_path,
        ""test_dataset"": test_dataset,
        ""training_dataset"": training_dataset,
        ""ids_seen"": list(ids_seen),
        ""overlap_ids"": list(overlap_ids),
    }
",experiments/train_test_overlap/dolma/aggregate_total.py,,1,7
survived,"    def __init__(
        self, loader: FileSystemLoader, autoescape: Any = None, **_kwargs: Any
    ) -> None:
        self.loader = loader
        self.globals: Dict[str, Any] = {}
",src/jinja2/__init__.py,Environment,1,7
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def export_command(
    db_path: Path,
    fmt: str,
    output_path: Path | None,
    start: str | None,
    end: str | None,
    metrics: tuple[str, ...],
) -> None:
    """"""Export telemetry data from the database.""""""
    db = TelemetryDB(db_path)
    if output_path is None:
        suffix = ""json"" if fmt == ""json"" else ""csv"" if fmt == ""csv"" else fmt
        output_path = Path(f""telemetry_export.{suffix}"")
    if fmt == ""json"":
        db.export_json(output_path, start=start, end=end, metrics=metrics or None)
    elif fmt == ""csv"":
        db.export_csv(output_path, start=start, end=end, metrics=metrics or None)
    else:
        db.export(output_path, fmt=fmt, start=start, end=end, metrics=metrics or None)
    click.echo(f""Exported telemetry to {output_path}"")
    db.close()
",src/meta_agent/cli/main.py,,1,7
survived,"    def export_csv(
        self,
        path: str | Path,
        *,
        start: datetime | str | None = None,
        end: datetime | str | None = None,
        metrics: Iterable[str] | None = None,
        compress: bool | None = None,
    ) -> str:
        """"""Export telemetry to a CSV file with optional compression.""""""
        compress = compress or str(path).endswith(("".gz"", "".gzip""))
        data = self.fetch_all(start=start, end=end, metrics=metrics)
        if not metrics:
            metrics = [""tokens"", ""cost"", ""latency"", ""guardrail_hits""]
        header = [""timestamp"", *metrics]
        open_fn = gzip.open if compress else open
        mode = ""wt""
        with open_fn(path, mode, encoding=""utf-8"", newline="""") as f:
            writer = csv.writer(f)
            writer.writerow(header)
            for row in data:
                writer.writerow([row.get(key, """") for key in header])
        return str(path)
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""model"", nargs=""?"", default=""117M"", help=""GPT-2 model size"")
    parser.add_argument(""--dest"", type=Path, default=Path(""models""), help=""Target directory"")
    args = parser.parse_args()
    try:
        download_openai_gpt2(args.model, args.dest)
    except Exception as exc:
        sys.exit(str(exc))
",scripts/download_openai_gpt2.py,,1,7
survived,"def run_discovery_once() -> None:
    """"""Run all discovery functions exactly once.""""""
    global _DISCOVERY_DONE
    if _DISCOVERY_DONE:
        return
    discover_local()
    discover_entrypoints()
    discover_hot_dir()
    discover_adk()
    _DISCOVERY_DONE = True",alpha_factory_v1/backend/agents/discovery.py,,1,7
survived,"    def run(cfg: TextLogitsConfig):
        import torch_xla.core.xla_model as xm
        import torch_xla.distributed.xla_multiprocessing as xmp

        def _mp_fn(index: int, cfg: TextLogitsConfig, tmp_dir: str):
            dataset = read_dataset(cfg.input_path)
            dataset = dataset.shard(xmp.xrt_world_size(), index)

            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)
            model = AutoModelForCausalLM.from_pretrained(cfg.model_name)
            device = xm.xla_device()
            model.to(device)
            model.eval()

            def _forward(batch):
                tokens = tokenizer(
                    batch[""text""],
                    truncation=True,
                    padding=True,
                    max_length=cfg.max_length,
                    return_tensors=""pt"",
                )
                tokens = {k: v.to(device) for k, v in tokens.items()}
                with torch.no_grad():
                    outputs = model(**tokens)
                xm.mark_step()
                batch[""logits""] = outputs.logits.cpu().tolist()
                return batch

            dataset = dataset.map(
                _forward, batched=True, batch_size=cfg.batch_size
            )

            shard_path = os.path.join(tmp_dir, f""logits_{index}.jsonl.gz"")
            write_dataset(dataset, shard_path)

        with tempfile.TemporaryDirectory() as tmp_dir:
            xmp.spawn(_mp_fn, args=(cfg, tmp_dir))
            import glob
            import datasets

            shard_files = sorted(glob.glob(os.path.join(tmp_dir, ""logits_*.jsonl.gz"")))
            shards = [read_dataset(p) for p in shard_files]
            combined = datasets.concatenate_datasets(shards)
            write_dataset(combined, cfg.output_path)
",marin/generation/logits.py,,1,7
survived,"def classify(n):
    return (""zero"" if n == 0 else (""one"" if n == 1 else ""many""))
",tests/transpiler/x/py/match_full.py,,1,7
survived,"def inc(x):
    return x + k
",tests/transpiler/x/py/pure_global_fold.py,,0,7
survived,"def twoSum(nums, target):
    n = len(nums)
    for i in range(0, n):
        for j in range(i + 1, n):
            if nums[i] + nums[j] == target:
                return [i, j]
    return [-1, -1]
",tests/transpiler/x/py/two-sum.py,,1,6
survived,"def propagate_shocks_to_tickers(shocks: Dict[str, float], *, map_path: str | Path = _MAP_PATH) -> str:
    """"""Propagate ``shocks`` to equity tickers and return the result as JSON.""""""

    mapping = load_sector_equity_map(map_path)
    impacts: Dict[str, float] = {}
    for sector, pct in shocks.items():
        tickers = mapping.get(sector, [])
        for ticker in tickers:
            impacts[ticker] = impacts.get(ticker, 0.0) + float(pct)
    return json.dumps(impacts)
",src/finance/adapter.py,,1,7
survived,"def _log_delta(delta: float, log_file: Path) -> None:
    """"""Append ``delta`` with timestamp to ``log_file`` (JSON list).""""""
    log: list[dict[str, float]]
    if log_file.exists():
        log = json.loads(log_file.read_text())
    else:
        log = []
    log.append({""ts"": time.time(), ""delta"": delta})
    log_file.write_text(json.dumps(log))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_improver.py,,1,6
survived,"def _evaluate(repo_path: Path, metric_file: str) -> float:
    """"""Return the numeric metric stored in ``metric_file`` inside ``repo_path``.""""""
    return float((repo_path / metric_file).read_text().strip())
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_improver.py,,1,7
survived,"    def __getitem__(cls, item: NamedArrayAxesSpec) -> typing.Annotated[""NamedArray"", NamedArrayAxes]:
        axes = _parse_namedarray_axes(item)
        return typing.Annotated[NamedArray, axes]
",src/haliax/core.py,NamedArrayMeta,1,6
survived,"def _data_dir() -> pathlib.Path:
    """"""Return offline CSV directory from env or default.""""""
    return pathlib.Path(os.getenv(""OFFLINE_DATA_DIR"", str(_DEFAULT_DATA_DIR)))
",alpha_factory_v1/demos/macro_sentinel/data_feeds.py,,1,8
survived,"        def embed_image(self, image):
            # Image should still be loaded when embed_image is called
            self.before_unload_image_none = image.image is None or image._image_as_numpy is None
            # simulate embedding
            _ = image.image_as_numpy
            image.unload_numpy_image()
            self.after_unload = image.image is None and image._image_as_numpy is None
            return ""hash""
",tests/inference/models_predictions_tests/test_owlv2.py,DummyOwl,1,6
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,,1,7
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,,1,7
survived,"    def search(
        self,
        query: str,
        *,
        category: str | None = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """"""Search the index using a simple token overlap ranking.""""""
        if not self._index:
            self.ensure_up_to_date()
        tokens = [t.lower() for t in query.split() if t]
        results = []
        for item in self._index:
            meta = item.get(""metadata"", {})
            if category and meta.get(""category"") != category:
                continue
            if tags and not all(t in meta.get(""tags"", []) for t in tags):
                continue
            haystack = "" "".join(
                [
                    item.get(""content"", """"),
                    meta.get(""title"", """"),
                    meta.get(""description"", """"),
                    meta.get(""slug"", """"),
                    "" "".join(meta.get(""tags"", [])),
                ]
            ).lower()
            score = sum(1 for tok in tokens if tok in haystack)
            if score:
                results.append({**item, ""score"": float(score)})
        results.sort(key=lambda r: r[""score""], reverse=True)
        return results[:limit]",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"def test_load_translations_and_translate(monkeypatch):
    monkeypatch.setenv('DEVICONS_LANG', 'fr')
    importlib.reload(devicons)

    translations = devicons.load_translations()
    assert translations == fr.translations

    assert devicons.translate_dir_name('Téléchargements') == 'Downloads'
    assert devicons.translate_dir_name('UnknownDir') == 'UnknownDir'
",tests/test_translations.py,,1,7
survived,"def evaluate(repo_path: Path) -> dict[str, float]:
    """"""Return average RMSE and lead-time for the Sector-Shock-10 dataset.""""""

    ds_dir = repo_path / ""data"" / ""sector_shock_10""
    rmses: list[float] = []
    leads: list[float] = []
    for path in sorted(ds_dir.glob(""*"")):
        if path.suffix not in {"".json"", "".csv""}:
            continue
        caps, shocks = _load_record(path)
        if not caps and not shocks:
            continue
        rmses.append(_rmse(caps, caps))
        leads.append(_lead_time(shocks, shocks))
    if not rmses:
        raise FileNotFoundError(ds_dir)
    return {""rmse"": statistics.mean(rmses), ""lead_time"": statistics.mean(leads)}",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evaluate_econ.py,,1,6
survived,"def _load_record(path: Path) -> tuple[list[float], list[bool]]:
    if path.suffix == "".json"":
        data = json.loads(path.read_text())
        caps = data.get(""capabilities"", [])
        shocks = data.get(""shocks"", [])
        return [float(c) for c in caps], [bool(s) for s in shocks]

    caps: list[float] = []
    shocks: list[bool] = []
    with path.open(newline="""", encoding=""utf-8"") as fh:
        reader = csv.reader(fh)
        rows = list(reader)
        if not rows:
            return caps, shocks
        header = [c.lower() for c in rows[0]]
        values = rows[1] if len(rows) > 1 else rows[0]
        for name, val in zip(header, values):
            if name.startswith(""cap""):
                try:
                    caps.append(float(val))
                except ValueError:
                    continue
            elif name.startswith(""shock""):
                shocks.append(val.strip().lower() in {""1"", ""true"", ""yes""})
    return caps, shocks
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evaluate_econ.py,,1,7
survived,"    def __init__(
        self,
        jobs: Iterable[Job],
        *,
        tokens_quota: int | None = None,
        time_quota: float | None = None,
        interval: str = ""1 second"",
        max_workers: int = 1,
    ) -> None:
        self.queue: asyncio.Queue[Job] = asyncio.Queue()
        self._initial_jobs = list(jobs)
        for job in self._initial_jobs:
            self.queue.put_nowait(job)
        self._results: Dict[Job, float] = {}
        self._stats: Dict[Job, tuple[int, int]] = {}
        self._active_jobs: list[Job] = []
        self._first_round_done = False
        self.tokens_quota = tokens_quota
        self.time_quota = time_quota
        self.max_workers = max_workers
        self.tokens_used = 0
        self.start_time = 0.0
        self.running: Set[asyncio.Task[None]] = set()
        self.app = Rocketry(execution=""async"")

        @self.app.task(every(interval))
        async def _spawn():  # pragma: no cover - Rocketry callback
            await self._spawn_jobs()
",src/scheduler/__init__.py,SelfImprovementScheduler,1,7
survived,"        def __init__(self, program_id: object, data: bytes, keys: list[object]):
            self.data = data
",tests/test_archive.py,DummyInstr,1,6
survived,"    def close(self) -> None:
        if self.conn:
            self.conn.close()
            self.conn = None  # type: ignore[assignment]
",src/archive/service.py,ArchiveService,1,6
survived,"        async def send_transaction(self, tx: object, *args: object) -> None:
            if raise_err:
                raise RuntimeError(""fail"")
            captured[""root""] = tx.instructions[0].data.decode()
",tests/test_archive.py,DummyClient,1,6
survived,"    async def _handle_rpc(self, request: bytes, _ctx: Any) -> bytes:
        data = json.loads(request.decode())
        result = self.score(data.get(""context"", """"), data.get(""response"", """"))
        return json.dumps(result).encode()
",src/critics/dual_critic_service.py,DualCriticService,1,6
survived,"    def __init__(self, docs: Iterable[str] | None = None) -> None:
        self.db = VectorDB(docs)
        self._server: ""grpc.aio.Server"" | None = None
",src/critics/dual_critic_service.py,DualCriticService,1,7
survived,"    def _score(a: str, b: str) -> float:
        a_tokens = set(a.lower().split())
        b_tokens = set(b.lower().split())
        if not a_tokens or not b_tokens:
            return 0.0
        return len(a_tokens & b_tokens) / len(a_tokens | b_tokens)
",src/critics/dual_critic_service.py,VectorDB,1,7
survived,"def create_weekly_scheduler(csv_path: str | Path = ""replay_metrics.csv"") -> Rocketry | None:
    """"""Return a ``Rocketry`` scheduler that emails the weekly report.""""""
    if Rocketry is None or every is None:
        return None
    app = Rocketry(execution=""async"")

    @app.task(every(""1 week""))
    def _report() -> None:  # pragma: no cover - scheduler callback
        weekly_report(csv_path)

    return app
",src/analysis/meta_foresight.py,,1,7
survived,"def test_macro_sentinel_first_cells(tmp_path: Path) -> None:
    """"""Execute the first two code cells of the macro sentinel notebook.""""""
    nb_path = Path(""alpha_factory_v1/demos/macro_sentinel/colab_macro_sentinel.ipynb"")
    assert nb_path.exists(), nb_path

    nb = nbformat.read(nb_path, as_version=4)

    # keep the first two code cells only
    code_cells = [cell for cell in nb.cells if cell.cell_type == ""code""][:2]
    nb.cells = code_cells

    ep = ExecutePreprocessor(timeout=60, kernel_name=""python3"")
    ep.preprocess(nb, {""metadata"": {""path"": str(tmp_path)}})",tests/test_notebooks.py,,0,6
survived,"        def do_POST(self):
            length = int(self.headers.get(""Content-Length"", 0))
            type(self).received_body = self.rfile.read(length)
            type(self).received_headers = dict(self.headers)
            self.send_response(status)
            self.end_headers()
            self.wfile.write(type(self).received_body)
",alpha_factory_v1/tests/test_requests_shim.py,Handler,1,6
survived,"    def test_add_and_search(self):
        mem = mv.VectorMemory(dsn=None)
        mem.add(""agent"", [""hello world"", ""foo""])
        self.assertEqual(len(mem), 2)
        hits = mem.search(""hello world"", k=1)
        self.assertTrue(hits)
        agent, text, score = hits[0]
        self.assertEqual(agent, ""agent"")
        self.assertEqual(text, ""hello world"")
",alpha_factory_v1/tests/test_vector_memory.py,VectorMemoryTest,1,7
survived,"    def test_discover_alpha_invalid_zero_default_model(self) -> None:
        with self.assertRaises(ValueError):
            stub.discover_alpha(num=0, ledger=None)
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha,1,7
survived,"        def __init__(self, host: str, port: int, app_id: str) -> None:
            captured[""a2a""] = f""{host}:{port}""  # pragma: no cover - record args
",tests/test_alpha_agi_business_3_v1.py,DummySock,0,6
survived,"            def __init__(self, program_id: object, data: bytes, keys: list[object]):
                self.data = data
",tests/test_insight_orchestrator_features.py,TestLedger.DummyInstr,1,7
survived,"    def _step(population: Population) -> Population:
        evaluate(population, fn)
        offspring: Population = []
        while len(offspring) < population_size:
            a, b = rng.sample(population, 2)
            cut = rng.randint(1, genome_length - 1)
            child_genome = a.genome[:cut] + b.genome[cut:]
            if rng.random() < mutation_rate:
                idx = rng.randrange(genome_length)
                child_genome[idx] += rng.uniform(-1, 1)
            offspring.append(Individual(child_genome))
        evaluate(offspring, fn)
        union = population + offspring
        fronts = _non_dominated_sort(union)
        new_pop: Population = []
        for front in fronts:
            _crowding(front)
            front.sort(key=lambda x: (-x.rank, -x.crowd))
            for ind in front:
                if len(new_pop) < population_size:
                    new_pop.append(ind)
        return new_pop
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,,1,7
survived,"def _load_config_timeout_minutes() -> int:
    env_val = os.getenv(""ANOMSTACK_KILL_RUN_AFTER_MINUTES"")
    if env_val:
        try:
            return int(env_val)
        except ValueError:
            pass

    dagster_home = Path(os.getenv(""DAGSTER_HOME"", """"))
    if not dagster_home:
        dagster_home = Path.cwd()
    config_path = dagster_home / ""dagster.yaml""
    minutes = DEFAULT_MINUTES
    if config_path.exists():
        try:
            with open(config_path, ""r"", encoding=""utf-8"") as f:
                cfg = yaml.safe_load(f)
            minutes = int(cfg.get(""kill_sensor"", {}).get(""kill_after_minutes"", minutes))
        except Exception:
            pass
    return minutes
",anomstack/sensors/timeout.py,,1,6
survived,"def test_authorize_button_state_mismatch(monkeypatch):
    st.session_state.clear()
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)

    monkeypatch.setattr(oauth.client, ""get_authorization_url"", AsyncMock(return_value=""http://auth""))
    monkeypatch.setattr(oauth.client, ""get_access_token"", AsyncMock(return_value={""access_token"": ""tok""}))
    monkeypatch.setattr(""streamlit_oauth._generate_state"", lambda key=None: ""GOOD"")
    monkeypatch.setattr(""streamlit_oauth._authorize_button"", lambda **kwargs: {""code"": ""CODE"", ""state"": ""BAD""})

    with pytest.raises(StreamlitOauthError):
        oauth.authorize_button(""Login"", ""http://cb"", ""scope"", key=""k"")
",tests/test_oauth_component.py,,1,7
survived,"    def __init__(self, d_model: int, num_heads: int, max_seq_len: int = 512):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.head_dim = d_model // num_heads
        assert (
            d_model % num_heads == 0
        ), ""d_model must be divisible by num_heads""

        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)
        self.dense = nn.Linear(d_model, d_model)

        inv_freq = 1.0 / (
            10000 ** (torch.arange(0, self.head_dim, 2, dtype=torch.float32) / self.head_dim)
        )
        t = torch.arange(max_seq_len, dtype=torch.float32)
        freqs = torch.einsum(""i,j->ij"", t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer(""cos_cached"", emb.cos()[None, None, :, :], persistent=False)
        self.register_buffer(""sin_cached"", emb.sin()[None, None, :, :], persistent=False)

        self._reset_parameters()
",src/model/u2tokenizer/rope.py,RotaryMultiheadAttention,1,7
survived,"    def forward(
        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, *, return_attn: bool = True, **kwargs
    ):
        if ""need_weights"" in kwargs:
            return_attn = kwargs.pop(""need_weights"")
        bsz, seq_len, _ = query.size()

        q = self.wq(query)
        k = self.wk(key)
        v = self.wv(value)

        q = self.split_heads(q, bsz)
        k = self.split_heads(k, bsz)
        v = self.split_heads(v, bsz)

        cos = self.cos_cached[:, :, :seq_len, :].to(q.dtype)
        sin = self.sin_cached[:, :, :seq_len, :].to(q.dtype)
        q = apply_rotary_pos_emb(q, cos, sin)
        k = apply_rotary_pos_emb(k, cos, sin)

        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        context = torch.matmul(attn, v)
        context = context.permute(0, 2, 1, 3).contiguous()
        context = context.view(bsz, seq_len, self.d_model)
        output = self.dense(context)

        if return_attn:
            return output, attn
        return output",src/model/u2tokenizer/rope.py,RotaryMultiheadAttention,1,7
survived,"    def __init__(self, threshold: float) -> None:
        self.threshold = threshold
        self.cost = 0.0
        self.gain = 0.0
        self.success = 1
        self.fail = 1
",alpha_factory_v1/core/simulation/loop.py,BanditEarlyStopper,1,8
survived,"    async def emit(self, recipient: str, payload: Any) -> None:
        env = messaging.Envelope(
            sender=self.name,
            recipient=recipient,
            payload=struct_pb2.Struct(),
            ts=time.time(),
        )
        if isinstance(payload, dict):
            env.payload.update(payload)
        self.ledger.log(env)
        self.bus.publish(recipient, env)
",alpha_factory_v1/core/agents/base_agent.py,BaseAgent,1,7
survived,"def _non_dominated_sort(pop: Population) -> List[Population]:
    """"""Group ``pop`` into Pareto fronts.""""""

    fronts: List[Population] = []
    S: dict[int, list[Individual]] = {id(ind): [] for ind in pop}
    n: dict[int, int] = {id(ind): 0 for ind in pop}
    for ind in pop:
        assert ind.fitness is not None
    for p in pop:
        for q in pop:
            if p is q:
                continue
            assert q.fitness is not None
            assert p.fitness is not None
            if all(pf <= qf for pf, qf in zip(p.fitness, q.fitness)):
                if any(pf < qf for pf, qf in zip(p.fitness, q.fitness)):
                    S[id(p)].append(q)
            elif all(qf <= pf for pf, qf in zip(p.fitness, q.fitness)):
                if any(qf < pf for pf, qf in zip(p.fitness, q.fitness)):
                    n[id(p)] += 1
        if n[id(p)] == 0:
            p.rank = 0
            if not fronts:
                fronts.append([])
            fronts[0].append(p)
    i = 0
    while i < len(fronts):
        nxt: Population = []
        for p in fronts[i]:
            for q in S[id(p)]:
                n[id(q)] -= 1
                if n[id(q)] == 0:
                    q.rank = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return fronts
",alpha_factory_v1/core/simulation/mats.py,,1,7
survived,"def _crowding(pop: Population) -> None:
    """"""Compute the crowding distance for a Pareto front.""""""

    if not pop or pop[0].fitness is None:
        return
    m = len(pop[0].fitness)
    for ind in pop:
        ind.crowd = 0.0
    for i in range(m):
        pop.sort(key=lambda x: (x.fitness or (0.0,) * m)[i])
        first_fit = pop[0].fitness
        last_fit = pop[-1].fitness
        assert first_fit is not None and last_fit is not None
        pop[0].crowd = pop[-1].crowd = float(""inf"")
        fmin = first_fit[i]
        fmax = last_fit[i]
        span = fmax - fmin or 1.0
        for j in range(1, len(pop) - 1):
            prev_fit = pop[j - 1].fitness
            next_fit = pop[j + 1].fitness
            assert prev_fit is not None and next_fit is not None
            prev_f = prev_fit[i]
            next_f = next_fit[i]
            pop[j].crowd += (next_f - prev_f) / span
",alpha_factory_v1/core/simulation/mats.py,,1,7
survived,"    def __init__(
        self,
        name: str,
        bus: messaging.A2ABus,
        ledger: ""Ledger"",
        *,
        backend: str = ""gpt-4o"",
        island: str = ""default"",
    ) -> None:
        self.island = island
        self.backend = backend
        self.name = f""{name}_{island}"" if island != ""default"" else name
        self.bus = bus
        self.ledger = ledger
        self.llm = None
        if backend.startswith(""gpt"") and AgentContext is not None:
            try:
                self.oai_ctx = AgentContext(
                    model=bus.settings.model_name,
                    temperature=bus.settings.temperature,
                    context_window=bus.settings.context_window,
                )
            except TypeError:
                try:
                    self.oai_ctx = AgentContext(
                        model=bus.settings.model_name,
                        temperature=bus.settings.temperature,
                    )
                except Exception:
                    self.oai_ctx = AgentContext()
        else:
            self.oai_ctx = None
            if LLMProvider is not None:
                try:
                    self.llm = LLMProvider(
                        temperature=bus.settings.temperature,
                        max_tokens=bus.settings.context_window,
                    )
                except Exception:
                    self.llm = LLMProvider() if LLMProvider is not None else None
        self.adk = ADKAdapter() if ADKAdapter.is_available() else None
        self.mcp = MCPAdapter() if MCPAdapter.is_available() else None
        self._handler = self._on_envelope
        self.bus.subscribe(self.name, self._handler)
",alpha_factory_v1/core/agents/base_agent.py,BaseAgent,1,7
survived,"def configure() -> None:
    """"""Initialise tracing and metrics if the SDK is installed.""""""
    global tracer, meter
    if trace is None or metrics is None:
        return

    endpoint = os.getenv(""OTEL_EXPORTER_OTLP_ENDPOINT"")
    if endpoint:
        span_exporter = OTLPSpanExporter(endpoint=endpoint)
        metric_exporter = OTLPMetricExporter(endpoint=endpoint)
    else:
        span_exporter = ConsoleSpanExporter()
        metric_exporter = ConsoleMetricExporter()

    resource = Resource.create({""service.name"": ""alpha-insight""})
    provider = TracerProvider(resource=resource)
    provider.add_span_processor(BatchSpanProcessor(span_exporter))
    trace.set_tracer_provider(provider)
    tracer = trace.get_tracer(""alpha_insight"")

    meter_provider = MeterProvider(
        resource=resource,
        metric_readers=[PeriodicExportingMetricReader(metric_exporter)],
    )
    metrics.set_meter_provider(meter_provider)
    meter = metrics.get_meter(""alpha_insight"")
",alpha_factory_v1/core/utils/tracing.py,,1,7
survived,"    def _basic_tool_from_spec(self, spec: Dict[str, Any]) -> GeneratedTool:
        """"""Create a very simple tool implementation directly from a spec.""""""
        name = spec.get(""name"", ""Generated"")
        description = spec.get(""description"", """")
        code = f""""""
import logging

logger_tool = logging.getLogger(__name__)

class {name}Tool:
    def __init__(self, salutation: str = \""Hello\""):
        self.salutation = salutation
        logger_tool.info(f\""{name}Tool initialized with {{self.salutation}}\"")

    def run(self, name: str) -> str:
        logger_tool.info(f\""{name}Tool.run called with {{name}}\"")
        return f\""{{self.salutation}}, {{name}} from {name}Tool!\""

def get_tool_instance():
    logger_tool.info(\""get_tool_instance called\"")
    return {name}Tool()
""""""

        return GeneratedTool(
            name=name,
            description=description,
            specification=spec.get(""specification"", {}),
            code=code,
        )
",src/meta_agent/orchestrator.py,MetaAgentOrchestrator,1,6
survived,"def test_merge_usage_entries_with_new_keys():
    """"""Ensure merging usage entries preserves unseen keys.""""""
    tracker = UsageTracker()

    tracker.add_usage(""model-x"", {""prompt_tokens"": 5})
    tracker.add_usage(""model-x"", {""completion_tokens"": 2})

    total_usage = tracker.get_total_tokens()

    assert total_usage[""model-x""][""prompt_tokens""] == 5
    assert total_usage[""model-x""][""completion_tokens""] == 2",tests/utils/test_usage_tracker.py,,1,7
survived,"def test_first_occurrence_positive() -> None:
    _reset()
    res = {""context"": ""run 5k"", ""time"": ""2025-04-22T07:00:00Z""}
    value = hc.reward(None, None, res)
    assert isinstance(value, float)
    assert 0.0 <= value <= 1.0
",tests/test_habit_consistency_reward.py,,1,7
survived,"async def _devnet_available() -> bool:
    try:
        from solana.rpc.async_api import AsyncClient
    except Exception:
        return False
    try:
        client = AsyncClient(""https://api.devnet.solana.com"")
        await client.get_version()
        await client.close()
        return True
    except Exception:
        return False
",tests/test_devnet_broadcast.py,,1,7
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_safety_guardian_property.py,DummyLedger,1,6
survived,"def _simulate(horizon: int, curve: str, pop_size: int, generations: int) -> list[Any]:
    """"""Run the disruption forecast and return the trajectory.""""""
    secs = [sector.Sector(f""s{i:02d}"") for i in range(pop_size)]
    return cast(
        list[Any],
        forecast.forecast_disruptions(
            secs,
            horizon,
            curve,
            pop_size=pop_size,
            generations=generations,
        ),
    )
",src/interface/minimal_ui.py,,0,6
survived,"def test_standard_json_format():
    source_files = {
        ""src/Contract.sol"": {""content"": ""contract C {}""},
        ""src/Dependency.sol"": {""content"": ""contract D {}""},
    }
    assert is_standard_json_contract(source_files)",tests/test_utils.py,,1,6
survived,"def test_percent_repr_call(state: State):
    s_in = """"""'%s' % repr(var)""""""
    s_expected = """"""f'{var!r}'""""""

    s_out, count = code_editor.fstringify_code_by_line(s_in, state)
    assert s_out == s_expected
",test/test_edits.py,,1,7
survived,"    async def handle(self, env):
        pass",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/planning_agent.py,PlanningAgent,0,7
survived,"    async def run_cycle(self) -> None:  # pragma: no cover - interface
        raise NotImplementedError",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent,1,6
survived,"    async def _start() -> None:
        orch_mod = importlib.import_module(""alpha_factory_v1.demos.alpha_agi_insight_v1.src.orchestrator"")
        app_f.state.orchestrator = orch_mod.Orchestrator()
        app_f.state.task = asyncio.create_task(app_f.state.orchestrator.run_forever())
        token = os.getenv(""API_TOKEN"")
        if not token:
            logging.getLogger(__name__).warning(""API_TOKEN not set; using insecure placeholder"")
            token = API_TOKEN_DEFAULT
        app_f.state.api_token = token
        _load_results()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    def publish(self, topic: str, env: Envelope) -> None:
        for h in list(self._subs.get(topic, [])):
            try:
                res = h(env)
                if asyncio.iscoroutine(res):
                    asyncio.create_task(res)
            except Exception:  # noqa: BLE001
                pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus,1,6
survived,"    async def handle(self, env):
        pass",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/research_agent.py,ResearchAgent,0,7
survived,"def _non_dominated_sort(pop: Population) -> List[Population]:
    fronts: List[Population] = []
    S = {id(ind): [] for ind in pop}
    n = {id(ind): 0 for ind in pop}
    for p in pop:
        for q in pop:
            if p is q:
                continue
            if all(pf <= qf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                if any(pf < qf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                    S[id(p)].append(q)
            elif all(qf <= pf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                if any(qf < pf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                    n[id(p)] += 1
        if n[id(p)] == 0:
            p.rank = 0
            if not fronts:
                fronts.append([])
            fronts[0].append(p)
    i = 0
    while i < len(fronts):
        nxt: Population = []
        for p in fronts[i]:
            for q in S[id(p)]:
                n[id(q)] -= 1
                if n[id(q)] == 0:
                    q.rank = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return fronts
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,,1,7
survived,"    def test_register_and_samples(self) -> None:
        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = MagicMock()

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Tool = _tool

        with patch.dict(sys.modules, {""openai_agents"": stub}):
            sys.modules.pop(
                ""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge"",
                None,
            )
            mod = importlib.import_module(
                ""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge""
            )
            agent = mod.CrossIndustryAgent()
            runtime = mod.AgentRuntime(api_key=None)
            runtime.register(agent)
            runtime.register.assert_called_once_with(agent)

            samples = asyncio.run(mod.list_samples())
            self.assertEqual(samples, mod.SAMPLE_ALPHA)
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime,1,7
survived,"    def test_acreate_uses_timeout(self) -> None:
        response = types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""{}""))])
        openai_mock = types.SimpleNamespace(
            ChatCompletion=types.SimpleNamespace(acreate=AsyncMock(return_value=response))
        )
        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""x""}):
            with patch.object(energy_agent, ""openai"", openai_mock):
                agent = EnergyAgent()
                agent.cfg.openai_enabled = True
                asyncio.run(agent._hedge())
        openai_mock.ChatCompletion.acreate.assert_awaited()
        kwargs = openai_mock.ChatCompletion.acreate.call_args.kwargs
        self.assertEqual(kwargs.get(""timeout""), energy_agent.OPENAI_TIMEOUT_SEC)
",tests/test_energy_agent.py,TestOpenAITimeout,0,7
survived,"    def get_player_location_global(self):
        '''
        get_player_location_global
        '''
        scale_factor = self.cfg.localize_downscale_factor
        # Downscale both template and search image
        img_roi = self.img_frame_gray[self.cfg.camera_ceiling:self.cfg.camera_floor, :]
        img_query = cv2.resize(img_roi, (0, 0), fx=scale_factor, fy=scale_factor)

        # Get previous frame result
        if self.is_first_frame or \
            time.time() - self.t_last_camera_missed > self.cfg.localize_cached_interval:
            last_result = None
            self.t_last_camera_missed = time.time()
        else:
            last_result = (
                int(self.loc_camera[0] * scale_factor),
                int(self.loc_camera[1] * scale_factor)
            )

        loc_camera, score, is_cached = find_pattern_sqdiff(
            self.img_map_resized,
            img_query,
            last_result=last_result,
            local_search_radius=20,
            global_threshold = 0.8)
        self.loc_camera = (
            int(loc_camera[0] / scale_factor),
            int(loc_camera[1] / scale_factor)
        )
        loc_player_global = (
            self.loc_camera[0] + self.loc_player[0],
            self.loc_camera[1] + self.loc_player[1] - self.cfg.camera_ceiling)

        # Draw camera rectangle
        camera_bottom_right = (
            self.loc_camera[0] + self.img_frame.shape[1],
            self.loc_camera[1] + self.img_frame.shape[0]
        )
        cv2.rectangle(self.img_route_debug, self.loc_camera,
                      camera_bottom_right, (0, 255, 255), 2)
        cv2.putText(
            self.img_route_debug,
            f""Camera, score={round(score, 2)}, {'cached' if is_cached else 'missed'}"",
            (self.loc_camera[0], self.loc_camera[1] + 60),
            cv2.FONT_HERSHEY_SIMPLEX, 2,
            (0, 255, 0), 2
        )

        # Draw player center
        cv2.circle(self.img_route_debug,
                   loc_player_global, radius=3,
                   color=(0, 0, 255), thickness=-1)
        cv2.putText(self.img_route_debug, ""Player"",
                    (loc_player_global[0] - 30, loc_player_global[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        return loc_player_global
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"    def update_window_region(self):
        '''
        Update window region
        '''
        self.region = get_window_region(self.window_title)
        if self.region is None:
            text = f""Cannot find window: {self.window_title}""
            logger.error(text)
            raise RuntimeError(text)
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor,1,7
survived,"    def get_nearest_color_code_on_minimap(self):
        '''
        get_nearest_color_code_on_minimap
        '''
        x0, y0 = self.loc_player_minimap
        h, w = self.img_route.shape[:2]
        x_min = max(0, x0 - self.cfg.minimap_color_code_search_range)
        x_max = min(w, x0 + self.cfg.minimap_color_code_search_range)
        y_min = max(0, y0 - self.cfg.minimap_color_code_search_range)
        y_max = min(h, y0 + self.cfg.minimap_color_code_search_range)

        nearest = None
        min_dist = float('inf')
        for y in range(y_min, y_max):
            for x in range(x_min, x_max):
                pixel = tuple(self.img_route[y, x])  # (R, G, B)
                if pixel in self.cfg.color_code:
                    dist = abs(x - x0) + abs(y - y0)
                    if dist < min_dist:
                        min_dist = dist
                        nearest = {
                            ""pixel"": (x, y),
                            ""color"": pixel,
                            ""action"": self.cfg.color_code[pixel],
                            ""distance"": dist
                        }

        # Debug
        draw_rectangle(
            self.img_route_debug,
            (x_min, y_min),
            (self.cfg.minimap_color_code_search_range*2,
             self.cfg.minimap_color_code_search_range*2),
            (0, 0, 255), ""Search Range"",
        )
        # Draw a straigt line from map_loc_player to color_code[""pixel""]
        if nearest is not None:
            cv2.line(
                self.img_route_debug,
                self.loc_player_minimap, # start point
                nearest[""pixel""],       # end point
                (0, 255, 0),            # green line
                1                       # thickness
            )

            # Print color code on debug image
            cv2.putText(
                self.img_frame_debug,
                f""Route Action: {nearest['action']}"",
                (720, 90),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),
                2, cv2.LINE_AA
            )
            cv2.putText(
                self.img_frame_debug, f""Route Index: {self.idx_routes}"",
                (720, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),
                2, cv2.LINE_AA
            )

        return nearest  # if not found return none
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"    def capture_frame(self):
        '''
        捕捉當前遊戲區域畫面
        '''
        img = self.capture.grab(self.region)
        frame = np.array(img)
        with self.lock:
            self.frame = frame
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor,1,7
survived,"def _build_local_site(repo_root: Path) -> bool:
    script = repo_root / ""scripts"" / ""build_gallery_site.sh""
    if not script.is_file():
        return False
    try:
        subprocess.run([str(script)], check=True)
    except Exception:
        return False
    return True
",scripts/open_subdir_demo.py,,1,6
survived,"        def run_generations(self, *_a) -> None:
            pass
",tests/test_aiga_openai_bridge_offline.py,DummyEvolver,0,7
survived,"        def __call__(self, prompt: str) -> str:
            if not self.base_url:
                return ""no base url""
            r = requests.post(
                f""{self.base_url}/chat/completions"",
                json={""model"": ""stub"", ""messages"": [{""role"": ""user"", ""content"": prompt}]},
                timeout=5,
            )
            r.raise_for_status()
            return r.json()[""choices""][0][""message""][""content""]
",tests/test_aiga_openai_bridge_offline.py,OpenAIAgent,1,7
survived,"def run_sync(coro: Awaitable[T]) -> T:
    """"""Run ``coro`` synchronously regardless of event loop state.""""""
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)

    result: list[T] = []

    def _worker() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        task = loop.create_task(coro)
        try:
            result.append(loop.run_until_complete(task))
        finally:
            loop.close()

    t = threading.Thread(target=_worker)
    t.start()
    t.join()
    return result[0]",alpha_factory_v1/backend/utils/sync.py,,1,6
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - test stub
        pass
",tests/test_alert_webhook.py,DummyLedger,0,7
survived,"def extract_summary(lines: list[str], title: str) -> str:
    """"""Return the first descriptive paragraph after the preview image.""""""
    after_preview = False
    paragraph: list[str] = []
    for line in lines:
        if not after_preview:
            if PREVIEW_RE.search(line):
                after_preview = True
            continue
        stripped = line.strip()
        if (
            not stripped
            or stripped.startswith(""#"")
            or stripped.startswith(""["")
            or stripped.startswith(""!"")
            or stripped.startswith(""---"")
            or stripped == title
            or stripped.lower().startswith(""each demo package"")
            or stripped.startswith(""<!--"")
            or stripped.startswith(""-->"")
            or stripped.startswith(""<"")
            or stripped.startswith(""```"")
        ):
            continue
        if stripped.startswith("">""):
            stripped = stripped.lstrip(""> "")
        stripped = re.sub(r""<[^>]+>"", """", stripped)
        paragraph.append(stripped)
        if not stripped or len(paragraph) >= 2:
            break
    return "" "".join(paragraph).strip()
",scripts/generate_gallery_html.py,,1,7
survived,"def test_compose_health(compose_stack: None) -> None:
    assert _wait(""http://localhost:8000/healthz""), ""/healthz endpoint not healthy""
    assert _wait(""http://localhost:8000/readiness""), ""/readiness endpoint not healthy""",tests/test_compose_health.py,,1,7
survived,"    def test_minimum_demo_count(self) -> None:
        base = validate_demos.DEFAULT_DIR
        demos = [
            d
            for d in os.listdir(base)
            if os.path.isdir(os.path.join(base, d))
            and not d.startswith(""."")
            and not d.startswith(""__"")
        ]
        self.assertGreaterEqual(len(demos), 10)
",tests/test_demo_quality.py,TestDemoDirectoryCount,0,6
survived,"    def test_ob_early_data(self):
        """"""Ensure early candles do not cause index errors in OB calculation.""""""
        short_df = pd.DataFrame(
            {
                ""open"": [1.0, 1.1, 1.2],
                ""high"": [1.05, 1.15, 1.25],
                ""low"": [0.95, 1.05, 1.15],
                ""close"": [1.02, 1.14, 1.24],
                ""volume"": [5, 6, 7],
            }
        )
        swing = smc.swing_highs_lows(short_df, swing_length=1)
        ob_df = smc.ob(short_df, swing)
        self.assertEqual(len(ob_df), len(short_df))
",tests/unit_tests.py,TestSmartMoneyConcepts,1,7
survived,"    def view_lines_task(self, *, path: str, start: int, end: Optional[int] = None) -> dict[str, str]:
        return {""text"": view_lines(path, start, end)}
",src/self_edit/tools.py,FileToolsADK,1,6
survived,"def archive_ls(proof: bool, db_path: str) -> None:
    """"""List pinned CIDs.""""""

    arch = HashArchive(db_path)
    entries = arch.list_entries()
    if not entries:
        click.echo(""No archive entries"")
        return
    headers = [""id"", ""cid""]
    rows = [(e[0], e[2]) for e in entries]
    if proof:
        root = arch.merkle_root()
        headers.append(""proof"")
        rows = [(r[0], r[1], root[:16]) for r in rows]
    _rich_table(headers, rows)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,,1,7
survived,"def xor_checksum(address: int, sig, d: bytearray) -> int:
  checksum = 0
  checksum_byte = sig.start_bit // 8
  for i in range(len(d)):
    if i != checksum_byte:
      checksum ^= d[i]
  return checksum",opendbc/car/volkswagen/mqbcan.py,,1,6
survived,"async def query(content, enable_thinking=False):
    if not enable_thinking:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[{""role"": ""user"", ""content"": content}],
            temperature=0.7,
            top_p=0.8,
            presence_penalty=1.5,
            extra_body={
                ""top_k"": 20, 
                ""chat_template_kwargs"": {""enable_thinking"": False},
            },
        )
    else:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[{""role"": ""user"", ""content"": content}],
            temperature=0.6,
            top_p=0.95,
            extra_body={
                ""top_k"": 20, 
                ""chat_template_kwargs"": {""enable_thinking"": True},
            },
        )
    return response.choices[0].message.content.strip(), response.choices[0].message.reasoning_content.strip() if enable_thinking else None
",src/preprocess/thinking_data_synthesis_refine_and_translation.py,,1,7
survived,"def prefer_smart_model() -> ModelPreferences:
    """"""Model preferences optimized for intelligence and capability.""""""

    return ModelPreferences(
        hints=[ModelHint(name=""gpt-4o""), ModelHint(name=""claude-3-opus"")],
        costPriority=0.2,
        speedPriority=0.3,
        intelligencePriority=0.9,
    )",src/enrichmcp/context.py,,1,8
survived,"        def predict(self, x: int) -> int:
            return x
",tests/trace/test_objs_query.py,MyModel,1,7
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,,1,7
survived,"    def execute_and_collect(self, path: Path, timeout: int = 60) -> CollectionResult:
        """"""Run tests via the execution module and gather outputs.""""""
        start = time.perf_counter()
        result = self.execution_module.run_tests(path, timeout=timeout)
        end = time.perf_counter()
        return CollectionResult(
            exit_code=result.exit_code,
            stdout=result.stdout,
            stderr=result.stderr,
            duration=end - start,
        )",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule,1,7
survived,"def test_execute_and_collect_propagates_error(monkeypatch, tmp_path):
    fake_exec = MagicMock()
    fake_exec.run_tests.side_effect = rc_mod.SandboxExecutionError(""boom"")
    module = ResultCollectionModule(fake_exec)
    with pytest.raises(rc_mod.SandboxExecutionError):
        module.execute_and_collect(tmp_path)",tests/unit/test_result_collection_module.py,,1,7
survived,"    def __init__(self, execution_module: Optional[ExecutionModule] = None) -> None:
        self.execution_module = execution_module or ExecutionModule()
",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule,1,7
survived,"def test_summarize_creates_report():
    module = ReportingModule()
    result = make_result()
    report = module.summarize(result)
    assert isinstance(report, SummaryReport)
    assert report.exit_code == 0
    assert report.passed is True
    assert report.duration == 1.23
    assert report.stdout == ""out""
    assert report.stderr == ""err""
",tests/unit/test_reporting_module.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/entropy-1.py,,1,6
survived,"def revInt(n):
    r = 0
    t = n
    while t > 0:
        r = r * 10 + t % 10
        t = int((t // 10))
    return r
",tests/rosetta/transpiler/Python/emirp-primes.py,,1,7
survived,"def dbl(p):
    if isZero(p):
        return p
    L = (3.0 * p.x * p.x) / (2.0 * p.y)
    x = L * L - 2.0 * p.x
    return Pt(x=x, y=L * (p.x - x) - p.y, inf=False)
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,,1,6
survived,"def sub(a, b):
    return a - b
",tests/rosetta/transpiler/Python/element-wise-operations.py,,1,6
survived,"def formatFloat(f, prec):
    scale = pow10(prec)
    scaled = (f * scale) + 0.5
    n = (int(scaled))
    digits = str(n)
    while len(digits) <= prec:
        digits = ""0"" + digits
    intPart = digits[0:len(digits) - prec]
    fracPart = digits[len(digits) - prec:len(digits)]
    return intPart + ""."" + fracPart
",tests/rosetta/transpiler/Python/element-wise-operations.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/empty-program.py,,1,6
survived,"    async def verify_token(
        credentials: HTTPAuthorizationCredentials = Depends(security),
    ) -> None:
        if credentials.credentials != API_TOKEN:
            raise HTTPException(status_code=403, detail=""Invalid token"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def ws_progress(websocket: WebSocket) -> None:
        auth = websocket.headers.get(""authorization"")
        token = getattr(app_f.state, ""api_token"", API_TOKEN_DEFAULT)
        if not auth or not auth.startswith(""Bearer "") or auth.split("" "", 1)[1] != token:
            await websocket.close(code=1008)
            return
        await websocket.accept()
        _progress_ws.add(websocket)
        try:
            while True:
                await websocket.receive_text()
        except Exception:
            pass
        finally:
            _progress_ws.discard(websocket)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def _background_run(sim_id: str, cfg: SimRequest) -> None:
        secs = [sector.Sector(f""s{i:02d}"") for i in range(cfg.pop_size)]
        traj: list[ForecastTrajectoryPoint] = []
        for year in range(1, cfg.horizon + 1):
            t = year / cfg.horizon
            cap = forecast.capability_growth(t)
            for sec in secs:
                if not sec.disrupted:
                    sec.energy *= 1.0 + sec.growth
                    if forecast.thermodynamic_trigger(sec, cap):
                        sec.disrupted = True
                        sec.energy += forecast._innovation_gain(cfg.pop_size, cfg.generations)
            snapshot = [sector.Sector(s.name, s.energy, s.entropy, s.growth, s.disrupted) for s in secs]
            point = forecast.TrajectoryPoint(year, cap, snapshot)
            traj.append(point)
            for ws in list(_progress_ws):
                try:
                    await ws.send_json({""id"": sim_id, ""year"": year, ""capability"": cap})
                except Exception:
                    _progress_ws.discard(ws)
            await asyncio.sleep(0)
        result = ResultsResponse(
            id=sim_id,
            forecast=[ForecastPoint(year=p.year, capability=p.capability) for p in traj],
        )
        _simulations[sim_id] = result
        _save_result(result)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,6
survived,"def get_explorer_chain_id(config):
    chain_id = None
    if ""explorer_chain_id_env_var"" in config:
        chain_id = load_env(
            config[""explorer_chain_id_env_var""], masked=False, required=False
        )
    elif ""explorer_chain_id"" in config:
        chain_id = config[""explorer_chain_id""]
    return chain_id",diffyscan/utils/explorer.py,,1,7
survived,"def ensure_deps() -> None:
    """"""Run the repo's dependency checker with auto-install enabled.""""""
    if os.getenv(""SKIP_DEPS_CHECK"") == ""1"":
        return
    checker = ROOT_DIR.parent / ""check_env.py""
    if checker.exists():
        env = os.environ.copy()
        env[""AUTO_INSTALL_MISSING""] = ""1""
        subprocess.run([sys.executable, str(checker)], env=env, check=False)
",alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py,,1,7
survived,"    async def step(self) -> None:
        if not self.evolver:
            return
        self.evolver.run_generations(1)
        _publish(
            ""aiga.best"",
            {""gen"": self.evolver.gen, ""fitness"": self.evolver.best_fitness},
        )",alpha_factory_v1/backend/agents/aiga_evolver_agent.py,AIGAEvolverAgent,1,7
survived,"            def __init__(self, program_id: Any, data: bytes, keys: list[Any]):
                self.data = data
",tests/test_ledger_broadcast.py,DummyInstr,1,7
survived,"def test_model_with_custom_bool_is_not_replaced(campaign_machine):
    class FalseyModel(MyModel):
        def __bool__(self):
            return False

    model = FalseyModel()
    machine = campaign_machine(model)

    assert machine.model is model
    assert model.state == ""draft""

    machine.produce()
    assert model.state == ""producing""",tests/test_statemachine.py,,1,7
survived,"        def __bool__(self):
            return False
",tests/test_statemachine.py,FalseyModel,1,6
survived,"    def config_mnemonic(cls, mnemonic_guess=None, closematch_cutoff=0.65, expected_len=None):
        """"""Configure globals for SLIP39 share recovery.""""""
        if not mnemonic_guess:
            init_gui()
            if tk_root:
                mnemonic_guess = tk.simpledialog.askstring(
                    ""SLIP39 share"", ""Please enter your best guess for the SLIP39 share:"")
            else:
                print(""No mnemonic guess specified... Exiting..."")
                exit()
            if not mnemonic_guess:
                sys.exit(""canceled"")

        cls._load_wordlist()
        mnemonic_guess = str(mnemonic_guess)

        global mnemonic_ids_guess, close_mnemonic_ids, num_inserts, num_deletes
        mnemonic_ids_guess = ()
        close_mnemonic_ids = {}
        for word in mnemonic_guess.lower().split():
            close_words = difflib.get_close_matches(word, cls._words, sys.maxsize, closematch_cutoff)
            if close_words:
                if close_words[0] != word:
                    print(f""'{word}' was in your guess, but it's not a valid SLIP39 word;\n    trying '{close_words[0]}' instead."")
                mnemonic_ids_guess += cls._word_to_id[close_words[0]],
                close_mnemonic_ids[mnemonic_ids_guess[-1]] = tuple((cls._word_to_id[w],) for w in close_words[1:])
            else:
                if word != 'seed_token_placeholder':
                    print(f""'{word}' was in your guess, but there is no similar SLIP39 word;\n    trying all possible seed words here instead."")
                mnemonic_ids_guess += None,

        if expected_len is None:
            expected_len = len(mnemonic_ids_guess)

        num_inserts = max(expected_len - len(mnemonic_ids_guess), 0)
        num_deletes = max(len(mnemonic_ids_guess) - expected_len, 0)
",btcrecover/btcrseed.py,WalletSLIP39Seed,1,7
survived,"    def visit_For(self, node):
        self.emit(f""for {self.expr(node.target)} in {self.expr(node.iter)} {{"")
        self.indent += 1
        for s in node.body:
            self.visit(s)
        self.indent -= 1
        self.emit(""}"")
",tools/any2mochi/py_simple.py,Conv,1,7
survived,"    def mutable_fields(cls) -> set[str]:
        """"""Return fields marked as mutable.""""""

        def _is_mutable(f: Any) -> bool:
            extra = getattr(f, ""json_schema_extra"", None)
            if extra is None:
                info = getattr(f, ""field_info"", None)
                extra = getattr(info, ""extra"", {}) if info is not None else {}
            return extra.get(""mutable"") is True

        return {
            name
            for name, field in cls.model_fields.items()
            if _is_mutable(field) and name not in cls.relationship_fields()
        }
",src/enrichmcp/entity.py,EnrichModel,1,7
survived,"    def create(
        self,
        func: Callable[..., Any] | None = None,
        *,
        name: str | None = None,
        description: str | None = None,
    ) -> Callable[..., Any] | DecoratorCallable:
        """"""Register a create operation.""""""

        def decorator(fn: Callable[..., Any]) -> Callable[..., Any]:
            return self.resource(fn, name=name, description=description)

        if func is not None:
            return decorator(func)
        return cast(""DecoratorCallable"", decorator)
",src/enrichmcp/app.py,EnrichMCP,1,7
survived,"    def load(self, project: str, note_id: str) -> MemoryNote | None:
        path = self._project_dir(project) / f""{note_id}.md""
        if not path.exists():
            return None
        text = path.read_text(encoding=""utf-8"")
        if not text.startswith(""---""):
            return None
        _, rest = text.split(""---"", 1)
        fm, content = rest.split(""---"", 1)
        meta = yaml.safe_load(fm) or {}
        return MemoryNote(
            id=note_id,
            title=meta.get(""title"", """"),
            tags=meta.get(""tags"", []),
            content=content.lstrip(),
        )
",examples/basic_memory/memory.py,FileMemoryStore,1,7
survived,"    def create_note(self, title: str, content: str, tags: list[str] | None = None) -> MemoryNote:
        note = MemoryNote(id=self.store.new_id(), title=title, content=content, tags=tags or [])
        self.store.save(self.name, note)
        return note
",examples/basic_memory/memory.py,MemoryProject,1,7
survived,"def save_ranking_plot(ranking: List[tuple[str, float]], path: Path) -> None:
    """"""Write a bar chart visualizing the ranking.

    Parameters
    ----------
    ranking:
        List of ``(sector, score)`` tuples sorted by descending score.
    path:
        Target image file path. ``.png`` extension is recommended.
    """"""

    if plt is None:  # pragma: no cover - optional
        return
    if not ranking:
        return

    sectors, scores = zip(*ranking)
    fig, ax = plt.subplots()
    ax.barh(sectors, scores, color=""#1e3a8a"")
    ax.invert_yaxis()
    ax.set_xlabel(""Impact Score"")
    ax.set_title(""AGI Disruption Ranking"")
    fig.tight_layout()
    fig.savefig(path)
    plt.close(fig)
",alpha_factory_v1/demos/alpha_agi_insight_v0/insight_demo.py,,1,7
survived,"    def run_sort(self):
        """"""Sort particle index by cell id and permute buffers accordingly.""""""
        order = torch.argsort(self.particle_index[:, 0])
        self.particle_index = self.particle_index[order]
        self.sorted_position = self.position[order]
        self.sorted_velocity = self.velocity[order]
        self.particle_index_back = order
",pytorch_solver.py,PytorchSolver,1,7
survived,"def test_simple_flow():
    pos = torch.tensor(
        [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.4, 1.0], [2.0, 2.0, 2.0, 1.0]]
    )
    vel = torch.zeros_like(pos)
    cfg = {
        ""xmin"": 0.0,
        ""ymin"": 0.0,
        ""zmin"": 0.0,
        ""hash_grid_cell_size_inv"": 1.0,
        ""grid_cells_x"": 4,
        ""grid_cells_y"": 4,
        ""grid_cells_z"": 4,
        ""grid_cell_count"": 64,
        ""h"": 0.5,
        ""mass_mult_Wpoly6Coefficient"": 1.0,
        ""mass_mult_gradWspikyCoefficient"": 1.0,
        ""rho0"": 1.0,
        ""delta"": 1.0,
        ""time_step"": 0.01,
    }
    solver = PytorchSolver(pos, vel, cfg)
    solver.run_hash_particles()
    solver.run_sort()
    solver.run_index()
    solver.run_index_post_pass()
    solver.run_find_neighbors()
    solver.run_compute_density()
    solver.run_compute_pressure()
    solver.run_compute_pressure_force_acceleration()
    solver.run_integrate()

    # the first two particles should be neighbours
    neigh0 = solver.neighbor_map[0]
    assert 1 in neigh0[:2]
    assert solver.position.shape == pos.shape
    # velocities should change due to gravity
    assert torch.allclose(solver.velocity[0, 1], torch.tensor(-0.098), atol=1e-3)",tests/test_pytorch_solver.py,,1,7
survived,"            def labels(self, *_a: Any, **_kw: Any) -> ""_N"":
                return self
",src/interface/api_server.py,_N,1,7
survived,"def test_safety_agent_emits_status() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(""codegen"", ""safety"", {""code"": ""import os""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_agent_handle_methods.py,,1,7
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:
        self.published.append((topic, env))
",tests/test_agent_handle_methods.py,DummyBus,1,6
survived,"async def _build_task_run_response(task_v2: TaskV2) -> TaskRunResponse:
    """"""Build TaskRunResponse object for webhook backward compatibility.""""""
    workflow_run_resp = None
    if task_v2.workflow_run_id:
        try:
            workflow_run_resp = await workflow_service.get_workflow_run_response(
                task_v2.workflow_run_id, organization_id=task_v2.organization_id
            )
        except Exception:
            LOG.warning(
                ""Failed to get workflow run response for task v2 webhook"",
                exc_info=True,
                task_v2_id=task_v2.observer_cruise_id,
            )

    app_url = None
    if task_v2.workflow_run_id and task_v2.workflow_permanent_id:
        app_url = (
            f""{settings.SKYVERN_APP_URL.rstrip('/')}/workflows/""
            f""{task_v2.workflow_permanent_id}/{task_v2.workflow_run_id}""
        )

    return TaskRunResponse(
        run_id=task_v2.observer_cruise_id,
        run_type=RunType.task_v2,
        status=task_v2.status,
        output=task_v2.output,
        failure_reason=workflow_run_resp.failure_reason if workflow_run_resp else None,
        created_at=task_v2.created_at,
        modified_at=task_v2.modified_at,
        recording_url=workflow_run_resp.recording_url if workflow_run_resp else None,
        screenshot_urls=workflow_run_resp.screenshot_urls if workflow_run_resp else None,
        downloaded_files=workflow_run_resp.downloaded_files if workflow_run_resp else None,
        app_url=app_url,
        run_request=TaskRunRequest(
            engine=RunEngine.skyvern_v2,
            prompt=task_v2.prompt or """",
            url=task_v2.url,
            webhook_url=task_v2.webhook_callback_url,
            totp_identifier=task_v2.totp_identifier,
            totp_url=task_v2.totp_verification_url,
            proxy_location=task_v2.proxy_location,
            data_extraction_schema=task_v2.extracted_information_schema,
            error_code_mapping=task_v2.error_code_mapping,
        ),
    )
",skyvern/services/task_v2_service.py,,1,6
survived,"    def generate(
        self,
        agent_code: str,
        tests: Optional[Mapping[str, str]] = None,
        requirements: Optional[Sequence[str]] = None,
        readme: str = """",
        guardrails_manifest: str = """",
        templates: Optional[Mapping[str, str]] = None,
    ) -> BundleMetadata:
        """"""Generate bundle files and return metadata.""""""

        checksums: dict[str, str] = {}

        checksums[""agent.py""] = self._write_file(""agent.py"", agent_code)

        tests = tests or {}
        if not tests:
            (self.bundle_dir / ""tests"").mkdir(parents=True, exist_ok=True)
        for name, content in tests.items():
            checksums[f""tests/{name}""] = self._write_file(Path(""tests"") / name, content)

        req_content = ""\n"".join(requirements or [])
        checksums[""requirements.txt""] = self._write_file(""requirements.txt"", req_content)

        checksums[""README.md""] = self._write_file(""README.md"", readme)

        (self.bundle_dir / ""traces"").mkdir(parents=True, exist_ok=True)

        (self.bundle_dir / ""guardrails"").mkdir(parents=True, exist_ok=True)
        if guardrails_manifest:
            checksums[""guardrails/manifest.json""] = self._write_file(
                ""guardrails/manifest.json"", guardrails_manifest
            )

        for rel, content in (templates or {}).items():
            checksums[str(rel)] = self._write_file(rel, content)

        metadata = BundleMetadata(meta_agent_version=__version__)
        metadata.custom[""checksums""] = checksums
        with open(self.bundle_dir / ""bundle.json"", ""w"", encoding=""utf-8"") as f:
            json.dump(json.loads(metadata.model_dump_json()), f, indent=2)
        return metadata",src/meta_agent/bundle_generator.py,BundleGenerator,1,7
survived,"    def resolve(
        self, packages: Iterable[str], include_hashes: bool = False
    ) -> Tuple[List[str], Dict[str, str], Optional[Dict[str, str]]]:
        """"""Return pinned requirements and license info for ``packages``.""""""

        pinned: Dict[str, str] = {}
        licenses: Dict[str, str] = {}
        hashes: Optional[Dict[str, str]] = {} if include_hashes else None
        visited: set[str] = set()

        for pkg in packages:
            base = pkg.split(""=="")[0].split("">="")[0].split(""<"")[0]
            base = base.split(""["")[0]
            self._collect_recursive(
                base, pinned, licenses, visited, include_hashes, hashes
            )

        reqs = [f""{name}=={ver}"" for name, ver in sorted(pinned.items())]
        return reqs, licenses, hashes",src/meta_agent/dependency_manager.py,DependencyManager,1,7
survived,"    def _write_detailed_memory_maps(self, f):
        """"""
        写入详细的内存映射分析
        """"""
        f.write(""3. 内存映射详细分析\n"")
        f.write(""-"" * 50 + ""\n"")
        
        process = psutil.Process()
        memory_maps = process.memory_maps()
        
        # 按权限分类
        perm_stats = {}
        file_stats = {}
        
        for mmap in memory_maps:
            size_mb = mmap.size / 1024 / 1024
            perms = mmap.perms
            
            # 按权限统计
            if perms not in perm_stats:
                perm_stats[perms] = {'count': 0, 'size': 0}
            perm_stats[perms]['count'] += 1
            perm_stats[perms]['size'] += size_mb
            
            # 按文件统计
            if mmap.path:
                if mmap.path not in file_stats:
                    file_stats[mmap.path] = {'count': 0, 'size': 0}
                file_stats[mmap.path]['count'] += 1
                file_stats[mmap.path]['size'] += size_mb
        
        f.write(""按权限分类的内存映射:\n"")
        f.write(f""{'权限':<10} {'数量':<8} {'大小(MB)':<12}\n"")
        f.write(""-"" * 35 + ""\n"")
        for perms, stats in sorted(perm_stats.items(), key=lambda x: x[1]['size'], reverse=True):
            f.write(f""{perms:<10} {stats['count']:<8} {stats['size']:<12.2f}\n"")
        
        f.write(f""\n按文件分类的内存映射 (前10个):\n"")
        f.write(f""{'文件路径':<50} {'大小(MB)':<12}\n"")
        f.write(""-"" * 70 + ""\n"")
        for path, stats in sorted(file_stats.items(), key=lambda x: x[1]['size'], reverse=True)[:10]:
            if len(path) > 47:
                path = path[:44] + ""...""
            f.write(f""{path:<50} {stats['size']:<12.2f}\n"")
        
        f.write(""\n"" + ""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper,1,7
survived,"    def _write_memory_leak_detection(self, f):
        """"""
        写入内存泄漏检测
        """"""
        f.write(""5. 内存泄漏检测\n"")
        f.write(""-"" * 50 + ""\n"")
        
        # tracemalloc分析
        current, peak = tracemalloc.get_traced_memory()
        f.write(f""tracemalloc当前内存: {current / 1024 / 1024:.2f} MB\n"")
        f.write(f""tracemalloc峰值内存: {peak / 1024 / 1024:.2f} MB\n"")
        
        try:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            
            f.write(f""\n内存分配最多的位置 (前15个):\n"")
            f.write(""-"" * 50 + ""\n"")
            for i, stat in enumerate(top_stats[:15], 1):
                f.write(f""{i:2d}. {stat.count:>8} 个对象, {stat.size / 1024 / 1024:>8.2f} MB\n"")
                for line in stat.traceback.format():
                    f.write(f""    {line}\n"")
                f.write(""\n"")
        except Exception as e:
            f.write(f""获取tracemalloc统计失败: {e}\n"")
        
        # 垃圾回收分析
        f.write(""垃圾回收分析:\n"")
        f.write(""-"" * 50 + ""\n"")
        gc_counts = gc.get_count()
        f.write(f""GC计数: {gc_counts}\n"")
        
        # 检查不可达对象
        unreachable = len(gc.garbage)
        f.write(f""不可达对象数量: {unreachable}\n"")
        if unreachable > 0:
            f.write(""不可达对象详情:\n"")
            for i, obj in enumerate(gc.garbage[:5], 1):  # 只显示前5个
                f.write(f""  {i}. {type(obj).__name__} - {id(obj)}\n"")
        
        f.write(""\n"" + ""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper,1,7
deleted,"    def avoid_incorrect_reward(prompt, response, answer, state):
        """"""Penalize if response contains known incorrect answers.""""""
        info = state.get(""info"", {})
        incorrect_answers = info.get(""incorrect_answers"", [])
        
        response_lower = response.lower()
        
        # Check if any incorrect answer is present
        for incorrect in incorrect_answers:
            if incorrect.lower() in response_lower:
                return 0.0  # Heavy penalty for including incorrect information
        
        return 1.0  # No incorrect information found
",environments/truthful_qa/truthful_qa.py,,1,7
survived,"    def make_decision(self, scenario: str, available_actions: List[str]) -> Dict[str, Any]:
        """"""
        Make a decision using the o3 model with tool calls.
        
        Args:
            scenario: Description of the current situation
            available_actions: List of possible actions to choose from
            
        Returns:
            Dictionary containing the chosen action and reasoning
        """"""
        
        # Define the tool for action selection
        tools = [{
            ""type"": ""function"",
            ""name"": ""select_action"",
            ""description"": ""Select the best action from the available options."",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""action"": {
                        ""type"": ""string"",
                        ""description"": ""The selected action from the available options""
                    },
                    ""reasoning"": {
                        ""type"": ""string"", 
                        ""description"": ""Detailed reasoning for why this action was chosen""
                    }
                },
                ""required"": [""action"", ""reasoning""],
                ""additionalProperties"": False
            }
        }]
        
        # Create the prompt
        system_prompt = create_decision_prompt(scenario, available_actions)
        user_message = f""Select the best action from these options: {available_actions}. Provide your reasoning and make your choice.""
        
        print(f""{self.color}Making decision with o3 model...{RESET_COLOR}"")
        print(f""{self.color}Scenario: {scenario}{RESET_COLOR}"")
        print(f""{self.color}Available actions: {available_actions}{RESET_COLOR}"")
        
        # Make the API call using the Responses API
        response = client.responses.create(
            model=self.model,
            input=[
                {""role"": ""system"", ""content"": system_prompt},
                {""role"": ""user"", ""content"": user_message}
            ],
            tools=tools,
            tool_choice=""required""
        )
        
        # Process the response
        tool_call = None
        reasoning_text = """"
        
        for output_item in response.output:
            if output_item.type == 'function_call':
                tool_call = output_item
            elif output_item.type == 'message' and hasattr(output_item, 'content'):
                for content in output_item.content:
                    if hasattr(content, 'text'):
                        reasoning_text += content.text
                        print(f""{self.color}Reasoning: {content.text}{RESET_COLOR}"")
        
        if tool_call:
            args = json.loads(tool_call.arguments)
            chosen_action = args[""action""]
            reasoning = args[""reasoning""]
            
            print(f""{self.color}Chosen action: {chosen_action}{RESET_COLOR}"")
            print(f""{self.color}Tool reasoning: {reasoning}{RESET_COLOR}"")
            
            return {
                ""action"": chosen_action,
                ""reasoning"": reasoning,
                ""full_reasoning"": reasoning_text,
                ""available_actions"": available_actions,
                ""scenario"": scenario
            }
        else:
            print(f""{self.color}No tool call found, using fallback{RESET_COLOR}"")
            return {
                ""action"": available_actions[0] if available_actions else ""no_action"",
                ""reasoning"": ""Fallback: No tool call received"",
                ""full_reasoning"": reasoning_text,
                ""available_actions"": available_actions,
                ""scenario"": scenario
            }
",examples/openai/o3_responses_example.py,O3DecisionAgent,1,8
survived,"    def returned_run(test_api_client: Any, mock_storage: Mock):
        run = task_run_ser(id=str(uuid7()), task_uid=1, task_schema_id=1, status=""success"")
        mock_storage.task_runs.fetch_task_run_resource.return_value = run
        mock_storage.tasks.get_task_info.return_value = TaskInfo(task_id=""bla"", uid=2)
        return run
",api/api/routers/runs_v1_test.py,TestGetRunByID,1,6
survived,"def generate_urls(
    config: Config, db: DB, import_id: str, distributable_url: str, logger: Logger
) -> list[str]:
    """"""For a pkgx import_id, generate a list of URLs it could have""""""
    urls: set[str] = set()

    # homepage
    similar = [config.package_managers.debian, config.package_managers.homebrew]
    maybe: list[str] = guess(db, similar, import_id)

    if maybe:
        homepage = maybe[0]
    else:
        homepage = ask_pkgx(import_id)

        if not homepage:
            homepage = special_case(import_id, logger)

    if homepage:
        canonical_homepage = canonicalize(homepage)
        urls.add(canonical_homepage)

    # source
    # NOTE: for non-GitHub source URLs, pkgx tells you where the version string for the
    # downloadable tarball is...right now, we don't do anything about that
    canonical_distributable = canonicalize(distributable_url)
    urls.add(canonical_distributable)

    return list(urls)",package_managers/pkgx/url.py,,1,7
survived,"    def diff_url(
        self, import_id: str, pkg: PkgxPackage, new_urls: dict[tuple[str, UUID], URL]
    ) -> dict[UUID, UUID]:
        """"""Given a package's URLs, returns the resolved URL for this specific package""""""
        resolved_urls: dict[UUID, UUID] = {}

        # Collect all URLs from the package
        urls_to_process = []

        # Add homepage URL if it exists
        homepage = self._get_homepage_url(import_id, pkg)
        if homepage:
            urls_to_process.append((homepage, self.config.url_types.homepage))

        # Add source URLs from distributables
        for distributable in pkg.distributable:
            if distributable.url:
                clean_url = self._canonicalize_url(distributable.url)
                if clean_url:
                    urls_to_process.append((clean_url, self.config.url_types.source))

                    # If it's a GitHub URL, also add as repository
                    if self._is_github_url(clean_url):
                        urls_to_process.append(
                            (clean_url, self.config.url_types.repository)
                        )

        # Process each URL
        for url, url_type in urls_to_process:
            url_key = URLKey(url, url_type)
            resolved_url_id: UUID

            if url_key in new_urls:
                resolved_url_id = new_urls[url_key].id
            elif url_key in self.caches.url_map:
                resolved_url_id = self.caches.url_map[url_key].id
            else:
                self.logger.debug(f""URL {url} for {url_type} is entirely new"")
                new_url = URL(
                    id=uuid4(),
                    url=url,
                    url_type_id=url_type,
                    created_at=self.now,
                    updated_at=self.now,
                )
                resolved_url_id = new_url.id
                new_urls[url_key] = new_url

            resolved_urls[url_type] = resolved_url_id

        return resolved_urls
",package_managers/pkgx/diff.py,PkgxDiff,1,7
survived,"def canonicalize(url: str) -> str:
    return normalize_url(url)
",package_managers/pkgx/url.py,,0,7
survived,"    def test_dependency_type_priority_new_package(self, mock_config, mock_logger):
        """"""Test case 4: p1 has no dependencies to p2 in cache,
        p1 has both runtime and build dependencies to p2 in parsed data.
        Expect one new runtime dependency (priority over build).""""""

        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""pkgx/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""pkgx/p2"", name=""p2"", import_id=""p2"")

        cache = Cache(
            package_map={""p1"": p1_pkg, ""p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={},  # No existing dependencies
        )

        # Parsed data has both runtime and build dependencies to p2
        new_pkg_data = create_pkgx_package(
            dependencies=[""p2""],  # runtime
            build_deps=[""p2""],  # build
        )

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""p1"", new_pkg_data)

        # Should only create one new dependency - runtime (higher priority)
        assert len(removed_deps) == 0
        assert len(new_deps) == 1
        assert new_deps[0].dependency_id == p2_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.runtime
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading,1,7
survived,"    def __init__(self, logger_name: str, config: Config):
        super().__init__(logger_name)
        self.config = config
",package_managers/pkgx/db.py,PkgxDB,1,7
survived,"    def test_sort_by_run_count_asc(self):
        """"""Test sorting by run count ascending (lowest first).""""""
        agents = [
            create_test_agent(""agent1"", run_count=5),
            create_test_agent(""agent2"", run_count=100),
            create_test_agent(""agent3"", run_count=50),
            create_test_agent(""agent4"", run_count=100),  # Same count as agent2
        ]

        sorted_agents = sort_agents(agents, ""run_count"", ""asc"")

        # Lowest count first
        assert [a.agent_id for a in sorted_agents] == [""agent1"", ""agent3"", ""agent2"", ""agent4""]
",api/api/routers/mcp/_utils/agent_sorting_test.py,TestSortAgents,1,7
survived,"    def test_sort_by_cost_desc(self):
        """"""Test sorting by combined cost (highest first).""""""
        models: list[ConciseModelResponse | ConciseLatestModelResponse] = [
            create_test_model(""model1"", cost_per_input_token_usd=0.002, cost_per_output_token_usd=0.003),  # 0.005 total
            create_test_model(""model2"", cost_per_input_token_usd=0.001, cost_per_output_token_usd=0.001),  # 0.002 total
            create_test_model(""model3"", cost_per_input_token_usd=0.001, cost_per_output_token_usd=0.002),  # 0.003 total
        ]

        sorted_models = sort_models(models, ""cost"", ""desc"")

        assert [m.id for m in sorted_models] == [""model1"", ""model3"", ""model2""]
",api/api/routers/mcp/_utils/model_sorting_test.py,TestSortModels,1,8
survived,"    def test_sort_by_last_active_at_asc_basic(self):
        """"""Test sorting by last active at ascending (oldest first) with basic scenarios.""""""
        agents = [
            create_test_agent(""agent1"", last_active_ats=[""2024-01-01T00:00:00""]),
            create_test_agent(""agent2"", last_active_ats=[""2024-01-03T00:00:00""]),
            create_test_agent(""agent3"", last_active_ats=[""2024-01-02T00:00:00""]),
        ]

        sorted_agents = sort_agents(agents, ""last_active_at"", ""asc"")

        assert [a.agent_id for a in sorted_agents] == [""agent1"", ""agent3"", ""agent2""]
",api/api/routers/mcp/_utils/agent_sorting_test.py,TestSortAgents,1,7
survived,"    def column_list_formats(self) -> Dict[str, str]:
        """"""Get column list formats from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_list_formats""
        ]
        if not config:
            return {}
        return config[0].get(""value"", {})
",keep/api/models/db/preset.py,PresetDto,1,7
survived,"def test_semantic_unnest_recursive():
    """"""Test semantic unnest operation with recursive unnesting.""""""
    df = pd.DataFrame({
        ""id"": [1],
        ""nested"": [[[1, 2], [3, 4]], [[5, 6]]]
    })
    
    result = df.semantic.unnest(
        unnest_key=""nested"",
        recursive=True,
        depth=2
    )
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) > 1  # Should create multiple rows
    assert ""nested"" in result.columns
    
    # Check that deeply nested values are flattened
    nested_values = result[""nested""].tolist()
    assert [1, 2] in nested_values or 1 in nested_values
",tests/test_pandas_accessors.py,,1,7
survived,"def test_semantic_gather_with_peripheral():
    """"""Test semantic gather operation with peripheral chunks configuration.""""""
    # Create test data with more chunks for context
    df = pd.DataFrame({
        ""doc_id"": [""doc1""] * 5,
        ""chunk_num"": [1, 2, 3, 4, 5],
        ""content"": [f""Chunk {i} content"" for i in range(1, 6)]
    })
    
    result = df.semantic.gather(
        content_key=""content"",
        doc_id_key=""doc_id"",
        order_key=""chunk_num"",
        peripheral_chunks={
            ""previous"": {""head"": {""count"": 1}, ""tail"": {""count"": 1}},
            ""next"": {""head"": {""count"": 1}, ""tail"": {""count"": 1}}
        }
    )
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(df)
    assert ""content_rendered"" in result.columns
    
    # Check that middle chunks have context from surrounding chunks
    middle_chunk = result[result[""chunk_num""] == 3].iloc[0]
    rendered = middle_chunk[""content_rendered""]
    
    # Should contain previous and next context markers
    assert ""--- Previous Context ---"" in rendered
    assert ""--- Next Context ---"" in rendered
    assert ""--- Begin Main Chunk ---"" in rendered
    assert ""--- End Main Chunk ---"" in rendered
",tests/test_pandas_accessors.py,,1,7
survived,"def test_new_operations_in_history():
    """"""Test that new operations are properly recorded in history.""""""
    df = pd.DataFrame({
        ""content"": [""Some text to split""],
        ""tags"": [[""a"", ""b""]]
    })
    
    # Test split operation history
    split_result = df.semantic.split(
        split_key=""content"",
        method=""token_count"",
        method_kwargs={""num_tokens"": 3}
    )
    
    assert len(split_result.semantic.history) == 1
    assert split_result.semantic.history[0].op_type == ""split""
    assert ""content_chunk"" in split_result.semantic.history[0].output_columns
    
    # Test unnest operation history
    unnest_result = split_result.semantic.unnest(unnest_key=""tags"")
    
    assert len(unnest_result.semantic.history) == 2
    assert unnest_result.semantic.history[1].op_type == ""unnest""
    
    # Test gather operation history (need appropriate data structure)
    gather_df = pd.DataFrame({
        ""doc_id"": [""doc1"", ""doc1""],
        ""chunk_num"": [1, 2],
        ""content"": [""chunk1"", ""chunk2""]
    })
    
    gather_result = gather_df.semantic.gather(
        content_key=""content"",
        doc_id_key=""doc_id"", 
        order_key=""chunk_num""
    )
    
    assert len(gather_result.semantic.history) == 1
    assert gather_result.semantic.history[0].op_type == ""gather""
    assert ""content_rendered"" in gather_result.semantic.history[0].output_columns
",tests/test_pandas_accessors.py,,1,7
survived,"    def test_cost_report_with_corrupted_resources_data(self):
        """"""Test cost report handles corrupted/unpicklable resources data.""""""
        mock_cluster_record = {
            'name': 'corrupted-cluster',
            'status': None,
            'num_nodes': 1,
            'resources': mock.Mock(),
            'total_cost': 0.0,
            'launched_at': 1640995200,
            'duration': 1800,
            'cluster_hash': 'def456',
            'usage_intervals': [(1640995200, 1640997000)],
            'user_hash': 'user456',
            'user_name': 'testuser2',
            'workspace': 'default',
        }
        
        # Mock resources with missing/invalid attributes
        mock_cluster_record['resources'].instance_type = None
        mock_cluster_record['resources'].cloud = None
        
        with mock.patch('sky.global_user_state.get_clusters_from_history', 
                      return_value=[mock_cluster_record]):
            
            # Should handle gracefully and not crash
            result = core.cost_report(days=30)
            self.assertIsInstance(result, list)
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness,1,7
survived,"    def test_cost_report_custom_days(self):
        """"""Test cost_report with custom days parameter.""""""
        with mock.patch('sky.global_user_state.get_clusters_from_history') as mock_get_history:
            mock_get_history.return_value = []
            
            result = core.cost_report(days=7)
            
            # Should call with custom 7 days
            mock_get_history.assert_called_once_with(days=7)
            self.assertEqual(result, [])
",tests/unit_tests/test_sky_cost_report.py,TestCostReportCore,1,7
survived,"    def test_cost_report_with_pickle_errors(self):
        """"""Test cost_report handles pickle errors gracefully when loading historical data.""""""
        import pickle
        
        # Mock get_clusters_from_history to simulate pickle errors being handled internally
        with mock.patch('sky.global_user_state.get_clusters_from_history') as mock_get_history:
            # Simulate the function handling pickle errors gracefully and returning empty list
            mock_get_history.return_value = []
            
            # Even if there are pickle errors internally, the function should not crash
            result = core.cost_report(days=30)
            
            self.assertEqual(result, [])
            mock_get_history.assert_called_once_with(days=30)
",tests/unit_tests/test_sky_cost_report.py,TestCostReportCore,0,6
survived,"    def test_cost_report_with_empty_usage_intervals(self):
        """"""Test cost report handles clusters with empty or malformed usage intervals.""""""
        mock_cluster_record = {
            'name': 'empty-intervals-cluster',
            'status': None,
            'num_nodes': 1,
            'resources': mock.Mock(),
            'total_cost': 0.0,
            'launched_at': None,  # Missing launch time
            'duration': 0,
            'cluster_hash': 'ghi789',
            'usage_intervals': [],  # Empty intervals
            'user_hash': 'user789',
            'user_name': 'testuser3',
            'workspace': 'default',
        }
        
        mock_cluster_record['resources'].instance_type = 'valid-type'
        mock_cluster_record['resources'].cloud = mock.Mock()
        mock_cluster_record['resources'].cloud.__str__ = lambda: 'gcp'
        
        with mock.patch('sky.global_user_state.get_clusters_from_history', 
                      return_value=[mock_cluster_record]):
            
            # Should handle gracefully
            result = core.cost_report(days=30)
            self.assertEqual(len(result), 1)
            self.assertEqual(result[0]['name'], 'empty-intervals-cluster')
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness,1,7
survived,"    def _create_alert(**properties):
        alert_data = {
            ""id"": ""test-alert-1"",
            ""source"": [""prometheus""],
            ""name"": ""test-alert"",
            ""status"": AlertStatus.FIRING,
            ""severity"": AlertSeverity.INFO,
            ""lastReceived"": datetime.datetime.now().isoformat(),
            ""fingerprint"": f""test-fingerprint-{datetime.datetime.now().timestamp()}"",
        }
        alert_data.update(properties)
        return AlertDto(**alert_data)
",tests/test_workflow_severity_comparisons.py,,1,7
survived,"def test_severity_less_than_high(
    db_session, workflow_manager, create_workflow, create_alert
):
    """"""Test severity < 'high' comparisons work correctly with numeric conversion""""""
    workflow = create_workflow(""test-severity-lt-high"", ""severity < 'high'"")

    # Should match: info, low, warning
    info_alert = create_alert(severity=AlertSeverity.INFO, fingerprint=""fp-info"")
    low_alert = create_alert(severity=AlertSeverity.LOW, fingerprint=""fp-low"")
    warning_alert = create_alert(severity=AlertSeverity.WARNING, fingerprint=""fp-warning"")

    # Should NOT match: high, critical
    high_alert = create_alert(severity=AlertSeverity.HIGH, fingerprint=""fp-high"")
    critical_alert = create_alert(severity=AlertSeverity.CRITICAL, fingerprint=""fp-critical"")

    # Test matching severities
    for alert in [info_alert, low_alert, warning_alert]:
        workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
        workflow_manager.insert_events(SINGLE_TENANT_UUID, [alert])
        assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1

    # Test non-matching severities  
    for alert in [high_alert, critical_alert]:
        workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
        workflow_manager.insert_events(SINGLE_TENANT_UUID, [alert])
        assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before
",tests/test_workflow_severity_comparisons.py,,1,7
survived,"def create_debian_package(
    package: str = ""test-package"",
    description: str = ""Test package"",
    homepage: str = """",
    vcs_git: str = """",
    vcs_browser: str = """",
    directory: str = """",
    filename: str = """",
    depends: list[str] | None = None,
    build_depends: list[str] | None = None,
    recommends: list[str] | None = None,
    suggests: list[str] | None = None,
) -> DebianData:
    """"""Helper to create DebianData instances for testing""""""

    debian_data = DebianData()
    debian_data.package = package
    debian_data.description = description
    debian_data.homepage = homepage
    debian_data.vcs_git = vcs_git
    debian_data.vcs_browser = vcs_browser
    debian_data.directory = directory
    debian_data.filename = filename

    # Convert string dependencies to Depends objects
    if depends:
        debian_data.depends = [Depends(package=dep, semver=""*"") for dep in depends]
    if build_depends:
        # build_depends is now list[Depends] like other dependency fields
        debian_data.build_depends = [
            Depends(package=dep, semver=""*"") for dep in build_depends
        ]
    if recommends:
        debian_data.recommends = [
            Depends(package=dep, semver=""*"") for dep in recommends
        ]
    if suggests:
        debian_data.suggests = [Depends(package=dep, semver=""*"") for dep in suggests]

    return debian_data",tests/package_managers/debian/conftest.py,,1,7
survived,"    def diff_deps(
        self, import_id: str, debian_data: DebianData
    ) -> tuple[list[LegacyDependency], list[LegacyDependency]]:
        """"""
        Takes in a debian package and figures out what dependencies have changed.

        The process is:
           1. Build a view of what the package's dependencies are according to
              the parsed debian data, using priority-based deduplication
           2. Get this package's ID from CHAI
           3. Get this package's existing dependencies from CHAI
           4. Compare the two sets, and identify new and removed dependencies

        Note: The database has a unique constraint on (package_id, dependency_id),
        so if a package depends on the same dependency with multiple types (e.g.,
        both runtime and build), we choose the highest priority type:
        Runtime > Build > Test

        Returns:
          - new_deps: a list of new dependencies
          - removed_deps: a list of removed dependencies
        """"""
        # First, collect all dependencies and deduplicate by dependency name
        # choosing the highest priority dependency type for each unique dependency
        dependency_map: dict[str, UUID] = {}

        # Priority order: Runtime > Build > Test
        priority_order = {
            self.config.dependency_types.runtime: 1,
            self.config.dependency_types.build: 2,
            self.config.dependency_types.test: 3,
        }

        def process_deps(dependencies: list[Depends], dep_type: UUID) -> None:
            """"""Helper to process dependencies of a given type with priority""""""
            for dep in dependencies:
                dep_name = f""debian/{dep.package}""  # bc the map is by import_id

                # Get the dependency package from cache
                dependency = self.caches.package_map.get(dep_name)

                # try debian/dependency
                if not dependency:
                    self.logger.debug(f""{dep_name} not loaded, will catch next time"")
                    continue

                # If this dependency already exists in our map, choose higher priority
                if dep_name in dependency_map:
                    existing_priority = priority_order.get(
                        dependency_map[dep_name], 999
                    )
                    new_priority = priority_order.get(dep_type, 999)

                    if new_priority < existing_priority:  # Lower is better!
                        old_type_id = dependency_map[dep_name]
                        dependency_map[dep_name] = dep_type
                        self.logger.debug(
                            f""Updated dependency type for {dep_name} from ""
                            f""{old_type_id} to {dep_type} (higher priority)""
                        )
                else:
                    dependency_map[dep_name] = dep_type

        # Process different types of dependencies with priority handling
        # Debian has: depends (runtime), build_depends (build), recommends, suggests, etc.
        process_deps(debian_data.depends, self.config.dependency_types.runtime)
        process_deps(debian_data.build_depends, self.config.dependency_types.build)
        # Map recommends and suggests to runtime for simplicity
        process_deps(debian_data.recommends, self.config.dependency_types.runtime)
        process_deps(debian_data.suggests, self.config.dependency_types.runtime)

        # Now build the actual set of dependencies with resolved types
        actual: set[tuple[UUID, UUID]] = set()
        for dep_name, dep_type in dependency_map.items():
            dependency = self.caches.package_map.get(dep_name)
            if dependency:  # Double-check it still exists
                actual.add((dependency.id, dep_type))

        # get the package ID for what we are working with
        package = self.caches.package_map.get(import_id)
        if not package:
            self.logger.debug(f""New package {import_id}, will grab its deps next time"")
            return [], []

        pkg_id: UUID = package.id

        # what are its existing dependencies?
        # specifically, existing dependencies IN THE SAME STRUCTURE as `actual`,
        # so we can do an easy comparison
        existing: set[tuple[UUID, UUID]] = {
            (dep.dependency_id, dep.dependency_type_id)
            for dep in self.caches.dependencies.get(pkg_id, set())
        }

        # we have two sets!
        # actual minus existing = new_deps
        # existing minus actual = removed_deps
        new = actual - existing
        removed = existing - actual

        new_deps: list[LegacyDependency] = [
            LegacyDependency(
                package_id=pkg_id,
                dependency_id=dep[0],
                dependency_type_id=dep[1],
                created_at=self.now,
                updated_at=self.now,
            )
            for dep in new
        ]

        # get the existing legacy dependency, and add it to removed_deps
        removed_deps: list[LegacyDependency] = []
        cache_deps: set[LegacyDependency] = self.caches.dependencies.get(pkg_id, set())
        for removed_dep_id, removed_dep_type in removed:
            try:
                existing_dep = next(
                    dep
                    for dep in cache_deps
                    if dep.dependency_id == removed_dep_id
                    and dep.dependency_type_id == removed_dep_type
                )
                removed_deps.append(existing_dep)
            except StopIteration as exc:
                cache_deps_str = ""\n"".join(
                    [
                        f""{dep.dependency_id} / {dep.dependency_type_id}""
                        for dep in cache_deps
                    ]
                )
                raise ValueError(
                    f""Removing {removed_dep_id} / {removed_dep_type} for {pkg_id} but not in Cache: \n{cache_deps_str}""
                ) from exc

        return new_deps, removed_deps
",package_managers/debian/diff.py,DebianDiff,1,7
survived,"def binutils():
    return """"""
Package: binutils
Binary: binutils-for-host, binutils-for-build,
 binutils-ia64-linux-gnu-dbg, binutils-m68k-linux-gnu,
 binutils-mips64el-linux-gnuabin32-dbg, binutils-mipsisa64r6-linux-gnuabin32,
 binutils-mipsisa64r6el-linux-gnuabi64-dbg

""""""
",package_managers/debian/scripts/test_investigate_sources.py,,1,6
survived,"    def test_debian_specific_dependencies(self, mock_config, mock_logger, mock_db):
        """"""Test Debian-specific dependency types: recommends, suggests""""""

        p1_id = uuid4()
        p2_id = uuid4()
        p3_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""debian/p1"", name=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""debian/p2"", name=""p2"")
        p3_pkg = Package(id=p3_id, derived_id=""debian/p3"", name=""p3"")

        cache = Cache(
            package_map={""debian/p1"": p1_pkg, ""debian/p2"": p2_pkg, ""debian/p3"": p3_pkg},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Parsed data with recommends and suggests (mapped to runtime)
        new_pkg_data = create_debian_package(
            package=""p1"",
            recommends=[""p2""],
            suggests=[""p3""],
        )

        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""debian/p1"", new_pkg_data)

        # Should create runtime dependencies for both recommends and suggests
        assert len(removed_deps) == 0
        assert len(new_deps) == 2

        # Both should be runtime dependencies
        for dep in new_deps:
            assert dep.dependency_type_id == mock_config.dependency_types.runtime
            assert dep.dependency_id in [p2_id, p3_id]
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading,1,7
survived,"def delete_project(project_id):
    """"""Delete a project""""""
    try:
        user_id = request.headers.get('X-User-ID')
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        success = DatabaseOperations.delete_project(project_id, user_id)
        if not success:
            return jsonify({'error': 'Project not found'}), 404
        
        return jsonify({
            'status': 'success',
            'message': 'Project deleted successfully'
        })
        
    except Exception as e:
        logger.error(f""Error deleting project {project_id}: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/projects.py,,1,7
survived,"def add_chat_message(task_id):
    """"""Add a chat message to a task""""""
    try:
        data = request.get_json()
        user_id = request.headers.get('X-User-ID')
        
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        content = data.get('content')
        role = data.get('role', 'user')
        
        if not content:
            return jsonify({'error': 'content is required'}), 400
        
        if role not in ['user', 'assistant']:
            return jsonify({'error': 'role must be either ""user"" or ""assistant""'}), 400
        
        task = DatabaseOperations.add_chat_message(task_id, user_id, role, content)
        if not task:
            return jsonify({'error': 'Task not found'}), 404
        
        return jsonify({
            'status': 'success',
            'task': task
        })
        
    except Exception as e:
        logger.error(f""Error adding chat message: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,,1,7
survived,"    def migrate_legacy_task(legacy_task: Dict, user_id: str) -> Optional[Dict]:
        """"""Migrate a legacy task from the JSON storage to Supabase""""""
        try:
            # Map legacy task structure to new structure
            task_data = {
                'user_id': user_id,
                'repo_url': legacy_task.get('repo_url'),
                'target_branch': legacy_task.get('branch', 'main'),
                'agent': legacy_task.get('model', 'claude'),
                'status': legacy_task.get('status', 'pending'),
                'container_id': legacy_task.get('container_id'),
                'commit_hash': legacy_task.get('commit_hash'),
                'git_diff': legacy_task.get('git_diff'),
                'git_patch': legacy_task.get('git_patch'),
                'changed_files': legacy_task.get('changed_files', []),
                'error': legacy_task.get('error'),
                'chat_messages': [{
                    'role': 'user',
                    'content': legacy_task.get('prompt', ''),
                    'timestamp': datetime.fromtimestamp(legacy_task.get('created_at', 0)).isoformat()
                }] if legacy_task.get('prompt') else [],
                'execution_metadata': {
                    'legacy_id': legacy_task.get('id'),
                    'migrated_at': datetime.utcnow().isoformat()
                }
            }
            
            # Set timestamps if available
            if legacy_task.get('created_at'):
                task_data['created_at'] = datetime.fromtimestamp(legacy_task['created_at']).isoformat()
            
            result = supabase.table('tasks').insert(task_data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error migrating legacy task: {e}"")
            raise",server/database.py,DatabaseOperations,1,6
survived,"    def update_task(task_id: int, user_id: str, updates: Dict) -> Optional[Dict]:
        """"""Update a task""""""
        try:
            # Handle timestamps
            if 'status' in updates:
                if updates['status'] == 'running' and 'started_at' not in updates:
                    updates['started_at'] = datetime.utcnow().isoformat()
                elif updates['status'] in ['completed', 'failed', 'cancelled'] and 'completed_at' not in updates:
                    updates['completed_at'] = datetime.utcnow().isoformat()
            
            updates['updated_at'] = datetime.utcnow().isoformat()
            result = supabase.table('tasks').update(updates).eq('id', task_id).eq('user_id', user_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error updating task {task_id}: {e}"")
            raise
",server/database.py,DatabaseOperations,1,8
deleted,"        def create_flow_tool(graph_obj: Graph, flow_name: str, flow_desc: str):
            """"""Create a tool function for a specific flow.""""""
            
            @mcp.tool()
            def flow_tool(input_data: FlowInput) -> FlowOutput:
                f""""""Execute the {flow_name} flow.
                
                {flow_desc}
                """"""
                try:
                    # Import here to avoid circular imports
                    import time
                    
                    start_time = time.time()
                    
                    # Execute the flow
                    # Note: This follows the same pattern as the REST API execution
                    result = graph_obj.run(
                        inputs={""input_value"": input_data.input_value},
                        tweaks=input_data.tweaks or {}
                    )
                    
                    execution_time = time.time() - start_time
                    
                    return FlowOutput(
                        result=result,
                        execution_time=execution_time
                    )
                    
                except Exception as e:
                    return FlowOutput(
                        result=None,
                        error=str(e)
                    )
            
            # Dynamically set the function name to match the flow
            flow_tool.__name__ = f""execute_{flow_name.replace(' ', '_').replace('-', '_').lower()}""
            return flow_tool
",src/backend/base/langflow/cli/mcp_server.py,,1,7
deleted,"    def test_mcp_server_tool_execution_success(self, mock_fastmcp, integration_graphs_and_metas):
        """"""Test successful tool execution through MCP server.""""""
        graphs, metas = integration_graphs_and_metas
        mock_mcp_instance = MagicMock()
        
        # Track registered tools
        registered_tools = []
        
        def mock_tool_decorator(func):
            registered_tools.append(func)
            return func
        
        mock_mcp_instance.tool.side_effect = lambda: mock_tool_decorator
        mock_fastmcp.return_value = mock_mcp_instance

        # Create the server
        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Integration Test Server""
        )

        # Verify tools were registered
        assert len(registered_tools) == len(graphs)

        # Test tool execution (simulate calling one of the registered tools)
        if registered_tools:
            tool_func = registered_tools[0]
            flow_input = FlowInput(input_value=""test input"")
            
            # Mock the graph execution context
            with patch(""time.time"", side_effect=[0, 1.5]):  # Mock execution time
                result = tool_func(flow_input)
            
            assert isinstance(result, FlowOutput)
            assert ""Processed: test input"" in str(result.result)
            assert result.execution_time == 1.5
            assert result.error is None
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration,1,7
survived,"    def test_mcp_valid_transports(self, runner, temp_python_script):
        """"""Test that valid MCP transports are accepted.""""""
        valid_transports = [""stdio"", ""sse"", ""websocket""]
        
        for transport in valid_transports:
            # We just test that the validation passes and the command would start
            # We'll use a timeout or patch to avoid actually starting the server
            with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
                mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
                
                result = runner.invoke(app, [
                    ""serve"", str(temp_python_script),
                    ""--mcp"", ""--mcp-transport"", transport,
                    ""--verbose""
                ])
                
                # Should either exit cleanly (0) or with KeyboardInterrupt handling
                assert result.exit_code in [0, 1]
                # Should show MCP mode is enabled
                assert f""MCP mode enabled with {transport} transport"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand,1,7
survived,"    def test_flow_output_model(self):
        """"""Test FlowOutput model validation.""""""
        # Success output
        output = FlowOutput(
            result=""test result"",
            execution_time=1.5
        )
        assert output.result == ""test result""
        assert output.execution_time == 1.5
        assert output.error is None

        # Error output
        error_output = FlowOutput(
            result=None,
            error=""Test error occurred""
        )
        assert error_output.result is None
        assert error_output.error == ""Test error occurred""
",src/backend/tests/unit/test_mcp_server.py,TestFlowModels,1,7
survived,"    def test_flow_output_with_both_result_and_error(self):
        """"""Test FlowOutput can have both result and error.""""""
        output = FlowOutput(
            result=""partial result"",
            error=""warning message"",
            execution_time=2.0
        )
        assert output.result == ""partial result""
        assert output.error == ""warning message""
        assert output.execution_time == 2.0
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerErrorHandling,1,7
survived,"    def test_mcp_server_missing_graph_attributes(self, mock_fastmcp):
        """"""Test MCP server creation with graphs missing expected attributes.""""""
        mock_graph = MagicMock()
        mock_graph.run.side_effect = AttributeError(""Graph has no run method"")
        
        mock_mcp_instance = MagicMock()
        mock_fastmcp.return_value = mock_mcp_instance

        graphs = {""broken_flow"": mock_graph}
        metas = {""broken_flow"": MagicMock()}

        # Should not fail during server creation
        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Broken Graph Test""
        )

        assert server == mock_mcp_instance",src/backend/tests/unit/test_mcp_server.py,TestMCPServerErrorHandling,1,7
deleted,"    def get_flow_schema(flow_id: str) -> str:
        """"""Get the schema (inputs/outputs) for a specific flow.""""""
        if flow_id not in graphs:
            return json.dumps({""error"": f""Flow '{flow_id}' not found""})
        
        graph = graphs[flow_id]
        
        # This could be expanded to provide detailed schema information
        # by analyzing the graph structure
        schema_info = {
            ""flow_id"": flow_id,
            ""inputs"": {
                ""input_value"": {
                    ""type"": ""string"",
                    ""description"": ""Main input value for the flow""
                },
                ""tweaks"": {
                    ""type"": ""object"",
                    ""description"": ""Optional parameter tweaks"",
                    ""optional"": True
                }
            },
            ""outputs"": {
                ""result"": {
                    ""type"": ""any"",
                    ""description"": ""Flow execution result""
                },
                ""execution_time"": {
                    ""type"": ""number"",
                    ""description"": ""Execution time in seconds"",
                    ""optional"": True
                },
                ""error"": {
                    ""type"": ""string"",
                    ""description"": ""Error message if execution failed"",
                    ""optional"": True
                }
            }
        }
        
        return json.dumps(schema_info, indent=2)
",src/backend/base/langflow/cli/mcp_server.py,,1,7
survived,"    def from_file(cls, file_path: Path) -> ""Registry"":
        """"""Load registry from a JSON file.

        Args:
            file_path: Path to the registry JSON file

        Returns:
            Registry: The loaded registry

        Raises:
            FileNotFoundError: If the file doesn't exist
            json.JSONDecodeError: If the file contains invalid JSON
            ValidationError: If the data doesn't match the expected schema
        """"""
        with open(file_path, ""r"") as f:
            data = json.load(f)
        return cls.from_json_list(data)
",terminal_bench/registry/client.py,Registry,1,7
survived,"def cost_per_token(model: str, usage: Usage) -> Tuple[float, float]:
    """"""
    Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Follows the same logic as other provider cost calculations.
    """"""
    return generic_cost_per_token(
        model=model, usage=usage, custom_llm_provider=""moonshot""
    )",litellm/llms/moonshot/cost_calculator.py,,1,7
survived,"    async def test_transfer_traces_fails_with_non_existent_trace_id(
        self,
        gql_client: AsyncGraphQLClient,
        trace_transfer_fixture: dict[str, int],
        db: DbSessionFactory,
    ) -> None:
        source_project_id = trace_transfer_fixture[""source_project_id""]
        dest_project_id = trace_transfer_fixture[""dest_project_id""]
        trace1_id = trace_transfer_fixture[""trace1_id""]

        async with db() as session:
            trace = await session.get(models.Trace, trace1_id)
            assert trace is not None
            assert trace.project_rowid == source_project_id

        result = await gql_client.execute(
            self.TRANSFER_TRACES_MUTATION,
            variables={
                ""traceIds"": [
                    str(GlobalID(""Trace"", str(trace1_id))),
                    str(GlobalID(""Trace"", ""99999"")),
                ],
                ""projectId"": str(GlobalID(""Project"", str(dest_project_id))),
            },
        )
        assert result.errors

        async with db() as session:
            trace = await session.get(models.Trace, trace1_id)
            assert trace is not None
            assert trace.project_rowid == source_project_id
",tests/unit/server/api/mutations/test_trace_transfer_mutations.py,TestTraceTransferMutationMixin,1,6
survived,"    async def transfer_traces_to_project(
        self,
        info: Info[Context, None],
        trace_ids: list[GlobalID],
        project_id: GlobalID,
    ) -> Query:
        if not trace_ids:
            raise BadRequest(""Must provide at least one trace ID to transfer"")
        trace_ids = list(set(trace_ids))
        try:
            trace_rowids = [
                from_global_id_with_expected_type(global_id=id, expected_type_name=""Trace"")
                for id in trace_ids
            ]
            dest_project_rowid = from_global_id_with_expected_type(
                global_id=project_id, expected_type_name=""Project""
            )
        except ValueError as error:
            raise BadRequest(str(error))
        
        async with info.context.db() as session:
            dest_project = await session.get(models.Project, dest_project_rowid)
            if dest_project is None:
                raise BadRequest(""Destination project does not exist"")
            
            traces = (
                await session.scalars(
                    select(models.Trace).where(models.Trace.id.in_(trace_rowids))
                )
            ).all()
            if len(traces) < len(trace_rowids):
                raise BadRequest(""Invalid trace IDs provided"")
            
            source_project_ids = set(trace.project_rowid for trace in traces)
            if len(source_project_ids) > 1:
                raise BadRequest(""Cannot transfer traces from multiple projects"")
            
            await session.execute(
                update(models.Trace)
                .where(models.Trace.id.in_(trace_rowids))
                .values(project_rowid=dest_project_rowid)
            )
            
        return Query()",src/phoenix/server/api/mutations/trace_mutations.py,TraceMutationMixin,1,6
survived,"    def test_csharp_operators(self):
        patch = """"""
@@ -152,10 +152,6 @@ public static ClassName operator+(ClassName a, ClassName b)

@@ -152,10 +152,6 @@ public static bool operator==(ClassName a, ClassName b)

@@ -152,10 +152,6 @@ public static implicit operator string(ClassName obj)

@@ -152,10 +152,6 @@ public static explicit operator int(ClassName obj)

@@ -152,10 +152,6 @@ public static ClassName operator++(ClassName obj)

@@ -152,10 +152,6 @@ public static bool operator<(ClassName a, ClassName b)

@@ -152,10 +152,6 @@ public static bool operator>(ClassName a, ClassName b)

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""+"",
            ""=="",
            ""implicit"",
            ""explicit"",
            ""++"",
            ""<"",
            "">"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase,1,7
survived,"def parse_args() -> tuple[RunConfig, TauBenchTrainingConfig, argparse.Namespace]:
    """"""Parse command line arguments for RL training""""""
    parser = argparse.ArgumentParser(description=""Train an agent on tau-bench using ART RL"")
    
    # tau-bench arguments (reuse from original run.py)
    parser.add_argument(""--num-trials"", type=int, default=1)
    parser.add_argument(
        ""--env"", type=str, choices=[""retail"", ""airline""], default=""retail""
    )
    parser.add_argument(
        ""--model"",
        type=str,
        help=""The model to use for the agent"",
        required=True,
    )
    parser.add_argument(
        ""--model-provider"",
        type=str,
        choices=provider_list,
        help=""The model provider for the agent"",
        required=True,
    )
    parser.add_argument(
        ""--user-model"",
        type=str,
        default=""gpt-4o"",
        help=""The model to use for the user simulator"",
    )
    parser.add_argument(
        ""--user-model-provider"",
        type=str,
        choices=provider_list,
        help=""The model provider for the user simulator"",
    )
    parser.add_argument(
        ""--agent-strategy"",
        type=str,
        default=""tool-calling"",
        choices=[""tool-calling"", ""act"", ""react"", ""few-shot""],
    )
    parser.add_argument(
        ""--temperature"",
        type=float,
        default=0.0,
        help=""The sampling temperature for the action model"",
    )
    parser.add_argument(
        ""--task-split"",
        type=str,
        default=""train"",  # Default to train for RL
        choices=[""train"", ""test"", ""dev""],
        help=""The split of tasks to run"",
    )
    parser.add_argument(""--start-index"", type=int, default=0)
    parser.add_argument(""--end-index"", type=int, default=100, help=""End index for training tasks"")
    parser.add_argument(""--task-ids"", type=int, nargs=""+"", help=""(Optional) run only the tasks with the given IDs"")
    parser.add_argument(""--log-dir"", type=str, default=""rl_results"")
    parser.add_argument(""--seed"", type=int, default=10)
    parser.add_argument(""--shuffle"", type=int, default=0)
    parser.add_argument(""--user-strategy"", type=str, default=""llm"", choices=[item.value for item in UserStrategy])
    parser.add_argument(""--few-shot-displays-path"", type=str, help=""Path to a jsonlines file containing few shot displays"")
    
    # RL-specific arguments
    parser.add_argument(""--model-name"", type=str, required=True, help=""Name for the trainable model"")
    parser.add_argument(""--base-model"", type=str, default=""Qwen/Qwen2.5-14B-Instruct"", help=""Base model for training"")
    parser.add_argument(""--trajectories-per-group"", type=int, default=6, help=""Number of trajectories per group"")
    parser.add_argument(""--groups-per-step"", type=int, default=8, help=""Number of groups per training step"")
    parser.add_argument(""--learning-rate"", type=float, default=1.2e-5, help=""Learning rate for training"")
    parser.add_argument(""--eval-steps"", type=int, default=30, help=""Evaluate every N steps"")
    parser.add_argument(""--val-set-size"", type=int, default=100, help=""Validation set size"")
    parser.add_argument(""--training-dataset-size"", type=int, default=1000, help=""Training dataset size"")
    parser.add_argument(""--num-epochs"", type=int, default=1, help=""Number of training epochs"")
    
    args = parser.parse_args()
    print(args)
    
    # Create RunConfig for tau-bench
    run_config = RunConfig(
        model_provider=args.model_provider,
        user_model_provider=args.user_model_provider,
        model=args.model,
        user_model=args.user_model,
        num_trials=args.num_trials,
        env=args.env,
        agent_strategy=args.agent_strategy,
        temperature=args.temperature,
        task_split=args.task_split,
        start_index=args.start_index,
        end_index=args.end_index,
        task_ids=args.task_ids,
        log_dir=args.log_dir,
        max_concurrency=1,  # RL training is sequential
        seed=args.seed,
        shuffle=args.shuffle,
        user_strategy=args.user_strategy,
        few_shot_displays_path=args.few_shot_displays_path,
    )
    
    # Create training config
    training_config = TauBenchTrainingConfig(
        trajectories_per_group=args.trajectories_per_group,
        groups_per_step=args.groups_per_step,
        learning_rate=args.learning_rate,
        eval_steps=args.eval_steps,
        val_set_size=args.val_set_size,
        training_dataset_size=args.training_dataset_size,
        num_epochs=args.num_epochs,
    )
    
    return run_config, training_config, args
",dev/tau-bench/run_rl.py,,1,8
survived,"    def test_env_group_empty_envs_fails(self):
        """"""Test that EnvGroup fails with empty environments list.""""""
        with pytest.raises(ValueError, match=""EnvGroup requires at least one environment""):
            EnvGroup(envs=[])
",tests/test_env_group.py,TestEnvGroup,1,7
survived,"            def env_response(self, messages, state, **kwargs):
                return "" Continue."", state
",tests/test_multiturn_env.py,TestMultiTurnEnv.CompletionMultiTurnEnv,1,6
survived,"    async def test_singleturn_stops_after_one_response(self, mock_openai_client, sample_dataset):
        """"""Test that SingleTurnEnv truly stops after one response.""""""
        # We'll verify this by checking the is_completed logic
        env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset
        )
        
        # Before any responses
        state = {""responses"": []}
        assert not env.is_completed([], state)
        
        # After one response
        state = {""responses"": [MagicMock()]}
        assert env.is_completed([], state)
        
        # Even with multiple responses (shouldn't happen), it's still completed
        state = {""responses"": [MagicMock(), MagicMock()]}
        assert env.is_completed([], state)",tests/test_singleturn_env.py,TestSingleTurnEnv,1,7
deleted,"def _get_azure_token_param_name(model_name: str) -> str:
    """"""
    Get the correct token parameter name for Azure OpenAI models.
    OpenAI o1 and o3 models use max_completion_tokens, while other models use max_tokens.
    However, Azure OpenAI models currently do not support max_completion_tokens unless it's an o1 or o3 model.
    """"""
    if model_name.startswith(""o1"") or model_name.startswith(""o3""):
        return ""max_completion_tokens""
    return ""max_tokens""
",src/phoenix/server/api/helpers/playground_clients.py,,1,8
survived,"    def supported_invocation_parameters(cls) -> list[InvocationParameter]:
        # For Azure OpenAI, we need to handle o1 and o3 models differently
        # They use max_completion_tokens instead of max_tokens
        return [
            BoundedFloatInvocationParameter(
                invocation_name=""temperature"",
                canonical_name=CanonicalParameterName.TEMPERATURE,
                label=""Temperature"",
                default_value=1.0,
                min_value=0.0,
                max_value=2.0,
            ),
            IntInvocationParameter(
                invocation_name=""max_tokens"",
                canonical_name=CanonicalParameterName.MAX_COMPLETION_TOKENS,
                label=""Max Tokens"",
            ),
            BoundedFloatInvocationParameter(
                invocation_name=""frequency_penalty"",
                label=""Frequency Penalty"",
                default_value=0.0,
                min_value=-2.0,
                max_value=2.0,
            ),
            BoundedFloatInvocationParameter(
                invocation_name=""presence_penalty"",
                label=""Presence Penalty"",
                default_value=0.0,
                min_value=-2.0,
                max_value=2.0,
            ),
            StringListInvocationParameter(
                invocation_name=""stop"",
                canonical_name=CanonicalParameterName.STOP_SEQUENCES,
                label=""Stop Sequences"",
            ),
            BoundedFloatInvocationParameter(
                invocation_name=""top_p"",
                canonical_name=CanonicalParameterName.TOP_P,
                label=""Top P"",
                default_value=1.0,
                min_value=0.0,
                max_value=1.0,
            ),
            IntInvocationParameter(
                invocation_name=""seed"",
                canonical_name=CanonicalParameterName.RANDOM_SEED,
                label=""Seed"",
            ),
            JSONInvocationParameter(
                invocation_name=""tool_choice"",
                label=""Tool Choice"",
                canonical_name=CanonicalParameterName.TOOL_CHOICE,
            ),
            JSONInvocationParameter(
                invocation_name=""response_format"",
                label=""Response Format"",
                canonical_name=CanonicalParameterName.RESPONSE_FORMAT,
            ),
        ]
",src/phoenix/server/api/helpers/playground_clients.py,AzureOpenAIStreamingClient,1,7
survived,"    async def chat_completion_create(
        self,
        messages: list[
            tuple[ChatCompletionMessageRole, str, Optional[str], Optional[list[JSONScalarType]]]
        ],
        tools: list[JSONScalarType],
        **invocation_parameters: Any,
    ) -> AsyncIterator[ChatCompletionChunk]:
        # Transform max_tokens to the correct parameter name for Azure OpenAI
        transformed_parameters = invocation_parameters.copy()
        if ""max_tokens"" in transformed_parameters:
            correct_param_name = _get_azure_token_param_name(self.model_name)
            if correct_param_name != ""max_tokens"":
                transformed_parameters[correct_param_name] = transformed_parameters.pop(""max_tokens"")
        
        # Call the parent method with transformed parameters
        async for chunk in super().chat_completion_create(messages, tools, **transformed_parameters):
            yield chunk
",src/phoenix/server/api/helpers/playground_clients.py,AzureOpenAIStreamingClient,1,7
survived,"    def path(self):
        return reverse(""sentry-mcp-json"")
",tests/sentry/web/test_api.py,McpJsonTest,1,6
survived,"    def test_mcp_json_self_hosted_mode(self):
        with override_settings(SENTRY_MODE=""self_hosted""):
            response = self.client.get(""/.well-known/mcp.json"")

        assert response.status_code == 404
",tests/sentry/web/test_api.py,McpJsonTest,1,6
survived,"    def __init__(self, runner, console: Console, llm_handler: LLMCallHandler, parser: ResponseParser):
        self.runner = runner
        self.console = console
        self.llm_handler = llm_handler
        self.parser = parser
",docetl/operations/utils/api.py,ValidationHandler,1,7
deleted,"    def _build_system_prompt(self, model: str, output_mode: OutputMode, scratchpad: Optional[str]) -> str:
        """"""Build the system prompt based on model and output mode.""""""
        persona = self.runner.config.get(""system_prompt"", {}).get(""persona"", ""a helpful assistant"")
        dataset_description = self.runner.config.get(""system_prompt"", {}).get(
            ""dataset_description"", ""a collection of unstructured documents""
        )
        
        base_prompt = (
            f""You are a {persona}, helping the user make sense of their data. ""
            f""The dataset description is: {dataset_description}. ""
            ""You will perform the specified task on the provided data, as precisely and exhaustively ""
            ""(i.e., high recall) as possible.""
        )

        if output_mode == OutputMode.STRUCTURED_OUTPUT or ""sagemaker"" in model or is_deepseek_r1(model):
            system_prompt = base_prompt
        else:
            system_prompt = (
                base_prompt +
                "" The result should be a structured output that you will send back to the user, ""
                ""with the `send_output` function. Do not influence your answers too much based on the ""
                ""`send_output` function parameter names; just use them to send the result back to the user.""
            )

        if scratchpad:
            system_prompt += self._build_scratchpad_instructions()

        return system_prompt
",docetl/operations/utils/api.py,LLMCallHandler,1,7
deleted,"    def _handle_model_specific_parsing(self, output_dict: Dict[str, Any]) -> None:
        """"""Handle specific parsing for certain models.""""""
        for key, value in output_dict.items():
            if not isinstance(value, str):
                continue
            try:
                output_dict[key] = ast.literal_eval(value)
            except Exception:
                try:
                    if value.startswith(""[""):
                        output_dict[key] = ast.literal_eval(value + ""]"")
                    else:
                        output_dict[key] = value
                except Exception:
                    pass
",docetl/operations/utils/api.py,ResponseParser,1,6
deleted,"    def _handle_model_error(self, model: str, error: Exception) -> None:
        """"""Handle model-specific errors.""""""
        if model not in BASIC_MODELS and ""/"" not in model:
            raise ValueError(
                f""Note: You may also need to prefix your model name with the provider, ""
                f""e.g. 'openai/gpt-4o-mini' or 'gemini/gemini-1.5-flash' to conform to ""
                f""LiteLLM API standards. Original error: {error}""
            )
        raise error
",docetl/operations/utils/api.py,LLMCallHandler,1,7
survived,"    def test_parse_empty_after_think(self, think_parser):
        """"""Test parsing when content after think tags is empty.""""""
        text = ""<think>Some thinking</think>""
        result = think_parser.parse(text)
        assert result == """"
",tests/test_think_parser.py,TestThinkParser,1,7
survived,"    async def test_call_reward_func_error_handling(self):
        """"""Test error handling in reward function calls.""""""
        def error_func(completion, **kwargs):
            raise ValueError(""Test error"")
        
        rubric = Rubric(funcs=[], weights=[])
        
        result = await rubric.call_reward_func(
            func=error_func,
            prompt=""test"",
            completion=""test"",
            answer=""test"",
            state={},
            task=""test"",
            info={}
        )
        
        assert result == 0.0  # Should return 0.0 on error
",tests/test_rubric.py,TestRubric,1,7
survived,"    def __init__(self):
        self.chat_completions = {}  # Maps conversation history to responses
        self.text_completions = {}  # Maps prompts to responses
        self.default_chat_response = ""This is a test response""
        self.default_text_response = ""This is a test completion""
        self.base_url = ""http://localhost/v1/""  # For testing URL parsing
        
        # Create mock structure
        self.chat = MagicMock()
        self.completions = MagicMock()
        self.chat.completions = MagicMock()
        
        # Set up async methods
        self.chat.completions.create = AsyncMock(side_effect=self._handle_chat_completion)
        self.completions.create = MagicMock(side_effect=self._handle_text_completion)
",tests/conftest.py,MockAsyncOpenAI,1,8
survived,"    def test_format_reward_function(self, xml_parser):
        """"""Test the format reward function.""""""
        reward_func = xml_parser.get_format_reward_func()
        
        # Well-formatted completion
        good_completion = [
            {""role"": ""assistant"", ""content"": ""<reasoning>Good reasoning</reasoning><answer>42</answer>""}
        ]
        good_reward = reward_func(good_completion)
        assert 0.0 <= good_reward <= 1.0
        
        # Poorly formatted completion - gets partial credit for proper spacing
        bad_completion = [
            {""role"": ""assistant"", ""content"": ""Just plain text without XML""}
        ]
        bad_reward = reward_func(bad_completion)
        assert bad_reward == 0.2  # Gets 0.2 for proper spacing (no XML tags to mess up)",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"    def test_format_method_missing_field(self, xml_parser):
        """"""Test format method with missing required field.""""""
        with pytest.raises(ValueError, match=""Missing value for field""):
            xml_parser.format(reasoning=""Only reasoning"")
",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"        def new_func(completion, **kwargs):
            return 0.9
",tests/test_rubric_group.py,TestRubricGroup,1,6
survived,"    async def test_environment_response_state_modification(self, mock_openai_client, sample_chat_dataset):
        """"""Test that environment can modify state between turns.""""""
        class StatefulMultiTurnEnv(MultiTurnEnv):
            def is_completed(self, messages, state, **kwargs):
                return state.get(""turn_count"", 0) >= 2
            
            def env_response(self, messages, state, **kwargs):
                state[""turn_count""] = state.get(""turn_count"", 0) + 1
                return {""role"": ""user"", ""content"": f""Turn {state['turn_count']}""}, state
        
        env = StatefulMultiTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_chat_dataset,
            max_turns=5,
            parser=Parser(),
            rubric=Rubric()
        )
        
        env.client.set_default_responses(chat_response=""Continue"")
        
        prompt = [{""role"": ""user"", ""content"": ""Start""}]
        completion, state = await env.rollout(
            client=env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""test""
        )
        
        # Should complete when turn_count reaches 2
        assert state[""turn_count""] == 2
        assert len(completion) >= 3  # Multiple turns with env responses
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    def test_parser_with_kwargs(self):
        """"""Test that Parser accepts arbitrary kwargs.""""""
        parser = Parser(custom_attr=""test_value"", number=42)
        assert parser.custom_attr == ""test_value""
        assert parser.number == 42
",tests/test_parser.py,TestParser,1,7
survived,"    async def rollout(self, client, model, prompt, answer, task=""default"", info={}, sampling_args={}, **kwargs):
        """"""Simple test rollout implementation.""""""
        response = await self.get_model_response(
            prompt=prompt,
            client=client,
            model=model,
            sampling_args=sampling_args
        )
        if self.message_type == 'chat':
            return [{'role': 'assistant', 'content': response}], {}
        return response, {}
",tests/test_environment.py,TestEnvironment,1,7
survived,"    def test_rubric_group_get_reward_func_names(self):
        """"""Test getting aggregated reward function names from all rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        def func3(completion, **kwargs):
            return 0.3
        
        rubric1 = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])
        rubric2 = Rubric(funcs=[func3], weights=[0.8])
        
        group = RubricGroup(rubrics=[rubric1, rubric2])
        names = group.get_reward_func_names()
        
        assert names == [""func1"", ""func2"", ""func3""]
",tests/test_rubric_group.py,TestRubricGroup,1,7
survived,"            def is_completed(self, messages, state, **kwargs):
                # Complete if we have any assistant message
                return any(msg.get(""role"") == ""assistant"" for msg in messages)
",tests/test_multiturn_env.py,TestMultiTurnEnv.ImmediateCompletionEnv,1,7
survived,"    def test_parse_without_think_tags(self, think_parser):
        """"""Test parsing text without think tags.""""""
        text = ""Just a simple answer without thinking tags.""
        result = think_parser.parse(text)
        assert result == text
",tests/test_think_parser.py,TestThinkParser,1,7
survived,"    async def test_score_rollouts_multiple(self):
        """"""Test scoring multiple rollouts.""""""
        def accuracy_func(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0
        
        def length_func(completion, **kwargs):
            return len(str(completion))
        
        rubric = Rubric(funcs=[accuracy_func, length_func], weights=[1.0, 0.1])
        
        prompts = [""prompt1"", ""prompt2"", ""prompt3""]
        completions = [""answer1"", ""answer2"", ""wrong""]
        answers = [""answer1"", ""answer2"", ""answer3""]
        states = [{}, {}, {}]
        tasks = [""task1"", ""task2"", ""task3""]
        infos = [{}, {}, {}]
        
        results = await rubric.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos
        )
        
        assert ""accuracy_func"" in results
        assert ""length_func"" in results
        assert ""reward"" in results
        assert len(results[""accuracy_func""]) == 3
        assert results[""accuracy_func""] == [1.0, 1.0, 0.0]  # First two match, third doesn't
        assert results[""length_func""] == [7.0, 7.0, 5.0]  # Lengths of completions
",tests/test_rubric.py,TestRubric,1,7
survived,"    def test_parse_answer_with_completion(self, basic_parser):
        """"""Test parse_answer with completion list.""""""
        completion = [
            {""role"": ""user"", ""content"": ""What is 2+2?""},
            {""role"": ""assistant"", ""content"": ""The answer is 4""}
        ]
        result = basic_parser.parse_answer(completion)
        assert result == ""The answer is 4""
",tests/test_parser.py,TestParser,1,7
survived,"        def analyze_large_objects():
            large_objects = []
            
            # 获取所有对象（限制数量）
            all_objects = muppy.get_objects()
            if len(all_objects) > self._max_objects_to_analyze:
                import random
                all_objects = random.sample(all_objects, self._max_objects_to_analyze)
            
            for obj in all_objects:
                try:
                    # 快速筛选
                    shallow_size = sys.getsizeof(obj)
                    if shallow_size < self._large_object_threshold:
                        continue
                    
                    # 深度计算
                    size = asizeof.asizeof(obj)
                    if size > self._large_object_threshold:
                        large_objects.append((obj, size))
                    
                    # 限制数量
                    if len(large_objects) >= 20:
                        break
                        
                except:
                    continue
            
            return large_objects
",app/helper/memory.py,MemoryHelper,1,7
survived,"def large_chai_graph() -> tuple[CHAI, dict[uuid.UUID, Decimal]]:
    """"""Creates a large CHAI graph with random edges and personalization.""""""
    G = CHAI()
    nodes = []
    initial_personalization_raw = {}

    # Create nodes
    for i in range(NUM_NODES):
        canon_id = uuid.uuid4()
        node = PackageNode(canon_id=canon_id)
        node.index = G.add_node(node)
        nodes.append(node)
        # Assign random initial weight for personalization
        initial_personalization_raw[canon_id] = Decimal(random.random())

    # Normalize personalization to sum to 1
    total_weight = sum(initial_personalization_raw.values())
    personalization = {
        uid: weight / total_weight
        for uid, weight in initial_personalization_raw.items()
    }
    assert (
        abs(sum(personalization.values()) - Decimal(1.0)) <= TOLERANCE
    ), f""Initial personalization should sum to 1 within tolerance: {sum(personalization.values())}""  # noqa: E501

    # Add random edges (potential cycles)
    node_indices = list(G.node_indices())
    for u_idx in node_indices:
        for v_idx in node_indices:
            if u_idx != v_idx and random.random() < EDGE_PROBABILITY:
                G.add_edge(u_idx, v_idx, None)  # Edge data is not used in distribute

    return G, personalization",tests/ranker/test_rx_graph.py,,0,6
survived,"    def create_crate(crate_id=""1048221"", dependencies=None):
        latest_version = CrateLatestVersion(
            id=9337571,
            checksum=""some-checksum"",
            downloads=1000,
            license=""MIT"",
            num=""1.0.0"",
            published_by=None,
            published_at=""2023-01-01"",
        )

        if dependencies:
            latest_version.dependencies = dependencies
        else:
            latest_version.dependencies = []

        crate = Crate(
            id=int(crate_id),
            name=""main_pkg"",
            readme=""Test readme"",
            homepage="""",
            repository="""",
            documentation="""",
            source=None,
        )
        crate.latest_version = latest_version

        return crate
",tests/package_managers/crates/test_diff_deps.py,,1,7
survived,"def test_map_operation_instance(
    map_config_with_batching, default_model, max_threads, runner
):
    return MapOperation(
        runner, map_config_with_batching, default_model, max_threads
    )
",tests/basic/test_basic_map.py,,1,6
survived,"    def test_go_simple(self):
        patch = """"""
@@ -152,10 +152,6 @@ func Hello(name string) string

@@ -152,10 +152,6 @@ func (r *Receiver) MethodName() error

@@ -152,10 +152,6 @@ func (r Receiver) ValueMethod() string

@@ -152,10 +152,6 @@ var myFunc = func() {

@@ -152,10 +152,6 @@ myFunc := func() {

@@ -152,10 +152,6 @@ func Calculate(x, y int) int

@@ -152,10 +152,6 @@ func ProcessData() (string, error)

@@ -152,10 +152,6 @@ func noParams()

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""Hello"",
            ""MethodName"",
            ""ValueMethod"",
            ""myFunc"",
            ""Calculate"",
            ""ProcessData"",
            ""noParams"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase,1,7
survived,"    def context_manager(self):
        """"""Create a mock context manager.""""""
        return ContextManager(tenant_id=""test_tenant"", workflow_id=""test_workflow"")
",tests/test_smtp_provider.py,TestSmtpProvider,1,6
survived,"    def test_ssl_encryption(self, mock_smtp_ssl_class, context_manager):
        """"""Test SMTP with SSL encryption.""""""
        # Create provider with SSL config
        ssl_config = ProviderConfig(
            description=""Test SMTP Provider"",
            authentication={
                ""smtp_server"": ""smtp.example.com"",
                ""smtp_port"": 465,
                ""encryption"": ""SSL"",
                ""smtp_username"": ""test@example.com"",
                ""smtp_password"": ""testpassword"",
            },
        )
        smtp_provider = SmtpProvider(
            context_manager=context_manager,
            provider_id=""test_smtp_provider"",
            config=ssl_config,
        )

        # Setup mock SMTP_SSL instance
        mock_smtp = MagicMock()
        mock_smtp_ssl_class.return_value = mock_smtp

        # Send email
        smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test SSL"",
            html=""<p>SSL test</p>"",
        )

        # Verify SMTP_SSL was used
        mock_smtp_ssl_class.assert_called_once_with(""smtp.example.com"", 465)
        mock_smtp.login.assert_called_once_with(""test@example.com"", ""testpassword"")
        mock_smtp.sendmail.assert_called_once()
",tests/test_smtp_provider.py,TestSmtpProvider,1,7
survived,"    def test_send_html_email(self, mock_smtp_class, smtp_provider):
        """"""Test sending an HTML email.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send HTML email
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test HTML Subject"",
            html=""<p>This is an <strong>HTML</strong> email</p>"",
        )

        # Verify SMTP was called correctly
        mock_smtp_class.assert_called_once_with(""smtp.example.com"", 587)
        mock_smtp.starttls.assert_called_once()
        mock_smtp.login.assert_called_once_with(""test@example.com"", ""testpassword"")
        
        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        assert call_args[0][0] == ""sender@example.com""
        assert call_args[0][1] == ""recipient@example.com""
        
        # Verify the email content contains HTML
        email_content = call_args[0][2]
        assert ""Content-Type: text/html"" in email_content
        assert ""<p>This is an <strong>HTML</strong> email</p>"" in email_content
        
        # Verify return value
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test HTML Subject"",
            ""html"": ""<p>This is an <strong>HTML</strong> email</p>"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider,1,6
deleted,"def test_agg_partition_by_string_notation(test_session):
    """"""Test that agg method supports string notation for partition_by.""""""
    class _ImageGroup(BaseModel):
        name: str
        size: int

    def func(key, val) -> Iterator[tuple[File, _ImageGroup]]:
        n = ""-"".join(key)
        v = sum(val)
        yield File(path=n), _ImageGroup(name=n, size=v)

    keys = [""n1"", ""n2"", ""n1""]
    values = [1, 5, 9]
    
    # Test using string notation (NEW functionality)
    ds = dc.read_values(key=keys, val=values, session=test_session).agg(
        x=func, partition_by=""key""  # String notation instead of C(""key"")
    )

    assert ds.order_by(""x_1.name"").to_values(""x_1.name"") == [""n1-n1"", ""n2""]
    assert ds.order_by(""x_1.size"").to_values(""x_1.size"") == [5, 10]
",tests/unit/lib/test_datachain.py,,1,7
survived,"def test_agg_partition_by_string_sequence(test_session):
    """"""Test that agg method supports sequence of strings for partition_by.""""""
    class _ImageGroup(BaseModel):
        name: str
        size: int

    def func(key1, key2, val) -> Iterator[tuple[File, _ImageGroup]]:
        n = f""{key1[0]}-{key2[0]}""
        v = sum(val)
        yield File(path=n), _ImageGroup(name=n, size=v)

    key1_values = [""a"", ""a"", ""b""]
    key2_values = [""x"", ""y"", ""x""]
    values = [1, 5, 9]
    
    # Test using sequence of strings (NEW functionality)
    ds = dc.read_values(
        key1=key1_values, key2=key2_values, val=values, session=test_session
    ).agg(
        x=func, partition_by=[""key1"", ""key2""]  # Sequence of strings
    )

    result_names = ds.order_by(""x_1.name"").to_values(""x_1.name"")
    result_sizes = ds.order_by(""x_1.size"").to_values(""x_1.size"")
    
    # Should have 3 partitions: (a,x), (a,y), (b,x)
    assert len(result_names) == 3
    assert len(result_sizes) == 3",tests/unit/lib/test_datachain.py,,1,7
survived,"def test_top_level_start_session_basic(sentry_init, capture_envelopes):
    """"""Test that top-level start_session starts a session on the isolation scope.""""""
    sentry_init(release=""test-release"", environment=""test-env"")
    envelopes = capture_envelopes()

    # Start a session using the top-level API
    sentry_sdk.start_session()

    # End the session
    sentry_sdk.end_session()
    sentry_sdk.flush()

    # Check that we got a session envelope
    assert len(envelopes) == 1
    sess = envelopes[0]
    assert len(sess.items) == 1
    sess_event = sess.items[0].payload.json

    assert sess_event[""attrs""] == {
        ""release"": ""test-release"",
        ""environment"": ""test-env"",
    }
    assert sess_event[""status""] == ""exited""
",tests/test_sessions.py,,1,7
survived,"def main():
    parser = argparse.ArgumentParser(description=""Benchmark RequestRepo implementations"")
    parser.add_argument(""--python-url"", default=DEFAULT_CONFIG[""python_url""],
                        help=""URL of the Python implementation"")
    parser.add_argument(""--rust-url"", default=DEFAULT_CONFIG[""rust_url""],
                        help=""URL of the Rust implementation"")
    parser.add_argument(""--concurrency"", type=int, nargs=""+"", default=DEFAULT_CONFIG[""concurrency""],
                        help=""Concurrency levels to test"")
    parser.add_argument(""--duration"", type=int, default=DEFAULT_CONFIG[""duration""],
                        help=""Duration of each benchmark in seconds"")
    parser.add_argument(""--output"", default=""benchmark_results.json"",
                        help=""Output file for results"")
    parser.add_argument(""--config"", help=""Path to benchmark configuration file"")
    parser.add_argument(""--python-only"", action=""store_true"", help=""Only benchmark Python implementation"")
    parser.add_argument(""--rust-only"", action=""store_true"", help=""Only benchmark Rust implementation"")
    
    args = parser.parse_args()
    
    config = DEFAULT_CONFIG
    if args.config:
        with open(args.config, 'r') as f:
            config = json.load(f)
    
    config[""python_url""] = args.python_url
    config[""rust_url""] = args.rust_url
    config[""concurrency""] = args.concurrency
    config[""duration""] = args.duration
    
    if not args.rust_only and not check_server(config[""python_url""]):
        console.print(f""[bold red]Error:[/bold red] Python server not running at {config['python_url']}"")
        console.print(""Start the Python server with: make start-backend"")
        if not args.python_only:
            console.print(""Continuing with Rust benchmarks only..."")
            args.python_only = False
            args.rust_only = True
        else:
            return 1
    
    if not args.python_only and not check_server(config[""rust_url""]):
        console.print(f""[bold red]Error:[/bold red] Rust server not running at {config['rust_url']}"")
        console.print(""Start the Rust server with: cd src && cargo run --release"")
        if not args.rust_only:
            console.print(""Continuing with Python benchmarks only..."")
            args.python_only = True
            args.rust_only = False
        else:
            return 1
    
    python_results = []
    rust_results = []
    
    for endpoint in config[""endpoints""]:
        for concurrency in config[""concurrency""]:
            if not args.rust_only:
                python_result = run_benchmark(
                    name=endpoint[""name""],
                    url=config[""python_url""],
                    method=endpoint[""method""],
                    path=endpoint[""path""],
                    data=endpoint.get(""data""),
                    headers=endpoint.get(""headers"", {}),
                    concurrency=concurrency,
                    duration=config[""duration""]
                )
                python_results.append(python_result)
            
            if not args.python_only:
                rust_result = run_benchmark(
                    name=endpoint[""name""],
                    url=config[""rust_url""],
                    method=endpoint[""method""],
                    path=endpoint[""path""],
                    data=endpoint.get(""data""),
                    headers=endpoint.get(""headers"", {}),
                    concurrency=concurrency,
                    duration=config[""duration""]
                )
                rust_results.append(rust_result)
    
    print_results(python_results, rust_results)
    save_results(python_results, rust_results, args.output)
    
    return 0
",benchmarks/benchmark.py,,1,7
survived,"def make_request(url: str, method: str, path: str, data: Optional[Dict] = None,
                headers: Optional[Dict] = None) -> Tuple[float, bool]:
    """"""Make a single HTTP request and return the latency and success status.""""""
    full_url = f""{url}{path}""
    headers = headers or {}
    
    start_time = time.time()
    try:
        if method.upper() == ""GET"":
            response = requests.get(full_url, headers=headers, timeout=5)
        elif method.upper() == ""POST"":
            response = requests.post(full_url, json=data, headers=headers, timeout=5)
        else:
            raise ValueError(f""Unsupported HTTP method: {method}"")
        
        success = 200 <= response.status_code < 300
    except Exception:
        success = False
    
    end_time = time.time()
    latency = (end_time - start_time) * 1000  # Convert to ms
    
    return latency, success
",benchmarks/benchmark.py,,1,7
survived,"def run_benchmark(name: str, url: str, method: str, path: str, data: Optional[Dict],
                 headers: Optional[Dict], concurrency: int, duration: int) -> BenchmarkResult:
    """"""Run a benchmark for a specific endpoint.""""""
    console.print(f""Running benchmark: [bold]{name}[/bold] with concurrency {concurrency}..."")
    
    start_time = time.time()
    end_time = start_time + duration
    
    latencies = []
    errors = 0
    requests_count = 0
    
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = []
        
        for _ in range(concurrency):
            futures.append(executor.submit(
                make_request, url, method, path, data, headers
            ))
        
        while time.time() < end_time:
            for i, future in enumerate(futures):
                if future.done():
                    latency, success = future.result()
                    latencies.append(latency)
                    requests_count += 1
                    
                    if not success:
                        errors += 1
                    
                    if time.time() < end_time:
                        futures[i] = executor.submit(
                            make_request, url, method, path, data, headers
                        )
            
            time.sleep(0.01)  # Small sleep to prevent CPU spinning
    
    actual_duration = time.time() - start_time
    
    return BenchmarkResult(
        name=name,
        concurrency=concurrency,
        requests=requests_count,
        duration=actual_duration,
        latencies=latencies,
        errors=errors
    )
",benchmarks/benchmark.py,,1,7
survived,"    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = ""https://api.opensea.io/api/v2""
",python/src/plugins/opensea/goat_plugins/opensea/service.py,OpenSeaService,1,7
survived,"    async def get_smart_money_status(self, parameters: dict):
        """"""Get the flows of tokens associated with smart money addresses""""""
        url = f""{self.base_url}/token_flows""
        params = {
            ""start_date"": parameters[""start_date""],
            ""end_date"": parameters[""end_date""]
        }
        if parameters.get(""token_address""):
            params[""token_address""] = parameters[""token_address""]
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService,1,6
survived,"def farcaster(options: FarcasterPluginOptions) -> FarcasterPlugin:
    return FarcasterPlugin(options)",python/src/plugins/farcaster/goat_plugins/farcaster/__init__.py,,1,7
survived,"    async def get_cast(self, parameters: dict):
        url = f""{self.base_url}/cast?identifier={parameters['identifier']}&type={parameters['type']}""
        return await self._make_request(""GET"", url)
",python/src/plugins/farcaster/goat_plugins/farcaster/service.py,FarcasterService,1,7
survived,"def extract_zip(zip_path, extract_dir):
    """"""Extract a zip file to a directory.""""""
    print(f""Extracting {zip_path} to {extract_dir}..."")
    with zipfile.ZipFile(zip_path, ""r"") as zip_ref:
        zip_ref.extractall(extract_dir)
    print(f""Extraction complete: {extract_dir}"")
    return extract_dir
",tests/replay_parser_test.py,,1,7
survived,"def test_remove_invalid_unicode_chars() -> None:
    """"""Test that invalid Unicode characters are properly removed.""""""
    # Test removal of illegal XML character 0xFDDB
    text_with_illegal_char = ""Valid text \uFDDB more text""
    sanitized = remove_invalid_unicode_chars(text_with_illegal_char)
    assert ""\uFDDB"" not in sanitized
    assert sanitized == ""Valid text  more text""

    # Test that valid characters are preserved
    valid_text = ""Hello, world! 你好世界""
    assert remove_invalid_unicode_chars(valid_text) == valid_text

    # Test multiple invalid characters including 0xFDDB
    text_with_multiple_illegal = ""\x00Hello\uFDDB World\uFFFE!""
    sanitized = remove_invalid_unicode_chars(text_with_multiple_illegal)
    assert all(c not in sanitized for c in [""\x00"", ""\uFDDB"", ""\uFFFE""])
    assert sanitized == ""Hello World!""",backend/tests/unit/onyx/document_index/vespa/shared_utils/test_utils.py,,1,7
survived,"    def create_source_tables(
        self,
        source: Source,
        streams: Literal[""*""] | list[str] | None = None,
    ) -> None:
        """"""Create tables in the cache for the provided source if they do not exist already.

        Tables are created based upon the Source's catalog.

        Args:
            source: The source to create tables for.
            streams: Stream names to create tables for. If None, use the Source's selected_streams
                or ""*"" if neither is set. If ""*"", all available streams will be used.
        """"""
        if streams is None:
            streams = source.get_selected_streams() or ""*""

        catalog_provider = CatalogProvider(source.get_configured_catalog(streams=streams))

        # Ensure schema exists
        self.processor._ensure_schema_exists()  # noqa: SLF001  # Accessing non-public member

        # Create tables for each stream if they don't exist
        for stream_name in catalog_provider.stream_names:
            self.processor._ensure_final_table_exists(  # noqa: SLF001
                stream_name=stream_name,
                create_if_missing=True,
            )
",airbyte/caches/base.py,CacheBase,0,6
survived,"    def __init__(self, jwt_token: str = """"):
        self.jwt_token = jwt_token
        self.base_url = ""https://api.rugcheck.xyz/v1""
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService,1,6
survived,"    async def get_trending_tokens_24h(self, parameters: dict):
        """"""Get trending tokens in the last 24h from RugCheck""""""
        return await self._make_request(""/stats/trending"")
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService,1,7
survived,"    def _are_both_versions_release_candidates(self, master_version: semver.Version, current_version: semver.Version) -> bool:
        """"""Check if both versions are release candidates.""""""
        return bool(
            master_version.prerelease
            and current_version.prerelease
            and ""rc"" in master_version.prerelease
            and ""rc"" in current_version.prerelease
        )
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionIncrementCheck,1,6
survived,"    def _should_run(self, connector: Connector) -> bool:
        """"""Determine if the check should run based on modified files.""""""
        if connector.metadata and connector.metadata.get(""ab_internal"", {}).get(""requireVersionIncrementsInPullRequests"") is False:
            return False
        
        return True
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionIncrementCheck,1,7
survived,"    def _get_master_metadata(self, connector: Connector) -> Optional[Dict[str, Any]]:
        """"""Get the metadata from the master branch.""""""
        response = requests.get(self._get_github_master_metadata_url(connector))
        
        if not response.ok:
            return None
        return yaml.safe_load(response.text)
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck,1,7
survived,"def decode_landm(pre, priors, variances):
    landms = np.concatenate(
        (
            priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],
        ),
        axis=1,
    )
    return landms
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,,1,7
survived,"def convert_to_onnx():
    model_path = download_model()
    
    model = SixDRepNet360(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 6)
    
    saved_state_dict = torch.load(model_path, map_location='cpu')
    if 'model_state_dict' in saved_state_dict:
        model.load_state_dict(saved_state_dict['model_state_dict'])
    else:
        model.load_state_dict(saved_state_dict)
    
    model.eval()
    
    dummy_input = torch.randn(1, 3, 224, 224)
    
    onnx_path = ""6DRepNet360.onnx""
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    print(f""Model converted to ONNX: {onnx_path}"")
    
    import onnx
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print(""ONNX model validation passed"")
    
    return onnx_path
",face_recognition/6d_repnet_360/convert_to_onnx.py,,1,7
survived,"def main():
    check_and_download_models(WEIGHT_PATH_6DRepNet360, MODEL_PATH_6DRepNet360, REMOTE_PATH_6DRepNet360)
    check_and_download_models(WEIGHT_PATH_FACE, MODEL_PATH_FACE, REMOTE_PATH_FACE)

    if args.video is not None:
        recognize_from_video()
    else:
        recognize_from_image()
",face_recognition/6d_repnet_360/6d_repnet_360.py,,1,7
survived,"    async def async_no_stream():
        await async_chat_client.create(
            model=""jamba-instruct"",
            system=""You are a helpful AI assistant"",
            messages=async_messages,
            maxTokens=10
        )
",tests/core_manual_tests/providers/ai21_canary.py,,1,6
deleted,"                async def __anext__(self):
                    if self.stream is None:
                        # Get the stream from the original method - it's already an async generator
                        response = original_method(self_client, *args, **kwargs_copy)
                        self.stream = aiter(response)

                    try:
                        # Get the next chunk
                        chunk = await anext(self.stream)
                        # Handle the chunk and track events
                        self.provider.handle_stream_chunk(chunk, self.session, self.llm_event, self.kwargs)
                        return chunk
                    except StopAsyncIteration:
                        # Record the LLM event when the stream completes
                        if self.session is not None:
                            self.llm_event.end_timestamp = get_ISO_time()
                            if not isinstance(self.llm_event.completion, dict):
                                self.llm_event.completion = {
                                    ""role"": ""assistant"",
                                    ""content"": self.llm_event.completion if isinstance(self.llm_event.completion, str) else """"
                                }
                            logger.info(f""Stream completed. Recording LLM event with completion: {self.llm_event.completion}"")
                            self.provider._safe_record(self.session, self.llm_event)
                            logger.info(""Successfully recorded async stream LLM event"")
                        raise
                    except Exception as e:
                        print(f""Error in AsyncStreamWrapper: {str(e)}"")
                        if not isinstance(self.llm_event, str):
                            self.provider._safe_record(self.session, ErrorEvent(trigger_event=self.llm_event, exception=e))
                        raise
",agentops/llms/providers/cohere.py,CohereProvider.AsyncStreamWrapper,1,7
survived,"    def sync_no_stream():
        anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from sync no stream"",
                }
            ],
            session=session
        )
",tests/core_manual_tests/providers/anthropic_canary.py,,1,7
survived,"                def __init__(self, provider, session, init_timestamp, kwargs):
                    self.provider = provider
                    self.session = session
                    self.init_timestamp = init_timestamp
                    self.kwargs = kwargs
                    self.stream = None
                    # Create a new LLM event for this stream
                    self.llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                    if session is not None:
                        self.llm_event.session_id = session.session_id
",agentops/llms/providers/cohere.py,CohereProvider.StreamWrapper,1,7
survived,"def test_groq_integration():
    """"""Integration test demonstrating all four Groq call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()

    # Initialize client and provider
    groq_client = Groq(api_key=os.getenv(""GROQ_API_KEY""))
    from agentops.llms.providers.groq import GroqProvider
    provider = GroqProvider(groq_client)
    provider.override()
    
    # Pass session to provider
    provider.client = session
    async_groq_client = AsyncGroq(api_key=os.getenv(""GROQ_API_KEY""))

    def sync_no_stream():
        groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from sync no stream""},
            ],
            session=session
        )

    def sync_stream():
        stream_response = groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from sync streaming""},
            ],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass

    async def async_no_stream():
        await async_groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from async no stream""},
            ],
            session=session
        )

    async def async_stream():
        async_stream_response = await async_groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from async streaming""},
            ],
            stream=True,
            session=session
        )
        async for _ in async_stream_response:
            pass

    async def run_async_tests():
        await async_no_stream()
        await async_stream()

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during Groq test: {str(e)}"")
        raise

    session.end_session(""Success"")
    analytics = session.get_analytics()
    print(analytics)
    # Verify that all LLM calls were tracked
    assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""
",tests/core_manual_tests/providers/groq_canary.py,,0,7
survived,"    def override(self):
        """"""Override Gemini's generate_content method to track LLM events.""""""
        if not hasattr(self.client, 'generate_content'):
            logger.warning(""Client does not have generate_content method. Skipping override."")
            return
            
        # Store original method
        self.original_generate = self.client.generate_content
        
        def patched_function(*args, **kwargs):
            init_timestamp = get_ISO_time()
            session = kwargs.pop(""session"", None) if ""session"" in kwargs else None
            
            # Handle positional content argument
            if args:
                kwargs[""contents""] = args[0]
                args = args[1:]  # Remove content from args
            
            # Ensure we have the original method
            if self.original_generate is None:
                logger.error(""Original generate_content method not found. Cannot proceed with override."")
                return None
            
            # Call original method and track event
            result = self.original_generate(*args, **kwargs)
            return self.handle_response(result, kwargs, init_timestamp, session=session)
        
        # Override the method
        self.client.generate_content = patched_function
",agentops/llms/providers/gemini.py,GeminiProvider,1,7
survived,"    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = ""https://quote-api.jup.ag/v6""
        self._timeout = aiohttp.ClientTimeout(total=10)  # 10 second timeout
        self._session_kwargs = {""timeout"": self._timeout}
",python/src/plugins/jupiter/goat_plugins/jupiter/service.py,JupiterService,1,7
survived,"    def __init__(self, context: PipelineContext) -> None:
        super().__init__(context)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,CheckIsInlineCandidate,0,7
survived,"    def test_pause_resume_state_initialization(self):
        """"""Test that _live_paused is properly initialized.""""""
        formatter = ConsoleFormatter()
        
        assert hasattr(formatter, '_live_paused')
        assert not formatter._live_paused",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume,1,7
survived,"            def track_resume():
                resume_calls.append(True)
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration,1,6
survived,"    def from_dict(cls, data: Dict[str, Any]) -> 'SecurityConfig':
        """"""
        Create a SecurityConfig from a dictionary.

        Args:
            data (Dict[str, Any]): Dictionary representation of a security config

        Returns:
            SecurityConfig: A new SecurityConfig instance
        """"""
        # Make a copy to avoid modifying the original
        data_copy = data.copy()

        fingerprint_data = data_copy.pop(""fingerprint"", None)
        fingerprint = Fingerprint.from_dict(fingerprint_data) if fingerprint_data else Fingerprint()

        return cls(fingerprint=fingerprint)",src/crewai/security/security_config.py,SecurityConfig,1,7
survived,"    def from_dict(cls, data: Dict[str, Any]) -> 'Fingerprint':
        """"""
        Create a Fingerprint from a dictionary representation.

        Args:
            data (Dict[str, Any]): Dictionary representation of a fingerprint

        Returns:
            Fingerprint: A new Fingerprint instance
        """"""
        if not data:
            return cls()

        fingerprint = cls(metadata=data.get(""metadata"", {}))

        # For consistency with existing stored fingerprints, we need to manually set these
        if ""uuid_str"" in data:
            object.__setattr__(fingerprint, 'uuid_str', data[""uuid_str""])
        if ""created_at"" in data and isinstance(data[""created_at""], str):
            object.__setattr__(fingerprint, 'created_at', datetime.fromisoformat(data[""created_at""]))

        return fingerprint",src/crewai/security/fingerprint.py,Fingerprint,1,7
deleted,"    async def get_embeddings_models(self) -> list[dict]:
        result = await self.ap.persistence_mgr.execute_async(sqlalchemy.select(persistence_model.EmbeddingsModel))

        models = result.all()
        return [self.ap.persistence_mgr.serialize_model(persistence_model.EmbeddingsModel, model) for model in models]
",pkg/api/http/service/model.py,EmbeddingsModelsService,1,6
deleted,"    async def remove_embeddings_model(self, model_uuid: str):
        """"""移除 Embeddings 模型""""""
        for model in self.embeddings_models:
            if model.model_entity.uuid == model_uuid:
                self.embeddings_models.remove(model)
                return
",pkg/provider/modelmgr/modelmgr.py,ModelManager,1,6
deleted,"    def validate_one_of_millisecs_selector(self) -> ""WaitAction"":
        if (self.milliseconds is None) == (self.selector is None):
            raise ValueError(""Exactly one of milliseconds or selector must be provided"")
        return self
",apps/python-sdk/firecrawl/firecrawl.py,WaitAction,1,7
survived,"def test_get_configured_catalog_without_overrides(mock_catalog, mock_stream):
    """"""Test that get_configured_catalog uses source-defined keys when no overrides exist.""""""
    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(executor=Mock(), name=""test-source"")

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.cursor_field == [""original_cursor""]
        assert configured_stream.primary_key == [[""original_pk""]]
",tests/unit_tests/sources/test_source_key_overrides.py,,1,7
survived,"    def set_cursor_key(
        self,
        stream_name: str,
        cursor_key: str,
    ) -> None:
        """"""Set the cursor for a single stream.""""""
        self._cursor_key_overrides[stream_name] = cursor_key
",airbyte/sources/base.py,Source,1,6
survived,"def test_tool_usage_limit():
    """"""Test that tools respect usage limits.""""""
    class LimitedTool(BaseTool):
        name: str = ""Limited Tool""
        description: str = ""A tool with usage limits for testing""
        max_usage_count: int = 2

        def _run(self, input_text: str) -> str:
            return f""Processed {input_text}""

    tool = LimitedTool()
    
    result1 = tool.run(input_text=""test1"")
    assert result1 == ""Processed test1""
    assert tool.current_usage_count == 1
    
    result2 = tool.run(input_text=""test2"")
    assert result2 == ""Processed test2""
    assert tool.current_usage_count == 2
",tests/tools/test_tool_usage_limit.py,,1,7
survived,"def test_tool_decorator_with_usage_limit():
    """"""Test usage limit with @tool decorator.""""""
    @tool(""Test Tool"", max_usage_count=3)
    def test_tool(input_text: str) -> str:
        """"""A test tool.""""""
        return f""Result: {input_text}""
    
    assert test_tool.max_usage_count == 3
    assert test_tool.current_usage_count == 0

    result = test_tool.run(input_text=""test"")
    assert result == ""Result: test""
    assert test_tool.current_usage_count == 1
",tests/tools/test_tool_usage_limit.py,,1,7
survived,"    def test_load_persistent_cache(self) -> None:
        """"""Test loading a persistent cache.""""""
        loader = JsonLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        
        # Create a valid JSON cache
        cache_dict = {
            ""defs"": {""var1"": ""value1""},
            ""hash"": ""hash1"",
            ""stateful_refs"": [],
            ""cache_type"": ""Pure"",
            ""hit"": True,
            ""meta"": {}
        }
        
        with open(cache_path, ""w"") as f:
            json.dump(cache_dict, f)
        
        # Load the cache
        loaded_cache = loader.load_persistent_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        assert isinstance(loaded_cache.stateful_refs, set)
        
        # Should raise for non-existent cache
        with pytest.raises(FileNotFoundError):
            loader.load_persistent_cache(""nonexistent"", ""Pure"")
        
        # Test with invalid JSON
        invalid_path = loader.build_path(""invalid"", ""Pure"")
        with open(invalid_path, ""w"") as f:
            f.write(""not valid json"")
        
        with pytest.raises(json.JSONDecodeError):
            loader.load_persistent_cache(""invalid"", ""Pure"")
        
        # Test with missing required fields
        missing_fields_path = loader.build_path(""missing"", ""Pure"")
        with open(missing_fields_path, ""w"") as f:
            json.dump({""hash"": ""missing"", ""stateful_refs"": []}, f)
        
        with pytest.raises(LoaderError, match=""Invalid json object""):
            loader.load_persistent_cache(""missing"", ""Pure"")
",tests/_save/loaders/test_json_loader.py,TestJsonLoader,1,8
survived,"    def test_cache_hit_miss(self) -> None:
        """"""Test cache hit and miss.""""""
        loader = MemoryLoader(""test"")
        assert not loader.cache_hit(""hash1"", ""Pure"")
        
        # Create and save a cache
        # Use string directly instead of Name constructor
        stateful_refs = set()
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            stateful_refs,
            ""Pure"",
            True,
            {}
        )
        loader.save_cache(cache)
        
        # Now it should hit
        assert loader.cache_hit(""hash1"", ""Pure"")
        # Different hash should miss
        assert not loader.cache_hit(""hash2"", ""Pure"")
        # Different cache type should miss
        assert not loader.cache_hit(""hash1"", ""Deferred"")
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader,1,7
survived,"    def test_build_path(self) -> None:
        """"""Test building the path for a cache file.""""""
        loader = JsonLoader(""test"", self.save_path)
        path = loader.build_path(""hash1"", ""Pure"")
        assert str(path).endswith(""P_hash1.json"")
        
        path = loader.build_path(""hash2"", ""Deferred"")
        assert str(path).endswith(""D_hash2.json"")
",tests/_save/loaders/test_json_loader.py,TestJsonLoader,1,7
deleted,"def mem0_storage_with_local_config(mock_mem0_memory):
    """"""Fixture to create a Mem0Storage instance with local mem0 config""""""

    # Patch the Memory class to return our mock
    with patch(""mem0.memory.main.Memory.from_config"", return_value=mock_mem0_memory) as mock_from_config:
        local_config = {
            ""vector_store"": {""provider"": ""mock_vector_store""},
            ""llm"": {""provider"": ""mock_llm""},
            ""embedder"": {""provider"": ""mock_embedder""},
        }
        
        crew = MockCrew(
            memory_config={
                ""provider"": ""mem0"",
                ""config"": {""user_id"": ""test_user"", ""local_mem0_config"": local_config},
            }
        )

        mem0_storage = Mem0Storage(type=""short_term"", crew=crew)
        return mem0_storage, mock_from_config, local_config
",tests/storage/test_mem0_storage.py,,1,7
survived,"def find_and_print_langsmith_run_url(client: Client, project_name: Optional[str] = None) -> Optional[str]:
    """"""Find the most recent LangSmith run and print its URL.

    Args:
        client: The LangSmith client
        project_name: Optional name of the project

    Returns:
        The URL for the run in LangSmith if found, None otherwise
    """"""
    separator = ""="" * 60
    
    try:
        # Get the most recent runs with proper filter parameters
        # We need to provide at least one filter parameter as required by the API
        recent_runs = list(
            client.list_runs(
                # Use the project name from environment variable
                project_name=project_name,
                # Limit to just the most recent run
                limit=1,
            )
        )

        if recent_runs and len(recent_runs) > 0:
            # Make sure we have a valid run object with an id attribute
            if hasattr(recent_runs[0], ""id""):
                # Convert the ID to string to ensure it's in the right format
                run_id = str(recent_runs[0].id)

                # Get the run URL using the run_id parameter
                run_url = get_langsmith_url(client, run_id=run_id, project_name=project_name)

                print(f""\n{separator}\n🔍 LangSmith Run URL: {run_url}\n{separator}"")
                return run_url
            else:
                print(f""\n{separator}\nRun object has no 'id' attribute: {recent_runs[0]}\n{separator}"")
                return None
        else:
            # If no runs found with project name, try a more general approach
            # Use a timestamp filter to get recent runs (last 10 minutes)
            ten_minutes_ago = datetime.datetime.now() - datetime.timedelta(minutes=10)

            recent_runs = list(client.list_runs(start_time=ten_minutes_ago.isoformat(), limit=1))

            if recent_runs and len(recent_runs) > 0 and hasattr(recent_runs[0], ""id""):
                # Convert the ID to string to ensure it's in the right format
                run_id = str(recent_runs[0].id)

                # Get the run URL using the run_id parameter
                run_url = get_langsmith_url(client, run_id=run_id, project_name=project_name)

                print(f""\n{separator}\n🔍 LangSmith Run URL: {run_url}\n{separator}"")
                return run_url
            else:
                print(f""\n{separator}\nNo valid runs found\n{separator}"")
                return None
    except Exception as e:
        print(f""\n{separator}\nCould not retrieve LangSmith URL: {e}"")
        import traceback

        print(traceback.format_exc())
        print(separator)
        return None",src/codegen/extensions/langchain/utils/get_langsmith_url.py,,1,7
survived,"    def validate_website_url(cls, v):
        if not v:
            raise ValueError(""Website URL cannot be empty"")
        
        if len(v) > 2048:  # Common maximum URL length
            raise ValueError(""URL is too long (max 2048 characters)"")
            
        if not re.match(r'^https?://', v):
            raise ValueError(""URL must start with http:// or https://"")
            
        try:
            result = urlparse(v)
            if not all([result.scheme, result.netloc]):
                raise ValueError(""Invalid URL format"")
        except Exception as e:
            raise ValueError(f""Invalid URL: {str(e)}"")
            
        if re.search(r'\s', v):
            raise ValueError(""URL cannot contain whitespace"")
            
        return v
",crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py,SeleniumScrapingToolSchema,1,7
survived,"def mock_driver_with_html(html_content):
    driver = MagicMock()
    mock_element = MagicMock()
    mock_element.get_attribute.return_value = html_content
    bs = BeautifulSoup(html_content, ""html.parser"")
    mock_element.text = bs.get_text()

    driver.find_elements.return_value = [mock_element]
    driver.find_element.return_value = mock_element

    return driver
",tests/tools/selenium_scraping_tool_test.py,,1,6
survived,"    def validate_metadata(cls, v):
        """"""Validate that metadata is a dictionary with string keys and valid values.""""""
        if not isinstance(v, dict):
            raise ValueError(""Metadata must be a dictionary"")
        
        # Validate that all keys are strings
        for key, value in v.items():
            if not isinstance(key, str):
                raise ValueError(f""Metadata keys must be strings, got {type(key)}"")
            
            # Validate nested dictionaries (prevent deeply nested structures)
            if isinstance(value, dict):
                # Check for nested dictionaries (limit depth to 1)
                for nested_key, nested_value in value.items():
                    if not isinstance(nested_key, str):
                        raise ValueError(f""Nested metadata keys must be strings, got {type(nested_key)}"")
                    if isinstance(nested_value, dict):
                        raise ValueError(""Metadata can only be nested one level deep"")
        
        # Check for maximum metadata size (prevent DoS)
        if len(str(v)) > 10000:  # Limit metadata size to 10KB
            raise ValueError(""Metadata size exceeds maximum allowed (10KB)"")
            
        return v
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def __eq__(self, other) -> bool:
        """"""Compare fingerprints by their UUID.""""""
        if isinstance(other, Fingerprint):
            return self.uuid_str == other.uuid_str
        return False
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        top_links = map_website[:3] if isinstance(map_website, list) else []
        print(f""{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}"")
        
        for link in top_links:
            print(f""{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}"")
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f""{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}"")
     
            
            check_prompt = f""""""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """"""
        
            completion = client.chat.completions.create(
            model=""qwen/qwen3-30b-a3b:free"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": [
                        {
                            ""type"": ""text"",
                            ""text"": check_prompt
                        }
                    ]
                }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != ""Objective not met"":
                print(f""{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}"")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f""{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}"")
            else:
                print(f""{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}"")
        
        print(f""{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}"")
        return None
    
    except Exception as e:
        print(f""{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}"")
        return None
",examples/qwen3-web-crawler/qwen3_web_crawler.py,,1,6
survived,"async def _search_for_implementation(tool: ToolDefinition) -> str:
    """"""
    Use web_search to find implementation details for a tool.
    
    Args:
        tool: The tool definition
        
    Returns:
        Implementation details as a string
    """"""
    try:
        try:
            from agents import Agent, Runner, function_tool
            from agents.tools import web_search
        except ImportError:
            return f""# Note: OpenAI Agents SDK not installed. Install with: pip install openai-agents\n""
            
        query = f""python implementation for {tool.name} function""
        if tool.description:
            query += f"" that {tool.description.lower()}""
            
        if tool.parameters:
            param_names = [p.name for p in tool.parameters]
            query += f"" with parameters {', '.join(param_names)}""
            
        search_results = await web_search(query, num_results=3)
        
        if not search_results or isinstance(search_results, str) and ""error"" in search_results.lower():
            return f""    # Could not find implementation details via web search\n    # Implement the {tool.name} functionality here\n    return f\""Implementation for {tool.name} with parameters: {{{', '.join([p.name + '=' + p.name for p in tool.parameters])}}}\""""
            
        return search_results
    except Exception as e:
        return f""    # Error during web search: {str(e)}\n    # Implement the {tool.name} functionality here\n    return f\""Implementation for {tool.name} with parameters: {{{', '.join([p.name + '=' + p.name for p in tool.parameters])}}}\""""
",meta_agent/generators/tool_generator.py,,0,6
survived,"    def test_call(
        self, mock_openai_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the openai class.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = ""Test response""
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response

        model = openai(""gpt-4"")
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig(
                max_tokens=100,
                temperature=0.7,
                top_p=0.9,
                frequency_penalty=0.5,
                presence_penalty=0.5,
            )

            result = model(messages, config)
            assert result == ""Test response""

            mock_openai_class.assert_called_once_with(
                api_key=""test-key"", base_url=None
            )
        mock_client.chat.completions.create.assert_called_once()
        call_args = mock_client.chat.completions.create.call_args[1]
        assert call_args[""model""] == ""gpt-4""
        assert len(call_args[""messages""]) == 2
        assert call_args[""messages""][0][""role""] == ""system""
        assert call_args[""messages""][0][""content""] == DEFAULT_SYSTEM_MESSAGE
        assert call_args[""messages""][1][""role""] == ""user""
        assert call_args[""messages""][1][""content""] == ""Test prompt""
        assert call_args[""max_tokens""] == 100
        assert call_args[""temperature""] == 0.7
        assert call_args[""top_p""] == 0.9
        assert call_args[""frequency_penalty""] == 0.5
        assert call_args[""presence_penalty""] == 0.5
        assert call_args[""stream""] is False
",tests/_ai/llm/_impl.py,TestOpenAI,1,7
survived,"    def test_call(
        self, mock_anthropic_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the anthropic class.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_anthropic_class.return_value = mock_client
        mock_response = MagicMock()
        mock_content = MagicMock()
        mock_content.type = ""text""
        mock_content.text = ""Test response""
        mock_response.content = [mock_content]
        mock_client.messages.create.return_value = mock_response

        model = anthropic(""claude-3-opus-20240229"")
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig(
                max_tokens=100,
                temperature=0.7,
                top_p=0.9,
                top_k=10,
            )

            result = model(messages, config)
            assert result == ""Test response""

            mock_anthropic_class.assert_called_once_with(
                api_key=""test-key"", base_url=None
            )
        mock_client.messages.create.assert_called_once()
        call_args = mock_client.messages.create.call_args[1]
        assert call_args[""model""] == ""claude-3-opus-20240229""
        assert call_args[""system""] == DEFAULT_SYSTEM_MESSAGE
        assert call_args[""max_tokens""] == 100
        assert call_args[""temperature""] == 0.7
        assert call_args[""top_p""] == 0.9
        assert call_args[""top_k""] == 10
        assert call_args[""stream""] is False
",tests/_ai/llm/_impl.py,TestAnthropic,1,8
survived,"    def test_call_azure(
        self, mock_azure_openai_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the openai class with Azure OpenAI.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_azure_openai_class.return_value = mock_client
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = ""Test response""
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response

        model = openai(
            ""gpt-4"",
            base_url=""https://example.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2023-05-15"",
        )
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig()

            result = model(messages, config)
            assert result == ""Test response""

            mock_azure_openai_class.assert_called_once_with(
                api_key=""test-key"",
                api_version=""2023-05-15"",
                azure_endpoint=""https://example.openai.azure.com"",
            )
        mock_client.chat.completions.create.assert_called_once()
        call_args = mock_client.chat.completions.create.call_args[1]
        assert call_args[""model""] == ""gpt-4""
",tests/_ai/llm/_impl.py,TestOpenAI,1,7
survived,"        def accepts_mime_bundle_or_tuple(
            bundle_or_tuple: MimeBundleOrTuple
        ) -> MimeBundleOrTuple:
            return bundle_or_tuple
",tests/_messaging/test_mimetypes.py,TestMimeTypes,1,6
survived,"    def test_initialization_without_completion_info(self) -> None:
        # Test initialization without completion_info
        option = CompletionOption(
            name=""test_var"",
            type=""variable"",
            completion_info=None,
        )

        assert option.name == ""test_var""
        assert option.type == ""variable""
        assert option.completion_info is None
",tests/_messaging/test_completion_option.py,TestCompletionOption,1,7
survived,"    def test_add_output_to_buffer_no_merge(self) -> None:
        # Test adding output for an existing cell with different stream or mimetype
        outputs_buffered_per_cell = {
            ""cell1"": [
                ConsoleMsg(
                    stream=CellChannel.STDOUT,
                    cell_id=""cell1"",
                    data=""Hello"",
                    mimetype=""text/plain"",
                )
            ]
        }
        msg = ConsoleMsg(
            stream=CellChannel.STDERR,
            cell_id=""cell1"",
            data=""Error"",
            mimetype=""text/plain"",
        )

        _add_output_to_buffer(msg, outputs_buffered_per_cell)

        assert len(outputs_buffered_per_cell[""cell1""]) == 2
        assert outputs_buffered_per_cell[""cell1""][0].data == ""Hello""
        assert outputs_buffered_per_cell[""cell1""][1].data == ""Error""
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker,1,7
survived,"    def write(self, op: str, data: dict) -> None:
        self.messages.append((op, data))
",tests/_messaging/test_console_output_worker.py,MockStream,1,7
survived,"def test_colorized_url() -> None:
    """"""Test the _colorized_url function.""""""
    # Test with a simple URL
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        result = _colorized_url(""http://localhost:8000"")
        mock_bold.assert_called_once_with(""http://localhost:8000"")
        assert result == ""BOLD_URL""
    
    # Test with a URL with a path
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        result = _colorized_url(""http://localhost:8000/path"")
        mock_bold.assert_called_once_with(""http://localhost:8000/path"")
        assert result == ""BOLD_URL""
    
    # Test with a URL with a query string
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        with patch(""marimo._server.print.muted"") as mock_muted:
            mock_muted.return_value = ""MUTED_QUERY""
            result = _colorized_url(""http://localhost:8000/path?query=value"")
            # The implementation separates the query part and applies muted() to it
            mock_bold.assert_called_once_with(""http://localhost:8000/pathMUTED_QUERY"")
            assert result == ""BOLD_URL""
",tests/_server/test_print.py,,0,7
survived,"def test_custodial_wallet_message_signing(custodial_api, test_email, test_message, solana_connection):
    """"""Test message signing with custodial wallet.""""""
    # Create wallet and client
    wallet = custodial_api.create_custodial_wallet(test_email)
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    
    # Sign message
    signature = client.sign_message(test_message)
    assert ""signature"" in signature
    assert len(signature[""signature""]) > 0  # Should be base58 encoded
",python/src/wallets/crossmint/tests/test_custodial_wallet.py,,1,7
survived,"def test_custodial_wallet_balance(custodial_api, test_email, solana_connection):
    """"""Test getting wallet balance.""""""
    # Create wallet and client
    wallet = custodial_api.create_custodial_wallet(test_email)
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    
    # Get balance
    balance = client.balance_of(wallet[""address""])
    assert ""value"" in balance
    assert ""symbol"" in balance
    assert balance[""symbol""] == ""SOL""
    assert ""decimals"" in balance
    assert balance[""decimals""] == 9
    assert ""name"" in balance
    assert balance[""name""] == ""Solana""
    assert ""in_base_units"" in balance
",python/src/wallets/crossmint/tests/test_custodial_wallet.py,,1,7
survived,"def compare_approval_responses(py_response: Dict[str, Any], ts_response: Dict[str, Any]) -> None:
    """"""Compare approval responses between implementations.
    
    Args:
        py_response: Response from Python implementation
        ts_response: Response from TypeScript implementation
        
    Raises:
        AssertionError: If responses don't match
    """"""
    # Compare pending approvals if present
    if ""approvals"" in py_response or ""approvals"" in ts_response:
        py_approvals = py_response.get(""approvals"", {}).get(""pending"", [])
        ts_approvals = ts_response.get(""approvals"", {}).get(""pending"", [])
        assert len(py_approvals) == len(ts_approvals), ""Number of pending approvals doesn't match""
        
        for py_approval, ts_approval in zip(py_approvals, ts_approvals):
            assert py_approval.get(""message"") == ts_approval.get(""message""), ""Approval messages don't match""
            assert py_approval.get(""signer"") == ts_approval.get(""signer""), ""Approval signers don't match""
",python/src/wallets/crossmint/tests/utils/helpers.py,,1,6
survived,"def test_custodial_wallet_raw_transaction(custodial_api, test_email, solana_connection):
    """"""Test sending raw transaction with custodial wallet.""""""
    # Create wallet and client
    wallet = custodial_api.create_custodial_wallet(test_email)
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    
    # Create a simple message
    message = Message(
        instructions=[],  # Empty for test
        payer=Pubkey.from_string(wallet[""address""])
    )
    
    # Serialize and encode
    serialized = b58encode(bytes(message)).decode()
    
    # Send raw transaction
    tx = client.send_raw_transaction(serialized)
    assert tx[""status""] in [""success"", ""pending""]
    if tx[""status""] == ""success"":
        assert len(tx[""hash""]) > 0
",python/src/wallets/crossmint/tests/test_custodial_wallet.py,,0,7
survived,"def sample_config() -> Mapping[str, Any]:
    return {
        ""client_id"": ""test_client_id"",
        ""client_secret"": ""test_client_secret"",
        ""box_subject_type"": ""user"",
        ""box_subject_id"": ""test_box_subject_id"",
        ""folder_id"": ""test_folder_id"",
    }
",airbyte-integrations/connectors/source-box-data-extract/unit_tests/test_streams.py,,1,6
survived,"    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:
        """"""
        :return: string if single primary key, list of strings if composite primary key, list of list of strings if composite primary key consisting of nested fields.
          If the stream has no primary keys, return None.
        """"""
        return ""id""
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractFolder,1,7
survived,"def box_folder_ai_extract_structured(
    client: BoxClient, folder_id: str, fields_json_str: str, is_recursive: bool = False, by_pass_text_extraction: bool = False
) -> Iterable[BoxFileExtended]:
    # folder items iterator
    for item in client.folders.get_folder_items(folder_id).entries:
        if item.type == ""file"":
            file = box_file_get_by_id(client=client, file_id=item.id)
            if not by_pass_text_extraction:
                text_representation = box_file_ai_extract_structured(client=client, file_id=item.id, fields_json_str=fields_json_str)
            else:
                text_representation = """"
            yield BoxFileExtended(file=file, text_representation=text_representation)
        elif item.type == ""folder"" and is_recursive:
            yield from box_folder_ai_extract_structured(
                client=client,
                folder_id=item.id,
                fields_json_str=fields_json_str,
                is_recursive=is_recursive,
                by_pass_text_extraction=by_pass_text_extraction,
            )",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,,1,7
survived,"def get_current_weather(location: str, unit: str) -> str:
    """"""
    Get the current weather in a given location.
    
    Args:
        location: The city and state, e.g. San Francisco, CA or country e.g., London, UK
        unit: The temperature unit to use. Either ""celsius"" or ""fahrenheit"".
        
    Returns:
        A string containing the weather information.
    """"""
    # This is a mock implementation - in a real application, you would call a weather API
    weather_data = {
        ""New York"": {""temperature"": 22, ""condition"": ""Sunny""},
        ""London"": {""temperature"": 15, ""condition"": ""Cloudy""},
        ""Tokyo"": {""temperature"": 28, ""condition"": ""Rainy""},
        ""Sydney"": {""temperature"": 31, ""condition"": ""Hot and sunny""},
    }
    
    # Default weather if location not found
    default_weather = {""temperature"": 20, ""condition"": ""Clear""}
    
    # Get weather for the location (case insensitive)
    location_key = next((k for k in weather_data.keys() if k.lower() == location.lower()), None)
    weather = weather_data.get(location_key, default_weather)
    
    # Convert temperature if needed
    temp = weather[""temperature""]
    if unit.lower() == ""fahrenheit"":
        temp = (temp * 9/5) + 32
    
    return f""The current weather in {location} is {weather['condition']} with a temperature of {temp}°{'F' if unit.lower() == 'fahrenheit' else 'C'}.""
",openai-agents-examples/05_agent_with_function_tools.py,,1,7
survived,"def main():
    """"""Main function to parse arguments and run the research blog system.""""""
    parser = argparse.ArgumentParser(description=""Research and Blog Agent System"")
    parser.add_argument(""--topic"", ""-t"", type=str, required=True, 
                        help=""The topic for the blog post"")
    parser.add_argument(""--output"", ""-o"", type=str, default=None,
                        help=""Optional file path to save the markdown blog post"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Create the blog post
        console.print(Panel(f""Creating a blog post about '{args.topic}'..."", title=""Status"", border_style=""blue""))
        blog_post = asyncio.run(create_research_blog(args.topic))
        
        # Display the blog post
        console.print(Panel(Markdown(blog_post), title=""Blog Post"", border_style=""green""))
        
        # Save to file if output path is provided
        if args.output:
            with open(args.output, ""w"") as f:
                f.write(blog_post)
            console.print(f""[green]Blog post saved to {args.output}[/green]"")
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"async def orchestrate_content_creation(prompt: str) -> str:
    """"""
    Orchestrate the content creation process with multiple specialized agents.
    
    Args:
        prompt: The content request
        
    Returns:
        The final polished content
    """"""
    # Create specialist agents
    research_agent = create_research_agent()
    outline_agent = create_outline_agent()
    content_agent = create_content_agent()
    editor_agent = create_editor_agent()
    
    # Create manager agent with specialists
    manager = create_manager_agent([research_agent, outline_agent, content_agent, editor_agent])
    
    # Create a context to track the workflow
    context = Context()
    
    # Run the manager agent with the prompt and context
    result = await Runner.run(manager, prompt, context=context)
    
    # Return the final content
    return result.final_output
",openai-agents-examples/11_agent_orchestration.py,,1,7
survived,"def create_agents_symlink():
    """"""Create a symlink from agents to openai.agents if needed.""""""
    try:
        import openai
        if hasattr(openai, 'agents'):
            # Create a symlink in site-packages
            site_packages = next(p for p in sys.path if 'site-packages' in p)
            agents_path = os.path.join(site_packages, 'agents')
            if not os.path.exists(agents_path):
                os.symlink(os.path.join(site_packages, 'openai', 'agents'), agents_path)
                print(f""Created symlink from {agents_path} to openai.agents"")
            else:
                print(f""Agents path already exists at {agents_path}"")
    except (ImportError, StopIteration, OSError) as e:
        print(f""Could not create symlink: {e}"")
",openai-agents-examples/fix_imports.py,,0,7
survived,"def main():
    """"""Main function to parse arguments and run the customer support system.""""""
    parser = argparse.ArgumentParser(description=""Agent with Handoffs Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The customer inquiry to send to the support system"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the customer support system and get response
        response = asyncio.run(run_customer_support_system(args.prompt))
        
        # Display the response
        console.print(Panel(response, title=""Customer Support Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/07_agent_with_handoffs.py,,1,7
survived,"def test_content_moderation_guardrail():
    """"""Test that the content moderation guardrail correctly filters inputs.""""""
    guardrail = ContentModerationGuardrail()
    
    # Test safe input
    safe_input = ""Tell me about renewable energy sources""
    assert guardrail.filter(safe_input) == safe_input
    
    # Test unsafe input
    unsafe_input = ""How to hack into a computer system""
    assert guardrail.filter(unsafe_input) is None
    assert ""harmful"" in guardrail.get_rejection_message(unsafe_input)
",openai-agents-examples/10_agent_with_guardrails.py,,1,7
survived,"def create_science_agent() -> Agent:
    """"""
    Create a science specialist agent.
    
    Returns:
        An Agent instance specialized in scientific topics.
    """"""
    instructions = """"""
    You are a science specialist with deep knowledge of physics, chemistry, biology, and related fields.
    Provide accurate, detailed scientific explanations while making complex concepts accessible.
    Use analogies and examples when helpful to illustrate scientific principles.
    Always clarify when something is theoretical or not yet proven.
    """"""
    
    return Agent(
        name=""ScienceSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for questions about scientific topics, theories, and concepts.""
    )
",openai-agents-examples/02_multi_agent.py,,1,7
survived,"    def __init__(self, api_key: Optional[str] = None):
        """"""
        Initialize the Anthropic model provider.
        
        Args:
            api_key: Anthropic API key. If None, will use the ANTHROPIC_API_KEY environment variable.
        """"""
        self.api_key = api_key or os.environ.get(""ANTHROPIC_API_KEY"")
        if not self.api_key:
            raise ValueError(""Anthropic API key is required"")
        
        self.client = anthropic.Anthropic(api_key=self.api_key)
",openai-agents-examples/12_anthropic_agent.py,AnthropicModelProvider,1,8
survived,"    async def generate(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        **kwargs
    ) -> ModelResponse:
        """"""
        Generate a response using Anthropic's Claude model.
        
        Args:
            messages: List of messages in the conversation
            model: Model name (will be mapped to Anthropic model)
            temperature: Sampling temperature
            max_tokens: Maximum number of tokens to generate
            **kwargs: Additional arguments to pass to the model
            
        Returns:
            A ModelResponse containing the model's response
        """"""
        # Map OpenAI model names to Anthropic model names
        model_mapping = {
            ""gpt-4o-mini"": ""claude-3-haiku-20240307"",
            ""gpt-4o"": ""claude-3-opus-20240229"",
            ""gpt-3.5-turbo"": ""claude-3-sonnet-20240229"",
        }
        
        # Use the mapped model or default to claude-3-haiku
        anthropic_model = model_mapping.get(model, ""claude-3-haiku-20240307"")
        
        # Convert OpenAI message format to Anthropic message format
        anthropic_messages = []
        for message in messages:
            role = message[""role""]
            # Map OpenAI roles to Anthropic roles
            if role == ""system"":
                # System messages are handled differently in Anthropic
                system_content = message.get(""content"", """")
                continue
            elif role == ""user"":
                anthropic_role = ""user""
            elif role == ""assistant"":
                anthropic_role = ""assistant""
            else:
                # Skip unsupported roles
                continue
            
            # Add the message
            anthropic_messages.append({
                ""role"": anthropic_role,
                ""content"": message.get(""content"", """")
            })
        
        # Create the message with system prompt if available
        try:
            response = await self.client.messages.create(
                model=anthropic_model,
                messages=anthropic_messages,
                system=system_content if 'system_content' in locals() else """",
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # Convert Anthropic response to OpenAI format
            output_message = {
                ""role"": ""assistant"",
                ""content"": response.content[0].text
            }
            
            # Create a ModelResponse
            return ModelResponse(
                output=[output_message],
                usage={
                    ""prompt_tokens"": response.usage.input_tokens,
                    ""completion_tokens"": response.usage.output_tokens,
                    ""total_tokens"": response.usage.input_tokens + response.usage.output_tokens
                },
                referenceable_id=None
            )
        
        except Exception as e:
            raise Exception(f""Error generating response from Anthropic: {str(e)}"")
",openai-agents-examples/12_anthropic_agent.py,AnthropicModelProvider,1,7
survived,"    async def kill(self):
        """"""停止适配器""""""
        await self.logger.info('WebChat调试适配器正在停止')
",pkg/platform/sources/webchat.py,WebChatAdapter,1,6
survived,"    def recreate_collection(self):
        """"""Recreate the docs collection.""""""
        try:
            self.client.collections['docs'].delete()
            logger.info(""Deleted existing 'docs' collection"")
        except Exception as e:
            logger.info(f""Collection 'docs' doesn't exist or couldn't be deleted: {e}"")
        
        self.client.collections.create(COLLECTION_SCHEMA)
        logger.info(""Created new 'docs' collection"")
",scripts/typesense_indexer.py,TypesenseIndexer,1,7
survived,"    def get_all(self, table_name):
        """"""Get all items from a table.""""""
        if table_name not in self.data:
            Logger.warning(self.logger, f""Table '{table_name}' not found"")
            return []
        
        items = list(self.data[table_name].values())
        Logger.debug(self.logger, f""Retrieved {len(items)} items from '{table_name}'"")
        return items
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase,1,7
survived,"    def format_as_summary(self):
        """"""
        Format the data as a summary report.
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data to format"")
            return self._create_result()
        
        try:
            # Create summary
            summary = {
                ""report_type"": ""summary"",
                ""generated_at"": datetime.now().isoformat(),
                ""data_source"": self.metadata.get(""input_metadata"", {}).get(""source"", ""unknown""),
                ""record_count"": len(self.data) if isinstance(self.data, list) else 1
            }
            
            # Add statistics if available
            if self.analysis and ""statistics"" in self.analysis:
                summary[""statistics""] = self.analysis[""statistics""]
            
            # Add processing information
            if ""processing_metadata"" in self.metadata:
                processing_meta = self.metadata[""processing_metadata""]
                if ""processing_steps"" in processing_meta:
                    summary[""processing_steps""] = processing_meta[""processing_steps""]
                if ""processing_time_seconds"" in processing_meta:
                    summary[""processing_time_seconds""] = processing_meta[""processing_time_seconds""]
            
            # Store the summary
            self.summary = summary
            
            # Update metadata
            self.metadata[""output_formats""].append(""summary"")
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Summary formatting error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/output_stage.py,OutputStage,1,7
survived,"    def get_all_categories():
        """"""Get all categories.""""""
        try:
            categories = CategoryService.get_all_categories()
            return {
                ""success"": True,
                ""data"": categories
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in get_all_categories: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while retrieving categories""
            }
",codebase-architectures/layered-architecture/api/category_api.py,CategoryAPI,1,7
survived,"    def query(self, table_name, filter_func):
        """"""Query items from a table using a filter function.""""""
        if table_name not in self.data:
            Logger.warning(self.logger, f""Table '{table_name}' not found for query"")
            return []
        
        items = list(self.data[table_name].values())
        filtered_items = [item for item in items if filter_func(item)]
        Logger.debug(self.logger, f""Query returned {len(filtered_items)} items from '{table_name}'"")
        return filtered_items
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase,1,7
survived,"    def change_password(token: str, current_password: str, new_password: str) -> Dict:
        """"""
        Change a user's password.
        
        Args:
            token: Authentication token
            current_password: The current password
            new_password: The new password
            
        Returns:
            Response with success status or error message
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Change password
        success, result = change_password(user_data[""id""], current_password, new_password)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Password changed successfully"",
                ""data"": None
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": result.get(""error"", ""Password change failed""),
                ""data"": None
            }",codebase-architectures/atomic-composable-architecture/endpoints/user_api.py,UserAPI,0,8
survived,"    def logout(token: str) -> Dict:
        """"""
        Logout a user.
        
        Args:
            token: Authentication token
            
        Returns:
            Response with success status
        """"""
        success = logout_user(token)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Logout successful"",
                ""data"": None
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid token"",
                ""data"": None
            }
",codebase-architectures/atomic-composable-architecture/endpoints/user_api.py,UserAPI,1,6
survived,"def authenticate(username: str, password: str) -> Optional[Dict]:
    """"""
    Authenticate a user with username and password.
    
    Args:
        username: The username to authenticate
        password: The password to authenticate
        
    Returns:
        User data dictionary if authentication succeeds, None otherwise
    """"""
    if username not in USER_STORE:
        return None
    
    user_data = USER_STORE[username]
    if verify_password(password, user_data[""hashed_password""], user_data[""salt""]):
        return {k: v for k, v in user_data.items() if k not in [""hashed_password"", ""salt""]}
    
    return None
",codebase-architectures/atomic-composable-architecture/modules/auth.py,,1,7
survived,"def validate_email(email: str) -> bool:
    """"""
    Validate an email address format.
    
    Args:
        email: The email address to validate
        
    Returns:
        True if the email is valid, False otherwise
    """"""
    # Simple regex for email validation
    # In a real application, consider using a more comprehensive validation
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, email))
",codebase-architectures/atomic-composable-architecture/modules/validation.py,,1,7
survived,"    def to_dict(self):
        """"""Convert user to dictionary.""""""
        return {
            ""id"": self.id,
            ""username"": self.username,
            ""email"": self.email,
            ""name"": self.name,
            ""created_at"": self.created_at,
            ""updated_at"": self.updated_at
        }
",codebase-architectures/vertical-slice-architecture/features/users/model.py,User,1,7
survived,"    def delete_alert(token: str, notification_id: str) -> Dict:
        """"""
        Delete an alert.
        
        Args:
            token: Authentication token
            notification_id: The ID of the notification
            
        Returns:
            Response with success status or error message
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Delete alert
        success = delete_user_alert(user_data[""id""], notification_id)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Alert deleted successfully"",
                ""data"": None
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": ""Alert not found"",
                ""data"": None
            }
",codebase-architectures/atomic-composable-architecture/endpoints/alerts_api.py,AlertsAPI,1,7
survived,"    def __init__(self, name, description=None, id=None):
        """"""Initialize a category.""""""
        self.id = id
        self.name = name
        self.description = description
        self.created_at = datetime.now().isoformat()
        self.updated_at = self.created_at
",codebase-architectures/layered-architecture/models/category.py,Category,1,7
survived,"    def mark_as_read(token: str, notification_id: str) -> Dict:
        """"""
        Mark an alert as read.
        
        Args:
            token: Authentication token
            notification_id: The ID of the notification
            
        Returns:
            Response with success status or error message
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Mark as read
        success = mark_alert_as_read(user_data[""id""], notification_id)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Alert marked as read"",
                ""data"": None
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": ""Alert not found"",
                ""data"": None
            }
",codebase-architectures/atomic-composable-architecture/endpoints/alerts_api.py,AlertsAPI,1,7
survived,"    def transform_data(self, transform_func):
        """"""
        Apply a transformation function to the data.
        
        Args:
            transform_func: Function to transform the data
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data loaded to transform"")
            return self._create_result()
        
        try:
            self.data = transform_func(self.data)
            self.metadata[""status""] = ""transformed""
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Transformation error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/input_stage.py,InputStage,1,7
survived,"def get_timestamp():
    """"""Get the current timestamp.""""""
    return datetime.now().isoformat()
",codebase-architectures/vertical-slice-architecture/shared/utils.py,,1,7
survived,"    def finalize(self):
        """"""
        Finalize the processing stage.
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.metadata[""status""] not in [""error"", ""skipped""]:
            self.metadata[""status""] = ""completed""
            self.metadata[""completed_at""] = datetime.now().isoformat()
            
            # Calculate processing time if we have start time
            if ""started_at"" in self.metadata:
                start_time = datetime.fromisoformat(self.metadata[""started_at""])
                end_time = datetime.fromisoformat(self.metadata[""completed_at""])
                processing_time = (end_time - start_time).total_seconds()
                self.metadata[""processing_time_seconds""] = processing_time
        
        return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage,1,7
survived,"    def configure_processing(self, config):
        """"""
        Configure the processing stage.
        
        Args:
            config: Dictionary with processing configuration
        """"""
        self.processing_config = config
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,DataProcessingPipeline,1,6
survived,"    def create_category(name, description=None):
        """"""Create a new category.""""""
        try:
            # Validate category name
            if not name or not isinstance(name, str):
                raise ValueError(""Category name is required and must be a string"")
            
            # Check if category with same name already exists
            existing_categories = db.query(""categories"", lambda c: c[""name""].lower() == name.lower())
            if existing_categories:
                raise ValueError(f""Category with name '{name}' already exists"")
            
            # Create and save category
            category = Category(name=name, description=description)
            saved_category = db.insert(""categories"", category.to_dict())
            Logger.info(app_logger, f""Created category: {name}"")
            return saved_category
        except Exception as e:
            Logger.error(app_logger, f""Error creating category: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/category_service.py,CategoryService,1,7
survived,"    def get_all_categories():
        """"""Get all categories.""""""
        try:
            categories = db.get_all(""categories"")
            Logger.info(app_logger, f""Retrieved {len(categories)} categories"")
            return categories
        except Exception as e:
            Logger.error(app_logger, f""Error getting all categories: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/category_service.py,CategoryService,1,7
survived,"    def load_data(self, source, source_type=""json""):
        """"""
        Load data from the specified source.
        
        Args:
            source: Path to the data file or raw data
            source_type: Type of data source (json, csv, raw)
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        try:
            self.metadata[""source""] = source
            self.metadata[""source_type""] = source_type
            
            # Load data based on source type
            if source_type == ""json"":
                if isinstance(source, str) and os.path.exists(source):
                    self.data = load_json_file(source)
                elif isinstance(source, str):
                    self.data = json.loads(source)
                else:
                    self.data = source
            elif source_type == ""csv"":
                self.data = load_csv_file(source)
            elif source_type == ""raw"":
                self.data = source
            else:
                raise ValueError(f""Unsupported source type: {source_type}"")
            
            self.metadata[""status""] = ""data_loaded""
            self.metadata[""record_count""] = len(self.data) if isinstance(self.data, list) else 1
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(str(e))
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/input_stage.py,InputStage,1,7
survived,"    def warning(logger, message):
        """"""Log a warning message.""""""
        logger.warning(message)
",codebase-architectures/layered-architecture/utils/logger.py,Logger,1,7
survived,"    def get_user(user_id):
        """"""Get a user by ID.""""""
        user = UserService.get_user(user_id)
        if not user:
            return {""error"": f""User with ID {user_id} not found""}
        return user
",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI,1,7
survived,"def validate_user_token(token: str) -> Tuple[bool, Optional[Dict]]:
    """"""
    Validate a user token and return user data.
    
    Args:
        token: The token to validate
        
    Returns:
        Tuple of (success, user_data) where user_data is None if validation fails
    """"""
    if not token:
        return False, None
    
    user_id = validate_token(token)
    if not user_id:
        return False, None
    
    user_data = get_user_by_id(user_id)
    if not user_data:
        return False, None
    
    return True, user_data
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,,1,7
survived,"def validate_data(data: Dict[str, Any], schema: Dict[str, Dict[str, Any]]) -> Dict[str, List[str]]:
    """"""
    Validate data against a schema.
    
    Args:
        data: The data to validate
        schema: Validation schema defining field types and constraints
        
    Returns:
        Dictionary mapping field names to lists of validation error messages
    """"""
    errors: Dict[str, List[str]] = {}
    
    for field_name, field_schema in schema.items():
        field_type = field_schema.get(""type"")
        required = field_schema.get(""required"", False)
        
        # Check if required field is missing
        if required and (field_name not in data or data[field_name] is None):
            errors.setdefault(field_name, []).append(""Field is required"")
            continue
        
        # Skip validation for optional fields that are not present
        if field_name not in data or data[field_name] is None:
            continue
        
        value = data[field_name]
        
        # Type validation
        if field_type == ""string"" and not isinstance(value, str):
            errors.setdefault(field_name, []).append(""Must be a string"")
        elif field_type == ""number"" and not isinstance(value, (int, float)):
            errors.setdefault(field_name, []).append(""Must be a number"")
        elif field_type == ""integer"" and not isinstance(value, int):
            errors.setdefault(field_name, []).append(""Must be an integer"")
        elif field_type == ""boolean"" and not isinstance(value, bool):
            errors.setdefault(field_name, []).append(""Must be a boolean"")
        elif field_type == ""array"" and not isinstance(value, list):
            errors.setdefault(field_name, []).append(""Must be an array"")
        elif field_type == ""object"" and not isinstance(value, dict):
            errors.setdefault(field_name, []).append(""Must be an object"")
        
        # String-specific validations
        if field_type == ""string"" and isinstance(value, str):
            min_length = field_schema.get(""min_length"")
            max_length = field_schema.get(""max_length"")
            pattern = field_schema.get(""pattern"")
            
            if min_length is not None and len(value) < min_length:
                errors.setdefault(field_name, []).append(f""Must be at least {min_length} characters"")
            
            if max_length is not None and len(value) > max_length:
                errors.setdefault(field_name, []).append(f""Must be at most {max_length} characters"")
            
            if pattern is not None and not re.match(pattern, value):
                errors.setdefault(field_name, []).append(""Does not match required pattern"")
        
        # Number-specific validations
        if field_type in [""number"", ""integer""] and isinstance(value, (int, float)):
            minimum = field_schema.get(""minimum"")
            maximum = field_schema.get(""maximum"")
            
            if minimum is not None and value < minimum:
                errors.setdefault(field_name, []).append(f""Must be at least {minimum}"")
            
            if maximum is not None and value > maximum:
                errors.setdefault(field_name, []).append(f""Must be at most {maximum}"")
    
    return errors",codebase-architectures/atomic-composable-architecture/modules/validation.py,,1,7
survived,"    def error(logger, message, exc_info=None):
        """"""Log an error message.""""""
        logger.error(message, exc_info=exc_info)
",codebase-architectures/layered-architecture/utils/logger.py,Logger,1,6
survived,"def get_timestamp():
    """"""Get the current timestamp.""""""
    return datetime.now().isoformat()
",codebase-architectures/pipeline-architecture/shared/utilities.py,,1,7
survived,"    def __init__(self, command: str, path: str = None, **kwargs):
        """"""
        Initialize a tool use request.
        
        Args:
            command: The command to execute
            path: The path to operate on
            **kwargs: Additional arguments for the command
        """"""
        self.command = command
        self.path = path
        self.kwargs = kwargs
",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/model.py,ToolUseRequest,1,7
survived,"    def _insert_text(self, path: str, insert_line: int, new_str: str) -> FileOperationResult:
        """"""
        Insert text at a specific location in a file.

        Args:
            path: The path to the file to modify
            insert_line: The line number after which to insert the text
            new_str: The text to insert

        Returns:
            FileOperationResult with result or error message
        """"""
        try:
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            if not os.path.exists(path):
                error_msg = f""File {path} does not exist""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            if insert_line is None:
                error_msg = ""No line number specified: insert_line is missing.""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            with open(path, ""r"") as f:
                lines = f.readlines()

            # Line is 0-indexed for this function, but Claude provides 1-indexed
            insert_line = min(max(0, insert_line - 1), len(lines))

            # Check that the index is within acceptable bounds
            if insert_line < 0 or insert_line > len(lines):
                error_msg = (
                    f""Insert line number {insert_line} out of range (0-{len(lines)}).""
                )
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Ensure new_str ends with newline
            if new_str and not new_str.endswith(""\n""):
                new_str += ""\n""

            lines.insert(insert_line, new_str)

            with open(path, ""w"") as f:
                f.writelines(lines)

            console.print(
                f""[green]Successfully inserted text at line {insert_line + 1} in {path}[/green]""
            )
            console.log(
                f""[insert_text] Successfully inserted text at line {insert_line + 1} in {path}""
            )
            return FileOperationResult(
                True, f""Successfully inserted text at line {insert_line + 1} in {path}""
            )
        except Exception as e:
            error_msg = f""Error inserting text: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[insert_text] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/pipeline-architecture/steps/processing_stage.py,ProcessingStage,1,7
survived,"    def handle_tool_use(self, tool_use: Dict[str, Any]) -> Dict[str, Any]:
        """"""
        Handle text editor tool use from Claude.

        Args:
            tool_use: The tool use request from Claude

        Returns:
            Dictionary with result or error to send back to Claude
        """"""
        console.log(f""[file_editor_pipeline] Handling tool use: {tool_use.get('command', 'unknown')}"")
        
        # Process the tool use through the pipeline
        result = self.pipeline.process(tool_use)
        
        console.log(f""[file_editor_pipeline] Tool use result: {result}"")
        return result",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/file_editor_pipeline.py,FileEditorPipeline,1,7
survived,"    def _create_file(self, path: str, file_text: str) -> FileOperationResult:
        """"""
        Create a new file with specified content.

        Args:
            path: The path where the new file should be created
            file_text: The content to write to the new file

        Returns:
            FileOperationResult with result or error message
        """"""
        try:
            # Check if the path is empty or invalid
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                console.log(f""[create_file] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            # Check if the directory exists
            directory = os.path.dirname(path)
            if directory and not os.path.exists(directory):
                console.log(f""[create_file] Creating directory: {directory}"")
                os.makedirs(directory)

            with open(path, ""w"") as f:
                f.write(file_text or """")

            console.print(f""[green]Successfully created file {path}[/green]"")
            console.log(f""[create_file] Successfully created file {path}"")
            return FileOperationResult(True, f""Successfully created file {path}"")
        except Exception as e:
            error_msg = f""Error creating file: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[create_file] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/pipeline-architecture/steps/processing_stage.py,ProcessingStage,1,7
survived,"def test_create_folder_structure_with_parent_folder():
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        parent_path = Path(temp_dir) / ""parent""
        parent_path.mkdir()
        
        folder_path, folder_name, class_name = create_folder_structure(""child/"", parent_folder=parent_path)
        
        assert folder_name == ""child""
        assert class_name == ""Child""
        assert folder_path.name == ""child""
        assert folder_path.parent == parent_path
        assert folder_path.exists()
",tests/cli/test_create_crew.py,,1,7
survived,"    def get_total_flow_rate(self, wallet_client: EVMWalletClient, parameters: dict):
        result = wallet_client.read(
            {
                ""address"": parameters[""poolAddress""],
                ""abi"": POOL_ABI,
                ""functionName"": ""getTotalFlowRate"",
                ""args"": [],
            }
        )
        return result[""value""]",python/src/plugins/superfluid/goat_plugins/superfluid/service.py,SuperfluidService,0,7
survived,"    def supports_chain(self, chain: Chain) -> bool:
        return chain[""type""] == ""evm""
",python/src/plugins/superfluid/goat_plugins/superfluid/__init__.py,SuperfluidPlugin,1,7
survived,"    async def JSONRpcFunc(self, parameters: dict):
        """"""Makes a POST request to the configured endpoint with the required JSON-RPC parameters.""""""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.endpoint, json=parameters) as response:
                    if not response.ok:
                        raise Exception(f""HTTP error! status: {response.status}, body: {await response.text()}"")
                    return await response.json()
        except Exception as e:
            raise Exception(f""Failed to call {self.endpoint}: {e}"")",python/src/plugins/jsonrpc/goat_plugins/jsonrpc/service.py,JSONRpcService,1,7
survived,"async def test_xai_raw_response_async(model, mode):
    """"""Test that _raw_response is attached to async XAI responses""""""
    client = instructor.from_provider(f""xai/{model}"", mode=mode, async_client=True)
    
    user = await client.chat.completions.create(
        response_model=User,
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""You are a helpful assistant that extracts information."",
            },
            {
                ""role"": ""user"",
                ""content"": ""Extract: Jason is 25 years old."",
            },
        ],
    )
    
    assert isinstance(user, User)
    assert user.name.lower() == ""jason""
    assert user.age == 25
    assert hasattr(user, ""_raw_response""), (
        ""The raw response should be available from XAI""
    )
    assert user._raw_response is not None
",tests/llm/test_xai/test_raw_response.py,,1,6
survived,"    def get_context_window_size(self) -> int:
        return 8192
",tests/custom_llm_test.py,JWTAuthLLM,1,6
survived,"    def supports_function_calling(self) -> bool:
        """"""Return True to indicate that function calling is supported.""""""
        return True
",tests/custom_llm_test.py,CustomLLM,1,7
