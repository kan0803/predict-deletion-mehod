status,method,filepath,class_name,predict,confidence
survived,"def test_rename_stops_further_positional_checks():
    old_code = ""def func(a, b=1, c=2): pass""
    new_code = ""def func(x, b, c): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert ""Positional param order/name changed: 'a' -> 'x'."" in errors[0].message",tests/dev/test_check_function_signatures.py,,1,7
survived,"def test_only_first_positional_rename_flagged():
    old_code = ""def func(a, b, c, d): pass""
    new_code = ""def func(x, y, z, w): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert ""Positional param order/name changed: 'a' -> 'x'."" in errors[0].message
",tests/dev/test_check_function_signatures.py,,1,6
survived,"def parse_args() -> Args:
    parser = argparse.ArgumentParser(
        description=""Check for breaking changes in Python function signatures""
    )
    parser.add_argument(""--base-branch"", default=os.environ.get(""GITHUB_BASE_REF"", ""master""))
    args = parser.parse_args()
    return Args(base_branch=args.base_branch)
",dev/check_function_signatures.py,,1,7
survived,"def test_positional_param_renamed():
    old_code = ""def func(a, b): pass""
    new_code = ""def func(x, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert ""Positional param order/name changed: 'a' -> 'x'."" in errors[0].message
    assert errors[0].param_name == ""x""
",tests/dev/test_check_function_signatures.py,,1,7
survived,"def get_visualization_data():
    """"""Return the visualization data and raw tensor data.""""""
    records, tensor_table, failures = collect_grid()
    visualization_data = {}
    raw_tensor_data = {}

    for grid_idx, program_records in records.items():
        viz_data, raw_data, kernel_src = prepare_visualization_data(
            program_records, tensor_table
        )
        visualization_data[str(grid_idx)] = viz_data
        raw_tensor_data.update(raw_data)

    # Get the kernel source code

    return {
        ""visualization_data"": visualization_data,
        ""raw_tensor_data"": raw_tensor_data,
        ""failures"": failures,
        ""kernel_src"": kernel_src,
    }
",triton_viz/visualizer/draw.py,,1,6
survived,"def pandas_static_corrmatrix(a):
    """"""Compute correlation matrix using pandas.""""""
    df = pandas_matrix_setup(a)
    return lambda: df.corr()
",numbagg/test/conftest.py,,1,6
survived,"    def test_rolling_broadcasting_higher_dims(self, move_func):
        """"""Test that rolling functions broadcast correctly over higher dimensions.""""""
        np.random.seed(42)
        window = 5

        # 3D array: (2, 4, 20) -> output (2, 20, 4, 4)
        data_3d = np.random.randn(2, 4, 20)
        result_3d = move_func(data_3d, window=window)
        assert result_3d.shape == (2, 20, 4, 4)

        # 4D array: (2, 3, 4, 20) -> output (2, 3, 20, 4, 4)
        data_4d = np.random.randn(2, 3, 4, 20)
        result_4d = move_func(data_4d, window=window)
        assert result_4d.shape == (2, 3, 20, 4, 4)

        # Verify correctness - each slice should match individual computation
        for i in range(2):
            single_result = move_func(data_3d[i], window=window)
            assert_allclose(result_3d[i], single_result, rtol=1e-10)",numbagg/test/test_matrix_functions.py,TestMatrixFunctions,1,7
survived,"def test_move_matrix_pandas_comp(array, func, window, min_count):
    """"""Test matrix functions against pandas with various parameters.""""""
    if array.ndim < 2:
        pytest.skip(""Matrix functions require at least 2D input"")

    c = COMPARISONS[func]

    if min_count == ""window"":
        min_count = window

    # Get numbagg result
    result = c[""numbagg""](array, window=window, min_count=min_count)()

    # Get pandas result - need to handle the different output format
    pandas_callable = c[""pandas""](array, window=window, min_count=min_count)
    pandas_result = pandas_callable()

    # Convert pandas MultiIndex DataFrame to 3D array for comparison
    n_obs = array.shape[-1]
    n_vars = array.shape[-2]
    expected_pandas = np.full((n_obs, n_vars, n_vars), np.nan)

    # Only include windows where we have at least min_count observations
    actual_min_count = min_count if min_count is not None else window
    for t in range(n_obs):
        # Check if we have enough observations in this window
        window_size = min(t + 1, window)
        if (
            window_size >= actual_min_count
            and t in pandas_result.index.get_level_values(0)
        ):
            expected_pandas[t] = pandas_result.loc[t].values

    assert_allclose(result, expected_pandas)
",numbagg/test/test_moving.py,,1,6
survived,"    def test_large_matrix_size(self):
        """"""Test with moderately large matrix to check scalability.""""""
        np.random.seed(999)
        n_vars = 10  # 10x10 matrix
        n_obs = 50

        data = np.random.randn(n_vars, n_obs)

        result = move_exp_nancorrmatrix(data, alpha=0.3)

        # Should complete without error and have correct shape
        assert result.shape == (n_obs, n_vars, n_vars)

        # Final matrix should have proper properties
        final_matrix = result[-1]
        assert_allclose(np.diag(final_matrix), np.ones(n_vars), rtol=1e-10)
        assert_allclose(final_matrix, final_matrix.T, rtol=1e-10)
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced,1,7
survived,"    def test_all_nan_input(self):
        """"""Test behavior with all-NaN input.""""""
        data = np.full((2, 4), np.nan, dtype=np.float64)

        result_corr = move_exp_nancorrmatrix(data, alpha=0.5)
        result_cov = move_exp_nancovmatrix(data, alpha=0.5)

        # All results should be NaN
        assert np.all(np.isnan(result_corr))
        assert np.all(np.isnan(result_cov))
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def test_covariance_with_nans(self):
        """"""Test consistency with NaN values.""""""
        np.random.seed(456)

        # Create two time series with some NaN values
        n_obs = 30
        a1 = np.random.randn(n_obs)
        a2 = np.random.randn(n_obs) * 2

        # Add some NaN values
        a1[5:8] = np.nan
        a2[15:17] = np.nan

        alpha = 0.3

        # Compute using non-matrix function
        cov_nonmatrix = move_exp_nancov(a1, a2, alpha=alpha)

        # Compute using matrix function
        data_matrix = np.array([a1, a2])
        cov_matrix_result = move_exp_nancovmatrix(data_matrix, alpha=alpha)
        cov_from_matrix = cov_matrix_result[:, 0, 1]

        # They should match
        assert_allclose(cov_nonmatrix, cov_from_matrix, rtol=1e-10)
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency,1,7
survived,"    def test_min_weight(self, func):
        """"""Test min_weight parameter.""""""
        data = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        alpha = 0.1  # Low alpha means slow buildup of weight

        # High min_weight should produce more NaNs initially
        result_high = func(data, alpha=alpha, min_weight=0.8)
        result_low = func(data, alpha=alpha, min_weight=0.1)

        # Check that high min_weight produces more NaNs initially
        nan_count_high = np.sum(np.isnan(result_high[0]))
        nan_count_low = np.sum(np.isnan(result_low[0]))
        assert nan_count_high >= nan_count_low
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def test_exponential_decay_property(self):
        """"""Test that older observations have less influence (exponential decay).""""""
        # Create a dataset where values change over time
        np.random.seed(42)  # For reproducibility
        early_data = np.random.randn(2, 20)
        late_data = np.random.randn(2, 20) + 10  # Different mean
        data = np.concatenate([early_data, late_data], axis=1)

        # With high alpha, recent values should dominate more than low alpha
        result_high = move_exp_nancovmatrix(data, alpha=0.9)
        result_low = move_exp_nancovmatrix(data, alpha=0.1)

        # The final covariance matrices should be different
        # (high alpha should weight recent data more)
        final_cov_high = result_high[-1]
        final_cov_low = result_low[-1]

        # Results should be different due to different weighting
        assert not np.allclose(final_cov_high, final_cov_low, rtol=1e-3)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def test_simple_matrix(self, func, expected_diag):
        """"""Test simple 2x2 matrix calculation with exponential decay.""""""
        data = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        alpha = 0.5
        result = func(data, alpha=alpha)

        # Check shape - should be (time, vars, vars)
        assert result.shape == (4, 2, 2)

        # Check diagonal at the end
        final_result = result[-1]
        if expected_diag is not None:
            assert_allclose(
                np.diag(final_result), [expected_diag, expected_diag], rtol=1e-10
            )
        else:
            # For covariance, just check diagonal is non-negative
            assert np.all(np.diag(final_result) >= 0)

        # Check symmetry at each time step
        for t in range(result.shape[0]):
            assert_allclose(result[t], result[t].T, rtol=1e-10)

        # For perfect linear relationship, correlation should be 1
        if func == move_exp_nancorrmatrix:
            # Check that off-diagonal elements approach 1 as we get more data
            assert_allclose(final_result, [[1.0, 1.0], [1.0, 1.0]], rtol=1e-10)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,8
survived,"    def test_different_alphas(self, func):
        """"""Test behavior with different alpha values.""""""
        data = np.array([[1, 2, 3, 4, 5], [1, 4, 9, 16, 25]], dtype=np.float64)

        # High alpha (fast decay) vs low alpha (slow decay)
        result_high = func(data, alpha=0.9)
        result_low = func(data, alpha=0.1)

        # Both should have same shape
        assert result_high.shape == result_low.shape == (5, 2, 2)

        # Results should be different
        assert not np.allclose(result_high[-1], result_low[-1], rtol=1e-3)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def __init__(
        self,
        func: Callable,
        signature: tuple[list[tuple], str],
        **kwargs,
    ):
        self.signature = signature
        super().__init__(func, **kwargs)
",numbagg/decorators.py,ndmoveexpmatrix,1,7
survived,"    def test_single_series_diagonal_consistency(self):
        """"""Test that diagonal elements match what we'd expect for a single series.""""""
        np.random.seed(333)

        # Create a single time series
        n_obs = 20
        a1 = np.random.randn(n_obs)

        alpha = 0.4

        # For correlation, diagonal should always be 1.0 (after enough data)
        data_matrix = np.array([a1])
        corr_matrix_result = move_exp_nancorrmatrix(data_matrix, alpha=alpha)

        # Diagonal elements should be 1.0 where finite
        diagonal_values = corr_matrix_result[:, 0, 0]
        finite_mask = np.isfinite(diagonal_values)
        assert_allclose(diagonal_values[finite_mask], 1.0, rtol=1e-10)
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency,1,7
survived,"    def create_example_yaml(self) -> Dict[str, Any]:
        """"""Create an example parallel-dev.yaml configuration""""""
        
        example_config = {
            'provider': 'claude',
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'source': 'haconiwa scan generate-parallel-config',
                'description': 'Example parallel development configuration'
            },
            'tasks': [
                {
                    'file': 'src/models/user.py',
                    'prompt': 'Add validation methods and type hints'
                },
                {
                    'file': 'src/models/product.py',
                    'prompt': 'Implement inventory tracking'
                },
                {
                    'file': 'src/models/order.py',
                    'prompt': 'Add status management'
                },
                {
                    'file': 'src/api/routes/users.py',
                    'prompt': 'Implement CRUD endpoints with validation'
                },
                {
                    'file': 'src/services/auth.py',
                    'prompt': 'Add JWT authentication'
                }
            ],
            'options': {
                'max_concurrent': 3,
                'timeout': 90,
                'allowed_tools': ['Read', 'Write', 'Edit', 'MultiEdit'],
                'permission_mode': 'confirmEach',
                'output_dir': './parallel-dev-results'
            }
        }
        
        return example_config",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator,1,7
survived,"    def test_generate_from_scan_results_with_files(self):
        """"""Test generation from directory analysis results""""""
        scan_results = {
            'files': {
                'src/utils/helper.py': {'category': 'utils'},
                'src/api/routes.py': {'category': 'api'},
                'src/models/user.py': {'category': 'model'}
            }
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=2
        )
        
        assert len(config['tasks']) == 2
        assert config['metadata']['action'] == 'refactor'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,1,7
survived,"    def test_generate_prompt_for_file_with_custom_prompts(self):
        """"""Test prompt generation with custom prompts""""""
        custom_prompts = {
            'src/special.py': 'Custom prompt for special file'
        }
        
        prompt = self.generator._generate_prompt_for_file(
            'src/special.py',
            'refactor',
            custom_prompts
        )
        
        assert prompt == 'Custom prompt for special file'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,1,7
survived,"    def test_generate_from_scan_results_with_matches(self):
        """"""Test generation from scan results with matches""""""
        scan_results = {
            'model_name': 'gpt-4',
            'matches': {
                'model': [
                    {'path': 'models/gpt4/model.py', 'type': 'python'},
                    {'path': 'models/gpt4/config.py', 'type': 'python'}
                ],
                'api': [
                    {'path': 'api/gpt4_api.py', 'type': 'python'}
                ]
            }
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='add_tests',
            max_files=2
        )
        
        assert config['provider'] == 'claude'
        assert len(config['tasks']) == 2
        assert config['metadata']['action'] == 'add_tests'
        assert config['options']['max_concurrent'] == 1  # min(5, max(1, 2//2))
        
        # Check that appropriate prompts were generated
        for task in config['tasks']:
            assert 'file' in task
            assert 'prompt' in task
            assert len(task['prompt']) > 0
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,1,7
survived,"def compare(
    models: List[str] = typer.Argument(..., help=""Model names to compare (2 or more)""),
    path: Optional[Path] = typer.Option(None, ""--path"", ""-p"", help=""Base path to search in""),
    output: Optional[Path] = typer.Option(None, ""--output"", ""-o"", help=""Output file path""),
    format: str = typer.Option(""table"", ""--format"", ""-f"", help=""Output format"")
):
    """"""Compare multiple AI models""""""
    if len(models) < 2:
        typer.echo(""Error: At least 2 models required for comparison"", err=True)
        raise typer.Exit(1)
    
    comparator = ModelComparator(base_path=path or Path.cwd())
    
    results = comparator.compare_models(models)
    
    formatter = OutputFormatter()
    output_text = formatter.format_comparison_results(results, format)
    
    if output:
        output.write_text(output_text)
        typer.echo(f""Results saved to: {output}"")
    else:
        typer.echo(output_text)
",src/haconiwa/scan/cli.py,,1,7
survived,"    def test_model_name_normalization(self, temp_model_dir):
        """"""Test model name normalization""""""
        scanner = ModelScanner(temp_model_dir)
        
        # Test various prefixes
        test_cases = [
            (""gpt-4"", ""4""),
            (""claude-3-opus"", ""3-opus""),
            (""llama-2-70b"", ""2-70b""),
            (""mistral-7b"", ""7b""),
            (""gemini-pro"", ""pro"")
        ]
        
        for original, expected in test_cases:
            normalized = scanner._normalize_model_name(original)
            assert normalized == expected
",tests/test_scan/test_scanner.py,TestModelScanner,1,7
survived,"    def test_scan_list_command(self, runner, temp_model_dir):
        """"""Test the scan list command""""""
        result = runner.invoke(
            scan_app,
            [""list"", ""--path"", str(temp_model_dir), ""--format"", ""table""]
        )
        
        assert result.exit_code == 0
        assert ""Available AI Models"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def __init__(self, 
                 base_path: Path,
                 strip_prefix: bool = True,
                 ignore_patterns: Optional[List[str]] = None,
                 whitelist: Optional[List[str]] = None):
        self.base_path = Path(base_path)
        self.strip_prefix = strip_prefix
        self.ignore_patterns = ignore_patterns or [
            ""*.pyc"", ""__pycache__"", "".git"", "".venv"", 
            ""node_modules"", ""*.egg-info"", "".pytest_cache""
        ]
        self.whitelist = whitelist or []
        
        # Common model name prefixes to strip
        self.model_prefixes = [
            ""gpt-"", ""claude-"", ""llama-"", ""mistral-"", ""gemini-"",
            ""palm-"", ""anthropic-"", ""openai-"", ""meta-"", ""google-""
        ]
        
        # File type mappings
        self.file_type_mappings = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.md': 'markdown',
            '.json': 'json',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.txt': 'text',
            '.sh': 'shell',
            '.dockerfile': 'docker',
            '.toml': 'toml',
            '.ini': 'config',
            '.conf': 'config'
        }
",src/haconiwa/scan/scanner.py,ModelScanner,1,7
survived,"    def test_scan_analyze_with_category(self, runner, temp_model_dir):
        """"""Test analyze with specific category""""""
        result = runner.invoke(
            scan_app,
            [""analyze"", ""--path"", str(temp_model_dir), ""--category"", ""general"", ""--format"", ""json""]
        )
        
        assert result.exit_code == 0
        output = json.loads(result.stdout)
        assert output['category'] == 'general'
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def _analyze_model_directory(self, path: Path, files: List[str]) -> Optional[Dict[str, Any]]:
        """"""Analyze a single model directory""""""
        model_info = {
            'path': str(path.relative_to(self.base_path)),
            'name': self._extract_model_name(path),
            'provider': self._extract_provider(path),
            'category': self._determine_category(path),
            'formats': [],
            'size': 0,
            'files': [],
            'config': None
        }
        
        # Analyze files
        for file in files:
            file_path = path / file
            
            try:
                file_stat = file_path.stat()
                file_size = file_stat.st_size
                model_info['size'] += file_size
                
                # Check for model files
                for ext, format_name in self.model_extensions.items():
                    if file.endswith(ext):
                        model_info['formats'].append(format_name)
                        model_info['files'].append({
                            'name': file,
                            'format': format_name,
                            'size': file_size
                        })
                        break
                
                # Load config if available
                if file in self.config_files and file.endswith(('.json', '.yaml', '.yml')):
                    try:
                        if file.endswith('.json'):
                            with open(file_path, 'r') as f:
                                model_info['config'] = json.load(f)
                        elif file.endswith(('.yaml', '.yml')):
                            with open(file_path, 'r') as f:
                                model_info['config'] = yaml.safe_load(f)
                    except:
                        pass
            
            except (PermissionError, OSError):
                continue
        
        # Remove duplicates from formats
        model_info['formats'] = list(set(model_info['formats']))
        
        return model_info if model_info['formats'] or model_info['config'] else None
",src/haconiwa/scan/analyzer.py,ModelAnalyzer,1,7
survived,"    def search_by_model_name(self, 
                           model_name: str, 
                           include_content: bool = False) -> Dict[str, Any]:
        """"""Search for files and directories related to a model name""""""
        normalized_name = self._normalize_model_name(model_name)
        results = {
            'model_name': model_name,
            'normalized_name': normalized_name,
            'matches': defaultdict(list),
            'total_files': 0,
            'categories': set()
        }
        
        # Search patterns
        patterns = [
            normalized_name,
            model_name.lower(),
            model_name.replace('-', '_'),
            normalized_name.replace('-', '_')
        ]
        
        for root, dirs, files in os.walk(self.base_path):
            root_path = Path(root)
            
            # Filter directories
            dirs[:] = [d for d in dirs if not self._should_ignore(root_path / d)]
            
            # Check directory names
            for pattern in patterns:
                if pattern in root_path.name.lower():
                    category = self._determine_category(root_path)
                    results['categories'].add(category)
                    
                    # Process files in matching directory
                    for file in files:
                        file_path = root_path / file
                        if not self._should_ignore(file_path):
                            file_info = self._get_file_info(file_path, include_content)
                            results['matches'][category].append(file_info)
                            results['total_files'] += 1
            
            # Check file names
            for file in files:
                file_path = root_path / file
                if self._should_ignore(file_path):
                    continue
                
                for pattern in patterns:
                    if pattern in file.lower():
                        category = self._determine_category(root_path)
                        results['categories'].add(category)
                        file_info = self._get_file_info(file_path, include_content)
                        results['matches'][category].append(file_info)
                        results['total_files'] += 1
                        break
        
        results['categories'] = list(results['categories'])
        results['matches'] = dict(results['matches'])
        return results
",src/haconiwa/scan/scanner.py,ModelScanner,1,7
survived,"    def test_generate_prompt_for_file_by_category(self):
        """"""Test prompt generation based on file category""""""
        # Model file
        prompt = self.generator._generate_prompt_for_file(
            'src/models/user.py',
            'validation',
            None
        )
        assert 'validation methods' in prompt
        
        # API file
        prompt = self.generator._generate_prompt_for_file(
            'src/api/routes.py',
            'endpoints',
            None
        )
        assert 'RESTful CRUD endpoints' in prompt
        
        # Utils file
        prompt = self.generator._generate_prompt_for_file(
            'src/utils/helpers.py',
            'type_hints',
            None
        )
        assert 'type hints' in prompt
        
        # Config file
        prompt = self.generator._generate_prompt_for_file(
            'src/config/settings.py',
            'validation',
            None
        )
        assert 'configuration validation' in prompt
        
        # Service file
        prompt = self.generator._generate_prompt_for_file(
            'src/services/auth.py',
            'implementation',
            None
        )
        assert 'service functionality' in prompt
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator,0,7
survived,"def model(
    model_name: str = typer.Argument(..., help=""Model name to search for""),
    path: Optional[Path] = typer.Option(None, ""--path"", ""-p"", help=""Base path to search in""),
    no_strip_prefix: bool = typer.Option(False, ""--no-strip-prefix"", help=""Don't strip common prefixes""),
    format: str = typer.Option(""text"", ""--format"", ""-f"", help=""Output format (text/json/yaml/tree)""),
    include_content: bool = typer.Option(False, ""--include-content"", help=""Include file contents in results""),
    ignore: Optional[List[str]] = typer.Option(None, ""--ignore"", ""-i"", help=""Patterns to ignore""),
    whitelist: Optional[List[str]] = typer.Option(None, ""--whitelist"", ""-w"", help=""Patterns to whitelist"")
):
    """"""Search for AI model by name with prefix stripping support""""""
    scanner = ModelScanner(
        base_path=path or Path.cwd(),
        strip_prefix=not no_strip_prefix,
        ignore_patterns=ignore,
        whitelist=whitelist
    )
    
    results = scanner.search_by_model_name(model_name, include_content=include_content)
    
    formatter = OutputFormatter()
    output = formatter.format_search_results(results, format)
    typer.echo(output)
",src/haconiwa/scan/cli.py,,1,7
survived,"    def test_scan_generate_parallel_config_pattern_fix(self, runner):
        """"""Test generate-parallel-config for pattern fix""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""pattern-fix.yaml""
            
            result = runner.invoke(
                scan_app,
                [""generate-parallel-config"",
                 ""--pattern-fix"", ""old_func:replace with new_func"",
                 ""--output"", str(output_path)]
            )
            
            assert result.exit_code == 0
            assert ""Generated pattern fix YAML"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI,1,6
survived,"    def test_simple_matrix(self, func, expected_diag):
        """"""Test simple 2x2 matrix calculation.""""""
        data = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        result = func(data)

        # Check shape
        assert result.shape == (2, 2)

        # Check diagonal
        if expected_diag is not None:
            assert_allclose(np.diag(result), [expected_diag, expected_diag])
        else:
            # For covariance, just check diagonal is non-negative
            assert np.all(np.diag(result) >= 0)

        # Check symmetry
        assert_allclose(result, result.T)

        # For perfect linear relationship, correlation should be 1
        if func == nancorrmatrix:
            assert_allclose(result, [[1.0, 1.0], [1.0, 1.0]])
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices,1,7
survived,"    def test_1d_array_raises_error(self, func):
        """"""Test that 1D arrays raise an appropriate error.""""""
        data_1d = np.array([1, 2, 3, 4, 5], dtype=np.float64)

        with pytest.raises(ValueError, match=""requires at least a 2D array""):
            func(data_1d)
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices,1,7
survived,"    def test_broadcasting_higher_dims(self, func):
        """"""Test that gufunc broadcasting works correctly for higher dimensional arrays.""""""
        np.random.seed(42)

        # 3D array: (2, 4, 10) -> broadcast dims (2,) + core dims (4, 10)
        data_3d = np.random.randn(2, 4, 10)
        result_3d = func(data_3d)
        assert result_3d.shape == (2, 4, 4)

        # 4D array: (2, 3, 4, 10) -> broadcast dims (2, 3) + core dims (4, 10)
        data_4d = np.random.randn(2, 3, 4, 10)
        result_4d = func(data_4d)
        assert result_4d.shape == (2, 3, 4, 4)

        # Check each broadcast element is valid
        for i in range(2):
            for j in range(3):
                matrix = result_4d[i, j]
                # Check symmetry
                assert_allclose(matrix, matrix.T, rtol=1e-10)

                if func == nancorrmatrix:
                    # Check diagonal is 1
                    assert_allclose(np.diag(matrix), np.ones(4), rtol=1e-10)
                    # Check bounds
                    assert np.all((matrix >= -1) & (matrix <= 1))
                else:
                    # Check diagonal (variance) is non-negative
                    assert np.all(np.diag(matrix) >= 0)

        # Verify correctness - each slice should match individual computation
        for i in range(2):
            single_result = func(data_3d[i])
            assert_allclose(result_3d[i], single_result, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices,1,7
survived,"    def test_rolling_simple(self, move_func, static_func, window):
        """"""Test rolling functions with simple data.""""""
        # Moving functions expect (obs, vars) format
        data = np.array(
            [[1, 2], [2, 4], [3, 6], [4, 8], [5, 10], [6, 12]], dtype=np.float64
        )
        result = move_func(data, window=window, min_count=2)

        # Shape should be (n_obs, n_vars, n_vars)
        assert result.shape == (6, 2, 2)

        # Check symmetry for each time point
        for t in range(6):
            assert_allclose(result[t], result[t].T, equal_nan=True)

        # For perfect linear relationship, correlation should be 1
        if move_func == move_nancorrmatrix:
            for i in range(1, 6):  # From second window onwards (min_count=2)
                assert_allclose(result[i], [[1.0, 1.0], [1.0, 1.0]], rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices,1,7
survived,"    def test_no_axis_parameter_accepted(self):
        """"""Test that axis parameter is no longer accepted.""""""
        data = np.random.randn(3, 100)

        # These should all raise TypeError since axis parameter removed
        with pytest.raises(TypeError):
            nancorrmatrix(data, axis=0)

        with pytest.raises(TypeError):
            nancorrmatrix(data, axis=-1)

        with pytest.raises(TypeError):
            nancovmatrix(data, axis=1)
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices,0,6
survived,"    def test_array_alpha(self, func):
        """"""Test with alpha as an array rather than scalar.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 2], [2, 4], [3, 6], [4, 8]], dtype=np.float64)
        alpha_array = np.array([0.1, 0.5, 0.9, 0.3])

        result = func(data, alpha=alpha_array)

        # Should work and produce expected shape
        assert result.shape == (4, 2, 2)

        # Should be different from constant alpha
        result_constant = func(data, alpha=0.5)
        assert not np.allclose(result, result_constant)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices,1,7
survived,"    def test_correlation_bounds(self):
        """"""Test that correlation values are properly bounded between -1 and 1.""""""
        # Create data with some negative correlation
        # Exponential moving functions expect (obs, vars) format
        np.random.seed(42)
        data = np.random.randn(100, 3)
        data[:, 1] = -data[:, 0] + 0.1 * np.random.randn(
            100
        )  # Strong negative correlation

        result = move_exp_nancorrmatrix(data, alpha=0.5)

        # All correlation values should be between -1 and 1
        finite_mask = np.isfinite(result)
        assert np.all(result[finite_mask] >= -1.0)
        assert np.all(result[finite_mask] <= 1.0)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices,1,8
survived,"    def test_run_with_uv_with_requirements(self, mock_run):
        """"""Test run_with_uv with requirements file.""""""
        mock_run.return_value = Mock(returncode=0)
        req_path = Path(""requirements.txt"")

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(""server.py"", with_requirements=req_path)

        assert exc_info.value.code == 0

        cmd = mock_run.call_args[0][0]
        expected = [
            ""uv"",
            ""run"",
            ""--with"",
            ""fastmcp"",
            ""--with-requirements"",
            ""requirements.txt"",
            ""fastmcp"",
            ""run"",
            ""server.py"",
        ]
        assert cmd == expected
",tests/cli/test_run_with_uv.py,TestRunWithUv,0,6
survived,"def visualise_from_checkpoint_manager(
    checkpoint_manager,
    meta_cluster_model,
    *,
    style: str = ""basic"",
    console: Optional[Console] = None
) -> None:
    """"""Visualize clusters using a CheckpointManager and meta cluster model.
    
    This function integrates with the v1 pipeline's CheckpointManager to automatically
    load and visualize clusters.
    
    Args:
        checkpoint_manager: CheckpointManager instance from v1 pipeline
        meta_cluster_model: Meta cluster model with checkpoint_filename
        style: Visualization style (""basic"", ""enhanced"", or ""rich"")
        console: Rich Console instance (for rich style)
        
    Raises:
        ValueError: If invalid style is provided
        FileNotFoundError: If checkpoint file doesn't exist
    """"""
    if not hasattr(meta_cluster_model, 'checkpoint_filename'):
        raise ValueError(""Meta cluster model must have checkpoint_filename attribute"")
    
    checkpoint_path = checkpoint_manager.get_checkpoint_path(meta_cluster_model.checkpoint_filename)
    
    if style == ""basic"":
        visualise_clusters(checkpoint_path=checkpoint_path)
    elif style == ""enhanced"":
        visualise_clusters_enhanced(checkpoint_path=checkpoint_path)
    elif style == ""rich"":
        visualise_clusters_rich(checkpoint_path=checkpoint_path, console=console)
    else:
        raise ValueError(f""Invalid style '{style}'. Must be one of: basic, enhanced, rich"")
",kura/v1/visualization.py,,0,6
survived,"    def get_checkpoint_path(self, filename: str) -> str:
        """"""Get full path for a checkpoint file.""""""
        return os.path.join(self.checkpoint_dir, filename)
",kura/v1/kura.py,CheckpointManager,1,8
survived,"    def setup_checkpoint_dir(self) -> None:
        """"""Create checkpoint directory if it doesn't exist.""""""
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)
            logger.info(f""Created checkpoint directory: {self.checkpoint_dir}"")
",kura/v1/kura.py,CheckpointManager,1,7
survived,"def _build_cluster_tree(clusters: List[Cluster]) -> dict[str, ClusterTreeNode]:
    """"""Build a tree structure from a list of clusters.
    
    Args:
        clusters: List of clusters to build tree from
        
    Returns:
        Dictionary mapping cluster IDs to tree nodes
    """"""
    node_id_to_cluster = {}

    # Create tree nodes
    for cluster in clusters:
        node_id_to_cluster[cluster.id] = ClusterTreeNode(
            id=cluster.id,
            name=cluster.name,
            description=cluster.description,
            count=len(cluster.chat_ids),
            children=[],
        )

    # Link parent-child relationships
    for cluster in clusters:
        if cluster.parent_id:
            node_id_to_cluster[cluster.parent_id].children.append(cluster.id)

    return node_id_to_cluster
",kura/v1/visualization.py,,1,7
survived,"        def numeric_arm_featurizer(X, action_tokens):
            """"""Add numeric arm features instead of string tokens.""""""
            n_contexts, n_features = X.shape
            n_arms = len(action_tokens)

            # Create 3D array: (n_contexts, n_features + 1, n_arms)
            result = np.zeros((n_contexts, n_features + 1, n_arms))

            for i, token in enumerate(action_tokens):
                result[:, :-1, i] = X  # Original features
                result[:, -1, i] = int(token.split(""_"")[1])  # Numeric arm ID

            return result
",tests/test_learner_pipeline.py,TestLearnerPipelineIntegration,1,7
survived,"def AgentPipeline(
    steps: List[Tuple[str, Any]],
    final_agent: Union[ContextualAgent[ContextType, TokenType], Agent[TokenType]],
) -> Union[
    ContextualAgentPipeline[ContextType, TokenType],
    NonContextualAgentPipeline[TokenType],
]:
    """"""Create a Pipeline that wraps an Agent or ContextualAgent.

    This factory function provides a clean API for creating pipelines
    while maintaining complete static typing based on the agent type.
    The pipeline can accept any input type and transform it to what the agent expects.

    The resulting Pipeline will have the same interface as the wrapped agent,
    allowing you to call `pull`, `update`, and other methods directly on it.

    Parameters
    ----------
    steps : List[Tuple[str, Any]]
        List of (name, transformer) tuples for preprocessing steps.
        All transformers must be either stateless or pre-fitted.
        The output of the transformation chain must match the agent's expected input type.
    final_agent : Agent[TokenType] or ContextualAgent[ContextType, TokenType]
        The agent to wrap. The pipeline type is determined by the agent type.

    Returns
    -------
    ContextualAgentPipeline or NonContextualAgentPipeline
        The appropriate pipeline type based on the final_agent type.

    Examples
    --------
    >>> from sklearn.feature_extraction import DictVectorizer
    >>> from bayesianbandits import Arm, NormalRegressor, ContextualAgent, ThompsonSampling
    >>>
    >>> # Pipeline accepting dict input, outputting sparse arrays for agent
    >>> arms = [Arm(i, learner=NormalRegressor(alpha=1.0, beta=1.0, sparse=True)) for i in range(3)]
    >>> agent = ContextualAgent(arms, ThompsonSampling())
    >>> vectorizer = DictVectorizer()
    >>> _ = vectorizer.fit([{'user': 'A'}, {'user': 'B'}])
    >>>
    >>> pipeline = AgentPipeline(
    ...     steps=[('vectorize', vectorizer)],
    ...     final_agent=agent
    ... )
    >>> # Can accept dict input: [{'user': 'A', 'item': 1}]
    >>> # Transforms to sparse matrix for agent
    """"""
    if isinstance(final_agent, Agent):
        return NonContextualAgentPipeline(steps, final_agent)
    return ContextualAgentPipeline(steps, final_agent)",bayesianbandits/pipelines/_agent.py,,1,7
survived,"    def arms(self):
        """"""Get the arms from the wrapped agent.""""""
        return self._agent.arms
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,1,7
survived,"        def multiply_two(X):
            return X * 2
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers,1,7
survived,"    def test_pull_without_top_k(self):
        """"""Test pull method without top_k.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling(), random_seed=42)
        steps = []

        pipeline = NonContextualAgentPipeline(steps, agent)

        actions = pipeline.pull()

        assert len(actions) == 1
        assert isinstance(actions[0], int)
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline,1,7
survived,"    def rng(self):
        """"""Get the random generator from the wrapped agent.""""""
        return self._agent.rng
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,6
survived,"    def sample(self, X, size=1):
        self.sample_calls.append((X, size))
        return np.random.randn(len(X), size).squeeze()
",tests/test_learner_pipeline.py,MockLearner,1,6
survived,"    def test_validate_duplicate_names(self):
        """"""Test validation with duplicate step names.""""""
        steps = [
            (""transform"", FunctionTransformer()),
            (""transform"", FunctionTransformer()),  # Duplicate
        ]
        with pytest.raises(ValueError, match=""Step names must be unique""):
            _validate_steps(steps)
",tests/test_agent_pipeline.py,TestValidateSteps,1,6
survived,"        def failing_transform(X):
            raise ValueError(""Custom transformation error"")
",tests/test_agent_pipeline.py,TestErrorHandling,0,9
survived,"    def test_empty_steps_allowed(self):
        """"""Test empty steps are allowed for non-contextual.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())

        # Should not raise
        pipeline = NonContextualAgentPipeline([], agent)
        assert len(pipeline) == 0
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline,1,7
survived,"    def select_for_update(self, token: TokenType) -> Self:
        """"""Set the arm to update and return self for chaining.""""""
        self._agent.select_for_update(token)
        return self
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,6
survived,"    def test_transform(self):
        """"""Test direct transform method.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling())
        steps = [(""double"", FunctionTransformer(lambda x: x * 2))]

        pipeline = ContextualAgentPipeline(steps, agent)

        X = np.array([[1], [2]])
        result = pipeline.transform(X)
        expected = np.array([[2], [4]])
        np.testing.assert_array_equal(result, expected)
",tests/test_agent_pipeline.py,TestContextualAgentPipeline,1,7
survived,"    def add_arm(self, arm) -> None:
        """"""Add an arm to the wrapped agent.""""""
        self._agent.add_arm(arm)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,6
survived,"    def update(
        self,
        y: NDArray[np.float64],
        sample_weight: Optional[NDArray[np.float64]] = None,
    ) -> None:
        """"""Update the wrapped agent with observed reward(s).

        Parameters
        ----------
        y : NDArray[np.float64]
            Reward(s) to use for updating the arm.
        sample_weight : Optional[NDArray[np.float64]], default=None
            Sample weights to use for updating the arm.
        """"""
        self._agent.update(y, sample_weight=sample_weight)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,0,7
survived,"    def test_valid_initialization(self):
        """"""Test valid initialization.""""""
        mock_learner = MockLearner()
        pipeline = LearnerPipeline(
            steps=[(""scale"", StandardScaler())],
            learner=mock_learner
        )

        assert len(pipeline.steps) == 1
        assert pipeline.learner is mock_learner
",tests/test_learner_pipeline.py,TestLearnerPipelineInit,1,7
survived,"    def test_decay(self):
        """"""Test decay method.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        steps = []

        pipeline = NonContextualAgentPipeline(steps, agent)

        # Should not raise
        pipeline.decay(decay_rate=0.5)
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline,1,7
survived,"    def _generate_implementation_steps(self, feature_request: str, analysis: Dict[str, Any]) -> List[str]:
        """"""Generate step-by-step implementation plan.""""""
        return [f""Step 1: Analyze {feature_request}"", ""Step 2: Implement"", ""Step 3: Test""]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def enhance_prompt_with_context(self, base_prompt: str, context_data: Dict[str, Any]) -> str:
        """"""
        Enhance a basic prompt with comprehensive contextual information.
        
        Args:
            base_prompt (str): Original prompt to enhance
            context_data (Dict[str, Any]): Contextual data to inject
            
        Returns:
            str: Enhanced prompt with rich context
        """"""
        enhanced_prompt = f""""""# Enhanced Prompt with Context Engineering

## Original Request
{base_prompt}

## Contextual Information
{self._format_context_data(context_data)}

## Implementation Context
Based on the analysis, when implementing this request:

### Architecture Considerations
{self._extract_architecture_guidance(context_data)}

### Pattern Adherence
{self._extract_pattern_guidance(context_data)}

### Quality Requirements
{self._extract_quality_guidance(context_data)}

## Enhanced Request
{base_prompt}

**Additional Context**: Implement following the patterns and conventions identified above. 
Ensure the solution integrates seamlessly with the existing codebase architecture.
""""""
        
        return enhanced_prompt
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"    def analyze_test_patterns(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze testing patterns and conventions in the project.""""""
        test_patterns = {
            ""test_structure"": self._analyze_test_structure(project_path),
            ""testing_frameworks"": self._identify_testing_frameworks(project_path),
            ""test_naming"": self._analyze_test_naming(project_path),
            ""coverage_patterns"": self._analyze_coverage_patterns(project_path)
        }
        return test_patterns
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,8
survived,"    def _analyze_documentation_style(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze documentation style and conventions.""""""
        doc_style = {""format"": ""markdown"", ""structure"": [], ""conventions"": []}
        # Implementation would analyze documentation patterns
        return doc_style
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"def test_basic_instantiation():
    """"""Test basic ContextAgent instantiation.""""""
    print(""\nðŸ§ª Testing ContextAgent Instantiation..."")
    
    try:
        from praisonaiagents import ContextAgent, create_context_agent
        
        # Test direct instantiation
        context_agent = ContextAgent()
        print(""âœ… Successfully created ContextAgent with default parameters"")
        
        # Test with custom parameters
        custom_agent = ContextAgent(
            name=""Test Context Engineer"",
            role=""Test Role"",
            goal=""Test Goal"",
            llm=""gpt-4o-mini""
        )
        print(""âœ… Successfully created ContextAgent with custom parameters"")
        
        # Test factory function
        factory_agent = create_context_agent(llm=""gpt-4o-mini"")
        print(""âœ… Successfully created ContextAgent using factory function"")
        
        return True, [context_agent, custom_agent, factory_agent]
        
    except Exception as e:
        print(f""âŒ Instantiation failed: {e}"")
        return False, []
",test_context_agent.py,,1,7
survived,"    def _get_default_context_tools(self) -> List[Any]:
        """"""Get default tools for Context Engineering operations.""""""
        return [
            self.analyze_codebase_patterns,
            self.generate_context_document,
            self.create_validation_loop,
            self.enhance_prompt_with_context,
            self.generate_prp,
            self.extract_documentation_patterns,
            self.analyze_test_patterns,
            self.create_implementation_blueprint
        ]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"    def _generate_prp_validation_framework(self, feature_request: str) -> str:
        """"""Generate validation framework for PRP.""""""
        return f""Validation framework for: {feature_request}""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    async def test_ai_text_summarizer_real_llm_call_stats(self):
        """"""Test AITextSummarizer with real LLM call mocking to verify llm_call_count.""""""
        from unittest.mock import AsyncMock, MagicMock, patch

        import backend.blocks.llm as llm

        block = llm.AITextSummarizerBlock()

        # Mock the actual LLM call instead of the llm_call method
        call_count = 0

        async def mock_create(*args, **kwargs):
            nonlocal call_count
            call_count += 1

            mock_response = MagicMock()
            # Return different responses for chunk summary vs final summary
            if call_count == 1:
                mock_response.choices = [
                    MagicMock(
                        message=MagicMock(
                            content='{""summary"": ""Test chunk summary""}', tool_calls=None
                        )
                    )
                ]
            else:
                mock_response.choices = [
                    MagicMock(
                        message=MagicMock(
                            content='{""final_summary"": ""Test final summary""}',
                            tool_calls=None,
                        )
                    )
                ]
            mock_response.usage = MagicMock(prompt_tokens=50, completion_tokens=30)
            return mock_response

        with patch(""openai.AsyncOpenAI"") as mock_openai:
            mock_client = AsyncMock()
            mock_openai.return_value = mock_client
            mock_client.chat.completions.create = mock_create

            # Test with very short text (should only need 1 chunk + 1 final summary)
            input_data = llm.AITextSummarizerBlock.Input(
                text=""This is a short text."",
                model=llm.LlmModel.GPT4O,
                credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore
                max_tokens=1000,  # Large enough to avoid chunking
            )

            outputs = {}
            async for output_name, output_data in block.run(
                input_data, credentials=llm.TEST_CREDENTIALS
            ):
                outputs[output_name] = output_data

            print(f""Actual calls made: {call_count}"")
            print(f""Block stats: {block.execution_stats}"")
            print(f""LLM call count: {block.execution_stats.llm_call_count}"")

            # Should have made 2 calls: 1 for chunk summary + 1 for final summary
            assert block.execution_stats.llm_call_count >= 1
            assert block.execution_stats.input_token_count > 0
            assert block.execution_stats.output_token_count > 0
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking,1,7
survived,"    def check(node: ast.Call, resolver: Resolver) -> bool:
        """"""
        Returns True if the call is ThreadPoolExecutor() without a thread_name_prefix parameter.
        """"""
        return (
            (resolved := resolver.resolve(node))
            and resolved == [""concurrent"", ""futures"", ""ThreadPoolExecutor""]
            and not any(keyword.arg == ""thread_name_prefix"" for keyword in node.keywords)
        )",dev/clint/src/clint/rules/thread_pool_executor_without_thread_name_prefix.py,ThreadPoolExecutorWithoutThreadNamePrefix,1,7
survived,"    def check(node: ast.FunctionDef | ast.AsyncFunctionDef, resolver: Resolver) -> bool:
        """"""
        Returns True if the function has @pytest.mark.repeat decorator.
        """"""
        return any(
            (res := resolver.resolve(deco)) and res == [""pytest"", ""mark"", ""repeat""]
            for deco in node.decorator_list
        )",dev/clint/src/clint/rules/pytest_mark_repeat.py,PytestMarkRepeat,1,7
survived,"    def __init__(self, params: set[str]) -> None:
        self.params = params
",dev/clint/src/clint/rules/extraneous_docstring_param.py,ExtraneousDocstringParam,1,7
survived,"    def check(cls, rules: set[str]) -> ""DoNotDisable"":
        if s := rules.intersection(DoNotDisable.DO_NOT_DISABLE):
            return cls(s)
",dev/clint/src/clint/rules/do_not_disable.py,DoNotDisable,1,6
survived,"    def _message(self) -> str:
        return (
            f""Generic type `{self.type_hint}` must be parameterized ""
            ""(e.g., `list[str]` rather than `list`).""
        )",dev/clint/src/clint/rules/unparameterized_generic_type.py,UnparameterizedGenericType,1,7
survived,"    def example(cls) -> ""ModelVersionAliasDeletedPayload"":
        return cls(
            name=""example_model"",
            alias=""example_alias"",
        )
",mlflow/webhooks/types.py,ModelVersionAliasDeletedPayload,1,7
survived,"    def example(cls) -> ""ModelVersionTagSetPayload"":
        return cls(
            name=""example_model"",
            version=""1"",
            key=""example_key"",
            value=""example_value"",
        )
",mlflow/webhooks/types.py,ModelVersionTagSetPayload,1,6
survived,"def test_webhook(webhook: Webhook, event: Optional[WebhookEvent] = None) -> WebhookTestResult:
    """"""Test a webhook by sending a test payload.

    Args:
        webhook: The webhook object to test
        event: Optional event type to test. If not specified, uses the first event from webhook.

    Returns:
        WebhookTestResult indicating success/failure and response details
    """"""
    try:
        # Use provided event or the first event type for testing
        test_event = event or webhook.events[0]

        # Generate example payload based on the event type
        if test_event == WebhookEvent.REGISTERED_MODEL_CREATED:
            from mlflow.webhooks.types import RegisteredModelCreatedPayload

            test_payload = RegisteredModelCreatedPayload.example()
        elif test_event == WebhookEvent.MODEL_VERSION_CREATED:
            from mlflow.webhooks.types import ModelVersionCreatedPayload

            test_payload = ModelVersionCreatedPayload.example()
        elif test_event == WebhookEvent.MODEL_VERSION_TAG_SET:
            from mlflow.webhooks.types import ModelVersionTagSetPayload

            test_payload = ModelVersionTagSetPayload.example()
        elif test_event == WebhookEvent.MODEL_VERSION_TAG_DELETED:
            from mlflow.webhooks.types import ModelVersionTagDeletedPayload

            test_payload = ModelVersionTagDeletedPayload.example()
        elif test_event == WebhookEvent.MODEL_VERSION_ALIAS_CREATED:
            from mlflow.webhooks.types import ModelVersionAliasCreatedPayload

            test_payload = ModelVersionAliasCreatedPayload.example()
        elif test_event == WebhookEvent.MODEL_VERSION_ALIAS_DELETED:
            from mlflow.webhooks.types import ModelVersionAliasDeletedPayload

            test_payload = ModelVersionAliasDeletedPayload.example()
        else:
            raise ValueError(f""Unknown event type: {test_event}"")

        return _send_webhook_request(webhook.url, test_payload, webhook.secret)
    except Exception as e:
        return WebhookTestResult(
            success=False,
            error_message=f""Failed to test webhook: {str(e)[:500]}"",
        )",mlflow/webhooks/dispatch.py,,1,7
survived,"            def w_NEW(vm: 'SPyVM', wop_cls: W_OpArg,
                     *args_wop: W_OpArg) -> W_OpImpl:
                # Support overloading based on argument count
                if len(args_wop) == 1:
                    # Point(x) -> Point(x, x)
                    @builtin_func('ext', 'new_point_single')
                    def w_new(vm: 'SPyVM', w_cls: W_Type, w_x: W_I32) -> W_Point:
                        return W_Point(w_x, w_x)
                    return W_OpImpl(w_new)
                else:
                    # Normal Point(x, y)
                    @builtin_func('ext', 'new_point')
                    def w_new(vm: 'SPyVM', w_cls: W_Type,
                              w_x: W_I32, w_y: W_I32) -> W_Point:
                        return W_Point(w_x, w_y)
                    return W_OpImpl(w_new)
",spy/tests/compiler/test_operator_call.py,TestCallOp.W_Point,1,7
survived,"    def w_meta_GETATTR(vm: 'SPyVM', wop_cls: W_OpArg, wop_attr: W_OpArg) -> 'W_OpImpl':
        """"""
        Handle class attribute lookups on OpImpl, like OpImpl.NULL
        """"""
        from spy.vm.str import W_Str

        attr_name = wop_attr.blue_unwrap_str(vm)

        if attr_name == 'NULL':
            # Return the NULL instance directly
            @builtin_func(W_OpImpl._w.fqn, 'get_null')
            def w_get_null(vm: 'SPyVM', w_cls: W_Type) -> W_OpImpl:
                return W_OpImpl.NULL

            return W_OpImpl(w_get_null, [wop_cls])

        return W_OpImpl.NULL
",spy/vm/opimpl.py,W_OpImpl,1,6
survived,"    def w_GET_color(vm: 'SPyVM', wop_x: 'W_OpArg',
                    wop_attr: 'W_OpArg') -> 'W_OpImpl':
        from spy.vm.builtin import builtin_func
        from spy.vm.str import W_Str

        @builtin_func(W_OpArg._w.fqn, 'get_color')
        def w_get_color(vm: 'SPyVM', w_oparg: W_OpArg) -> W_Str:
            return vm.wrap(w_oparg.color)  # type: ignore

        return W_OpImpl(w_get_color, [wop_x])
",spy/vm/opimpl.py,W_OpArg,1,6
survived,"            def w_get_null(vm: 'SPyVM', w_cls: W_Type) -> W_OpImpl:
                return W_OpImpl.NULL
",spy/vm/opimpl.py,W_OpImpl,1,6
survived,"    def test_opimpl_null(self):
        mod = self.compile(
        """"""
        from operator import OpImpl

        @blue
        def get_null() -> OpImpl:
            return OpImpl.NULL
        """""")
        w_null = mod.get_null(unwrap=False)
        assert w_null is W_OpImpl.NULL",spy/tests/compiler/test_opimpl.py,TestOpImpl,1,7
survived,"    def test_with_nans(self):
        # Test with NaN values
        data = np.array(
            [[1, 2, np.nan, 4], [2, 4, 6, np.nan], [np.nan, 1, 2, 3]], dtype=np.float64
        )
        result = nancorrmatrix(data)

        # Check diagonal is 1
        assert_allclose(np.diag(result), [1.0, 1.0, 1.0])

        # Check symmetry
        assert_allclose(result, result.T)

        # Correlation values should be between -1 and 1
        assert np.all((result >= -1) & (result <= 1) | np.isnan(result))
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix,1,7
survived,"def nancorrmatrix(a, out):
    """"""
    Compute correlation matrix treating NaN as missing values.

    For 2D input, correlates variables (rows) across observations (columns).
    Uses pairwise complete observations (like pandas.DataFrame.corr).
    """"""
    n_vars, n_obs = a.shape

    # Compute correlation matrix
    for i in range(n_vars):
        for j in range(i, n_vars):  # Only compute upper triangle
            if i == j:
                # Diagonal: correlation with itself is 1.0 if any valid values exist
                for k in range(n_obs):
                    if not np.isnan(a[i, k]):
                        out[i, j] = 1.0
                        break
                else:
                    # No valid values found
                    out[i, j] = np.nan
                continue

            # Find pairwise complete observations and compute sums in one pass
            sum_i = 0.0
            sum_j = 0.0
            count = 0

            for k in range(n_obs):
                val_i = a[i, k]
                val_j = a[j, k]
                if not np.isnan(val_i) and not np.isnan(val_j):
                    sum_i += val_i
                    sum_j += val_j
                    count += 1

            if count > 1:
                # Compute means using only pairwise complete observations
                mean_i = sum_i / count
                mean_j = sum_j / count

                # Compute correlation components in second pass
                cov_sum = 0.0
                var_i_sum = 0.0
                var_j_sum = 0.0

                for k in range(n_obs):
                    val_i = a[i, k]
                    val_j = a[j, k]
                    if not np.isnan(val_i) and not np.isnan(val_j):
                        diff_i = val_i - mean_i
                        diff_j = val_j - mean_j
                        cov_sum += diff_i * diff_j
                        var_i_sum += diff_i * diff_i
                        var_j_sum += diff_j * diff_j

                # Use count - 1 for sample correlation
                var_i = var_i_sum / (count - 1)
                var_j = var_j_sum / (count - 1)

                if var_i > 0 and var_j > 0:
                    corr = cov_sum / (count - 1) / np.sqrt(var_i * var_j)
                    out[i, j] = corr
                    out[j, i] = corr  # Symmetric
                else:
                    out[i, j] = np.nan
                    out[j, i] = np.nan
            else:
                out[i, j] = np.nan
                out[j, i] = np.nan
",numbagg/funcs.py,,1,7
survived,"    async def log_tool(context: Context) -> None:
        await context.info(message=""test log"")
",tests/server/middleware/test_middleware.py,,1,6
survived,"    async def test_list_resources(
        self, mcp_server: FastMCP, recording_middleware: RecordingMiddleware
    ):
        async with Client(mcp_server) as client:
            await client.list_resources()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_resources"", times=1)
",tests/server/middleware/test_middleware.py,TestMiddlewareHooks,1,7
survived,"    def test_resource_with_path(x: int) -> str:
        return f""test resource with {x}""
",tests/server/middleware/test_middleware.py,,1,6
deleted,"    def _collect_all_related_parameters(
        self, 
        interdeps: ""InterDependencies_"", 
        initial_params: set[ParamSpecBase], 
        result_dict: Mapping[ParamSpecBase, npt.NDArray]
    ) -> set[ParamSpecBase]:
        """"""
        Transitively collect all parameters that are related to the initial set of parameters.
        This includes parameters that any parameter in the set is inferred from, and parameters
        that depend on or are inferred from those parameters, etc.
        
        Only includes parameters that are present in result_dict.
        """"""
        collected = set(initial_params)
        to_process = set(initial_params)
        
        while to_process:
            current = to_process.pop()
            
            # Add parameters that current parameter is inferred from
            inferred_from = set(interdeps.inferences.get(current, ()))
            new_inferred = inferred_from - collected
            # Only add if they're in result_dict
            new_inferred = new_inferred.intersection(result_dict.keys())
            collected.update(new_inferred)
            to_process.update(new_inferred)
            
            # Add parameters that depend on current parameter
            dependents = set(interdeps._dependencies_inv.get(current, ()))
            new_dependents = dependents - collected
            # Only add if they're in result_dict
            new_dependents = new_dependents.intersection(result_dict.keys())
            collected.update(new_dependents)
            to_process.update(new_dependents)
            
            # Add parameters that are inferred from current parameter
            infers = set(interdeps._inferences_inv.get(current, ()))
            new_infers = infers - collected
            # Only add if they're in result_dict
            new_infers = new_infers.intersection(result_dict.keys())
            collected.update(new_infers)
            to_process.update(new_infers)
        
        return collected
",src/qcodes/dataset/data_set_in_memory.py,DataSetInMem,1,7
survived,"def test_export_datasets_preserve_experiment_structure():
    """"""Test that experiment structure is preserved in the target database""""""
    with tempfile.TemporaryDirectory() as temp_dir:
        source_db_path = Path(temp_dir) / ""source.db""
        target_db_path = Path(temp_dir) / ""target.db""
        export_path = Path(temp_dir) / ""exports""
        
        # Create source database with multiple experiments
        source_conn = connect(source_db_path)
        
        # Create first experiment
        exp1 = load_or_create_experiment(
            experiment_name=""exp1"",
            sample_name=""sample1"",
            conn=source_conn
        )
        
        # Create second experiment
        exp2 = load_or_create_experiment(
            experiment_name=""exp2"",
            sample_name=""sample2"",
            conn=source_conn
        )
        
        # Create interdependencies
        x = ParamSpec(""x"", ""numeric"", unit=""V"")
        y = ParamSpec(""y"", ""numeric"", unit=""A"")
        interdeps = InterDependencies_(dependencies={y: (x,)})
        
        # Create datasets in both experiments
        datasets = []
        for exp in [exp1, exp2]:
            for i in range(2):  # 2 datasets per experiment
                dataset = DataSet(conn=source_conn, exp_id=exp.exp_id)
                dataset.set_interdependencies(interdeps)
                dataset.mark_started()
                
                # Add some data
                for j in range(5):
                    dataset.add_results([{""x"": j, ""y"": j * (i + 1)}])
                
                dataset.mark_completed()
                datasets.append(dataset)
        
        source_conn.close()
        
        # Run the export function
        result = export_datasets_and_create_metadata_db(
            source_db_path=source_db_path,
            target_db_path=target_db_path,
            export_path=export_path,
        )
        
        # Check that all datasets were processed
        assert len(result) == 4
        
        # Check that target database has all runs
        target_conn = connect(target_db_path)
        target_runs = get_runs(target_conn)
        assert len(target_runs) == 4
        target_conn.close()
",tests/dataset/test_export_datasets_and_create_metadata_db.py,,1,7
survived,"def test_export_datasets_empty_database():
    """"""Test behavior with empty source database""""""
    with tempfile.TemporaryDirectory() as temp_dir:
        source_db_path = Path(temp_dir) / ""empty.db""
        target_db_path = Path(temp_dir) / ""target.db""
        export_path = Path(temp_dir) / ""exports""
        
        # Create empty database
        source_conn = connect(source_db_path)
        source_conn.close()
        
        # Run the export function
        result = export_datasets_and_create_metadata_db(
            source_db_path=source_db_path,
            target_db_path=target_db_path,
            export_path=export_path,
        )
        
        # Should return empty result
        assert result == {}
",tests/dataset/test_export_datasets_and_create_metadata_db.py,,1,7
survived,"async def no_sleep(_: float) -> None:
    return None
",tests/test_register_mesh_backoff.py,,0,6
survived,"    def add_check(self, check: Callable[[str], Awaitable[str]]) -> None:
        """"""Register a custom check plugin.""""""

        self.checks.append(check)
",src/meta_agent/policy.py,PolicyChecker,1,7
survived,"        def __init__(
            self, name: str | None = None, tools: list[Any] | None = None
        ) -> None:
            self.name = name or ""StubAgent""
            self.tools = tools or []
",src/meta_agent/agents/guardrail_designer_agent.py,Agent,1,7
survived,"def test_agent_inherits_from_agents():
    router = GuardrailModelRouter({""gpt"": DummyAdapter()}, default_model=""gpt"")
    agent = GuardrailDesignerAgent(model_router=router)

    assert isinstance(agent, agents.Agent)",tests/test_guardrail_designer_agent.py,,1,7
survived,"def test_settings_vault_auto(monkeypatch):
    class FakeKV:
        def read_secret_version(self, path):
            return {""data"": {""data"": {""OPENAI_API_KEY"": ""vault""}}}

    class FakeClient:
        def __init__(self, url, token):
            self.secrets = types.SimpleNamespace(kv=FakeKV())

    monkeypatch.setenv(""VAULT_TOKEN"", ""tok"")
    monkeypatch.setenv(""VAULT_ADDR"", ""http://vault"")
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    monkeypatch.setitem(sys.modules, ""hvac"", types.SimpleNamespace(Client=FakeClient))
    import src.utils.config as cfg
    importlib.reload(cfg)
    settings = cfg.Settings()
    assert settings.openai_api_key == ""vault""
",tests/test_root_config.py,,1,7
survived,"def main(directory: Path) -> int:
    service_worker = directory / ""service-worker.js""
    workbox = directory / ""lib"" / ""workbox-sw.js""
    if not service_worker.exists():
        raise FileNotFoundError(service_worker)
    if not workbox.exists():
        raise FileNotFoundError(workbox)
    expected = parse_expected_hash(service_worker)
    actual = compute_hash(workbox)
    if expected != actual:
        print(f""Hash mismatch: expected {expected}, got {actual}"")
        return 1
    return 0
",scripts/verify_workbox_hash.py,,1,7
survived,"def _load_yaml(path):
    items = []
    obj = None
    for line in open(path):
        line = line.strip()
        if not line:
            continue
        if line.startswith('- '):
            if obj is not None:
                items.append(obj)
            obj = {}
            line = line[2:]
        if not line:
            continue
        key, val = line.split(':', 1)
        key = key.strip()
        val = val.strip()
        if val.isdigit():
            val = int(val)
        elif val.startswith('""') and val.endswith('""'):
            val = val[1:-1]
        obj[key] = val
    if obj is not None:
        items.append(obj)
    return items
",tests/transpiler/x/py/load_yaml.py,,0,7
survived,"    def _args(self):
        return argparse.Namespace(
            agents=""A,B"",
            port=123,
            metrics_port=456,
            a2a_port=789,
            cycle=5,
            loglevel=""DEBUG"",
            version=False,
            list_agents=False,
        )
",alpha_factory_v1/tests/test_edge_runner_main.py,EdgeRunnerMainInvokesRun,1,7
survived,"async def test_self_improver_agent_rollback(monkeypatch, tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)
    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text(patch)
    bus = messaging.A2ABus(config.Settings(bus_port=0))
    agent = SelfImproverAgent(bus, DummyLedger(), str(repo_dir), str(patch_file), allowed=[""metric.txt""])
    orig_commit = git.Repo(repo_dir).head.commit.hexsha
    def fail_commit(self, *a, **k):
        raise RuntimeError(""boom"")
    monkeypatch.setattr(git.index.base.IndexFile, ""commit"", fail_commit)
    with pytest.raises(RuntimeError):
        await agent.run_cycle()
    assert git.Repo(repo_dir).head.commit.hexsha == orig_commit
    assert (repo_dir / ""metric.txt"").read_text().strip() == ""1""
    REGISTRY._names_to_collectors.clear()
    REGISTRY._collector_to_names.clear()",tests/test_self_improver.py,,1,7
survived,"    def close(self) -> None:
        pass
",tests/test_self_improver.py,DummyLedger,0,7
survived,"    def generate(
        self,
        prompt: str,
        amount: int = 1,
        model: str = ""stabilityai/stable-diffusion-xl-base-1.0"",
        guidance_scale: Optional[float] = None,
        negative_prompt: Optional[str] = None,
        num_inference_steps: Optional[int] = None,
        width: Optional[int] = None,
        height: Optional[int] = None,
        scheduler: Optional[str] = None,
        seed: Optional[int] = None,
    ) -> List[bytes]:
        """"""Generate some fire images! ðŸŽ¨

        Args:
            prompt (str): Your lit image description
            amount (int): How many images to generate (default: 1)
            model (str): Which model to use (default: ""stabilityai/stable-diffusion-xl-base-1.0"")
            guidance_scale (float, optional): Control how much to follow your prompt
            negative_prompt (str, optional): What you don't want in the image
            num_inference_steps (int, optional): More steps = better quality but slower
            width (int, optional): Image width
            height (int, optional): Image height
            scheduler (str, optional): Which scheduler to use
            seed (int, optional): Random seed for reproducibility

        Returns:
            List[bytes]: Your generated images as bytes
        """"""
        assert bool(prompt), ""Yo fam, prompt can't be empty! ðŸš«""
        assert isinstance(amount, int), f""Amount gotta be an integer, not {type(amount)} ðŸ¤”""
        assert amount > 0, ""Amount gotta be greater than 0! ðŸ“ˆ""

        self.prompt = prompt
        response = []
        if self.logging:
            logger.info(f""Generating {amount} images with {model}... ðŸŽ¨"")

        for _ in range(amount):
            url = self.base_url + model
            payload: Dict[str, Any] = {""inputs"": prompt}
            parameters = {}

            if guidance_scale is not None:
                parameters[""guidance_scale""] = guidance_scale
            if negative_prompt is not None:
                parameters[""negative_prompt""] = negative_prompt
            if num_inference_steps is not None:
                parameters[""num_inference_steps""] = num_inference_steps
            if width is not None and height is not None:
                parameters[""target_size""] = {""width"": width, ""height"": height}
            if scheduler is not None:
                parameters[""scheduler""] = scheduler
            if seed is not None:
                parameters[""seed""] = seed

            if parameters:
                payload[""parameters""] = parameters

            try:
                resp = self.session.post(url, headers=self.headers, json=payload, timeout=self.timeout)
                resp.raise_for_status()
                response.append(resp.content)
                if self.logging:
                    logger.success(""Image generated successfully! ðŸŽ‰"")
            except requests.RequestException as e:
                if self.logging:
                    logger.error(f""Failed to generate image: {e} ðŸ˜¢"")
                raise

        return response
",webscout/Provider/TTI/huggingface.py,HFimager,1,7
survived,"    def get_auth_file(self) -> Path:
        """"""Get path to authentication file""""""
        path = Path(os.path.join(os.path.expanduser(""~""), "".ai_arta_cookies""))
        path.mkdir(exist_ok=True)
        filename = f""auth_{self.__class__.__name__}.json""
        return path / filename
",webscout/Provider/TTI/aiarta.py,AIArtaImager,1,7
survived,"    def save(
        self,
        response: List[bytes],
        name: Optional[str] = None,
        dir: Optional[Union[str, Path]] = None,
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire generated images! ðŸ’¾

        Examples:
            >>> provider = ImgSys()
            >>> images = provider.generate(""Cool art"")
            >>> # Save with default settings
            >>> paths = provider.save(images)
            >>> # Save with custom name and directory
            >>> paths = provider.save(
            ...     images,
            ...     name=""my_art"",
            ...     dir=""my_images"",
            ...     filenames_prefix=""test_""
            ... )

        Args:
            response (List[bytes]): Your generated images
            name (Optional[str]): Custom name for your images
            dir (Optional[Union[str, Path]]): Where to save the images (default: current directory)
            filenames_prefix (str): Prefix for your image files

        Returns:
            List[str]: Paths to your saved images
        """"""
        save_dir = dir if dir else os.getcwd()
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        saved_paths = []
        timestamp = int(time.time())
        
        for i, image_bytes in enumerate(response):
            if name:
                filename = f""{filenames_prefix}{name}_{i}.{self.image_extension}""
            else:
                filename = f""{filenames_prefix}imgsys_{timestamp}_{i}.{self.image_extension}""
            
            filepath = os.path.join(save_dir, filename)
            
            with open(filepath, ""wb"") as f:
                f.write(image_bytes)
            
            saved_paths.append(filepath)

        return saved_paths ",webscout/Provider/TTI/imgsys.py,ImgSys,1,7
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}, logging: bool = True):
        """"""Initialize your TalkAI provider with custom settings! âš™ï¸

        Args:
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
            logging (bool): Enable fire logging (default: True)
        """"""
        self.api_endpoint = ""https://talkai.info/chat/send/""
        self.headers = {
            'accept': 'application/json',
            'accept-language': 'en-US,en;q=0.9',
            'content-type': 'application/json',
            'origin': 'https://talkai.info',
            'referer': 'https://talkai.info/image/',
            'user-agent': agent.random(),  # Using our fire random agent! ðŸ”¥
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
        self.logging = logging
        if self.logging:
            logger.info(""TalkaiImager initialized! Ready to create some fire art! ðŸš€"")
",webscout/Provider/TTI/talkai.py,TalkaiImager,1,7
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}, logging: bool = True):
        """"""Initialize your Nexra provider with custom settings! âš™ï¸

        Args:
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
            logging (bool): Enable fire logging (default: True)
        """"""
        self.url = ""https://nexra.aryahcr.cc/api/image/complements""
        self.headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
            ""User-Agent"": agent.random()
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
        self.logging = logging
        if self.logging:
            logger.info(""Nexra provider initialized! ðŸš€"")
",webscout/Provider/TTI/nexra.py,NexraImager,1,7
survived,"def test_request_patch_handles_openai_error(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
    class FailError(Exception):
        pass

    openai_stub = types.ModuleType(""openai"")
    openai_stub.Error = FailError

    def create(*_a: object, **_k: object) -> None:
        raise FailError(""boom"")

    openai_stub.ChatCompletion = types.SimpleNamespace(create=create)
    monkeypatch.setitem(sys.modules, ""openai"", openai_stub)
    monkeypatch.setenv(""OPENAI_API_KEY"", ""x"")
    monkeypatch.setenv(""USE_LOCAL_LLM"", ""false"")

    client = _reload_client(monkeypatch, """")

    caplog.set_level(logging.ERROR)
    out = client.request_patch([{""role"": ""user"", ""content"": ""fix""}])

    assert out == """"
    assert any(""OpenAI API request failed"" in r.getMessage() for r in caplog.records)",tests/test_llm_client_error_handling.py,,1,7
survived,"def eval_dyad_grad(klong, a, b):
    """"""

        aâˆ‡b                                                    [Grad]

        Compute the numeric gradient of the monadic function ``b`` at ``a``.

    """"""
    if isinstance(a, KGSym):
        orig = klong[a]

        def func(v):
            klong[a] = v
            try:
                return klong.call(KGCall(b, [v], 1)) if isinstance(b, (KGSym, KGLambda, KGFn, KGCall)) else b(v)
            finally:
                klong[a] = orig

        return numeric_grad(func, orig)
    else:
        return grad_of_fn(klong, b, a)
",klongpy/dyads.py,,1,7
survived,"def test_with_retry_sync(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    calls = {""n"": 0}

    def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 3:
            raise ValueError(""boom"")
        return ""ok""

    wrapped = retry.with_retry(func, max_tries=3)
    assert wrapped() == ""ok""
    assert calls[""n""] == 3
",tests/test_retry_wrapper.py,,1,7
survived,"def test_insight_invalid_token() -> None:
    _setup_simulations()
    client = _make_client()
    resp = client.post(""/insight"", json={}, headers={""Authorization"": ""Bearer bad""})
    assert resp.status_code == 403",tests/test_insight_endpoint.py,,1,7
survived,"def test_insight_aggregates_results() -> None:
    _setup_simulations()
    client = _make_client()
    headers = {""Authorization"": ""Bearer test-token""}
    resp = client.post(""/insight"", json={""ids"": [""a"", ""b""]}, headers=headers)
    assert resp.status_code == 200
    assert resp.json() == {""forecast"": [{""year"": 1, ""capability"": 0.5}]}
",tests/test_insight_endpoint.py,,1,7
survived,"        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()
",tests/unit/test_telemetry_client.py,FakeRunner,1,6
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"async def test_attach_runner(monkeypatch):
    # Fake runner class
    class FakeRunner:
        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()

    client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
    send_mock = AsyncMock(return_value={""ok"": True})
    monkeypatch.setattr(client, ""send"", send_mock)

    client.attach_runner(FakeRunner, ""trace"")
    res = await FakeRunner().run(None)
    assert hasattr(res, ""span_graph"")
    send_mock.assert_awaited_once_with(""trace"", {""span"": 1})
    await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"async def test_attach_runner(monkeypatch):
    # Fake runner class
    class FakeRunner:
        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()

    client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
    send_mock = AsyncMock(return_value={""ok"": True})
    monkeypatch.setattr(client, ""send"", send_mock)

    client.attach_runner(FakeRunner, ""trace"")
    res = await FakeRunner().run(None)
    assert hasattr(res, ""span_graph"")
    send_mock.assert_awaited_once_with(""trace"", {""span"": 1})
    await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"async def test_send_success(telemetry_client):
    result = await telemetry_client.send(""trace"", {""data"": 1})
    assert result == {""ok"": True}
",tests/unit/test_telemetry_client.py,,1,7
survived,"        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()
",tests/unit/test_telemetry_client.py,FakeRunner,1,6
survived,"        async def close(self) -> None:
            pass
",src/meta_agent/services/telemetry_client.py,ClientSession,1,6
survived,"    def attach_runner(self, runner_cls: Any, endpoint: str = ""traces"") -> None:
        """"""Patch ``runner_cls.run`` to send span data to ``endpoint``.

        The patched ``run`` method forwards all arguments to the original
        implementation, awaits the result, and if the result object exposes a
        ``span_graph``/``spans``/``trace`` attribute, it will be posted to the
        configured telemetry endpoint using :meth:`send`.
        """"""

        orig_run = getattr(runner_cls, ""run"")

        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result

        setattr(runner_cls, ""run"", wrapped_run)
        runner_cls._meta_agent_orig_run = orig_run  # type: ignore[attr-defined]
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"            def set(self, v):
                self.value = v
",tests/test_base_helpers.py,TestPromMetrics.Dummy,1,6
survived,"    def tearDown(self):
        AGENT_REGISTRY.clear()
        AGENT_REGISTRY.update(self._backup)
",tests/test_agents_registry.py,TestVersionOverride,1,7
survived,"    def test_validate_demos_cli(self) -> None:
        """"""Running the module as a CLI should succeed.""""""
        import sys
        import subprocess

        result = subprocess.run(
            [sys.executable, ""-m"", ""alpha_factory_v1.demos.validate_demos""],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""validated"", result.stdout.lower())",tests/test_demos.py,TestDemos,1,7
survived,"    def test_empty_demo_fails(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            demo_dir = Path(tmpdir) / ""demo""
            demo_dir.mkdir()
            (demo_dir / ""README.md"").write_text(""""""# Title\nMore than ten lines\nline3\nline4\nline5\nline6\nline7\nline8\nline9\nline10\n"""""")
            (demo_dir / ""__init__.py"").write_text(""# package\n"")
            ret = validate_demos.main(str(tmpdir), min_lines=10)
            self.assertEqual(ret, 1)
",alpha_factory_v1/tests/test_validate_demos.py,TestValidateDemos,0,6
survived,"    def test_montecarlo_hedge_basic(self):
        sim = simulation_core.MonteCarloSimulator(n_paths=500, horizon=5)
        factors = sim.simulate({
            ""yield_10y"": 4.0,
            ""yield_3m"": 4.5,
            ""stable_flow"": 10.0,
            ""es_settle"": 5000.0,
        })
        hedge = sim.hedge(factors, 1_000_000)
        self.assertIn(""es_notional"", hedge)
        self.assertIn(""dv01_usd"", hedge)
        self.assertIn(""metrics"", hedge)
        self.assertEqual(len(sim.scenario_table(factors)), 3)
",tests/test_macro_sentinel.py,TestMacroSentinel,0,7
survived,"async def get_order_items(order_id: int, ctx: EnrichContext) -> list[""OrderItemEnrichModel""]:
    """"""Get all items in a specific order.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        result = await session.execute(select(OrderItem).where(OrderItem.order_id == order_id))
        items = result.scalars().all()

        return [
            OrderItemEnrichModel(
                id=item.id,
                order_id=item.order_id,
                product_id=item.product_id,
                quantity=item.quantity,
                unit_price=item.unit_price,
                total_price=item.total_price,
            )
            for item in items
        ]
",examples/sqlalchemy_shop/app.py,,1,7
survived,"    def test_relationship_without_description(self):
        """"""Test relationship with no description gets a default one.""""""

        class Base(DeclarativeBase):
            pass

        class User(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""users""

            id: Mapped[int] = mapped_column(primary_key=True)
            posts: Mapped[list[""Post""]] = relationship()

        class Post(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""posts""

            id: Mapped[int] = mapped_column(primary_key=True)
            user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))

        UserEnrichModel = User.__enrich_model__()
        fields = UserEnrichModel.model_fields

        assert ""posts"" in fields
        assert isinstance(fields[""posts""].default, Relationship)
        assert fields[""posts""].default.description == ""Relationship to PostEnrichModel""
",tests/test_sqlalchemy_integration.py,TestRelationships,1,7
survived,"async def seed_database(session: AsyncSession) -> None:
    """"""Populate the database with example data.""""""

    users = [
        User(
            username=""john_doe"",
            email=""john@example.com"",
            full_name=""John Doe"",
            created_at=datetime.now(),
        ),
        User(
            username=""jane_smith"",
            email=""jane@example.com"",
            full_name=""Jane Smith"",
            created_at=datetime.now(),
        ),
    ]
    session.add_all(users)

    products = [
        Product(
            name=""Laptop"",
            description=""High-performance laptop"",
            price=999.99,
            stock_quantity=50,
            category=""Electronics"",
            created_at=datetime.now(),
        ),
        Product(
            name=""Wireless Mouse"",
            description=""Ergonomic wireless mouse"",
            price=29.99,
            stock_quantity=200,
            category=""Electronics"",
            created_at=datetime.now(),
        ),
        Product(
            name=""USB-C Cable"",
            description=""Fast charging USB-C cable"",
            price=19.99,
            stock_quantity=500,
            category=""Accessories"",
            created_at=datetime.now(),
        ),
        Product(
            name=""Coffee Maker"",
            description=""Programmable coffee maker"",
            price=79.99,
            stock_quantity=30,
            category=""Appliances"",
            created_at=datetime.now(),
        ),
    ]
    session.add_all(products)
    await session.flush()

    order1 = Order(
        order_number=""ORD-001"",
        user_id=users[0].id,
        status=""delivered"",
        total_amount=1029.98,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        shipping_address=""123 Main St, City, State 12345"",
    )
    order2 = Order(
        order_number=""ORD-002"",
        user_id=users[1].id,
        status=""processing"",
        total_amount=99.98,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        shipping_address=""456 Oak Ave, Town, State 67890"",
    )
    session.add_all([order1, order2])
    await session.flush()

    items = [
        OrderItem(
            order_id=order1.id,
            product_id=products[0].id,
            quantity=1,
            unit_price=999.99,
            total_price=999.99,
        ),
        OrderItem(
            order_id=order1.id,
            product_id=products[1].id,
            quantity=1,
            unit_price=29.99,
            total_price=29.99,
        ),
        OrderItem(
            order_id=order2.id,
            product_id=products[3].id,
            quantity=1,
            unit_price=79.99,
            total_price=79.99,
        ),
        OrderItem(
            order_id=order2.id,
            product_id=products[2].id,
            quantity=1,
            unit_price=19.99,
            total_price=19.99,
        ),
    ]
    session.add_all(items)
",examples/sqlalchemy_shop/app.py,,1,6
survived,"    def test_model_with_no_docstring(self):
        """"""Test model without docstring gets a default one.""""""

        class Base(DeclarativeBase):
            pass

        class NoDoc(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""no_doc""
            id: Mapped[int] = mapped_column(primary_key=True)

        NoDocEnrichModel = NoDoc.__enrich_model__()
        assert NoDocEnrichModel.__doc__ == ""NoDoc entity""
",tests/test_sqlalchemy_integration.py,TestEdgeCases,1,7
survived,"    def test_non_declarative_base_raises_error(self):
        """"""Test that using the mixin without DeclarativeBase raises an error.""""""

        class NotSQLAlchemy(EnrichSQLAlchemyMixin):
            """"""This is not a SQLAlchemy model.""""""

            pass

        with pytest.raises(TypeError) as exc_info:
            NotSQLAlchemy.__enrich_model__()

        assert ""must inherit from SQLAlchemy DeclarativeBase"" in str(exc_info.value)
",tests/test_sqlalchemy_integration.py,TestEdgeCases,1,7
survived,"    def publish(cls, topic: str, msg: dict):
        with cls._lock:
            for cb in list(cls._subs.get(topic, [])):
                try:
                    cb(msg)
                except Exception as exc:  # pragma: no cover
                    LOG.error(""[A2A] handler error on %s: %s"", topic, exc)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,A2ABus,1,7
survived,"    def __init__(self, ledger_path: str | pathlib.Path):
        self.path = pathlib.Path(ledger_path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,LineageTracer,1,7
survived,"            def __init__(self): super().__init__(""llm_planner"")
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,LLMPlanner,1,7
survived,"    def __init__(self, input_dim: int, hidden: int):
        super().__init__(); self.l = nn.Linear(input_dim, hidden)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Repr,1,7
survived,"def _linear(value: float, target: float, cap: float | None = None) -> float:
    """"""Linearly scales ``value`` â†’ [0, 1] with 1 at ``target``.""""""
    if cap is None:
        cap = 2 * target  # allow 2Ã— target to reach 0
    v = max(min(value, cap), 0)
    return max(0.0, 1.0 - abs(v - target) / (cap - target + _EPS))
",alpha_factory_v1/demos/era_of_experience/reward_backends/fitness_reward.py,,1,6
survived,"    def reset(self):
        self.agent = (0,0)
        return self._obs()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,MiniWorld,1,7
survived,"    def __init__(self, env: MiniWorld):
        self.net = MuZeroTiny(env.size**2, 4).to(CFG.device)
        self.opt = optim.Adam(self.net.parameters(), CFG.lr)
        self.buffer : List[Tuple[np.ndarray,float]]=[]
        self.step_count=0
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Learner,1,7
survived,"    def forward(self, h, a):
        x = torch.cat([h, a], -1)
        return self.r(x), torch.tanh(self.h(x))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Dyn,1,6
survived,"    def run(self, prompt: str, context: Optional[Iterable[Dict[str,str]]]=None, **kw) -> Dict[str,Any]:
        ctx: List[Dict[str,str]] = list(context or [])
        ctx.append({""role"":""user"", ""content"": prompt})
        t0 = time.perf_counter()
        output = self.lm.chat(ctx, **kw)
        latency = time.perf_counter()-t0
        tokens_in = _str_tkn(prompt)
        tokens_out = _str_tkn(output)
        cost = self._estimate_cost(tokens_in,tokens_out)
        carbon = cost*0.00015 # placeholder multiplier (avg kgCO2 per $ cloud)
        risk = self._risk_assess(prompt, output)
        metrics = dict(latency=latency, cost=cost, carbon=carbon, risk=risk)
        score = self.objectives.score(metrics)
        self.tracer.log(""run"", prompt=prompt[:120], response=output[:120], metrics=metrics, score=score)
        return {""response"": output, ""metrics"": metrics, ""score"": score}
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent,1,7
survived,"def test_revive_rate() -> None:
    rng = random.Random(12345)
    agents = {""A"": True, ""B"": False}
    result = loop.run_loop(
        cost_budget=100.0,
        cost_per_cycle=1.0,
        revive_rate=10,
        agents=agents,
        rng=rng,
    )
    assert result.revives >= 1",tests/test_revive_rate.py,,1,7
survived,"def test_rejects_low_entropy_patch() -> None:
    diff = _read(""red_team.diff"")
    assert not is_patch_safe(diff)",tests/test_patch_entropy.py,,1,7
survived,"def evaluate_agent(code: str) -> dict[str, float]:
    """"""Return accuracy, novelty SimHash and execution latency.""""""

    import random
    import time
    from hashlib import blake2b

    start = time.perf_counter()
    h = blake2b(code.encode(), digest_size=8).digest()
    simhash = int.from_bytes(h, ""big"")
    rng = random.Random(simhash & 0xFFFF)
    accuracy = 0.5 + rng.random() * 0.5
    latency_ms = (time.perf_counter() - start) * 1000
    return {
        ""accuracy"": accuracy,
        ""novelty_simhash"": float(simhash),
        ""latency_ms"": latency_ms,
    }
",src/eval/fitness.py,,1,7
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:
        self.published.append((topic, env))
",tests/test_codegen_safety.py,DummyBus,1,7
survived,"    def close(self) -> None:  # pragma: no cover - dummy
        pass
",tests/test_codegen_safety.py,DummyLedger,0,7
survived,"def test_allows_normal_message() -> None:
    agent = _make_agent()
    env = messaging.Envelope(
        sender=""market"",
        recipient=""safety"",
        payload={""analysis"": ""hold position""},
        ts=0.0,
    )
    asyncio.run(agent.handle(env))
    assert agent.bus.published[-1][1].payload[""status""] == ""ok""",tests/test_codegen_safety.py,,1,7
survived,"def play_episode(agent: MiniMu, render: bool = True, max_steps: int = 500) -> Tuple[List, float]:
    """"""Run a full episode using the agent.""""""
    obs = agent.reset()
    frames: List = []
    total_reward = 0.0
    done = False
    truncated = False
    while not done and not truncated and len(frames) < max_steps:
        if render:
            frames.append(agent.env.render())
        action = agent.act(obs)
        obs, reward, done, truncated, _ = agent.env.step(action)
        total_reward += float(reward)
    if render:
        frames.append(agent.env.render())
    agent.env.close()
    return frames, total_reward
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,,1,7
survived,"    def reset(self):
        obs, _ = self.env.reset()
        return obs
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,MiniMu,1,7
survived,"    def test_load_job(self):
        path = Path(""alpha_factory_v1/demos/alpha_agi_marketplace_v1/examples/sample_job.json"")
        job = load_job(path)
        self.assertEqual(job[""agent""], ""finance"")
",alpha_factory_v1/tests/test_marketplace_client.py,MarketplaceClientTest,1,6
survived,"    def test_parse_args_defaults(self):
        args = parse_args([])
        sample = Path(
            ""alpha_factory_v1/demos/alpha_agi_marketplace_v1/examples/sample_job.json""
        ).resolve()
        self.assertEqual(Path(args.job_file), sample)
        self.assertEqual(args.host, ""localhost"")
        self.assertEqual(args.port, 8000)
",alpha_factory_v1/tests/test_marketplace_client.py,MarketplaceClientTest,1,7
survived,"    def test_mcts_policy_bounds(self):
        if not dependencies_available:
            self.skipTest(""demo dependencies missing"")
        net = demo.MuZeroTiny(obs_dim=9, act_dim=4)
        obs = [0.0] * 9
        act = demo.mcts_policy(net, obs, simulations=4)
        self.assertIsInstance(act, int)
        self.assertGreaterEqual(act, 0)
        self.assertLess(act, 4)
",alpha_factory_v1/tests/test_alpha_asi_world_model.py,TestAlphaASIWorldModel,1,7
survived,"    async def publish(self, topic, msg):
        self.messages.append((topic, msg))
",alpha_factory_v1/tests/test_ping_agent.py,DummyOrchestrator,1,7
survived,"    def test_check_docker_daemon(self):
        with mock.patch('shutil.which', return_value=None):
            self.assertFalse(preflight.check_docker_daemon())
        with mock.patch('shutil.which', return_value='/bin/docker'):
            with mock.patch('subprocess.run') as run:
                run.return_value = mock.Mock(returncode=0)
                self.assertTrue(preflight.check_docker_daemon())
            with mock.patch('subprocess.run', side_effect=Exception):
                self.assertFalse(preflight.check_docker_daemon())
",alpha_factory_v1/tests/test_preflight.py,PreflightTest,1,7
survived,"    def test_import_success(self):
        with NamedTemporaryFile('w', delete=False) as tmp:
            json.dump({'title': 't'}, tmp)
            tmp_path = tmp.name
        with mock.patch.dict(os.environ, {'GRAFANA_TOKEN': 'tok', 'GRAFANA_HOST': 'http://h'}, clear=True):
            with mock.patch.object(sys, 'argv', ['script', tmp_path]):
                with mock.patch('alpha_factory_v1.scripts.import_dashboard.post') as post:
                    post.return_value = mock.Mock(raise_for_status=lambda: None)
                    import_dashboard.main()
                    post.assert_called_once()
                    args, kwargs = post.call_args
                    self.assertEqual(args[0], 'http://h/api/dashboards/import')
                    self.assertEqual(kwargs['headers']['Authorization'], 'Bearer tok')
",alpha_factory_v1/tests/test_import_dashboard.py,ImportDashboardTest,1,7
survived,"    def __init__(
        self,
        result_collector: Optional[ResultCollectionModule] = None,
        reporter: Optional[ReportingModule] = None,
    ) -> None:
        self.result_collector = result_collector or ResultCollectionModule()
        self.reporter = reporter or ReportingModule()
        self.logger = logging.getLogger(__name__)
",src/meta_agent/evaluation/harness.py,EvaluationHarness,1,7
survived,"def test_run_macro_demo_help() -> None:
    """"""`run_macro_demo.sh --help` should exit successfully.""""""
    subprocess.run([str(RUN_SCRIPT), ""--help""], check=True, capture_output=True)",tests/test_macro_compose_config.py,,1,7
survived,"def doNeg(x):
    pass
",tests/rosetta/transpiler/Python/conditional-structures-4.py,,0,7
survived,"def main():
    print(""For primes < 1 million:\n"")
    for dir in [""ascending"", ""descending""]:
        longestSeq(dir)
",tests/rosetta/transpiler/Python/consecutive-primes-with-ascending-or-descending-differences.py,,1,6
survived,"def main():
    example10()
",tests/rosetta/transpiler/Python/conditional-structures-10.py,,1,6
survived,"def example6(value):
    (None if value == 1 else (None if value == 2 else (None if value == 3 else (None if value == 4 else None))))",tests/rosetta/transpiler/Python/conditional-structures-6.py,,0,7
survived,"def setCell(f, x, y, b):
    rows = f.s
    row = rows[y]
    row[x] = b
    rows[y] = row
    f = dataclasses.replace(f, s=rows)
",tests/rosetta/transpiler/Python/conways-game-of-life.py,,0,7
survived,"def newTerm(a, b):
    return {""a"": a, ""b"": b}
",tests/rosetta/transpiler/Python/continued-fraction.py,,1,6
survived,"def example3(a, b):
    if a:
        None
    else:
        if b:
            None",tests/rosetta/transpiler/Python/conditional-structures-3.py,,0,8
survived,"def main() -> None:
    repo_root = Path(__file__).resolve().parent.parent
    manifest_path = repo_root / (""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build_assets.json"")
    manifest = json.loads(manifest_path.read_text())
    checksums = manifest.get(""checksums"", {})
    checksums.update(fa.CHECKSUMS)
    manifest[""checksums""] = {k: checksums[k] for k in sorted(checksums)}
    manifest_path.write_text(json.dumps(manifest, indent=2) + ""\n"")
    print(f""Updated {manifest_path}"")
",scripts/generate_build_manifest.py,,1,6
survived,"def check_network(host: str = ""pypi.org"", timeout: float = 2.0) -> bool:
    """"""Return True if *host* can be resolved within *timeout* seconds.""""""
    try:
        with suppress(Exception):
            prev = socket.getdefaulttimeout()
        socket.setdefaulttimeout(timeout)
        socket.gethostbyname(host)
    except Exception:
        banner(
            f""WARNING: Unable to resolve {host}. Use --wheelhouse for offline installs."",
            ""YELLOW"",
        )
        return False
    finally:
        with suppress(Exception):
            socket.setdefaulttimeout(prev)
    banner(f""{host} resolved"", ""GREEN"")
    return True
",alpha_factory_v1/scripts/preflight.py,,1,7
survived,"    def test_check_network(self) -> None:
        with mock.patch(""socket.gethostbyname"", return_value=""1.2.3.4""):
            self.assertTrue(preflight.check_network())
        with mock.patch(""socket.gethostbyname"", side_effect=OSError):
            with mock.patch.object(preflight, ""banner"") as b:
                self.assertFalse(preflight.check_network())
                b.assert_called()
",alpha_factory_v1/tests/test_preflight.py,PreflightTest,1,7
survived,"    def __init__(
        self,
        config: SanskritPoetryEnvConfig,
        server_configs: List[APIServerConfig],
        slurm: bool = True,
        testing: bool = False,
    ):
        super().__init__(config, server_configs, slurm, testing)
        # Create reward function using registry for easy configuration
        self.reward_fn = registry.create(
            {""type"": ""chandas_meter"", ""params"": {""meter"": config.meter}}
        )
        self.iter = 0
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv,1,7
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Delete stale entries from the SQL cache.""""""
        threshold = datetime.now() - stale_after
        with self._lock, self._Session() as session:
            session.execute(
                delete(CacheTable).where(
                    and_(
                        CacheTable.function_id == self._func_str,
                        CacheTable.timestamp < threshold,
                    )
                )
            )
            session.commit()",src/cachier/cores/sql.py,_SQLCore,1,7
survived,"def test_close_stops_consumer() -> None:
    bus = EventBus(None, True)
    called = False

    async def dummy_stop() -> None:
        nonlocal called
        called = True
        bus._consumer_task = None

    bus._consumer_task = object()  # type: ignore[assignment]
    bus.stop_consumer = dummy_stop  # type: ignore[assignment]
    bus._close()
    assert called
    assert bus._consumer_task is None",tests/test_eventbus.py,,1,6
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""Return order as plain dictionary.""""""
        return asdict(self)
",alpha_factory_v1/backend/broker.py,Order,1,8
survived,"    async def get_cash(self) -> float:
        """"""Return the available cash balance in the account currency.""""""
",alpha_factory_v1/backend/types.py,TradeBrokerProtocol,1,7
survived,"def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Alpha-Factory backend entry point"")
    parser.add_argument(""--dev"", action=""store_true"", help=""Enable development mode (DEV_MODE)"")
    parser.add_argument(""--preflight"", action=""store_true"", help=""Run environment checks and exit"")
    parser.add_argument(""--port"", type=int, help=""REST API port (PORT)"")
    parser.add_argument(""--metrics-port"", type=int, help=""Prometheus metrics port (METRICS_PORT)"")
    parser.add_argument(""--a2a-port"", type=int, help=""A2A gRPC port (A2A_PORT)"")
    parser.add_argument(""--disable-tls"", action=""store_true"", help=""Disable TLS for gRPC (INSECURE_DISABLE_TLS)"")
    parser.add_argument(""--kafka-broker"", help=""Kafka bootstrap servers (ALPHA_KAFKA_BROKER)"")
    parser.add_argument(""--cycle-seconds"", type=int, help=""Default agent cycle period (ALPHA_CYCLE_SECONDS)"")
    parser.add_argument(""--max-cycle-sec"", type=int, help=""Hard limit per agent run (MAX_CYCLE_SEC)"")
    parser.add_argument(""--enabled"", help=""Comma-separated list of enabled agents (ALPHA_ENABLED_AGENTS)"")
    parser.add_argument(""--loglevel"", default=""INFO"", help=""Logging level (LOGLEVEL)"")
    parser.add_argument(""--version"", action=""store_true"", help=""Print version and exit"")
    return parser.parse_args()
",alpha_factory_v1/backend/main.py,,1,7
survived,"def Tool(*_args, **_kwargs):
    def decorator(func):
        return func

    return decorator",stubs/openai_agents/__init__.py,,1,6
survived,"    async def __call__(self, text: str) -> str:  # pragma: no cover - demo stub
        return ""ok""
",stubs/openai_agents/__init__.py,OpenAIAgent,0,6
survived,"        def _decorator(func):
            return func
",alpha_factory_v1/demos/self_healing_repo/agent_selfheal_entrypoint.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_items_iteration.py,Data,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/cross_join.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join.py,Nation,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/json_builtin.py,Auto1,1,7
survived,"        def add_heading(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyDocxDocument,0,7
survived,"    def test_timestamp_quoting(self):
        df = pd.DataFrame(
            {
                ""metric_timestamp"": [
                    pd.Timestamp(""2023-01-01""),
                    pd.Timestamp(""2023-01-02""),
                ],
                ""value"": [10, 20],
            }
        )
        result = generate_insert_sql(df, ""metrics"", batch_size=1)
        expected = [
            (
                ""INSERT INTO metrics (metric_timestamp, value) ""
                ""VALUES ('2023-01-01 00:00:00', 10);""
            ),
            (
                ""INSERT INTO metrics (metric_timestamp, value) ""
                ""VALUES ('2023-01-02 00:00:00', 20);""
            ),
        ]
        assert result == expected",tests/test_df_utils.py,TestGenerateInsertSQL,1,7
survived,"    def emit(self, line: str) -> None:
        self.lines.append(""  "" * self.indent + line)
",tools/any2mochi/py/py2mochi.py,Converter,1,7
survived,"    def strip_comments(s: str) -> str:
        return ""\n"".join([ln.split(""//"")[0].rstrip() for ln in s.splitlines()])
",tools/any2mochi/py/run_all.py,,1,7
survived,"    def visit_Pass(self, node: ast.Pass) -> None:
        pass
",tools/any2mochi/py/py2mochi.py,Converter,1,6
survived,"    def _clear_tool_stats() -> None:
        clear_tool_stats()
",src/serena/dashboard.py,SerenaDashboardAPI,0,7
survived,"    def __init__(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyBlocks,0,7
deleted,"    def validate_validation_dataset(cls, dataset_name: str | None) -> str | None:
        if dataset_name is None:
            return None
        try:
            url = f""https://huggingface.co/api/datasets/{dataset_name}/tree/main""
            response = requests.get(url, timeout=5)
            if response.status_code != 200:
                raise ValueError()
            return dataset_name
        except Exception:
            raise ValueError(
                f""Dataset {dataset_name} is not a valid, public Hugging Face dataset. Please check the URL and try again. Your dataset name should be in the format <username>/<dataset_name>"",
            )
",phosphobot/phosphobot/am/base.py,TrainingRequest,1,7
survived,"def _make_client(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> TestClient:
    monkeypatch.setenv(""STORAGE_PATH"", str(tmp_path))
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src import evolution_worker

    return TestClient(evolution_worker.app)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_evolution_worker.py,,1,6
survived,"    def _run_runtime(
        episodes: int, target: int, model: str | None = None, rewriter: str | None = None
    ) -> None:
        print(""openai-agents package is missing. Running offline demo..."")
        run(episodes=episodes, target=target, model=model, rewriter=rewriter)
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,,1,6
survived,"    async def run_insight_search(
        episodes: int = 5,
        target: int = 3,
        model: str | None = None,
        rewriter: str | None = None,
        sectors: str | None = None,
    ) -> str:
        return run(
            episodes=episodes,
            target=target,
            model=model,
            rewriter=rewriter,
            sectors=(sectors.split("","") if sectors else None),
        )
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,,1,6
survived,"def _check_ollama(url: str) -> None:
    """"""Verify an Ollama server is reachable.""""""
    base = url.rstrip(""/"")
    if base.endswith(""/v1""):
        base = base[:-3]
    try:
        request.urlopen(f""{base}/api/tags"", timeout=3)
    except Exception as exc:  # pragma: no cover - network check
        raise RuntimeError(
            f""Ollama not reachable at {base}. ""
            ""Install it from https://ollama.com and run 'ollama serve'.""
        ) from exc
",alpha_factory_v1/demos/macro_sentinel/agent_macro_entrypoint.py,,1,7
survived,"def test_template_metadata_invalid_category() -> None:
    try:
        TemplateMetadata(
            slug=""bad"",
            title=""Bad"",
            description=""Bad"",
            category=""invalid"",  # type: ignore[arg-type]
            subcategory=""x"",
            complexity=TemplateComplexity.BASIC,
        )
    except ValidationError:
        pass
    else:  # pragma: no cover - should not succeed
        assert False, ""ValidationError not raised""",tests/test_template_schema.py,,1,7
survived,"    def add_path(self):
        ''' Adds a document to a path starting from this node '''
        node = self
        node.customers += 1
        for level in range(1, self.num_levels):
            node = node.parent
            node.customers += 1
",src/hlda/sampler.py,NCRPNode,1,6
survived,"def run_demo(args):
    corpus = load_documents(args.data_dir)
    vocab, index = build_vocab(corpus)
    int_corpus = convert_corpus(corpus, index)

    hlda = HierarchicalLDA(
        int_corpus,
        vocab,
        alpha=args.alpha,
        gamma=args.gamma,
        eta=args.eta,
        num_levels=args.num_levels,
        seed=args.seed,
    )

    hlda.estimate(
        args.iterations,
        display_topics=args.display_topics,
        n_words=args.n_words,
        with_weights=False,
    )

    print(""\nFinal topic hierarchy:"")
    hlda.print_nodes(args.n_words, with_weights=False)

    return hlda
",scripts/run_hlda.py,,1,7
survived,"    def select(self, gamma):
        ''' Selects an existing child or create a new one according to the CRP '''

        weights = np.zeros(len(self.children)+1)
        weights[0] = float(gamma) / (gamma+self.customers)
        i = 1
        for child in self.children:
            weights[i] = float(child.customers) / (gamma + self.customers)
            i += 1

        choice = self.random_state.multinomial(1, weights).argmax()
        if choice == 0:
            return self.add_child()
        else:
            return self.children[choice-1]
",src/hlda/sampler.py,NCRPNode,1,7
survived,"    def drop_path(self):
        ''' Removes a document from a path starting from this node (leaf) upwards '''
        node = self
        while node is not None:
            node.customers -= 1
            if node.customers == 0 and node.parent is not None:
                node.parent.remove(node)
            node = node.parent
",src/hlda/sampler.py,NCRPNode,1,7
survived,"def _write_notebook(path: str) -> None:
    """"""Create a simple notebook with one formattable code cell.""""""
    nb = {
        ""cells"": [
            {""cell_type"": ""code"", ""source"": [""print('{}'.format(1))\n""]},
            {""cell_type"": ""markdown"", ""source"": [""# header""]},
        ]
    }
    with open(path, ""w"", encoding=""utf-8"") as f:
        json.dump(nb, f)
",test/integration/test_api.py,,1,7
survived,"    def padded(start, stop):
        pos = hax.arange(Pos, dtype=jnp.int32, start=start)
        return hax.where(pos >= stop, -1, pos)
",tests/test_attention.py,,1,6
survived,"    def close(self):
        self.connected = False
",tests/test_sys_fn_kdb.py,DummyQConnection,1,6
survived,"    def __call__(self, qexp):
        self.queries.append(qexp)
        return f""EXEC:{qexp}""
",tests/test_sys_fn_kdb.py,DummyQConnection,1,7
survived,"    def __init__(self, host='localhost', port=5000, username=None, password=None):
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.connected = False
        self.queries = []
",tests/test_sys_fn_kdb.py,DummyQConnection,1,7
survived,"def test_no_pep585_generics():
    viols = []
    for root, _, files in os.walk(os.path.dirname(os.path.dirname(__file__))):
        for file in files:
            if file.endswith('.py') and not file.startswith('test_'):
                path = os.path.join(root, file)
                with open(path, 'r', encoding='utf-8') as f:
                    source = f.read()
                try:
                    tree = ast.parse(source)
                except SyntaxError:
                    continue
                for node in ast.walk(tree):
                    if isinstance(node, ast.Subscript) and isinstance(node.value, ast.Name):
                        if node.value.id in ALLOWED_BUILTINS:
                            viols.append(f""{path}:{node.lineno}"")
    assert not viols, ""PEP585 generics found: "" + "", "".join(viols)",tests/test_no_pep585.py,,0,7
survived,"    def __init__(self) -> None:
        self.dim = _DIM
        self.index = faiss.IndexFlatIP(self.dim) if faiss else None
        self.mean = np.zeros(self.dim, dtype=""float32"")
        self.count = 0
",src/evaluators/novelty.py,NoveltyIndex,1,7
survived,"    def divergence(self, text: str) -> float:
        vec = embed(text)
        if self.count == 0:
            return 1.0
        p = _softmax(vec[0])
        q = _softmax(self.mean)
        kl = float(np.sum(p * np.log((p + 1e-12) / (q + 1e-12))))
        return kl",src/evaluators/novelty.py,NoveltyIndex,1,7
survived,"def _non_dominated_sort(values: Sequence[Sequence[float]]) -> tuple[list[int], list[list[int]]]:
    n = len(values)
    ranks = [0] * n
    S = [set() for _ in range(n)]
    dominated = [0] * n
    for i, a in enumerate(values):
        for j, b in enumerate(values):
            if i == j:
                continue
            if all(ai <= bj for ai, bj in zip(a, b)) and any(ai < bj for ai, bj in zip(a, b)):
                S[i].add(j)
            elif all(bj <= ai for ai, bj in zip(a, b)) and any(bj < ai for ai, bj in zip(a, b)):
                dominated[i] += 1
        if dominated[i] == 0:
            ranks[i] = 0
    fronts = [[i for i, d in enumerate(dominated) if d == 0]]
    i = 0
    while i < len(fronts):
        nxt: list[int] = []
        for p in fronts[i]:
            for q in S[p]:
                dominated[q] -= 1
                if dominated[q] == 0:
                    ranks[q] = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return ranks, fronts
",src/simulation/surrogate_fitness.py,,1,6
survived,"def test_download_error(tmp_path: Path, requests_mock: ""requests_mock.Mocker"") -> None:
    monkeypatch_file_list = [""dummy.txt""]
    url = dg.model_urls(""117M"")[0].replace(""checkpoint"", ""dummy.txt"")
    requests_mock.get(url, status_code=404)

    dest_dir = tmp_path / ""models""
    with pytest.MonkeyPatch.context() as m:
        m.setattr(dg, ""_FILE_LIST"", monkeypatch_file_list)
        with pytest.raises(Exception):
            dg.download_openai_gpt2(""117M"", dest=dest_dir, attempts=1)",tests/test_download_openai_gpt2.py,,1,7
survived,"def _make_client(monkeypatch: pytest.MonkeyPatch, token: str = ""secret"") -> TestClient:
    monkeypatch.setenv(""API_TOKEN"", token)
    app = orchestrator._build_rest({""dummy"": DummyRunner()})
    assert app is not None
    return TestClient(app)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,,1,7
survived,"    def Depends(*_a, **_kw):  # type: ignore
        return None
",alpha_factory_v1/backend/orchestrator.py,,1,6
survived,"            def run(self) -> None:
                pass
",tests/test_alpha_opportunity_stub.py,TestAlphaOpportunityStub.DummyRuntime,0,7
survived,"    def prompt_video(self) -> str | None:
        prompt = self.properties.get(""prompt_video"")
        if prompt is None:
            return None
        if not isinstance(prompt, str):
            raise ValueError(""Invalid prompt_video. prompt_video must be a string."")
        return prompt
",libs/core/kiln_ai/datamodel/extraction.py,ExtractorConfig,1,7
survived,"def test_env_validation_fails(env_var: str, value: str, message: str) -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    script = browser_dir / ""build"" / ""env_validate.js""
    env = os.environ.copy()
    env[env_var] = value
    res = subprocess.run(
        [""node"", str(script)],
        cwd=browser_dir,
        env=env,
        capture_output=True,
        text=True,
    )
    assert res.returncode == 1
    assert message in res.stderr",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_env_validate.py,,1,6
survived,"def test_merkle_root_ignores_corrupt_rows(tmp_path: Path) -> None:
    ledger = Ledger(str(tmp_path / ""ledger.db""), broadcast=False)
    env1 = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
    env2 = messaging.Envelope(sender=""b"", recipient=""c"", payload={""v"": 2}, ts=1.0)
    ledger.log(env1)
    ledger.log(env2)

    baseline = ledger.compute_merkle_root()

    # insert rows with missing hash and invalid hash
    ledger.conn.execute(
        ""INSERT INTO messages (ts, sender, recipient, payload) VALUES (?, ?, ?, ?)"",
        (2.0, ""x"", ""y"", ""{oops""),
    )
    ledger.conn.execute(
        ""INSERT INTO messages (ts, sender, recipient, payload, hash) VALUES (?, ?, ?, ?, ?)"",
        (3.0, ""y"", ""z"", ""{}"", ""zz""),
    )
    ledger.conn.commit()

    root = ledger.compute_merkle_root()
    assert root == baseline

    # further logging should still succeed
    ledger.log(messaging.Envelope(sender=""c"", recipient=""d"", payload={""v"": 3}, ts=2.0))
    ledger.compute_merkle_root()
",tests/test_ledger_malformed_rows.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Lineitem,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Nation,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Supplier,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q3.py,Order,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Part,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto4,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto8,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q23.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto3,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q16.py,,0,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto1,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q13.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto4,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto2,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q28.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto7,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto4,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto10,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto9,1,7
survived,"def test_Q25_finds_male_horror_writer_with_violent_keywords():
    assert result == [
        Auto1(
            movie_budget=""Horror"",
            movie_votes=100,
            male_writer=""Mike"",
            violent_movie_title=""Scary Movie"",
        )
    ]
",tests/dataset/job/compiler/py/q25.py,,0,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto10,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto6,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto3,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q30.py,,0,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto6,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto5,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto5,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto8,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto12,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q12.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto1,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q33.py,,1,6
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q22.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto9,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto12,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,HouseholdDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,StoreSale,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q1.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q26.py,Promotion,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q4.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,StoreSale,1,7
survived,"def test_TPCDS_Q32_simplified():
    assert result == 20.0
",tests/dataset/tpc-ds/compiler/py/q32.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Item,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,Customer,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q61.py,Sale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Store,1,7
survived,"def test_TPCDS_Q20_revenue_ratio():
    assert result == [
        Auto1(
            i_item_id=""ITEM1"",
            i_item_desc=""Item One"",
            i_category=""A"",
            i_class=""X"",
            i_current_price=10.0,
            itemrevenue=600.0,
            revenueratio=66.66666666666667,
        ),
        Auto1(
            i_item_id=""ITEM2"",
            i_item_desc=""Item Two"",
            i_category=""A"",
            i_class=""X"",
            i_current_price=20.0,
            itemrevenue=300.0,
            revenueratio=33.333333333333336,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q20.py,,1,7
survived,"def abs(x):
    if x >= 0.0:
        return x
    return -x
",tests/dataset/tpc-ds/compiler/py/q57.py,,1,7
survived,"def test_TPCDS_Q84_sample():
    assert result == 84.0
",tests/dataset/tpc-ds/compiler/py/q84.py,,0,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q40.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,DateDim,1,6
survived,"def test_TPCDS_Q96_count():
    assert result == 3
",tests/dataset/tpc-ds/compiler/py/q96.py,,1,7
survived,"def _q0():
    _groups = {}
    _order = []
    for j in joined:
        _k = Auto3(
            i_item_id=j.i_item_id,
            ca_country=j.ca_country,
            ca_state=j.ca_state,
            ca_county=j.ca_county,
        )
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(j)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            i_item_id=g.key[""i_item_id""],
            ca_country=g.key[""ca_country""],
            ca_state=g.key[""ca_state""],
            ca_county=g.key[""ca_county""],
            agg1=(
                sum([x.q for x in g]) / len([x.q for x in g]) if [x.q for x in g] else 0
            ),
            agg2=(
                sum([x.lp for x in g]) / len([x.lp for x in g])
                if [x.lp for x in g]
                else 0
            ),
            agg3=(
                sum([x.cp for x in g]) / len([x.cp for x in g])
                if [x.cp for x in g]
                else 0
            ),
            agg4=(
                sum([x.sp for x in g]) / len([x.sp for x in g])
                if [x.sp for x in g]
                else 0
            ),
            agg5=(
                sum([x.np for x in g]) / len([x.np for x in g])
                if [x.np for x in g]
                else 0
            ),
            agg6=(
                sum([x.by for x in g]) / len([x.by for x in g])
                if [x.by for x in g]
                else 0
            ),
            agg7=(
                sum([x.dep for x in g]) / len([x.dep for x in g])
                if [x.dep for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q18.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,StoreReturn,1,7
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q2.py,,1,7
survived,"def _q2():
    _groups = {}
    _order = []
    for f in filtered:
        _k = f.i_class
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(f)
    _items1 = [_groups[k] for k in _order]
    return [Auto4(_class=g.key, total=sum([x.itemrevenue for x in g])) for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q12.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,StoreSale,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q27.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,DateDim,1,6
survived,"def test_TPCDS_Q30_simplified():
    assert result == [
        Auto1(
            c_customer_id=""C1"",
            c_first_name=""John"",
            c_last_name=""Doe"",
            ctr_total_return=150.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q30.py,,1,7
survived,"def _q1():
    _groups = {}
    _order = []
    for r in by_customer:
        _k = Auto4(seg=int(r.revenue / 50))
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            segment=g.key[""seg""], num_customers=len(g), segment_base=g.key[""seg""] * 50
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q54.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto5,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q52.py,_Group,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q98.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Inventory,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,DateDim,1,6
survived,"def test_TPCDS_Q68_simplified():
    assert result == 68
",tests/dataset/tpc-ds/compiler/py/q68.py,,1,6
survived,"def test_TPCDS_Q12_revenue_ratio():
    assert result == [
        Auto1(
            i_item_id=""ITEM1"",
            i_item_desc=""Item One"",
            i_category=""A"",
            i_class=""C1"",
            i_current_price=10.0,
            itemrevenue=200.0,
            revenueratio=50.0,
        ),
        Auto1(
            i_item_id=""ITEM2"",
            i_item_desc=""Item Two"",
            i_category=""A"",
            i_class=""C1"",
            i_current_price=20.0,
            itemrevenue=200.0,
            revenueratio=50.0,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q12.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Auto2,1,7
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q3.py,,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q77.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,DateDim,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q64.py,StoreReturn,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q40.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q81.py,CatalogReturn,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q89.py,StoreSale,1,7
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q21.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto6,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q55.py,_Group,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q25.py,,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q95.py,,1,6
survived,"def _q6():
    _src = catalog_returns
    _rows = _query(
        _src,
        [
            {
                ""items"": date_dim,
                ""on"": lambda cr, d: d.d_date_sk == cr.cr_returned_date_sk,
            }
        ],
        {""select"": lambda cr, d: (cr, d)},
    )
    _groups = _group_by(_rows, lambda cr, d: cr.cr_call_center_sk)
    _items7 = _groups
    return [
        Auto5(
            cr_call_center_sk=g.key,
            returns=_sum([x[0].cr_return_amount for x in g]),
            profit_loss=_sum([x[0].cr_net_loss for x in g]),
        )
        for g in _items7
    ]
",tests/dataset/tpc-ds/compiler/py/q77.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q65.py,_Group,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q26.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,StoreSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Auto2,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q6.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,Auto1,1,7
survived,"def test_TPCDS_Q98_revenue():
    assert result == [
        Auto1(
            i_item_id=""I1"",
            i_item_desc=""desc1"",
            i_category=""CatA"",
            i_class=""Class1"",
            i_current_price=100.0,
            itemrevenue=50.0,
            revenueratio=33.333333333333336,
        ),
        Auto1(
            i_item_id=""I2"",
            i_item_desc=""desc2"",
            i_category=""CatB"",
            i_class=""Class1"",
            i_current_price=200.0,
            itemrevenue=100.0,
            revenueratio=66.66666666666667,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q98.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,Auto2,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q73.py,_Group,1,7
survived,"def test_TPCDS_Q4_result():
    assert result == [
        Auto1(
            customer_id=""C1"",
            customer_first_name=""Alice"",
            customer_last_name=""A"",
            customer_login=""alice"",
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q4.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,StoreSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Auto1,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q70.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q24.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,W,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,CustomerAddres,1,6
survived,"def test_TPCDS_Q28_buckets():
    assert result == Auto1(
        B1_LP=100.0, B1_CNT=1, B1_CNTD=1, B2_LP=80.0, B2_CNT=1, B2_CNTD=1
    )
",tests/dataset/tpc-ds/compiler/py/q28.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Customer,1,7
survived,"def test_TPCDS_Q41_simplified():
    assert result == [""Blue Shirt"", ""Red Dress""]
",tests/dataset/tpc-ds/compiler/py/q41.py,,1,6
survived,"def test_TPCDS_Q69_simplified():
    assert result == 69
",tests/dataset/tpc-ds/compiler/py/q69.py,,0,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q25.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,Store,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Item,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q24.py,,1,6
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q28.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,HouseholdDemographic,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q71.py,_Group,1,6
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q65.py,,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q53.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q86.py,WebSale,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q91.py,_Group,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q70.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q70.py,_Group,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q71.py,_Group,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q13.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,DateDim,1,6
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q27.py,,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q72.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,DateDim,1,7
survived,"def _q0():
    _src = web_returns
    _rows = _query(
        _src,
        [
            {
                ""items"": date_dim,
                ""on"": lambda wr, d: wr.wr_returned_date_sk == d.d_date_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda wr, d, ca: wr.wr_returning_addr_sk == ca.ca_address_sk,
            },
        ],
        {
            ""select"": lambda wr, d, ca: (wr, d, ca),
            ""where"": lambda wr, d, ca: d.d_year == 2000 and ca.ca_state == ""CA"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda wr, d, ca: Auto3(cust=wr.wr_returning_customer_sk, state=ca.ca_state),
    )
    _items1 = _groups
    return [
        Auto2(
            ctr_customer_sk=g.key[""cust""],
            ctr_state=g.key[""state""],
            ctr_total_return=sum([x[0].wr_return_amt for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q30.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto2,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q52.py,,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q55.py,,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q72.py,_Group,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q57.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,DateDim,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q21.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,HouseholdDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Customer,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q96.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q87.py,CatalogSale,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q75.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,Customer,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q99.py,_Group,1,8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Customer,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q79.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,Item,1,6
survived,"def _q0():
    _src = all_rows
    _rows = _query(_src, [], {""select"": lambda r: r})
    _groups = _group_by(
        _rows,
        lambda r: Auto3(
            channel=r.get(""channel"") if isinstance(r, dict) else getattr(r, ""channel""),
            col_name=(
                r.get(""col_name"") if isinstance(r, dict) else getattr(r, ""col_name"")
            ),
            d_year=r.get(""d_year"") if isinstance(r, dict) else getattr(r, ""d_year""),
            d_qoy=r.get(""d_qoy"") if isinstance(r, dict) else getattr(r, ""d_qoy""),
            i_category=(
                r.get(""i_category"") if isinstance(r, dict) else getattr(r, ""i_category"")
            ),
        ),
    )
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: _sort_key(g.key[""channel""]))
    return [
        Auto1(
            channel=g.key[""channel""],
            col_name=g.key[""col_name""],
            d_year=g.key[""d_year""],
            d_qoy=g.key[""d_qoy""],
            i_category=g.key[""i_category""],
            sales_cnt=len(g),
            sales_amt=_sum(
                [
                    (
                        (x.get(""r"") if isinstance(x, dict) else getattr(x, ""r"")).get(
                            ""ext_sales_price""
                        )
                        if isinstance(
                            x.get(""r"") if isinstance(x, dict) else getattr(x, ""r""), dict
                        )
                        else getattr(
                            x.get(""r"") if isinstance(x, dict) else getattr(x, ""r""),
                            ""ext_sales_price"",
                        )
                    )
                    for x in g
                ]
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q76.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,CatalogSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CatalogSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,SalesYear1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,CatalogSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,Auto2,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q16.py,_Group,1,8
survived,"def test_TPCDS_Q63_simplified():
    assert result == 63
",tests/dataset/tpc-ds/compiler/py/q63.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CustomerAddress,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q26.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,Inventory,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Item,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q78.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Auto2,1,6
survived,"def test_TPCDS_Q82_sample():
    assert result == 82
",tests/dataset/tpc-ds/compiler/py/q82.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto4,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Item,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,Customer,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,CatalogSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,CatalogSale,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q28.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q26.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q61.py,Sale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q87.py,CatalogSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,Customer,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,Auto1,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q26.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q63.py,Auto2,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q12.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,CustomerAddres,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,StoreSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,Auto1,1,6
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q37.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,DateDim,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q1.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,StoreSale,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q97.py,_Group,1,8
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q14.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q64.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,StoreReturn,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q68.py,CatalogSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Inventory,1,6
survived,"def test_TPCDS_Q24_customer_net_paid():
    assert result == [
        Auto1(
            c_last_name=""Smith"", c_first_name=""Ann"", s_store_name=""Store1"", paid=100.0
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q24.py,,1,7
survived,"    async def async_step_user(self, user_input: dict | None = None) -> FlowResult:
        """"""Handle the initial step.""""""
        if user_input is not None:
            self._data.update(user_input)
            return self.async_create_entry(title=user_input.get(CONF_NAME) or ""Gree Climate"", data=self._data)

        data_schema = vol.Schema(
            {
                vol.Required(CONF_HOST): str,
                vol.Required(CONF_MAC): str,
                vol.Required(CONF_PORT, default=DEFAULT_PORT): int,
                vol.Optional(CONF_NAME): str,
                vol.Optional(CONF_TIMEOUT, default=DEFAULT_TIMEOUT): int,
                vol.Optional(CONF_ENCRYPTION_KEY): str,
                vol.Optional(CONF_UID): int,
                vol.Optional(CONF_ENCRYPTION_VERSION, default=1): int,
            }
        )
        return self.async_show_form(step_id=""user"", data_schema=data_schema)
",custom_components/gree/config_flow.py,ConfigFlow,1,7
survived,"    def create(self, *args, **kwargs):  # pragma: no cover - stub method
        raise NotImplementedError
",openai/__init__.py,_ChatCompletions,0,7
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    failing_test = bundle_dir / ""tests"" / ""test_main.py""
    failing_test.write_text(""def test_fail():\n    assert False"")
    # update checksum so validation reaches test execution
    import hashlib
    import json

    bundle_file = bundle_dir / ""bundle.json""
    data = json.loads(bundle_file.read_text())
    digest = hashlib.sha256(failing_test.read_bytes()).hexdigest()
    data[""custom""][""checksums""][""tests/test_main.py""] = digest
    bundle_file.write_text(json.dumps(data))
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _run_tests(self, errors: List[str]) -> None:
        env = os.environ.copy()
        for var in (
            ""COVERAGE_FILE"",
            ""COVERAGE_PROCESS_START"",
            ""COV_CORE_SOURCE"",
            ""COV_CORE_CONFIG"",
            ""COV_CORE_DATAFILE"",
        ):
            env.pop(var, None)

        env[""PYTHONPATH""] = (
            str(self.bundle_dir) + os.pathsep + env.get(""PYTHONPATH"", """")
        )

        result = subprocess.run(
            [""pytest"", ""-x"", ""-c"", ""/dev/null""],
            cwd=self.bundle_dir,
            capture_output=True,
            text=True,
            env=env,
        )
        if result.returncode != 0:
            errors.append(""tests failed"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,6
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,6
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""tests"" / ""test_main.py"").write_text(
        ""def test_fail():\n    assert False""
    )
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def unicode_escape_map(literal: str) -> Dict[str, str]:
    """"""Return mapping of characters to their unicode escape sequences.""""""
    quote = get_quote_type(literal)
    if quote is None:
        return {}
    idx = 0
    while idx < len(literal) and literal[idx] in ""furbFURB"":
        idx += 1
    body = literal[idx + len(quote) : -len(quote)]
    mapping: Dict[str, str] = {}
    for m in unicode_escape_re.finditer(body):
        esc = m.group(0)
        try:
            char = codecs.decode(esc, ""unicode_escape"")
        except Exception:  # noqa: S112
            continue
        mapping[char] = esc
    return mapping
",src/flynt/utils/utils.py,,1,7
survived,"def test_is_yaml_returns_false_for_other_extensions() -> None:
    """"""Test that non-YAML extensions return ``False``.""""""
    assert not yaml_utils.is_yaml(""config.txt"")
    assert not yaml_utils.is_yaml(""notyaml"")",tests/unit/utils/test_yaml_utils.py,,1,7
survived,"    def query_params(self) -> QueryParams:
        return dict(self.request.GET.items())
",src/graphql_server/webob/views.py,WebobHTTPRequestAdapter,1,7
survived,"    def get_context(self, request: Request, response: Response) -> Context:
        return {""request"": request, ""response"": response}  # type: ignore
",src/graphql_server/webob/views.py,GraphQLView,1,6
survived,"    async def _graphql_request(
        self,
        method: Literal[""get"", ""post""],
        query: Optional[str] = None,
        operation_name: Optional[str] = None,
        variables: Optional[dict[str, object]] = None,
        files: Optional[dict[str, BytesIO]] = None,
        headers: Optional[dict[str, str]] = None,
        extensions: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> ClientResponse:
        body = self._build_body(
            query=query,
            operation_name=operation_name,
            variables=variables,
            files=files,
            method=method,
            extensions=extensions,
        )

        data: Union[dict[str, object], str, None] = None

        url = ""/graphql""

        if body and files:
            body.update({name: (file, name) for name, file in files.items()})

        if method == ""get"":
            body_encoded = urllib.parse.urlencode(body or {})
            url = f""{url}?{body_encoded}""
        else:
            if body:
                data = body if files else json.dumps(body)
            kwargs[""body""] = data

        headers = self._get_headers(method=method, headers=headers, files=files)

        return await self.request(url, method, headers=headers, **kwargs)
",src/tests/http/clients/webob.py,WebobHttpClient,1,7
survived,"    def _do_request(
        self,
        url: str,
        method: Literal[""get"", ""post"", ""patch"", ""put"", ""delete""],
        headers: Optional[dict[str, str]] = None,
        **kwargs: Any,
    ) -> ClientResponse:
        body = kwargs.get(""body"", None)
        req = Request.blank(
            url, method=method.upper(), headers=headers or {}, body=body
        )
        resp = self.view.dispatch_request(req)
        return ClientResponse(
            status_code=resp.status_code, data=resp.body, headers=resp.headers
        )
",src/tests/http/clients/webob.py,WebobHttpClient,1,7
survived,"    def _log_retry(details: dict[str, Any]) -> None:
        _log.warning(
            ""Retry %d/%d for %s due to %s"",
            details[""tries""],
            max_tries,
            getattr(details.get(""target""), ""__name__"", ""call""),
            details.get(""exception""),
        )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/retry.py,,1,7
survived,"    async def run(self, stop_event: asyncio.Event) -> None:
        """"""Run agent cycles until ``stop_event`` is set.""""""
        await self.start()
        await self.manager.run(stop_event)
        await self.stop()
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler,1,7
survived,"    async def run(self, stop_event: asyncio.Event) -> None:
        """"""Drive the orchestrator until ``stop_event`` is set.""""""
        await self.start()
        await self.scheduler.run(stop_event)
        await self.stop()
",alpha_factory_v1/backend/orchestrator.py,Orchestrator,1,7
survived,"    async def stop(self) -> None:
        """"""Stop the underlying manager.""""""
        await self.manager.stop()
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler,1,6
survived,"    async def start(self) -> None:
        """"""Launch REST/gRPC servers and background tasks.""""""
        self._rest_task, self._grpc_server = await start_servers(
            self.scheduler.manager.runners,
            MODEL_MAX_BYTES,
            mem,
            PORT,
            A2A_PORT,
            LOGLEVEL,
            SSL_DISABLE,
        )
        await self.scheduler.start()
",alpha_factory_v1/backend/orchestrator.py,Orchestrator,1,7
survived,"def reload_devicons(lang):
    os.environ['DEVICONS_LANG'] = lang
    from ranger_devicons import devicons
    importlib.reload(devicons)
    return devicons
",tests/test_devicons.py,,1,6
survived,"def test_llm_mutator_offline(tmp_path: Path, monkeypatch) -> None:
    ledger = _ledger(tmp_path)
    target = tmp_path / ""demo.py""
    target.write_text(""def demo():\n    return 1\n"", encoding=""utf-8"")
    monkeypatch.setenv(""AGI_INSIGHT_OFFLINE"", ""1"")
    mut = llm_mutator.LLMMutator(ledger, rng=random.Random(1))
    diff = mut.generate_diff(str(tmp_path), ""demo.py:feat"")
    patcher_core.apply_patch(diff, repo_path=tmp_path)
    assert ""feat"" in target.read_text(encoding=""utf-8"")
",tests/test_mutator.py,,1,6
survived,"def test_llm_mutator_online(tmp_path: Path, monkeypatch) -> None:
    ledger = _ledger(tmp_path)
    target = tmp_path / ""demo.py""
    target.write_text(""def demo():\n    return 1\n"", encoding=""utf-8"")
    patch = propose_diff(str(target), ""improve"")
    monkeypatch.setenv(""OPENAI_API_KEY"", ""k"")
    with mock.patch.object(llm_mutator, ""_sync_chat"", return_value=patch):
        mut = llm_mutator.LLMMutator(ledger, rng=random.Random(2))
        diff = mut.generate_diff(str(tmp_path), ""demo.py:improve"")
    patcher_core.apply_patch(diff, repo_path=tmp_path)
    assert ""improve"" in target.read_text(encoding=""utf-8"")",tests/test_mutator.py,,1,7
survived,"def apply_patch(repo: str | Path, diff: str) -> Tuple[bool, Path]:
    """"""Apply ``diff`` to ``repo`` inside a sandbox and run tests.""""""
    src = Path(repo).resolve()
    tmp = Path(tempfile.mkdtemp(prefix=""self-evo-""))
    shutil.copytree(src, tmp, dirs_exist_ok=True)
    patcher_core.apply_patch(diff, repo_path=str(tmp))
    run_preflight(tmp)
    rc = _run_tests(tmp)
    return rc == 0, tmp
",src/self_evolution/harness.py,,1,7
survived,"def self_test(patch: str) -> None:
    """"""Apply PATCH and run sandboxed tests.""""""
    registry = StakeRegistry()
    registry.set_stake(""orch"", 1.0)
    diff = Path(patch).read_text(encoding=""utf-8"")
    accepted = harness.vote_and_merge(Path.cwd(), diff, registry)
    click.echo(""accepted"" if accepted else ""rejected"")
",src/interface/cli.py,,1,7
survived,"def title_case(text: str) -> str:
    words = re.split(r'(\s+|-)', text)
    # Count real words (exclude spaces/hyphens)
    real_words = [w for w in words if w and not re.match(r'(\s+|-)', w)]
    total = len(real_words)
    result = []
    index = 0
    for part in words:
        if re.match(r'(\s+|-)', part):
            result.append(part)
            continue
        word = part
        is_first = index == 0
        is_last = index == total - 1
        lower = word.lower()
        if word.isupper() or any(c.isupper() for c in word[1:]):
            result.append(word)
        elif not is_first and not is_last and lower in SMALL_WORDS:
            result.append(lower)
        else:
            result.append(word.capitalize())
        index += 1
    return ''.join(result)
",scripts/fix_titlecase.py,,1,7
survived,"def boom():
    print(""boom"")
    return True
",tests/human/python/bool_chain.py,,1,6
survived,"def test_consilience_values(tmp_path: Path) -> None:
    script = tmp_path / ""run.mjs""
    script.write_text(
        f""import {{ consilience }} from '{CRITICS.resolve().as_posix()}';\n""
        ""const r1 = consilience({a:0.5,b:0.5,c:0.5});\n""
        ""const r2 = consilience({a:0,b:1});\n""
        ""console.log(JSON.stringify({r1,r2}));\n""
    )
    result = subprocess.run([""node"", script], capture_output=True, text=True, check=True)
    data = json.loads(result.stdout)
    assert data[""r1""] > 0.99
    assert data[""r2""] < data[""r1""]",tests/test_consilience.py,,1,7
survived,"def request_with_proxy_fallback(
    session: requests.Session,
    method: str,
    url: str,
    *,
    timeout: Optional[int] = None,
    retries: int = 3,
    **kwargs,
) -> requests.Response:
    """"""Perform a request using rotating proxies.

    The request first uses the session's current proxy configuration. If the
    request fails due to connectivity issues, a new proxy is fetched using
    :func:`get_auto_proxy` and the request is retried. This continues up to
    ``retries`` times before raising an exception.
    """"""

    if retries < 1:
        retries = 1

    last_error: Exception | None = None
    for attempt in range(retries):
        try:
            resp = session.request(method, url, timeout=timeout, **kwargs)
            resp.raise_for_status()
            return resp
        except (
            requests.exceptions.ConnectionError,
            requests.exceptions.ProxyError,
            requests.exceptions.Timeout,
        ) as exc:
            last_error = exc
            try:
                proxy = get_auto_proxy()
                session.proxies.update({""http"": proxy, ""https"": proxy})
            except Exception as fetch_err:  # pragma: no cover - network dependent
                last_error = fetch_err
        except Exception:
            # Other errors should not trigger proxy retry
            raise

    raise RuntimeError(f""All proxy attempts failed: {last_error}"")
",webscout/Provider/TTI/utils.py,,1,7
survived,"def test_call_summary_deep_merge(client):
    @weave.op()
    def my_op():
        call = call_context.get_current_call()
        call.summary[""nested""] = {""foo"": 1}
        return ""done""

    my_op()
    calls = list(client.get_calls())
    assert len(calls) == 1
    summary = calls[0].summary
    assert summary[""nested""][""foo""] == 1
    assert summary[RESERVED_SUMMARY_STATUS_COUNTS_KEY][tsi.TraceStatus.SUCCESS] == 1",tests/trace/test_current_call.py,,1,7
survived,"        async def get_event():
            gen = demo.experience_stream()
            return await anext(gen)
",tests/test_era_experience.py,TestEraOfExperience,1,6
survived,"    def test_idempotent(self) -> None:
        logger = logging.getLogger(""alpha_factory.agents"")
        stream = io.StringIO()
        handler = logging.StreamHandler(stream)
        logger.addHandler(handler)

        demo.register_demo_agents()
        stream.truncate(0)
        stream.seek(0)
        demo.register_demo_agents()

        logger.removeHandler(handler)
        logs = stream.getvalue()
        self.assertNotIn(""Duplicate agent name"", logs)
",tests/test_demo_registration.py,TestRegisterDemoAgents,1,7
survived,"    def test_reject_call(self) -> None:
        with self.assertRaises(ValueError):
            safe_eval(""__import__('os').system('echo hi')"")
",tests/test_safe_eval_security.py,TestSafeEval,1,7
survived,"    def test_close_closes_connection(self):
        self.assertEqual(self.fabric.vector._mode, ""sqlite"")
        conn = self.fabric.vector._sql
        self.fabric.close()
        self.assertIsNone(self.fabric.vector._sql)
        with self.assertRaises(sqlite3.ProgrammingError):
            conn.execute(""SELECT 1"")
",tests/test_memory_fabric_sqlite.py,TestMemoryFabricSQLiteClose,1,6
survived,"def test_env_float_invalid(monkeypatch):
    monkeypatch.setenv(""AF_TEST"", ""bad"")
    assert _env_float(""AF_TEST"", 3.0) == 3.0
",tests/test_agent_runner_utils.py,,1,7
survived,"    def __init__(self) -> None:
        super().__init__()
        path = Path(__file__).with_name(""examples"") / ""alpha_opportunities.json""
        try:
            self._opportunities = json.loads(path.read_text(encoding=""utf-8""))
        except Exception:  # pragma: no cover - fallback when file missing
            self._opportunities = [
                {""alpha"": ""generic supply-chain inefficiency""}
            ]
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,AlphaOpportunityAgent,1,7
survived,"    def test_launcher_compiles(self) -> None:
        path = Path('alpha_factory_v1/demos/alpha_agi_business_v1/run_business_v1_local.py')
        py_compile.compile(path, doraise=True)
",tests/test_alpha_business_v1_script.py,TestAlphaBusinessV1Script,0,6
survived,"def test_metrics_update_during_sim() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_selector(""#simulator-panel"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-gen').value=2"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-pop').value=3"")
            page.click(""#simulator-panel #sim-start"")
            page.wait_for_function(""document.querySelector('#worker-time').textContent.includes('ms')"")
            page.wait_for_function(""document.querySelector('#heap').textContent !== ''"")
            page.wait_for_function(""document.querySelector('#fps-value').textContent.includes('fps')"")
            page.click('#simulator-panel #sim-cancel')
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",tests/test_browser_ui.py,,0,7
survived,"def test_workbox_sri() -> None:
    index_file = BROWSER / ""dist/index.html""
    html = index_file.read_text()
    match = re.search(r'<script[^>]*src=[""\']lib/workbox-sw.js[""\'][^>]*>', html)
    assert match, ""lib/workbox-sw.js script tag missing""
    tag = match.group(0)
    integrity = re.search(r'integrity=[""\']([^""\']+)[""\']', tag)
    assert integrity, ""integrity attribute missing""
    sri = integrity.group(1)
    expected = json.loads((BROWSER / ""build_assets.json"").read_text())[""checksums""][""lib/workbox-sw.js""]
    assert sri == expected and ""placeholder"" not in sri.lower(), ""integrity mismatch""",tests/test_integrity.py,,0,6
survived,"async def test_stream_options_not_injected_for_non_openai_base_url_async() -> None:
    captured = {}

    async def dummy_fn(completion, **kwargs):
        captured.update(kwargs)
        return ""ok""

    wrapped = create_wrapper_async(OpSettings())(dummy_fn)

    await wrapped(DummyCompletion(""https://api.mistral.ai""), stream=True)

    assert ""stream_options"" not in captured",tests/integrations/openai/test_openai_sdk.py,,1,7
survived,"    def __init__(self, base_url: str):
        self._base_url = base_url
",tests/integrations/openai/test_openai_sdk.py,DummyClient,1,7
survived,"async def test_pending_safety_check_acknowledged() -> None:
    """"""Safety checks should be acknowledged via the callback.""""""

    computer = LoggingComputer(screenshot_return=""img"")
    called: list[ComputerToolSafetyCheckData] = []

    def on_sc(data: ComputerToolSafetyCheckData) -> bool:
        called.append(data)
        return True

    tool = ComputerTool(computer=computer, on_safety_check=on_sc)
    safety = PendingSafetyCheck(id=""sc"", code=""c"", message=""m"")
    tool_call = ResponseComputerToolCall(
        id=""t1"",
        type=""computer_call"",
        action=ActionClick(type=""click"", x=1, y=1, button=""left""),
        call_id=""t1"",
        pending_safety_checks=[safety],
        status=""completed"",
    )
    run_action = ToolRunComputerAction(tool_call=tool_call, computer_tool=tool)
    agent = Agent(name=""a"", tools=[tool])
    ctx = RunContextWrapper(context=None)

    results = await RunImpl.execute_computer_actions(
        agent=agent,
        actions=[run_action],
        hooks=RunHooks[Any](),
        context_wrapper=ctx,
        config=RunConfig(),
    )

    assert len(results) == 1
    raw = results[0].raw_item
    assert isinstance(raw, dict)
    assert raw.get(""acknowledged_safety_checks"") == [{""id"": ""sc"", ""code"": ""c"", ""message"": ""m""}]
    assert len(called) == 1
    assert called[0].safety_check.id == ""sc""",tests/test_computer_action.py,,0,6
survived,"def main() -> int:
    if not BUNDLE.is_file() or not INDEX.is_file():
        print(""insight bundle or index.html missing"", file=sys.stderr)
        return 1
    html = INDEX.read_text()
    match = re.search(r""integrity=['\""]([^'\""]+)['\""]"", html)
    if not match:
        print(""integrity attribute missing"", file=sys.stderr)
        return 1
    expected = ""sha384-"" + _sha384(BUNDLE)
    if match.group(1) != expected:
        print(f""SRI mismatch: {match.group(1)} != {expected}"", file=sys.stderr)
        return 1
    print(""insight bundle integrity verified"")
    return 0
",scripts/check_insight_sri.py,,1,7
survived,"def test_duckdb_merkle_root(tmp_path) -> None:
    ledger = Ledger(tmp_path / ""log.duckdb"", db=""duckdb"", broadcast=False)
    env = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
    ledger.log(env)
    assert ledger.compute_merkle_root() == _expected_root([env])
",tests/test_ledger_backends.py,,0,6
survived,"def test_cycle_emits_blocked_to_memory() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    chaos = chaos_agent.ChaosAgent(bus, led, burst=1)
    guardian = safety_agent.SafetyGuardianAgent(bus, led)

    asyncio.run(chaos.run_cycle())
    topic, env = bus.published[0]
    assert topic == ""safety""

    asyncio.run(guardian.handle(env))
    assert bus.published[-1][0] == ""memory""
    assert bus.published[-1][1].payload[""status""] == ""blocked""",tests/test_chaos_agent.py,,1,7
survived,"    def vote(self, proposal_id: str, agent_id: str, support: bool) -> None:
        """"""Record ``agent_id``'s vote for ``proposal_id``.""""""
        if agent_id not in self.stakes:
            raise ValueError(f""unknown agent {agent_id}"")
        self.votes.setdefault(proposal_id, {})[agent_id] = bool(support)
",src/governance/stake_registry.py,StakeRegistry,1,7
survived,"    def history(self, start_hash: str) -> Iterator[ArchiveEntry]:
        current = self.get(start_hash)
        while current is not None:
            yield current
            if not current.parent:
                break
            current = self.get(current.parent)",src/archive/db.py,ArchiveDB,1,7
survived,"    def fake_run(*args, **kwargs):
        raise subprocess.TimeoutExpired(cmd=args[0], timeout=120)
",tests/test_secure_run.py,,1,6
survived,"def _load_entries(db_path: Path) -> List[ArchiveEntry]:
    """"""Return all archive entries.""""""
    with sqlite3.connect(db_path) as cx:
        rows = list(
            cx.execute(
                ""SELECT hash, parent, score, novelty, is_live, ts FROM archive""
            )
        )
    return [
        ArchiveEntry(
            hash=r[0],
            parent=r[1],
            score=float(r[2]),
            novelty=float(r[3]),
            is_live=bool(r[4]),
            ts=float(r[5]),
        )
        for r in rows
    ]
",src/tools/analyse_backtrack.py,,1,7
survived,"    def _validate_inputs(
        self,
        code_directory: Path,
        command: list[str],
        mem_limit: str,
        cpu_shares: int,
    ) -> None:
        """"""Validate inputs and resource limits for sandbox execution.""""""



        if not code_directory.is_dir():
            raise FileNotFoundError(f""Code directory not found: {code_directory}"")

        if not command or any(
            not isinstance(c, str) or any(x in c for x in ["";"", ""&"", ""|"", ""`"", ""\n""])
            for c in command
        ):
            raise ValueError(""Invalid command passed to sandbox"")

        if cpu_shares <= 0 or cpu_shares > MAX_CPU_SHARES:
            raise ValueError(""cpu_shares out of allowed range"")

        if mem_limit[-1].lower() not in {""m"", ""g""} or not mem_limit[:-1].isdigit():
            raise ValueError(""mem_limit must be like '256m' or '1g'"")
",src/meta_agent/sandbox/sandbox_manager.py,SandboxManager,1,7
survived,"    def _validate_inputs(
        self,
        code_directory: Path,
        command: list[str],
        mem_limit: str,
        cpu_shares: int,
    ) -> None:
        """"""Validate inputs and resource limits for sandbox execution.""""""

        if not command or any(
            not isinstance(c, str) or any(x in c for x in ["";"", ""&"", ""|"", ""`"", ""\n""])
            for c in command
        ):
            raise ValueError(""Invalid command passed to sandbox"")

        if cpu_shares <= 0 or cpu_shares > MAX_CPU_SHARES:
            raise ValueError(""cpu_shares out of allowed range"")

        if mem_limit[-1].lower() not in {""m"", ""g""} or not mem_limit[:-1].isdigit():
            raise ValueError(""mem_limit must be like '256m' or '1g'"")
",src/meta_agent/sandbox/sandbox_manager.py,SandboxManager,1,7
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,,1,7
survived,"def test_invalid_command(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python; rm -rf /""])
",tests/unit/test_sandbox_manager.py,,1,8
survived,"    def __init__(self, host: str, port: int, use_https: bool = False, level: LogLevel = LogLevel.DEBUG):
        super().__init__(level)
        self.host = host
        self.port = port
        self.use_https = use_https
",webscout/litlogger/handlers.py,NetworkHandler,1,7
survived,"    def emit(self, message: str, level: LogLevel):
        color = LEVEL_COLORS.get(level, """")
        self.stream.write(f""{color}{message}{RESET}\n"")
        self.stream.flush()
",webscout/litlogger/handlers.py,ConsoleHandler,1,7
survived,"def test_load_from_url():
    sample = [built_in_models[0].model_dump(mode=""json"")]

    class FakeResponse:
        def raise_for_status(self):
            pass

        def json(self):
            return {""model_list"": sample}

    with patch(
        ""kiln_ai.adapters.remote_config.requests.get"", return_value=FakeResponse()
    ):
        models = load_from_url(""http://example.com/models.json"")
    assert [m.model_dump(mode=""json"") for m in models] == sample
",libs/core/kiln_ai/adapters/test_remote_config.py,,1,7
survived,"        def raise_for_status(self):
            pass
",libs/core/kiln_ai/adapters/test_remote_config.py,FakeResponse,0,7
survived,"def test_update_appends_rules():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX, wanted_list=[""Banana""])
    count = len(scraper.stack_list)
    scraper.build(html=HTML_COMPLEX, wanted_list=[""Apple""], update=True)
    assert len(scraper.stack_list) == count + 1
",tests/integration/test_complex_features.py,,0,7
survived,"def test_similar_keep_order():
    scraper = AutoScraper()
    scraper.build(html=HTML, wanted_list=[""Banana""])
    result = scraper.get_result_similar(html=HTML, contain_sibling_leaves=True, keep_order=True)
    assert result == [""Banana"", ""Apple"", ""Orange""]",tests/unit/test_additional_features.py,,1,7
survived,"    def __init__(self):
        super().__init__()
        self.root = _Node(""[document]"", {})
        self.current = self.root
",tests/conftest.py,_Parser,1,7
survived,"    async def _loop(self, interval: int) -> None:
        while True:
            await asyncio.sleep(interval)
            await self.broadcast_merkle_root()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger,1,6
survived,"def test_schema_checker_invalid_network_alias():
    usage_scenario_name = 'schema_checker_invalid_network_alias.yml'
    usage_scenario_path = os.path.join(CURRENT_DIR, '../data/usage_scenarios/schema_checker/', usage_scenario_name)
    with open(usage_scenario_path, encoding='utf8') as file:
        usage_scenario = yaml.safe_load(file)
    schema_checker = SchemaChecker(validate_compose_flag=True)
    with pytest.raises(SchemaError) as error:
        schema_checker.check_usage_scenario(usage_scenario)
    expected_exception = ""bad!alias includes disallowed values: ['!']""
    assert expected_exception in str(error.value), \
        Tests.assertion_info(f""Exception: {expected_exception}"", str(error.value))
",tests/lib/test_schema_checker.py,,1,7
survived,"def test_parsing_manifest_when_raw_json_is_plain_string() -> None:
    # given
    raw_manifest = {
        ""name"": ""parser"",
        ""type"": ""roboflow_core/json_parser@v1"",
        ""raw_json"": ""{\""a\"": 1}"",
        ""expected_fields"": [""a""],
    }

    # when
    result = BlockManifest.model_validate(raw_manifest)

    # then
    assert result == BlockManifest(
        name=""parser"",
        type=""roboflow_core/json_parser@v1"",
        raw_json=""{\""a\"": 1}"",
        expected_fields=[""a""],
    )
",tests/workflows/unit_tests/core_steps/formatters/test_json_parser.py,,1,7
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q9.py,,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q4.py,,1,7
survived,"        def __call__(self, query: str, *args: Any, **kwargs: Any) -> str:
            return ""Hosted tool unavailable in this environment.""
",src/meta_agent/sub_agent_manager.py,FileSearchTool,1,6
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return int(s.getsockname()[1])
",tests/test_adk_gateway.py,,1,7
survived,"def _kafka_producer() -> Optional[KafkaProducer]:
    if not _KAFKA_BROKER or KafkaProducer is None:
        return None
    try:
        return KafkaProducer(
            bootstrap_servers=_KAFKA_BROKER,
            value_serializer=lambda v: v.encode() if isinstance(v, str) else v,
        )
    except Exception:  # noqa: BLE001
        logger.exception(""Kafka producer init failed"")
        return None
",alpha_factory_v1/backend/agents/registry.py,,1,7
survived,"def list_agents(detail: bool = False):
    with _REGISTRY_LOCK:
        metas = sorted(AGENT_REGISTRY.values(), key=lambda m: m.name)
    return [m.as_dict() if detail else m.name for m in metas]
",alpha_factory_v1/backend/agents/registry.py,,1,7
survived,"    def add(self, capability: str, agent_name: str) -> None:
        self.setdefault(capability, []).append(agent_name)
",alpha_factory_v1/backend/agents/registry.py,CapabilityGraph,1,7
survived,"    def decorator(inner_cls):
        from_backend_base = _agent_base()
        if not issubclass(inner_cls, from_backend_base):
            raise TypeError(""register() only allowed on AgentBase subclasses"")

        cond_result = condition() if callable(condition) else bool(condition)
        if cond_result:
            meta = AgentMetadata(
                name=getattr(inner_cls, ""NAME"", inner_cls.__name__),
                cls=inner_cls,
                version=getattr(inner_cls, ""__version__"", ""0.1.0""),
                capabilities=list(getattr(inner_cls, ""CAPABILITIES"", [])),
                compliance_tags=list(getattr(inner_cls, ""COMPLIANCE_TAGS"", [])),
                requires_api_key=getattr(inner_cls, ""REQUIRES_API_KEY"", False),
            )
            _register(meta, overwrite=False)
        else:
            logger.info(
                ""Agent %s not registered (condition=false)"",
                getattr(inner_cls, ""NAME"", inner_cls.__name__),
            )
        return inner_cls
",alpha_factory_v1/backend/agents/registry.py,,1,6
survived,"    def __init__(self, bus: A2ABus, ledger: Ledger) -> None:
        super().__init__(""freeze"", bus, ledger)
",tests/test_insight_orchestrator_restart.py,FreezeAgent,0,6
survived,"        def __init__(self, data: dict) -> None:
            self._data = data
",tests/test_cli_runner_ext.py,Dummy,1,7
survived,"        def json(self) -> dict:
            return payload
",tests/test_api_status.py,Dummy,1,6
survived,"def test_cli_agents_status_parses_mapping() -> None:
    from src.interface import cli

    payload = {""agents"": {""agent1"": {""last_beat"": 1.0, ""restarts"": 0}}}

    class Dummy:
        status_code = 200

        def json(self) -> dict:
            return payload

    with patch.object(cli.requests, ""get"", return_value=Dummy()):
        result = CliRunner().invoke(cli.main, [""agents-status""])
    assert ""agent1"" in result.output",tests/test_api_status.py,,1,7
survived,"    def test_matrix_grad_torch(self):
        self._check_matrix_grad(""torch"")
",tests/test_autograd.py,TestAutograd,1,6
survived,"    def test_bridge_enable_adk(self) -> None:
        """"""Bridge accepts the --enable-adk flag.""""""
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge"",
                ""--episodes"",
                ""1"",
                ""--enable-adk"",
            ],
            check=True,
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo,1,6
survived,"def apply_env(args: argparse.Namespace) -> None:
    if args.dev:
        os.environ[""DEV_MODE""] = ""true""
    if args.port is not None:
        os.environ[""PORT""] = str(args.port)
    if args.metrics_port is not None:
        os.environ[""METRICS_PORT""] = str(args.metrics_port)
    if args.a2a_port is not None:
        os.environ[""A2A_PORT""] = str(args.a2a_port)
    if args.enabled is not None:
        os.environ[""ALPHA_ENABLED_AGENTS""] = args.enabled
    if args.loglevel:
        os.environ[""LOGLEVEL""] = args.loglevel.upper()
",alpha_factory_v1/run.py,,1,7
survived,"    async def func() -> str:
        calls[""n""] += 1
        if calls[""n""] <= failures:
            raise ValueError(""boom"")
        return ""ok""
",tests/test_retry_property.py,,1,6
survived,"async def summarize_thread(thread_ts: str, conversation: list[ModelMessage]) -> str:
    """"""Summarize a Slack conversation and store the result.""""""

    summary = await summarize_async(""\n\n"".join(m.content for m in conversation))

    add_asset_metadata(
        THREAD_SUMMARY,
        {
            ""thread_ts"": thread_ts,
            ""message_count"": len(conversation),
            ""timestamp"": datetime.now().isoformat(),
        },
    )

    return summary",examples/slackbot/src/slackbot/assets.py,,1,7
survived,"        def process(value: list[float] | None) -> FloatVector | None:
            return np.asarray(value, dtype=np.float32) if value is not None else None
",src/raglite/_typing.py,DuckDBVec,1,7
deleted,"    def l1_distance(self, other: FloatVector) -> Operators:
        """"""Compute the L1 distance.""""""
        if self._is_postgres():
            return self.op(""<+>"", return_type=Float)(other)
        return func.abs(func.sum(self.expr - other))
",src/raglite/_typing.py,EmbeddingComparator,1,7
survived,"def test_deploy_script_preserves_api_key(tmp_path: Path) -> None:
    out = _run_deploy_script(tmp_path, {""OPENAI_API_KEY"": ""x""})
    assert ""NO_LLM"" not in out",tests/test_world_model_safety.py,,1,6
survived,"def _run_deploy_script(tmp_path: Path, env_vars: dict[str, str]) -> str:
    script = Path(
        ""alpha_factory_v1/demos/alpha_asi_world_model/deploy_alpha_asi_world_model_demo.sh""
    )
    assert script.exists(), script
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()
    capture = tmp_path / ""env.txt""
    _write_executable(
        bin_dir / ""python"",
        ""#!/usr/bin/env bash\nprintenv > \""$CAPTURE\""\n"",
    )
    env = os.environ.copy()
    env.update(env_vars)
    env.update({""PATH"": f""{bin_dir}:{os.environ.get('PATH', '')}"", ""CAPTURE"": str(capture)})
    subprocess.run([""bash"", str(script)], env=env, check=True, timeout=5)
    return capture.read_text()
",tests/test_world_model_safety.py,,0,6
survived,"async def test_run_cycle_negative_delta_g_posts_job() -> None:
    class LowFin(demo.AgentFin):
        def latent_work(self, bundle):
            return 0.0

    class CaptureOrch(demo.Orchestrator):
        def __init__(self) -> None:
            self.called = False

        def post_alpha_job(self, bundle_id: int, delta_g: float) -> None:
            self.called = True

    orch = CaptureOrch()
    await demo.run_cycle_async(
        orch,
        LowFin(),
        demo.AgentRes(),
        demo.AgentEne(),
        demo.AgentGdl(),
        DummyModel(),
    )
    assert orch.called
",tests/test_alpha_agi_business_3_v1.py,,1,6
survived,"        def post_alpha_job(self, bundle_id: int, delta_g: float) -> None:
            self.called = True
",tests/test_alpha_agi_business_3_v1.py,CaptureOrch,1,6
survived,"    def __init__(self, agent: object) -> None:
        self.cls: Callable[..., object] = type(agent)
        self.agent = agent
        self.period = getattr(agent, ""CYCLE_SECONDS"", 1.0)
        self.capabilities = getattr(agent, ""CAPABILITIES"", [])
        self.last_beat = time.time()
        self.restarts = 0
        self.task: asyncio.Task[None] | None = None
        self.error_count = 0
        self.restart_streak = 0
",alpha_factory_v1/backend/agent_supervisor.py,AgentRunner,1,7
survived,"def _remote_available(url: str) -> bool:
    try:
        req = Request(url, method=""HEAD"")
        with urlopen(req, timeout=3) as resp:
            status = getattr(resp, ""status"", None)
        return bool(status and 200 <= int(status) < 300)
    except Exception:
        return False
",scripts/open_demo.py,,1,6
survived,"def run(cmd: Sequence[str], **kwargs: Any) -> None:
    """"""Run ``cmd`` and raise ``CalledProcessError`` on failure.""""""
    print(""+"", "" "".join(cmd))
    subprocess.run(cmd, check=True, **kwargs)
",scripts/edge_human_knowledge_pages_sprint.py,,1,6
survived,"def _build_local_site(repo_root: Path) -> bool:
    script = repo_root / ""scripts"" / ""build_gallery_site.sh""
    if not script.is_file():
        return False
    try:
        subprocess.run([str(script)], check=True)
    except Exception:
        return False
    return True
",scripts/open_subdir_gallery.py,,1,7
survived,"def load_translations(lang=None):
    """"""Load directory name translations for the given language.""""""
    if lang is None:
        lang = os.getenv('DEVICONS_LANG')
        if not lang:
            loc = locale.getdefaultlocale()[0]
            if loc:
                lang = loc.split('_')[0]
    if not lang:
        return {}
    try:
        module = importlib.import_module(f'ranger_devicons.locales.{lang}')
        return getattr(module, 'translations', {})
    except ModuleNotFoundError:
        return {}
",devicons.py,,1,7
survived,"def test_bandit_early_stop_reduces_cost() -> None:
    gains = [1.0, 0.2, 0.0, 0.0]

    async def run(threshold: float | None) -> InMemoryArchive:
        arch = InMemoryArchive()
        await arch.accept(Candidate(0.0, fitness=0.0, novelty=1.0))
        await evolve(_op, eval_genome, arch, max_cost=5.0, cost_threshold=threshold)
        return arch

    log: list[float] = []

    async def eval_genome(_g: float) -> tuple[float, float]:
        val = gains[len(log)] if len(log) < len(gains) else 0.0
        log.append(val)
        return val, 1.0

    naive_arch = asyncio.run(run(None))
    naive_cost = sum(c.cost for c in naive_arch.all()[1:])
    naive_gain = max(c.fitness for c in naive_arch.all())

    log.clear()
    early_arch = asyncio.run(run(1.5))
    early_cost = sum(c.cost for c in early_arch.all()[1:])
    early_gain = max(c.fitness for c in early_arch.all())

    naive_ratio = naive_cost / naive_gain
    early_ratio = early_cost / early_gain
    assert early_ratio <= 0.75 * naive_ratio",tests/test_evolve.py,,0,6
survived,"def test_check_patch_in_sandbox_ok(monkeypatch):
    def fake_run(cmd, capture_output=True, text=True):
        return subprocess.CompletedProcess(cmd, 0, """", """")

    monkeypatch.setattr(subprocess, ""run"", fake_run)
    assert preflight.check_patch_in_sandbox(""img"")
",tests/test_preflight_sandbox.py,,1,7
survived,"    async def maybe_step(self) -> None:
        if time.time() < self.next_ts:
            return
        self._calc_next()

        async def _cycle() -> None:
            t0 = time.time()
            span_cm = tracer.start_as_current_span(self.name) if tracer else contextlib.nullcontext()
            with span_cm:
                try:
                    await asyncio.wait_for(maybe_await(self.inst.run_cycle), timeout=self._max_cycle_sec)
                except asyncio.TimeoutError:
                    MET_ERR.labels(self.name).inc()
                    log.error(""%s run_cycle exceeded %ss budget â€“ skipped"", self.name, self._max_cycle_sec)
                except Exception as exc:  # noqa: BLE001
                    MET_ERR.labels(self.name).inc()
                    log.exception(""%s.run_cycle crashed: %s"", self.name, exc)
                finally:
                    dur_ms = (time.time() - t0) * 1_000
                    MET_LAT.labels(self.name).observe(dur_ms)
                    self.last_beat = time.time()
                    self._publish(""agent.cycle"", {""agent"": self.name, ""latency_ms"": dur_ms, ""ts"": utc_now()})

        self.task = asyncio.create_task(_cycle())
",alpha_factory_v1/backend/agent_runner.py,AgentRunner,1,7
survived,"                    def __init__(self) -> None:
                        self.nodes: set[str] = set()
                        self._edges: list[tuple[str, str, str, dict[str, Any]]] = []
",alpha_factory_v1/backend/memory_graph.py,GraphMemory._Stub,1,7
survived,"async def get_order(order_id: int):
    order = next((o for o in ORDERS if o[""id""] == order_id), None)
    if not order:
        raise HTTPException(status_code=404, detail=""Order not found"")
    return order",examples/shop_api_gateway/server.py,,1,7
survived,"async def list_products(ctx: EnrichContext) -> list[Product]:
    client = await _client(ctx)
    resp = await client.get(""/products"")
    resp.raise_for_status()
    return [Product(**p) for p in resp.json()]
",examples/shop_api_gateway/app.py,,1,7
survived,"async def list_products():
    return PRODUCTS
",examples/shop_api_gateway/server.py,,1,6
survived,"async def list_users():
    return USERS
",examples/shop_api_gateway/server.py,,1,7
survived,"def get_image_analysis_azure(api_endpoint, api_key, api_version, deployment_name, prompt, base64_image):
    client = AzureOpenAI(
        azure_endpoint=api_endpoint,
        api_key=api_key,
        api_version=api_version,
    )

    response = client.chat.completions.create(
        model=deployment_name,
        messages=[
            {
                ""role"": ""user"",
                ""content"": [
                    {""type"": ""text"", ""text"": prompt},
                    {""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{base64_image}""}},
                ],
            }
        ],
        max_tokens=4000,
    )

    return {
        ""choices"": [
            {""message"": {""content"": response.choices[0].message.content}}
        ]
    }
",threat_model.py,,1,6
survived,"def get_image_analysis_anthropic(api_key, model_name, prompt, base64_image):
    client = Anthropic(api_key=api_key)
    response = client.messages.create(
        model=model_name,
        max_tokens=4000,
        messages=[
            {
                ""role"": ""user"",
                ""content"": [
                    {
                        ""type"": ""image"",
                        ""source"": {
                            ""type"": ""base64"",
                            ""media_type"": ""image/jpeg"",
                            ""data"": base64_image,
                        },
                    },
                    {""type"": ""text"", ""text"": prompt},
                ],
            }
        ],
    )

    text = """".join(block.text for block in response.content if getattr(block, ""text"", None))
    return {""choices"": [{""message"": {""content"": text}}]}
",threat_model.py,,1,7
survived,"def main():
    p = {}
    p[""a""] = 1
    p[""c""] = 9
    print(str(doIt(p)))
",tests/rosetta/transpiler/Python/call-a-function-5.py,,0,7
survived,"def evaluate(repo_path: Path) -> dict[str, float]:
    """"""Return average RMSE and lead-time for the Sector-Shock-10 dataset.""""""

    ds_dir = repo_path / ""data"" / ""sector_shock_10""
    rmses: list[float] = []
    leads: list[float] = []
    for path in sorted(ds_dir.glob(""*.json"")):
        data = json.loads(path.read_text())
        truth_caps = data.get(""capabilities"", [])
        truth_shocks = data.get(""shocks"", [])
        pred_caps = truth_caps[:]  # deterministic baseline
        pred_shocks = truth_shocks[:]
        rmses.append(_rmse(truth_caps, pred_caps))
        leads.append(_lead_time(truth_shocks, pred_shocks))
    if not rmses:
        raise FileNotFoundError(ds_dir)
    return {""rmse"": statistics.mean(rmses), ""lead_time"": statistics.mean(leads)}
",src/eval/foresight.py,,1,7
survived,"def stub_adk(monkeypatch):
    """"""Provide a minimal google_adk stub.""""""
    dummy = types.ModuleType(""google_adk"")

    class _Router:
        def __init__(self):
            self.app = object()

        def register_agent(self, _agent):
            pass

    dummy.Router = _Router
    dummy.Agent = object
    dummy.AgentException = Exception
    monkeypatch.setitem(sys.modules, ""google_adk"", dummy)
    monkeypatch.setenv(""ALPHA_FACTORY_ENABLE_ADK"", ""1"")
    yield
    monkeypatch.delenv(""ALPHA_FACTORY_ENABLE_ADK"", raising=False)
    sys.modules.pop(""google_adk"", None)
",tests/test_adk_gateway_startup.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/exceptions-catch-an-exception-thrown-in-a-nested-call.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""The listed extensions are:"")
    print(extensions)
    tests = [""MyData.a##"", ""MyData.tar.Gz"", ""MyData.gzip"", ""MyData.7z.backup"", ""MyData..."", ""MyData"", ""MyData_v1.0.tar.bz2"", ""MyData_v1.0.bz2""]
    for t in tests:
        res = fileExtInList(t)
        ok = bool(res[0])
        ext = str(res[1])
        print(pad(t, 20) + "" => "" + str(ok) + ""  (extension = "" + ext + "")"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,,1,6
survived,"def if2(cond1, cond2, f):
    if cond1 and cond2:
        f()
    return If2(cond1=cond1, cond2=cond2)
",tests/rosetta/transpiler/Python/extend-your-language.py,,1,6
survived,"def hailstone(n):
    seq = []
    x = n
    seq = seq + [x]
    while x > 1:
        if x % 2 == 0:
            x = x // 2
        else:
            x = 3 * x + 1
        seq = seq + [x]
    return seq
",tests/rosetta/transpiler/Python/executable-library.py,,1,7
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    n = 100000
    gamma = harmonic(n) - ln(float(n))
    print(str(gamma))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/eulers-constant-0.5772....py,,1,6
survived,"def pad(s, w):
    t = s
    while len(t) < w:
        t = t + "" ""
    sys.exit(t)
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,,0,8
survived,"def indexOfSub(s, sub):
    if len(sub) == 0:
        sys.exit(0)
    i = 0
    while i + len(sub) <= len(s):
        if s[i:i + len(sub)] == sub:
            sys.exit(i)
        i = i + 1
    sys.exit(0 - 1)
",tests/rosetta/transpiler/Python/execute-a-markov-algorithm.py,,0,8
survived,"def fib(a):
    if a < 2:
        return a
    return fib(a - 1) + fib(a - 2)
",tests/rosetta/transpiler/Python/fibonacci-sequence-1.py,,1,7
survived,"def show(xs):
    s = """"
    i = 0
    while i < len(xs):
        s = s + str(xs[i])
        if i < len(xs) - 1:
            s = s + "" ""
        i = i + 1
    sys.exit(s)
",tests/rosetta/transpiler/Python/fibonacci-n-step-number-sequences.py,,0,7
survived,"def fibonacciWord(n):
    a = ""1""
    b = ""0""
    i = 1
    while i < n:
        tmp = b
        b = b + a
        a = tmp
        i = i + 1
    sys.exit(a)
",tests/rosetta/transpiler/Python/fibonacci-word.py,,0,8
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/execute-snusp.py,,1,6
survived,"def faceToPerim(face):
    le = len(face)
    if le == 0:
        return None
    edges = []
    i = 0
    while i < le:
        e = face[i]
        if e.b <= e.a:
            return None
        edges = edges + [e]
        i = i + 1
    edges = sortEdges(edges)
    firstEdge = edges[0]
    perim = [firstEdge.a, firstEdge.b]
    first = firstEdge.a
    last = firstEdge.b
    edges = edges[1:len(edges)]
    le = len(edges)
    done = False
    while le > 0 and (not done):
        idx = 0
        found = False
        while idx < le:
            e = edges[idx]
            if e.a == last:
                perim = perim + [e.b]
                last = e.b
                found = True
            else:
                if e.b == last:
                    perim = perim + [e.a]
                    last = e.a
                    found = True
            if found:
                edges = concat(edges[:idx], edges[idx + 1:len(edges)])
                le = le - 1
                if last == first:
                    if le == 0:
                        done = True
                    else:
                        return None
                break
            idx = idx + 1
        if not found:
            return None
    return perim[:len(perim) - 1]
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,,1,6
survived,"def test_sqlalchemy_lifespan_cleanup(tmp_path):
    db = tmp_path / ""db.sqlite""
    engine = create_async_engine(f""sqlite+aiosqlite:///{db}"")

    lifespan = sqlalchemy_lifespan(Base, engine, cleanup_db_file=True)
    app = EnrichMCP(""Test"", ""Desc"")

    async def run():
        async with lifespan(app) as ctx:
            session_factory = ctx[""session_factory""]
            async with session_factory() as session:
                await session.execute(text(""SELECT 1""))

    import asyncio

    asyncio.run(run())

    assert not db.exists()",tests/test_sqlalchemy_autogen_extra.py,,1,7
survived,"    def start_merkle_task(self, *a, **kw) -> None:  # pragma: no cover - stub
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,DummyLedger,0,7
survived,"    def verify_ledger(self, expected: str, agent_id: str) -> None:
        """"""Slash ``agent_id`` when the current ledger root mismatches ``expected``.""""""
        actual = self.ledger.compute_merkle_root()
        if actual != expected:
            log.warning(""Merkle mismatch for %s"", agent_id)
            self.slash(agent_id)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator,1,7
survived,"    def fake_exec(code: str) -> tuple[str, str]:
        nonlocal called
        called = True
        return """", """"
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,,1,6
survived,"def test_get_context_returns_enrich_context():
    """"""app.get_context should return an EnrichContext""""""

    app = EnrichMCP(""Test API"", description=""Test API description"")
    ctx = app.get_context()

    assert isinstance(ctx, EnrichContext)
    assert ctx.fastmcp is app.mcp

    with pytest.raises(ValueError):
        _ = ctx.request_context
",tests/test_core.py,,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an error or informational event.""""""
        self.events.append(
            TelemetryCollector.Event(category=category, severity=severity, message=message)
        )
        log_method = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log_method = self.logger.error
        elif severity is self.Severity.WARNING:
            log_method = self.logger.warning
        log_method(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an error or informational event.""""""
        self.events.append(
            TelemetryCollector.Event(category=category, severity=severity, message=message)
        )
        log_method = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log_method = self.logger.error
        elif severity is self.Severity.WARNING:
            log_method = self.logger.warning
        log_method(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an informational or error event.""""""
        self.events.append(
            TelemetryCollector.Event(
                category=category,
                severity=severity,
                message=message,
            )
        )
        log = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log = self.logger.error
        elif severity is self.Severity.WARNING:
            log = self.logger.warning
        log(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"def _groups_equal(a: RolloutGroup, b: RolloutGroup) -> bool:
    """"""Deep equality helper via dataclasses.asdict with float tolerance.""""""

    da, db = asdict(a), asdict(b)
    # Allow tiny float differences in ""created""
    if abs(da[""created""] - db[""created""]) > 1e-9:
        return False
    da[""created""] = db[""created""] = 0  # normalise
    return da == db
",tests/rl/test_parquet_store.py,,0,6
survived,"    async def stop(self) -> None:
        """"""Signal the event loop to terminate gracefully.""""""

        self._stop_event.set()
        logger.info(""Stop signal received"")
",marin/rl/env.py,AbstractMarinEnv,1,7
survived,"    def __init__(self, inference: InferenceEndpoint, rollout_sink: RolloutSink):
        self._inference = inference
        self._rollout_sink = rollout_sink
        self._stop_event: asyncio.Event = asyncio.Event()
        logger.info(""Environment initialized with inference %s"", inference.address)
",marin/rl/env.py,AbstractMarinEnv,1,7
survived,"    def build(self, inference: InferenceEndpoint, rollout_sink: RolloutSink, seed: int) -> ray.actor.ActorHandle:
        ActorCls = ray.remote(num_cpus=1)(HelloWorldEnv)
        actor = ActorCls.remote(inference, rollout_sink)
        actor.run.remote()  # kick off event loop
        return actor",marin/rl/envs/hello.py,HelloEnvConfig,1,7
survived,"    def resources(self) -> RayResources:
        return RayResources(cpu=1)
",marin/rl/envs/openai_echo.py,ChatEchoEnvConfig,1,7
survived,"    def set_env_var(self, key: str, value: str) -> None:
        """"""Helper to track environment variable overrides.""""""
        os.environ[key] = value
        self.env_vars[key] = value
",tests/test_alpha_opportunity_env.py,TestAlphaOpportunityEnv,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitwise-operations.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-flood-fill.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-2.py,,1,6
survived,"    def __str__(self) -> str:  # pragma: no cover - simple formatting
        """"""Return a human-readable Markdown summary.""""""
        lines = [f""# {self.title}""]
        if self.description:
            lines.append(self.description)
        lines.append("""")
        lines.append(f""**Entity count:** {self.entity_count}"")
        if self.entities:
            lines.append("""")
            lines.append(""## Entities"")
            for name in sorted(self.entities):
                lines.append(f""- {name}"")
        lines.append("""")
        lines.append(self.model)
        lines.append("""")
        lines.append(self.usage_hint)
        return ""\n"".join(lines)",src/enrichmcp/datamodel.py,DataModelSummary,0,6
survived,"def test_research_agent_adapters_invoked(monkeypatch) -> None:
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils import config, messaging
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.agents import research_agent

    class DummyLedger:
        def __init__(self, *_a, **_kw) -> None:
            pass

        def log(self, _env) -> None:  # type: ignore[override]
            pass

        def start_merkle_task(self, *_a, **_kw) -> None:
            pass

        async def stop_merkle_task(self) -> None:
            pass

        def close(self) -> None:
            pass

    settings = config.Settings(bus_port=0)
    bus = messaging.A2ABus(settings)
    agent = research_agent.ResearchAgent(bus, DummyLedger())

    adk_mock = type(""A"", (), {""heartbeat"": lambda self: None})()
    mcp_mock = type(""M"", (), {""heartbeat"": lambda self: None})()
    monkeypatch.setattr(agent, ""adk"", adk_mock, raising=False)
    monkeypatch.setattr(agent, ""mcp"", mcp_mock, raising=False)
    with patch.object(adk_mock, ""heartbeat"") as adk_hb, patch.object(mcp_mock, ""heartbeat"") as mcp_hb:
        asyncio.run(agent.run_cycle())
        adk_hb.assert_called_once()
        mcp_hb.assert_called_once()",tests/test_agents.py,,0,7
survived,"    def list_profiles(path: str | None = None):
        """"""Return available profile names.""""""
        _path = os.path.expanduser(path or ""~/.dhapi/credentials"")
        if not os.path.exists(_path):
            raise FileNotFoundError(f""{_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."")

        with open(_path, ""r"", encoding=""UTF-8"") as f:
            config = tomli.loads(f.read())

        return list(config.keys())",src/dhapi/port/credentials_provider.py,CredentialsProvider,1,6
survived,"def show_profiles():
    try:
        profiles = CredentialsProvider.list_profiles()
    except FileNotFoundError as e:
        print(f""âŒ {e.args[0]}"")
        raise typer.Exit(code=1)

    console = Console()
    table = Table(""profiles"")
    for name in profiles:
        table.add_row(name)

    console.print(table)
",src/dhapi/router/router.py,,1,6
survived,"def triple(x):
    return x * 3
",tests/transpiler/x/py/pure_fold.py,,1,7
survived,"def add(a, b):
    return a + b
",tests/transpiler/x/py/partial_application.py,,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        steps = [
            f""Practice {payload['skill']} at level {lvl}""
            for lvl in range(payload[""current_level""], payload[""target_level""] + 1)
        ]
        measures = [""Take breaks"", ""Monitor progress""]
        return {
            ""scaffold_steps"": steps,
            ""safety_measures"": measures,
            ""review_intervals"": ""weekly"",
        }",servers/server_clear_thought/tools/safe_struggle_designer.py,SafeStruggleDesigner,1,7
survived,"def load_tools() -> List[Type[BaseTool]]:
    tools: List[Type[BaseTool]] = []
    pkg_dir = Path(__file__).parent / ""tools""
    for module_info in pkgutil.iter_modules([str(pkg_dir)]):
        module = importlib.import_module(f""{__package__}.tools.{module_info.name}"")
        for attr in module.__dict__.values():
            if isinstance(attr, type) and issubclass(attr, BaseTool) and attr is not BaseTool:
                tools.append(attr)
    return tools
",servers/server_clear_thought/app.py,,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        voi_score = sum(payload[""payoffs""]) / (len(payload[""uncertainties""]) or 1)
        questions = [f""Resolve {u}?"" for u in payload[""uncertainties""]]
        return {
            ""voi_score"": round(voi_score, 2),
            ""high_impact_questions"": questions,
        }",servers/server_clear_thought/tools/value_of_information.py,ValueOfInformation,1,7
survived,"        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)
",servers/server_clear_thought/core/base_tool.py,BaseTool,1,6
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        k = payload.get(""k"") or 3
        analogies = [
            {""domain"": d, ""analogy"": f""{payload['problem']} ~ {d}""}
            for d in (payload.get(""seed_domains"") or [""math"", ""biology"", ""art""])[:k]
        ]
        prompts = [f""How would {a['domain']} approach it?"" for a in analogies]
        return {
            ""analogies"": analogies,
            ""suggested_prompts"": prompts,
        }",servers/server_clear_thought/tools/analogical_mapper.py,AnalogicalMapper,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        result = []
        skills = payload[""skills""]
        for task, _ in payload[""tasks""].items():
            best = max(skills, key=lambda k: skills[k])
            result.append({""task"": task, ""assignee"": best})
        return {""advantage_map"": result}",servers/server_clear_thought/tools/comparative_advantage.py,ComparativeAdvantage,1,6
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        points = [
            {""category"": c or ""general"", ""count"": i}
            for i, c in enumerate(payload.get(""categories"") or [""general""], start=1)
        ]
        score = sum(p[""count""] for p in points) / len(points)
        return {
            ""drag_points"": points,
            ""summary_score"": round(score, 2),
        }",servers/server_clear_thought/tools/drag_point_audit.py,DragPointAudit,1,7
survived,"def test_existing_tool_example():
    app = create_app()
    client = TestClient(app)
    resp = client.post(""/existing-tool-example/execute"", json={""text"": ""hi""})
    assert resp.status_code == 200
    assert resp.json() == {""echoed"": ""hi""}",servers/server_clear_thought/tests/test_existing_tools.py,,1,7
survived,"def test_drag_point_audit():
    client = get_client()
    resp = client.post(
        ""/drag-point-audit/execute"",
        json={""log"": ""...""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""drag_points"", ""summary_score""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,6
survived,"def random_confidences(n: int) -> List[float]:
    return [round(random.uniform(0.5, 1.0), 2) for _ in range(n)]",servers/server_clear_thought/core/utils.py,,1,6
survived,"def test_assumption_xray():
    client = get_client()
    resp = client.post(
        ""/assumption-xray/execute"",
        json={""claim"": ""A"", ""context"": ""B""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""assumptions"", ""confidence"", ""tests""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"    async def meme_usage(_: None = Depends(verify_token)) -> dict[str, int]:
        """"""Return meme usage counts.""""""
        return _meme_usage
",src/interface/api_server.py,,1,7
survived,"def test_generate_docs(tmp_path, monkeypatch):
    repo = tmp_path
    demos = repo / ""alpha_factory_v1"" / ""demos"" / ""demo_a""
    demos.mkdir(parents=True)
    (demos / ""README.md"").write_text(""# Demo A\nHello"", encoding=""utf-8"")
    assets = repo / ""docs"" / ""demo_a"" / ""assets""
    assets.mkdir(parents=True)
    (assets / ""preview.png"").write_text(""data"", encoding=""utf-8"")
    docs_demos = repo / ""docs"" / ""demos""

    monkeypatch.setattr(gdd, ""REPO_ROOT"", repo)
    monkeypatch.setattr(gdd, ""DEMOS_DIR"", repo / ""alpha_factory_v1"" / ""demos"")
    monkeypatch.setattr(gdd, ""DOCS_DIR"", docs_demos)

    gdd.generate_docs()

    page = docs_demos / ""demo_a.md""
    text = page.read_text(encoding=""utf-8"")
    assert ""# Demo A"" in text
    assert ""![preview](../demo_a/assets/preview.png){.demo-preview}"" in text
    assert ""[View README](../../alpha_factory_v1/demos/demo_a/README.md)"" in text",tests/test_generate_demo_docs.py,,1,7
survived,"    def execute_in_sandbox(self, code: str) -> tuple[str, str]:
        """"""Run ``code`` inside a subprocess with resource limits.""""""

        def _apply_limits() -> None:  # pragma: no cover - platform dependent
            try:
                import resource

                resource.setrlimit(resource.RLIMIT_CPU, (2, 2))
                mem = 128 * 1024 * 1024
                resource.setrlimit(resource.RLIMIT_AS, (mem, mem))
            except Exception:
                pass

        with tempfile.NamedTemporaryFile(""w"", suffix="".py"", delete=False) as fh:
            fh.write(code)
            code_path = fh.name

        helper = tempfile.NamedTemporaryFile(""w"", suffix="".py"", delete=False)
        helper.write(
            ""import json,sys,contextlib,io,textwrap,resource\n""
            ""code=open(sys.argv[1]).read()\n""
            ""try:\n""
            ""    resource.setrlimit(resource.RLIMIT_CPU,(2,2))\n""
            ""    resource.setrlimit(resource.RLIMIT_AS,(256*1024*1024,256*1024*1024))\n""
            ""except Exception:\n""
            ""    pass\n""
            ""wrapped='def snippet():\\n'+textwrap.indent(code,'    ')\n""
            ""env={'__builtins__':{'print':print,'range':range,'len':len}}\n""
            ""loc={}\n""
            ""out,err=io.StringIO(),io.StringIO()\n""
            ""with contextlib.redirect_stdout(out), contextlib.redirect_stderr(err):\n""
            ""    try:\n""
            ""        exec(compile(wrapped,'<agent>','exec'),env,loc)\n""
            ""        loc['snippet']()\n""
            ""    except Exception as e:\n""
            ""        err.write(type(e).__name__)\n""
            ""print(json.dumps({'stdout':out.getvalue(),'stderr':err.getvalue()}))\n""
        )
        helper.flush()
        helper_path = helper.name
        helper.close()

        try:
            proc = subprocess.run(
                [sys.executable, helper_path, code_path],
                text=True,
                capture_output=True,
                timeout=3,
                preexec_fn=_apply_limits if os.name == ""posix"" else None,
            )
            try:
                data = json.loads(proc.stdout or ""{}"")
                out = data.get(""stdout"", """")
                err = data.get(""stderr"", """")
            except json.JSONDecodeError:
                out, err = proc.stdout, proc.stderr
        except Exception as exc:  # pragma: no cover - runtime errors
            out, err = """", str(exc)
        finally:
            os.unlink(code_path)
            os.unlink(helper_path)

        env = messaging.Envelope(self.name, ""exec"", {""stdout"": out, ""stderr"": err}, time.time())
        self.ledger.log(env)
        return out, err",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/codegen_agent.py,CodeGenAgent,1,7
survived,"async def test_send_http_error():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        mock_session.return_value.close = AsyncMock()
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        with pytest.raises(ValueError):
            await client.send(""trace"", {""d"": 1})
        await client.close()
",tests/unit/test_telemetry_client.py,,1,7
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def __init__(self, *args, **kwargs):
        pass
",src/aiohttp/__init__.py,ClientSession,1,6
survived,"    def __init__(
        self,
        endpoints: Dict[str, EndpointConfig],
        *,
        rate_limit: int = 5,
        timeout: int = 10,
    ) -> None:
        if not endpoints:
            raise ValueError(""At least one endpoint must be configured"")
        self.endpoints = endpoints
        self.timeout = timeout
        self._sem = asyncio.Semaphore(rate_limit)
        self._session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=None)
        )
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def __init__(self, cost_cap: float = 0.5) -> None:
        self.cost_cap = cost_cap
        self.token_count = 0
        self.cost = 0.0
        self.guardrail_hits = 0
        self.latency = 0.0
        self._start: Optional[float] = None
        self.logger = logging.getLogger(__name__)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"def test_record_event():
    t = TelemetryCollector()
    t.record_event(
        TelemetryCollector.Category.EXECUTION,
        ""failed"",
        severity=TelemetryCollector.Severity.ERROR,
    )
    assert len(t.events) == 1
    ev = t.events[0]
    assert ev.category == TelemetryCollector.Category.EXECUTION
    assert ev.severity == TelemetryCollector.Severity.ERROR",tests/unit/test_telemetry_collector.py,,1,7
survived,"    def increment_guardrail_hits(self) -> None:
        """"""Increment guardrail hit counter and record an event.""""""
        self.guardrail_hits += 1
        self.record_event(
            self.Category.GUARDRAIL,
            ""guardrail violation"",
            severity=self.Severity.WARNING,
        )
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"async def test_send_retry_failure():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        mock_session.return_value.close = AsyncMock()

        client = TelemetryAPIClient(
            {""trace"": EndpointConfig(""http://example.com"")}, retries=1, backoff=0
        )
        with pytest.raises(Exception):
            await client.send(""trace"", {""d"": 1})
        assert mock_session.return_value.post.call_count == 2
        await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def test_purge_old(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(1, 0.01, 0.1, 0)
    # update timestamp to old date
    old_ts = ""2000-01-01T00:00:00""
    db.conn.execute(""UPDATE telemetry SET timestamp=?"", (old_ts,))
    db.conn.commit()
    db.purge_old()
    assert db.fetch_all() == []
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"    def archive(self, path: Optional[str] = None) -> str:
        """"""Export all telemetry records to a gzipped JSON file.""""""
        data = self.fetch_all()
        if path is None:
            name = datetime.utcnow().isoformat().replace("":"", """").replace(""."", """")
            path = f""telemetry_{name}.json.gz""
        with gzip.open(path, ""wt"", encoding=""utf-8"") as f:
            json.dump(data, f)
        return path
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an informational or error event.""""""
        self.events.append(
            TelemetryCollector.Event(
                category=category,
                severity=severity,
                message=message,
            )
        )
        log = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log = self.logger.error
        elif severity is self.Severity.WARNING:
            log = self.logger.warning
        log(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,8
survived,"def test_cost_cap_threshold_events(caplog):
    t = TelemetryCollector(cost_cap=0.02)
    with caplog.at_level(logging.INFO):
        t.add_usage(1000, 0, model=""o3"")
        assert len(t.events) == 0
        t.add_usage(500, 0, model=""o3"")
        assert len(t.events) == 1
        assert t.events[0].severity == TelemetryCollector.Severity.WARNING
        t.add_usage(300, 0, model=""o3"")
        assert len(t.events) == 2
        assert t.events[1].severity == TelemetryCollector.Severity.ERROR
        with pytest.raises(RuntimeError):
            t.add_usage(200, 0, model=""o3"")
        assert len(t.events) == 3
        assert t.events[-1].severity == TelemetryCollector.Severity.CRITICAL
",tests/unit/test_telemetry_collector.py,,1,7
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def verify(self) -> bool:
        cur = self.conn.cursor()
        res = cur.execute(""PRAGMA integrity_check"").fetchone()
        return res[0] == ""ok""
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"        def __enter__(self) -> ""_Resp"":
            return self
",tests/test_check_env_network.py,_Resp,1,7
survived,"def test_storage_access_toast() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(storage_state=None)
        context.add_init_script(""document.hasStorageAccess = () => Promise.resolve(false)"")
        page = context.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(
            ""document.getElementById('toast').textContent.includes('no storage access')""
        )
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_storage_access_toast.py,,1,6
survived,"def main(argv: List[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Run the Meta-Agentic Tree Search demo"")
    parser.add_argument(""--episodes"", type=int, default=10, help=""Number of search iterations"")
    args = parser.parse_args(argv)
    run(args.episodes)
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/run_demo.py,,1,7
survived,"    def rollout(self, agents: List[int]) -> float:
        """"""Return a pseudo reward after a single rollout.""""""
        distance = sum(abs(a - self.target) for a in agents)
        noise = random.random() * 0.1
        return -distance + noise",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/env.py,NumberLineEnv,1,7
survived,"    def test_run_demo_with_seed(self) -> None:
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--seed"",
                ""123"",
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo,1,6
survived,"    def publish(self, topic: str, env: EnvelopeLike) -> None:
        with span(""bus.publish""):
            bus_messages_total.labels(topic).inc()
            if self._producer:
                if isinstance(env, pb.Envelope):
                    payload = json_format.MessageToDict(env, preserving_proto_field_name=True)
                else:  # support SimpleNamespace in tests
                    payload = env.__dict__
                data = json.dumps(payload).encode()
                asyncio.create_task(self._producer.send_and_wait(topic, data))
            for h in list(self._subs.get(topic, [])):
                try:
                    res = h(env)
                    if asyncio.iscoroutine(res):
                        try:
                            asyncio.get_running_loop().create_task(res)
                        except RuntimeError:  # pragma: no cover - sync context
                            asyncio.run(res)
                except Exception:  # noqa: BLE001
                    logger.exception(
                        ""handler error %s -> %s on %s"",
                        env.sender,
                        env.recipient,
                        topic,
                    )
",alpha_factory_v1/common/utils/messaging.py,A2ABus,1,7
survived,"    def tail(self, count: int = 10) -> List[dict[str, object]]:
        """"""Return the last ``count`` ledger entries.""""""

        assert self.conn is not None
        if self.db_type == ""postgres"":
            with self.conn.cursor() as cur:
                cur.execute(
                    ""SELECT ts, sender, recipient, payload FROM messages ORDER BY id DESC LIMIT %s"",
                    (count,),
                )
                rows = cur.fetchall()
        else:
            cur = self.conn.execute(
                ""SELECT ts, sender, recipient, payload FROM messages ORDER BY id DESC LIMIT ?"",
                (count,),
            )
            rows = cur.fetchall()
        result: List[dict[str, object]] = []
        for ts, sender, recipient, payload in reversed(rows):
            try:
                data = json.loads(payload)
            except Exception:
                data = payload
            result.append({""ts"": ts, ""sender"": sender, ""recipient"": recipient, ""payload"": data})
        return result
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"    async def __aenter__(self) -> ""Ledger"":
        """"""Start the Merkle broadcast task and return ``self``.""""""
        self.start_merkle_task()
        return self
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"    def __init__(
        self,
        path: str,
        rpc_url: str | None = None,
        wallet: str | None = None,
        broadcast: bool = True,
        db: str | None = None,
    ) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        db_type = db or os.getenv(""AGI_INSIGHT_DB"", ""sqlite"")
        self.db_type = db_type
        if db_type == ""duckdb"" and duckdb is not None:
            self.conn = duckdb.connect(str(self.path))
            self.conn.execute(
                """"""
                CREATE TABLE IF NOT EXISTS messages (
                    id BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
                    ts DOUBLE,
                    sender TEXT,
                    recipient TEXT,
                    payload TEXT,
                    hash TEXT
                )
                """"""
            )
        elif db_type == ""postgres"":
            if ""psycopg2"" not in globals():
                _log.warning(""AGI_INSIGHT_DB=postgres but psycopg2 not installed â€“ falling back to sqlite"")
                self.conn = sqlite3.connect(str(self.path))
                self.conn.execute(
                    """"""
                    CREATE TABLE IF NOT EXISTS messages (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        ts REAL,
                        sender TEXT,
                        recipient TEXT,
                        payload TEXT,
                        hash TEXT
                    )
                    """"""
                )
            else:
                params = {
                    ""host"": os.getenv(""PGHOST""),
                    ""port"": os.getenv(""PGPORT"", ""5432""),
                    ""user"": os.getenv(""PGUSER""),
                    ""password"": os.getenv(""PGPASSWORD""),
                    ""dbname"": os.getenv(""PGDATABASE"", ""insight""),
                }
                self.conn = psycopg2.connect(**{k: v for k, v in params.items() if v is not None})
                with self.conn, self.conn.cursor() as cur:
                    cur.execute(
                        """"""
                        CREATE TABLE IF NOT EXISTS messages (
                            id BIGSERIAL PRIMARY KEY,
                            ts DOUBLE PRECISION,
                            sender TEXT,
                            recipient TEXT,
                            payload TEXT,
                            hash TEXT
                        )
                        """"""
                    )
        else:
            if db_type == ""duckdb"" and duckdb is None:
                _log.warning(""AGI_INSIGHT_DB=duckdb but duckdb not installed â€“ falling back to sqlite"")
            self.conn = sqlite3.connect(str(self.path))
            self.conn.execute(
                """"""
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    ts REAL,
                    sender TEXT,
                    recipient TEXT,
                    payload TEXT,
                    hash TEXT
                )
                """"""
            )
        self.conn.commit()
        self._task: asyncio.Task[None] | None = None
        self.rpc_url = rpc_url
        self.wallet = wallet
        self.broadcast = broadcast
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"    def call_stub(prompt: str, s: Settings) -> str:
        return f""[offline] {prompt}""
",alpha_factory_v1/common/utils/local_llm.py,,1,6
survived,"def chat(prompt: str, cfg: Settings | None = None) -> str:
    """"""Return a completion using the local model or a simple echo.""""""
    cfg = cfg or config.CFG
    if _CALL is None:
        _load_model(cfg)
    assert _CALL is not None
    try:
        with span(""local_llm.chat""):
            return _CALL(prompt, cfg)
    except Exception as exc:  # pragma: no cover - runtime error
        _log.exception(""Local chat failed: %s"", exc)
        return f""[offline] {prompt}""",alpha_factory_v1/common/utils/local_llm.py,,1,7
survived,"    async def start(self) -> None:  # pragma: no cover - no async setup
        return None
",alpha_factory_v1/backend/services/kafka_service.py,KafkaService,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q22.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q22.py,Auto2,1,7
survived,"def _hash_embed(text: str, dim: int) -> List[float]:
    """"""Create a simple hashed bag-of-words embedding.""""""
    vec = [0.0] * dim
    for word in text.lower().split():
        idx = hash(word) % dim
        vec[idx] += 1.0
    return vec
",src/meta_agent/embedding_models.py,,1,6
survived,"    def test_accumulate_entries(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / 'log.json'
            for seed in ('1', '2'):
                result = subprocess.run(
                    [
                        sys.executable,
                        STUB,
                        '-n',
                        '1',
                        '--seed',
                        seed,
                        '--ledger',
                        str(ledger),
                        '--model',
                        'gpt-4o-mini',
                    ],
                    capture_output=True,
                    text=True,
                )
                self.assertEqual(result.returncode, 0, result.stderr)

            logged = json.loads(ledger.read_text())
            self.assertIsInstance(logged, list)
            self.assertEqual(len(logged), 2)
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub,1,7
survived,"        def Tool(*_args, **_kw):  # type: ignore
            def _decorator(func):
                return func

            return _decorator
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,,1,7
survived,"            def _decorator(func):
                return func
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,,1,6
survived,"    def test_alpha_factory_import(self) -> None:
        mod = importlib.import_module(""alpha_factory_v1"")
        self.assertTrue(hasattr(mod, ""__version__""))
",tests/test_imports.py,TestImports,1,7
survived,"    def setUp(self):
        self._registry_backup = AGENT_REGISTRY.copy()
        self._cap_backup = {k: v[:] for k, v in CAPABILITY_GRAPH.items()}
        AGENT_REGISTRY.clear()
        CAPABILITY_GRAPH.clear()
",tests/test_agents_registry.py,TestAgentRegistryFunctions,1,7
survived,"    def test_list_agents_detail(self):
        class DAgent(AgentBase):
            NAME = ""detail""
            CAPABILITIES = [""bar""]

            async def step(self):
                return None

        meta = AgentMetadata(
            name=DAgent.NAME,
            cls=DAgent,
            version=""1.2"",
            capabilities=DAgent.CAPABILITIES,
            compliance_tags=[""x""],
        )
        register_agent(meta)
        detail = list_agents(detail=True)
        self.assertEqual(detail[0][""name""], DAgent.NAME)
        self.assertEqual(detail[0][""version""], ""1.2"")
        self.assertIn(""bar"", detail[0][""capabilities""])
",tests/test_agents_registry.py,TestAgentRegistryFunctions,1,7
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestRegisterDecorator.OkAgent,0,7
survived,"    def test_list_capabilities(self):
        caps = list_capabilities()
        self.assertIsInstance(caps, list)
        self.assertTrue(all(isinstance(c, str) for c in caps))
        # Should include at least one known capability from PingAgent
        self.assertIn(""diagnostics"", caps)
",tests/test_agents_integrity.py,TestAgentsIntegrity,1,7
survived,"    def set(self, val) -> None:
        self.value = val
",tests/test_agent_base.py,_Gauge,1,7
survived,"    def __init__(self):
        self.value = None
",tests/test_agent_base.py,_Gauge,1,6
survived,"        async def _sleep(_: float) -> None:
            raise asyncio.CancelledError()
",tests/test_agent_runner.py,,0,7
survived,"  def update_strings(self, strings, sendcan: bool = False):
    try:
      if strings and not isinstance(strings[0], list | tuple):
        strings = [strings]

      for addr in self.addresses:
        for k in self.vl_all[addr]:
          self.vl_all[addr][k].clear()

      updated_addrs: set[int] = set()
      for entry in strings:
        t = entry[0]
        frames = entry[1]
        bus_empty = True
        for address, dat, src in frames:
          if src != self.bus:
            continue
          bus_empty = False
          state = self.message_states.get(address)
          if state is None or len(dat) > 64:
            continue
          if state.parse(t, dat):
            updated_addrs.add(address)
            msgname = state.name
            for i, sig in enumerate(state.signals):
              val = state.vals[i]
              self.vl[address][sig.name] = val
              self.vl[msgname][sig.name] = val
              self.vl_all[address][sig.name] = state.all_vals[i]
              self.vl_all[msgname][sig.name] = state.all_vals[i]
              self.ts_nanos[address][sig.name] = state.timestamps[-1]
              self.ts_nanos[msgname][sig.name] = state.timestamps[-1]

        if not bus_empty:
          self.last_nonempty_nanos = t
        bus_timeout_threshold = 500 * 1_000_000
        for st in self.message_states.values():
          if st.timeout_threshold > 0:
            bus_timeout_threshold = min(bus_timeout_threshold, st.timeout_threshold)
        self.bus_timeout = (t - self.last_nonempty_nanos) > bus_timeout_threshold
        self.update_valid(t)

      return updated_addrs
    except (TypeError, IndexError):
      raise RuntimeError(""invalid parameter"") from None
",opendbc/can/parser.py,CANParser,1,7
survived,"async def test_sequential_defaults():
    called = {}

    async def runner(prompt, user_id=None, session_id=None, llm=None, sdk_context=None):
        called['params'] = (user_id, session_id, llm, sdk_context)
        return prompt + ""-done""

    wf = Workflow(
        name=""wf"",
        instruction=""instr"",
        description=""desc"",
        default_llm=""llm"",
        sdk_context=SDKContext(""./swarmzero_config_test.toml""),
        default_user_id=""u"",
        default_session_id=""s"",
        steps=[WorkflowStep(runner=runner)],
    )

    result = await wf.run(""hi"")
    assert result == ""hi-done""
    assert called[""params""][:3] == (""u"", ""s"", ""llm"")
    assert isinstance(called[""params""][3], SDKContext)
",tests/test_workflow.py,,1,7
survived,"    async def run_forever(self) -> None:
        await asyncio.Event().wait()
",tests/test_api_server.py,DummyOrch,1,6
survived,"def ensure_config(directory: Path) -> tuple[Path, bool]:
    """"""Ensure ``config.env`` exists in ``directory``.

    Returns the path and whether it was created.
    """"""
    config = directory / ""config.env""
    if config.exists():
        return config, False
    sample = directory / ""config.env.sample""
    shutil.copyfile(sample, config)
    return config, True
",alpha_factory_v1/demos/alpha_agi_business_v1/scripts/setup_config.py,,1,7
survived,"def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Create config.env if missing"")
    parser.add_argument(
        ""--dir"",
        type=Path,
        default=Path(__file__).resolve().parents[1],
        help=""Demo directory (default: parent of this script)"",
    )
    args = parser.parse_args(argv)
    path, created = ensure_config(args.dir)
    if created:
        print(f""Created {path}. Edit this file to set secrets."")
    else:
        print(f""{path} already exists. Edit it to update secrets."")
",alpha_factory_v1/demos/alpha_agi_business_v1/scripts/setup_config.py,,1,7
survived,"def test_two_contiguous_selectors():
    B, X, Y = Axis(""batch"", 3), Axis(""x"", 5), Axis(""y"", 7)
    a = hax.arange((B, X, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a[""x"", ix, ""y"", iy]
    assert out.axes == (B,)
    ref = a.array[jnp.arange(3), ix.array, iy.array]
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,,1,7
survived,"def test_multiselector_broadcast():
    B, S, V = Axis(""batch"", 2), Axis(""seq"", 3), Axis(""vocab"", 6)
    a = hax.arange((B, S, V))
    idx1 = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = a[""vocab"", idx1]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(a, V, idx1))
",tests/test_scatter_gather.py,,1,7
survived,"    def init_state(self):
        import jax.numpy as jnp
        from dataclasses import dataclass

        @dataclass
        class State:
            token_ids: jnp.ndarray  # (max_seqs, max_len)
            lengths: jnp.ndarray  # (max_seqs,)
            active: jnp.ndarray  # (max_seqs,)
            head: jnp.ndarray  # ()
            tail: jnp.ndarray  # ()

        return State(
            token_ids=jnp.full((self.max_seqs, self.max_len), self.eos, dtype=jnp.int32),
            lengths=jnp.zeros((self.max_seqs,), dtype=jnp.int32),
            active=jnp.zeros((self.max_seqs,), dtype=jnp.bool_),
            head=jnp.array(0, dtype=jnp.int32),
            tail=jnp.array(0, dtype=jnp.int32),
        )
",src/levanter/inference/scheduler.py,JittedScheduler,1,7
survived,"def main():
    parser = argparse.ArgumentParser(description=""Run simple LLM inference"")
    parser.add_argument(""model"", help=""Path to HF checkpoint"")
    parser.add_argument(""prompts"", help=""Text file with one prompt per line"")
    parser.add_argument(""--temperature"", type=float, default=0.6)
    parser.add_argument(""--max_tokens"", type=int, default=256)
    args = parser.parse_args()

    with open(Path(args.prompts), ""r"", encoding=""utf-8"") as f:
        prompts = [line.strip() for line in f if line.strip()]

    llm = LLM(args.model)
    sp = SamplingParams(temperature=args.temperature, max_tokens=args.max_tokens)
    outputs = llm.generate(prompts, sp)

    for prompt, out in zip(prompts, outputs):
        print(f""Prompt: {prompt}\nCompletion: {out['text']}\n"")
",src/levanter/main/nano_inference.py,,1,7
survived,"    def _prefill(self, seq: Sequence, cache, page_table):
        real_len = len(seq.prompt_token_ids)
        padded_len = _round_preferred(real_len)
        pos_axis = Axis(""position"", padded_len)
        padded_tokens = list(seq.prompt_token_ids) + [self.eos] * (padded_len - real_len)
        tokens = hax.NamedArray(jnp.array(padded_tokens, dtype=jnp.int32), axes=(pos_axis,))
        seq_named = hax.named([seq.seq_id], ""seq"")
        temps = hax.full((), seq.sampling_params.temperature, dtype=jnp.float32)
        key = jrandom.PRNGKey(0)
        tok, page_table, cache = do_prefill(self.model, cache, page_table, tokens, self.sampler, seq_named, temps, key)
        return int(tok.array), cache, page_table
",src/levanter/inference/llm_engine.py,LLMEngine,1,7
survived,"    def __init__(self, h, neighbours):
        """""" Iterative-deepening A* search.

        h(n) is the heuristic that gives the cost between node n and the goal node. It must be admissable, meaning that h(n) MUST NEVER OVERSTIMATE the true cost. Underestimating is fine.

        neighbours(n) is an iterable giving a pair (cost, node, descr) for each node neighbouring n
        IN ASCENDING ORDER OF COST. descr is not used in the computation but can be used to
        efficiently store information about the path edges (e.g. up/left/right/down for grids).
        """"""

        self.h = h
        self.neighbours = neighbours
        self.FOUND = object()
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,IDAStar,1,7
survived,"def gen_wd_table(n):
    goal = [[0] * i + [n] + [0] * (n - 1 - i) for i in range(n)]
    goal[-1][-1] = n - 1
    goal = tuple(sum(goal, []))

    table = {}
    to_visit = [(goal, 0, n-1)]
    while to_visit:
        cfg, cost, e = to_visit.pop(0)
        enccfg = encode_cfg(cfg, n)
        if enccfg in table: continue
        table[enccfg] = cost

        for d in [-1, 1]:
            if 0 <= e + d < n:
                for c in range(n):
                    if cfg[n*(e+d) + c] > 0:
                        ncfg = list(cfg)
                        ncfg[n*(e+d) + c] -= 1
                        ncfg[n*e + c] += 1
                        to_visit.append((tuple(ncfg), cost + 1, e+d))

    return table
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,,1,6
survived,"def slide_neighbours(n):
    movelist = []
    for gap in range(n*n):
        x, y = gap % n, gap // n
        moves = []
        if x > 0: moves.append(-1)    # Move the gap left.
        if x < n-1: moves.append(+1)  # Move the gap right.
        if y > 0: moves.append(-n)    # Move the gap up.
        if y < n-1: moves.append(+n)  # Move the gap down.
        movelist.append(moves)

    def neighbours(p):
        gap = p.index(0)
        l = list(p)

        for m in movelist[gap]:
            l[gap] = l[gap + m]
            l[gap + m] = 0
            yield (1, tuple(l), (l[gap], m))
            l[gap + m] = l[gap]
            l[gap] = 0

    return neighbours
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,,1,7
survived,"    def heuristic(self, start):
        """"""

        Estimates the number of moves from start to goal.
        The goal was preprocessed in __init__.

        """"""

        distance = 0

        # local variables for instance variables

        t = start.tiles
        g = self.goal_map
        rc = self.row_conflicts
        cc = self.col_conflicts

        # calculate manhattan distance

        for row in range(4):
            for col in range(4):
                start_tilenum = t[row][col]
                if start_tilenum != 0:
                    (grow, gcol) = g[start_tilenum]
                    distance += abs(row - grow) + abs(col - gcol)

        # add linear conflicts

        for row in range(4):
            curr_row = t[row]
            distance += rc[row][curr_row]

        for col in range(4):
            col_tuple = (t[0][col], t[1][col], t[2][col], t[3][col])
            distance += cc[col][col_tuple]

        return distance
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,HeuristicObj,1,8
survived,"    def __repr__(self):
        # printable version of self
        strrep = """"
        for e in self.qheap:
          fscore, tiles = e
          strrep += str(fscore)+"":""+str(tiles)+""\n""

        return strrep
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue,1,7
survived,"    def __missing__(self, key):
        return 0
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,lcmap,1,7
survived,"    def get_tag(self):
        python, abi, platform = super().get_tag()
        if python.startswith(""cp""):
            python, abi = ""cp39"", ""abi3""
        return python, abi, platform
",third_party/tree-sitter-racket/setup.py,BdistWheel,1,6
survived,"    def export_tree(self):
        """"""Return the current hierarchy as a JSONâ€‘serialisable structure.""""""

        def visit(node):
            return {
                ""id"": int(node.node_id),
                ""level"": int(node.level),
                ""customers"": int(node.customers),
                ""total_words"": int(node.total_words),
                ""children"": [visit(child) for child in node.children],
            }

        return visit(self.root_node)
",src/hlda/sampler.py,HierarchicalLDA,1,8
survived,"        def visit(node):
            return {
                ""id"": int(node.node_id),
                ""level"": int(node.level),
                ""customers"": int(node.customers),
                ""total_words"": int(node.total_words),
                ""children"": [visit(child) for child in node.children],
            }
",src/hlda/sampler.py,HierarchicalLDA,1,7
survived,"def test_visualize_shardings_inside_jit(capsys):
    mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))

    @named_jit(out_axis_resources={""dim1"": ResourceAxis.DATA})
    def fn(x):
        visualize_shardings(x)
        return x

    with axis_mapping({""dim1"": ResourceAxis.DATA}), mesh:
        x = hax.ones(Dim1)
        fn(x)

    out = capsys.readouterr().out
    assert ""dim1"" in out
",tests/test_visualize_sharding.py,,0,6
survived,"    def fn(x):
        visualize_shardings(x)
        return x
",tests/test_visualize_sharding.py,,1,6
survived,"            def reset(self):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver,0,7
survived,"def discover_alpha() -> dict:
    """"""Return a randomly selected alpha opportunity.""""""
    pick = random.choice(SAMPLE_ALPHA)
    LEDGER.write_text(json.dumps(pick, indent=2))
    return pick
",alpha_factory_v1/demos/omni_factory_demo/alpha_discovery_stub.py,,1,6
survived,"    def test_sanity_check_patch_nonexistent(self):
        with tempfile.TemporaryDirectory() as repo:
            open(os.path.join(repo, ""file.py""), ""w"").close()
            bad_patch = """"""--- a/missing.py
+++ b/missing.py
@@
-print('x')
+print('y')
""""""
            with self.assertRaises(ValueError):
                patcher_core._sanity_check_patch(bad_patch, pathlib.Path(repo))
",tests/test_self_healing_patcher.py,TestPatcherCore,1,7
survived,"    def test_basic_convergence(self) -> None:
        coop = run_sim(agents=50, rounds=500, delta=0.85, stake=2.5, seed=1)
        self.assertGreater(coop, 0.7)
",tests/test_governance_sim.py,TestGovernanceSim,1,6
survived,"async def list_agents() -> list[str]:
    resp = requests.get(f""{HOST}/agents"", timeout=5)
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,,1,6
survived,"    def test_plugins_load_and_function(self):
        demo._load_plugins.cache_clear()
        plugins = demo._load_plugins()
        self.assertTrue(len(plugins) >= 1)
        plugin = plugins[0]
        heur = getattr(plugin, ""heuristic_policy"", None)
        self.assertTrue(callable(heur))
        result = heur([0.1, 0.5, 0.0])
        self.assertIn(""action"", result)
",tests/test_omni_factory_plugins.py,TestPluginLoader,1,7
survived,"        def run(self) -> None:
            pass
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,AgentRuntime,0,7
survived,"def main(argv: list[str] | None = None) -> None:
    """"""Launch the MuZero dashboard with optional CLI overrides.""""""

    parser = argparse.ArgumentParser(description=""Run MuZero planning demo"")
    parser.add_argument(
        ""--env"",
        default=os.getenv(""MUZERO_ENV_ID"", ""CartPole-v1""),
        help=""Gymnasium environment ID"",
    )
    parser.add_argument(
        ""--episodes"",
        type=int,
        default=int(os.getenv(""MUZERO_EPISODES"", 3)),
        help=""Number of episodes to run"",
    )
    parser.add_argument(
        ""--port"",
        type=int,
        default=int(os.getenv(""HOST_PORT"", 7861)),
        help=""Dashboard port"",
    )
    args = parser.parse_args(argv)

    os.environ[""MUZERO_ENV_ID""] = args.env
    os.environ[""MUZERO_EPISODES""] = str(args.episodes)
    os.environ[""HOST_PORT""] = str(args.port)

    launch_dashboard()
",alpha_factory_v1/demos/muzero_planning/__main__.py,,1,7
survived,"    def reset(self) -> None:
        """"""Return to generation zero with a fresh population.""""""
        self._init_population()
        self.gen = 0
        self.history.clear()
        self._archive.clear()
        self._best_fitness = -math.inf
        self.best_genome = self.population[0]
        self._last_scores.clear()
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"def add(a, b):
    return a + b
",tests/machine/x/python/fun_call.py,,1,7
survived,"def outer(x: int) -> int:
    def inner(y: int) -> int:
        return x + y
    return inner(5)
",tests/machine/x/python/nested_function.py,,1,7
survived,"    async def run() -> None:
        await bus.start()
        try:
            creds = grpc.ssl_channel_credentials(root_certificates=ca)
            async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                stub = ch.unary_unary(""/bus.Bus/Send"")
                payload = {
                    ""sender"": ""a"",
                    ""recipient"": ""b"",
                    ""payload"": {},
                    ""ts"": 0.0,
                    ""token"": ""bad"",
                }
                with pytest.raises(grpc.aio.AioRpcError):
                    await stub(json.dumps(payload).encode())
        finally:
            await bus.stop()
",tests/test_agents.py,,1,6
survived,"    def test_cpu_alignment(self):
        sw = SmithWatermanGPU()
        score, _ = sw.align(""ACACACTA"", ""AGCACACA"")
        self.assertEqual(score, 17)
",src/test/python/test_gpu_smith_waterman.py,TestSmithWatermanGPU,1,6
survived,"    def _setup_opencl(self) -> None:
        """"""Initialize OpenCL context and compile kernel.""""""
        self._ctx = cl.create_some_context()
        self._queue = cl.CommandQueue(self._ctx)
        kernel = r""""""
        __kernel void sw_diag(
            __global const char* seq1,
            __global const char* seq2,
            __global int* H,
            const int len1,
            const int len2,
            const int diag,
            const int start_i,
            const int match,
            const int mismatch,
            const int gap)
        {
            int gid = get_global_id(0);
            int i = start_i + gid;
            int j = diag - i + 1;
            if(i<=len1 && j>=1 && j<=len2) {
                int diag_idx = (i-1)*(len2+1) + (j-1);
                int up_idx   = (i-1)*(len2+1) + j;
                int left_idx = i*(len2+1) + (j-1);
                int idx      = i*(len2+1) + j;
                int match_val = seq1[i-1]==seq2[j-1] ? match : mismatch;
                int diag_val  = H[diag_idx] + match_val;
                int up_val    = H[up_idx] + gap;
                int left_val  = H[left_idx] + gap;
                int val = diag_val;
                if(up_val > val) val = up_val;
                if(left_val > val) val = left_val;
                if(val < 0) val = 0;
                H[idx] = val;
            }
        }
        """"""
        self._prog = cl.Program(self._ctx, kernel).build()
",src/python/gpu_smith_waterman.py,SmithWatermanGPU,1,7
survived,"    def _align_gpu(self, seq1: str, seq2: str) -> tuple[int, ""np.ndarray""]:
        len1, len2 = len(seq1), len(seq2)
        seq1_buf = np.frombuffer(seq1.encode(""ascii""), dtype=np.int8)
        seq2_buf = np.frombuffer(seq2.encode(""ascii""), dtype=np.int8)
        H = np.zeros((len1 + 1) * (len2 + 1), dtype=np.int32)
        d_seq1 = cl.Buffer(self._ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=seq1_buf)
        d_seq2 = cl.Buffer(self._ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=seq2_buf)
        d_H = cl.Buffer(self._ctx, cl.mem_flags.READ_WRITE | cl.mem_flags.COPY_HOST_PTR, hostbuf=H)
        for diag in range(2, len1 + len2 + 1):
            start_i = max(1, diag - len2)
            end_i = min(len1, diag - 1)
            diag_size = max(0, end_i - start_i + 1)
            if diag_size == 0:
                continue
            self._prog.sw_diag(
                self._queue,
                (diag_size,),
                None,
                d_seq1,
                d_seq2,
                d_H,
                np.int32(len1),
                np.int32(len2),
                np.int32(diag),
                np.int32(start_i),
                np.int32(self.match),
                np.int32(self.mismatch),
                np.int32(self.gap),
            )
        cl.enqueue_copy(self._queue, H, d_H)
        H = H.reshape((len1 + 1, len2 + 1))
        return int(H.max()), H
",src/python/gpu_smith_waterman.py,SmithWatermanGPU,0,7
survived,"    def labels(self, *a, **kw):
        return self
",tests/test_eventbus.py,_M,1,6
survived,"def test_queue_max_size() -> None:
    bus = EventBus(None, True, max_queue_size=2)
    bus.publish(""x"", {""v"": 1})
    bus.publish(""x"", {""v"": 2})
    bus.publish(""x"", {""v"": 3})
    events = bus.read_and_clear(""x"")
    assert events == {""x"": [{""v"": 2}, {""v"": 3}]}",tests/test_eventbus.py,,1,7
survived,"            def assign(carry):
                token_dests, seq_cursors = carry
                page_idx = seq_cursors[seq_id] // self.page_size
                page_offset = seq_cursors[seq_id] % self.page_size
                page = new_table.page_indices[""seq"", seq_id, ""page"", page_idx]
                dest = hax.where(page < 0, -1, page * self.page_size + page_offset)
                token_dests = token_dests.at[""position"", i].set(dest)
                seq_cursors = seq_cursors.at[seq_id].add(1)
                return token_dests, seq_cursors
",src/levanter/layers/page_table.py,PageTable,1,6
survived,"async def test_guardrails_called_in_order():
    adapter = MockAdapter()
    router = GuardrailModelRouter({""a"": adapter}, default_model=""a"")
    order: list[str] = []

    async def input_guardrail(prompt: str):
        order.append(f""in:{prompt}"")

    async def output_guardrail(output: str):
        order.append(f""out:{output}"")

    router.add_input_guardrail(input_guardrail)
    router.add_output_guardrail(output_guardrail)

    res = await router.invoke(""test"")

    assert res == ""test:ok""
    assert order == [""in:test"", ""out:test:ok""]",tests/test_guardrail_router.py,,1,8
survived,"    async def output_guardrail(output: str):
        order.append(f""out:{output}"")
",tests/test_guardrail_router.py,,1,6
survived,"async def test_router_selects_model():
    a1 = MockAdapter()
    a2 = MockAdapter()
    router = GuardrailModelRouter({""a"": a1, ""b"": a2}, default_model=""a"")

    res = await router.invoke(""hi"", model=""b"")

    assert res == ""hi:ok""
    assert a2.prompts == [""hi""]
    assert not a1.prompts
",tests/test_guardrail_router.py,,1,7
survived,"def square(x):
    return x * x
",tests/kgtests/autograd/helpers.py,,1,7
survived,"def to_numpy(val):
    """"""Return ``val`` as a NumPy array if backed by torch tensors.""""""
    if hasattr(val, ""detach""):
        val = val.detach().cpu().numpy()
    return val
",tests/utils.py,,1,7
survived,"    def _check_mixed_args_grad(self, name: str):
        """"""Verify gradient of the dot product xÂ·y with respect to each argument.""""""
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x, y):
            return b.sum(b.mul(x, y))

        gx = b.grad(f, wrt=0)
        gy = b.grad(f, wrt=1)
        x = b.array([1.0, 2.0, 3.0], requires_grad=True)
        y = b.array([4.0, 5.0, 6.0], requires_grad=True)
        gradx = to_numpy(gx(x, y))
        grady = to_numpy(gy(x, y))
        np.testing.assert_allclose(np.array(gradx), np.array([4.0, 5.0, 6.0]))
        np.testing.assert_allclose(np.array(grady), np.array([1.0, 2.0, 3.0]))
",tests/test_autograd.py,TestAutograd,1,7
survived,"    async def root():
        return {""ok"": True}
",tests/test_rate_lock.py,,1,7
survived,"def test_stub_compiles() -> None:
    py_compile.compile(str(STUB), doraise=True)
",tests/test_alpha_opportunity_stub.py,,1,7
survived,"        def __exit__(self, *exc: object) -> None:
            pass
",tests/test_check_env_network.py,_Sock,1,6
survived,"def test_logistic_curve_parameters() -> None:
    """"""Custom ``k`` and ``x0`` should shift the curve.""""""
    base = forecast.logistic_curve(0.0)
    shifted = forecast.logistic_curve(0.5, x0=0.5)
    steep = forecast.logistic_curve(0.1, k=2.0)
    assert shifted == pytest.approx(base)
    assert steep > forecast.logistic_curve(0.1)
",tests/test_forecast.py,,1,7
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - test helper
        pass
",tests/test_agent_runner.py,_Ledger,0,7
survived,"def test_load_dotenv(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    env = tmp_path / ""sample.env""
    env.write_text(""FOO=bar\n"", encoding=""utf-8"")
    monkeypatch.delenv(""FOO"", raising=False)
    cfg._load_dotenv(str(env))
    assert os.environ[""FOO""] == ""bar""
",tests/test_config_utils.py,,1,7
survived,"    def _model_dump(self: BaseModel, *args: Any, **kwargs: Any) -> Any:
        return self.dict(*args, **kwargs)
",src/meta_agent/__init__.py,,1,6
survived,"        def _model_dump(self: BaseModel, *args: Any, **kwargs: Any) -> Any:
            return self.dict(*args, **kwargs)
",src/meta_agent/__init__.py,,0,7
survived,"    def _model_dump_json(self: BaseModel, *args: Any, **kwargs: Any) -> str:
        return self.json(*args, **kwargs)
",src/meta_agent/__init__.py,,0,7
survived,"def backtrack_boost(pop: List[Any], archive: List[Any], rate: float) -> Any:
    """"""Return a parent possibly selected from weaker individuals.

    With probability ``rate`` the parent is drawn uniformly from the
    lower half of ``archive`` based on fitness.  Otherwise the regular
    ``select_parent`` mechanism chooses from ``pop``.
    """"""

    if not pop:
        raise ValueError(""population is empty"")
    if rate <= 0.0:
        return select_parent(pop, temp=1.0)
    if random.random() < rate:
        ranked = sorted(archive, key=lambda c: getattr(c, ""fitness"", 0.0))
        bottom = ranked[: max(1, len(ranked) // 2)]
        return random.choice(bottom)
    return select_parent(pop, temp=1.0)",src/simulation/mats_ops.py,,1,6
survived,"def _run_bench(repo: Path, flags: Dict[str, bool]) -> float:
    """"""Return SWE pass rate for the given repo and feature flags.""""""

    env = os.environ.copy()
    env[""PYTHONPATH""] = str(repo)
    for name, enabled in flags.items():
        env[f""ENABLE_{name.upper()}""] = ""1"" if enabled else ""0""
    proc = subprocess.run(
        [sys.executable, str(repo / ""benchmarks"" / ""run_benchmarks.py"")],
        capture_output=True,
        text=True,
        env=env,
        check=True,
    )
    results = json.loads(proc.stdout)
    metrics = compute_fitness(results)
    score = metrics.get(""swe_mini"", {}).get(""pass_rate"", 0.0)
    # simple synthetic penalty when features disabled
    for enabled in flags.values():
        if not enabled:
            score = max(0.0, score - 0.05)
    return score
",src/tools/ablation_runner.py,,1,7
survived,"def test_fsm_cycles_three() -> None:
    result = loop.run_loop(cost_budget=3.0, cost_per_cycle=1.0)
    assert result.cycles == 3
    assert result.state is loop.State.SELECT",tests/test_loop_fsm.py,,1,7
survived,"        async def run() -> None:
            async with messaging.A2ABus(cfg):
                pass
",tests/test_bus_logging.py,,0,6
survived,"        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime,1,6
survived,"    def test_index_3d_wildcard(self):
        """"""Indexing into 3D array with wildcard""""""
        expr = '[[[1 2] [3 4]] [[5 6] [7 8]]]:@[1 0 []]'
        self.assert_eval_cmp(expr, '[5 6]')",tests/test_numpy_slice.py,TestNumpySliceBehavior,1,6
survived,"    def test_index_negative_column(self):
        """"""Use negative index for last column""""""
        self.assert_eval_cmp('[[1 2 3] [4 5 6]]:@[[] -1]', '[3 6]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior,1,7
survived,"    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)
",tests/test_business_bridge_offline.py,,1,7
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,,1,7
survived,"    def search(
        self,
        query: str,
        *,
        category: str | None = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """"""Search the index using a simple token overlap ranking.""""""
        if not self._index:
            self.ensure_up_to_date()
        tokens = [t.lower() for t in query.split() if t]
        results = []
        for item in self._index:
            meta = item.get(""metadata"", {})
            if category and meta.get(""category"") != category:
                continue
            if tags and not all(t in meta.get(""tags"", []) for t in tags):
                continue
            haystack = "" "".join(
                [
                    item.get(""content"", """"),
                    meta.get(""title"", """"),
                    meta.get(""description"", """"),
                    meta.get(""slug"", """"),
                    "" "".join(meta.get(""tags"", [])),
                ]
            ).lower()
            score = sum(1 for tok in tokens if tok in haystack)
            if score:
                results.append({**item, ""score"": float(score)})
        results.sort(key=lambda r: r[""score""], reverse=True)
        return results[:limit]",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex,1,6
survived,"    def _append_enrichparameter_hints(self, description: str, fn: Callable[..., Any]) -> str:
        """"""Append ``EnrichParameter`` metadata to a description string.""""""

        hints: list[str] = []
        try:
            sig = inspect.signature(fn)
        except (TypeError, ValueError):  # pragma: no cover - defensive
            return description

        for param in sig.parameters.values():
            default = param.default
            annotation = param.annotation

            if isinstance(default, EnrichParameter):
                if annotation is EnrichContext:
                    # Context parameters are stripped from the final tool
                    # interface so hints would be confusing to the agent.
                    continue

                param_type = ""Any""
                if annotation is not inspect.Parameter.empty:
                    if get_origin(annotation) is Literal:
                        values = "", "".join(repr(v) for v in get_args(annotation))
                        param_type = f""Literal[{values}]""
                    else:
                        param_type = getattr(annotation, ""__name__"", str(annotation))

                parts = [param_type]
                if default.description:
                    parts.append(default.description)
                if default.examples:
                    joined = "", "".join(map(str, default.examples))
                    parts.append(f""examples: {joined}"")
                if default.metadata:
                    meta = "", "".join(f""{k}: {v}"" for k, v in default.metadata.items())
                    parts.append(meta)

                hints.append(f""{param.name} - {'; '.join(parts)}"")

        if hints:
            description = (
                description.rstrip() + ""\n\nParameter hints:\n"" + ""\n"".join(f""- {h}"" for h in hints)
            )

        return description
",src/enrichmcp/app.py,EnrichMCP,1,7
survived,"    def _mp_eval(self):
        with ProcessPoolExecutor() as pool:
            results = list(pool.map(self._simulate, self.population))
        return self._post_eval(results)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"    def _post_eval(self, results):
        scores, bcs = zip(*results)
        self._archive.extend(bcs[-64:])
        return list(scores)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"    def _init_population(self):
        seed = Genome()
        self.population = [seed.mutate() for _ in range(self.pop_size)]
        self.best_genome = self.population[0]
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"    def sha(self) -> str:
        return hashlib.sha256(self.to_json().encode()).hexdigest()[:12]
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,Genome,1,7
survived,"def check_cmd(cmd: str) -> bool:
    if shutil.which(cmd):
        banner(f""{cmd} found"", 'GREEN')
        return True
    banner(f""{cmd} missing"", 'RED')
    return False
",alpha_factory_v1/scripts/preflight.py,,1,7
survived,"def test_escaped_newline(state):
    out, expected = try_on_file(
        ""escaped_newline.py"",
        partial(fstringify_code_by_line, state=state),
    )
    assert out == expected",test/integration/test_issue83.py,,1,7
survived,"def _base_url() -> str:
    return os.environ.get(
        ""HF_GPT2_BASE_URL"",
        ""https://huggingface.co/openai-community/gpt2/resolve/main"",
    ).rstrip(""/"")
",scripts/download_hf_gpt2.py,,1,7
survived,"def main() -> None:
    """"""Run the asynchronous ``_main`` function via ``asyncio.run``.""""""
    asyncio.run(_main())",alpha_factory_v1/demos/alpha_agi_business_3_v1/cli.py,,1,7
survived,"    def render(self, task):
        from .convert_to_native_type import TaskToDict
        import base64

        png_bytes = base64.b64decode(
            ""iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR4nGNgYGBgAAAABQABRDE8UwAAAABJRU5ErkJggg==""
        )
        img_src = TaskToDict().parse_image(png_bytes)
        return f""<html><img src='{img_src}' /></html>""",metaflow/plugins/cards/card_modules/test_cards.py,TestImageCard,1,6
survived,"def sonarr_import(csv_path: str, cfg: configparser.ConfigParser) -> None:
    baseurl = cfg[""sonarr""][""baseurl""]
    urlbase = cfg[""sonarr""].get(""urlbase"", """")
    api_key = cfg[""sonarr""][""api_key""]
    root = cfg[""sonarr""][""rootfolderpath""]
    profile = cfg[""sonarr""][""qualityProfileId""]
    search = cfg.getboolean(""sonarr"", ""searchForShow"", fallback=False)

    headers = {""Content-type"": ""application/json"", ""X-Api-Key"": api_key}
    session = requests.Session()

    with open(csv_path, encoding=""utf-8"") as f:
        reader = csv.DictReader(f)
        for row in reader:
            title = row.get(""title"")
            year = row.get(""year"")
            imdbid = row.get(""imdbid"")
            if imdbid:
                url = f""{baseurl}{urlbase}/api/v3/series/lookup?term=imdb:{imdbid}""
            else:
                term = urllib.parse.quote_plus(f""{title} {year}"" if year else title)
                url = f""{baseurl}{urlbase}/api/v3/series/lookup?term={term}""
            rsp = session.get(url, headers=headers)
            if rsp.status_code != 200 or rsp.text in ("""", ""[]""):
                messagebox.showwarning(""Sonarr"", f""{title} not found"")
                continue
            data = rsp.json()
            if isinstance(data, list):
                data = data[0]
            payload = {
                ""title"": data.get(""title""),
                ""tvdbId"": data.get(""tvdbId""),
                ""year"": data.get(""year""),
                ""titleSlug"": data.get(""titleSlug""),
                ""qualityProfileId"": int(profile),
                ""rootFolderPath"": root,
                ""monitored"": True,
                ""seasonFolder"": True,
                ""images"": data.get(""images"", []),
                ""seasons"": data.get(""seasons"", []),
                ""addOptions"": {""searchForMissingEpisodes"": search},
            }
            add_url = f""{baseurl}{urlbase}/api/v3/series""
            session.post(add_url, headers=headers, json=payload)
",arr_gui.py,,1,7
survived,"    def __call__(self, genome: List[float]) -> List[float]:
        low, high = self.bounds
        return [min(high, max(low, g + self.rng.gauss(0.0, self.std))) for g in genome]
",src/simulation/mats_ops.py,GaussianParam,1,7
survived,"        def json(self) -> dict:
            return {""choices"": [{""message"": {""content"": ""ok""}}]}
",tests/test_llm_client_offline.py,DummyResp,1,7
survived,"        def __init__(self, text: str = ""local"") -> None:
            self._data = {""choices"": [{""message"": {""content"": text}}]}
",tests/test_selfheal_entrypoint_offline.py,DummyResp,1,6
survived,"def test_publish_grpc() -> None:
    port = _free_port()
    cfg = config.Settings(bus_port=port, allow_insecure=True)
    bus = messaging.A2ABus(cfg)
    received: list[messaging.Envelope] = []

    def handler(env: messaging.Envelope) -> None:
        received.append(env)

    bus.subscribe(""x"", handler)

    async def run() -> None:
        async with bus:
            async with grpc.aio.insecure_channel(f""localhost:{port}"") as ch:
                stub = ch.unary_unary(""/bus.Bus/Send"")
                payload = {
                    ""sender"": ""a"",
                    ""recipient"": ""x"",
                    ""payload"": {""v"": 1},
                    ""ts"": 0.0,
                }
                await stub(json.dumps(payload).encode())
                await asyncio.sleep(0)

    asyncio.run(run())

    assert len(received) == 1
    assert received[0].payload[""v""] == 1
",tests/test_message_bus.py,,1,7
survived,"    def tearDown(self):
        for p in reversed(self.patches):
            p.stop()
",tests/test_memory_vector.py,TestVectorMemoryOffline,1,7
survived,"    def setUp(self):
        self.patches = [
            mock.patch.object(mv, ""_HAS_PG"", False),
            mock.patch.object(mv, ""_HAS_FAISS"", False),
            mock.patch.object(mv, ""_HAS_OPENAI"", False),
            mock.patch.object(mv, ""SentenceTransformer"", None),
            mock.patch.object(mv, ""_DIM_OPENAI"", 3),
            mock.patch.object(mv, ""_DIM_SBERT"", 3),
            mock.patch.object(mv, ""_embed"", lambda texts: mv._np.ones((len(texts), 3), dtype=""float32"")),
        ]
        for p in self.patches:
            p.start()
",tests/test_memory_vector.py,TestVectorMemoryOffline,1,7
survived,"    def test_init_with_dsn_no_pg(self):
        mem = mv.VectorMemory(dsn=""postgres://user:pass@localhost/db"")
        self.assertEqual(mem.backend, ""numpy"")
",tests/test_memory_vector.py,TestVectorMemoryOffline,1,6
survived,"  async def init_shakers(self):
    return hex_to_binary(await self.send_command(""ll"", ""vi"", """"))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,6
survived,"def cytomat_rack_18mm_26(name: str):
  return _cytomat_rack(name=name, site_height=18, num_sites=26, model=""cytomat_rack_18mm_26"")
",pylabrobot/storage/cytomat/racks.py,,1,6
survived,"  async def action_storage_to_transfer(  # used by retrieve_plate
    self, site: PlateHolder
  ) -> OverviewRegisterState:
    """"""Retrieve from storage, open door, move to transfer, close door""""""
    return await self.send_action(""mv"", ""st"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,7
survived,"  async def initialize(self):
    await self._send_command(""ST 1900"")
    await self._send_command(""ST 1801"")
    await self._wait_ready()
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend,1,6
survived,"  async def stop(self):
    print(""Stopping incubator backend"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,1,6
survived,"  async def _wait_ready(self, timeout: int = 60):
    """"""
    Poll the ready flag (RD 1915) until it becomes '1' or timeout.
    """"""
    start = time.time()
    while True:
      resp = await self._send_command(""RD 1915"")
      if resp == ""1"":
        return
      await asyncio.sleep(0.1)
      if time.time() - start > timeout:
        raise TimeoutError(""Legacy Cytomat did not become ready in time"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend,1,7
survived,"  def __init__(self):
    super().__init__()
    self._racks: Optional[List[PlateCarrier]] = None
",pylabrobot/storage/backend.py,IncubatorBackend,1,7
survived,"  async def get_temperature(self) -> float:
    print(""Getting temperature"")
    return self._dummy_temperature
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,1,6
survived,"  def serialize(self) -> dict:
    return {
      **IncubatorBackend.serialize(self),
      ""model"": self.model.value,
      ""port"": self.io.port,
    }
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,7
survived,"    def __init__(self, registry: TemplateRegistry) -> None:
        self.registry = registry
",src/meta_agent/template_mixer.py,_RegistryLoader,1,7
survived,"    def dependency_graph(
        self, slug: str, *, version: str = ""latest""
    ) -> Dict[str, List[str]]:
        """"""Return a mapping of template to templates it references via ``extends`` or ``include``.""""""

        visited: Dict[str, List[str]] = {}
        pattern = re.compile(r""{%\s*(?:extends|include)\s+'([^']+)'"")

        def _walk(name: str) -> None:
            if name in visited:
                return
            s, v = _split_name(name)
            source = self.registry.load_template(s, v) or """"
            deps = pattern.findall(source)
            visited[name] = deps
            for dep in deps:
                _walk(dep)

        root = slug if version == ""latest"" else f""{slug}@{version}""
        _walk(root)
        return visited",src/meta_agent/template_mixer.py,TemplateMixer,1,7
survived,"    def validate(
        self,
        content: str,
        test_cases: Optional[List[TemplateTestCase]] = None,
        *,
        max_render_seconds: float = 1.0,
    ) -> ValidationResult:
        """"""Validate ``content`` and optionally run ``test_cases``.""""""
        errors: List[str] = []
        try:
            parsed = self.env.parse(content)
        except TemplateSyntaxError as exc:  # pragma: no cover - jinja2 message
            errors.append(f""syntax error: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        undeclared = meta.find_undeclared_variables(parsed)
        if test_cases:
            template = self.env.from_string(content)
            for case in test_cases:
                missing = undeclared - case.context.keys()
                if missing:
                    errors.append(f""missing variables {sorted(missing)}"")
                    continue
                start = time.perf_counter()
                output = template.render(**case.context)
                duration = time.perf_counter() - start
                if duration > max_render_seconds:
                    errors.append(""template rendering too slow"")
                if (
                    case.expected_output is not None
                    and output.strip() != case.expected_output.strip()
                ):
                    errors.append(""output mismatch"")
        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/template_validator.py,TemplateValidator,1,7
survived,"def test_template_validator_syntax_error() -> None:
    validator = TemplateValidator()
    result = validator.validate(""{% for x in %}"")
    assert not result.success
    assert any(""syntax error"" in e for e in result.errors)
",tests/test_template_validator.py,,1,7
survived,"def test_template_validator_success() -> None:
    validator = TemplateValidator()
    case = TemplateTestCase(context={""name"": ""Bob""}, expected_output=""Hello Bob"")
    result = validator.validate(""Hello {{ name }}"", [case])
    assert result.success
    assert result.errors == []
",tests/test_template_validator.py,,1,7
survived,"    def __init__(self, env: Optional[Environment] = None) -> None:
        self.env = env or Environment()
",src/meta_agent/template_validator.py,TemplateValidator,1,7
survived,"def test_merge_versions(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    manager = TemplateSharingManager(reg)
    meta = _meta(""demo"")
    reg.register(meta, ""first"", version=""0.1.0"")
    reg.register(meta, ""second"", version=""0.2.0"")

    merged = manager.merge_versions(""demo"", ""0.1.0"", ""0.2.0"")
    assert merged.strip() == ""second""",tests/test_template_sharing.py,,1,7
survived,"    def showcase(self, limit: int = 5) -> List[Tuple[str, float]]:
        """"""Return top rated templates as ``[(slug, average)]``.""""""
        ratings = self._load_ratings()
        avgs = [(slug, sum(vals) / len(vals)) for slug, vals in ratings.items() if vals]
        avgs.sort(key=lambda t: t[1], reverse=True)
        return avgs[:limit]
",src/meta_agent/template_sharing.py,TemplateSharingManager,1,8
survived,"        async def run(self, prompt: str) -> str:  # pragma: no cover - async stub
            return ""done""
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_agents.py,Ctx,1,6
survived,"def chat(prompt: str) -> str:
    """"""Return a completion using the local model or a simple echo.""""""
    if _CALL is None:
        _load_model()
    assert _CALL is not None
    try:
        return _CALL(prompt)
    except Exception:  # pragma: no cover - runtime error
        return f""[offline] {prompt}""",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/local_llm.py,,1,7
survived,"def format_values(node, values):
    return '{}({})'.format(node.__class__.__name__, ',\n    '.join(values))
",test/integration/expected_out_single_line/issue192.py,,1,6
survived,"def percent_conditional(line):
    return f""{line}\n"" if not line.endswith('\\') or line.endswith('\\\\') else f""{line[:-1]}""
",test/integration/expected_out/issue192.py,,1,6
survived,"def test_page_cache_extend_multi_page():
    Seq = Axis(""seq"", 2)
    Page = Axis(""page"", 3)
    MaxPage = Axis(""max_page"", 3)
    Slot = Axis(""slot"", 2)
    KVH = Axis(""kv_head"", 1)
    HD = Axis(""head_dim"", 1)

    cache = PageCache.init(Seq, Page, Slot, KVH, HD, MaxPage, dtype=jnp.float32)

    Tok = Axis(""tok"", 4)
    new_k = hax.arange(Tok).broadcast_axis((KVH, HD)).rearrange((Tok, KVH, HD)) + 1
    new_v = hax.arange(Tok).broadcast_axis((KVH, HD)).rearrange((Tok, KVH, HD)) + 101

    cu = jnp.array([0, 3, 4], dtype=jnp.int32)
    pages = jnp.array([0, 1, 2], dtype=jnp.int32)

    jit_extend = eqx.filter_jit(PageCache.extend)
    cache = jit_extend(cache, new_k, new_v, cu, pages, 2)

    assert jnp.all(cache.kv_lens.array == jnp.array([3, 1], dtype=jnp.int32))
    assert cache.page_indices.array[0, 0] == 0
    assert cache.page_indices.array[0, 1] == 1
    assert cache.page_indices.array[1, 0] == 2

    assert cache.kv_pages.array[0, 0, 0, 0] == 1
    assert cache.kv_pages.array[0, 1, 0, 0] == 2
    assert cache.kv_pages.array[1, 0, 0, 0] == 3
    assert cache.kv_pages.array[2, 0, 0, 0] == 4

    assert cache.kv_pages.array[0, 0, 1, 0] == 101
    assert cache.kv_pages.array[0, 1, 1, 0] == 102
    assert cache.kv_pages.array[1, 0, 1, 0] == 103
    assert cache.kv_pages.array[2, 0, 1, 0] == 104",tests/test_page_cache.py,,1,6
survived,"            def _replace_include(m: re.Match[str]) -> str:
                inc = self.registry.load_template(m.group(1), version) or """"
                return _render_source(inc)
",src/meta_agent/template_mixer.py,TemplateMixer,1,7
survived,"def _cli_output(seed: int) -> list[dict]:
    with patch.object(cli.orchestrator, ""Orchestrator""):
        res = CliRunner().invoke(
            cli.main,
            [
                ""simulate"",
                ""--horizon"",
                ""1"",
                ""--offline"",
                ""--sectors"",
                ""1"",
                ""--pop-size"",
                ""1"",
                ""--generations"",
                ""1"",
                ""--seed"",
                str(seed),
                ""--curve"",
                ""linear"",
                ""--export"",
                ""json"",
            ],
        )
    return json.loads(res.output)
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_wasm_bridge.py,,1,6
survived,"    def open_logs():
        try:
            open_logs_folder()
            return {""message"": ""opened""}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))",app/desktop/studio_server/settings_api.py,,1,6
survived,"def test_enqueue_and_pack():
    sched = JitScheduler.init(max_tokens=8, max_seqs=2, key=jax.random.PRNGKey(0))
    toks = hax.named(jnp.array([1, 2], dtype=jnp.int32), ""position"")
    seqs = hax.named(jnp.array([0, 1], dtype=jnp.int32), ""position"")
    sched = sched.enqueue_tokens(toks, seqs, 2)

    pack = eqx.filter_jit(lambda s: s.pack_next_sequence(2))
    sched, ptoks, pseqs = pack(sched)
    assert jnp.array_equal(ptoks.array, jnp.array([1, 2], dtype=jnp.int32))
    assert jnp.array_equal(pseqs.array, jnp.array([0, 1], dtype=jnp.int32))
    assert sched.num_queued_tokens == 0
",tests/test_jit_scheduler.py,,0,6
survived,"def validate_identifier(value: str, field_name: str) -> None:
    """"""Ensure simple alphanumeric names to avoid path traversal.""""""
    if not _SAFE_NAME_RE.match(value):
        raise HTTPException(status_code=400, detail=f""Invalid {field_name} provided."")
",no-ocr-api/np_ocr/api.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-octal-1.py,,1,6
survived,"def _lambda1():
    draw.get(1)()
    draw.get(6)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,1,6
survived,"def _lambda5():
    draw.get(10)()
    draw.get(60)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,0,6
survived,"def _lambda12():
    draw.get(1000)()
    draw.get(4000)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,0,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-the-coins-2.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/cramers-rule.py,,1,6
survived,"def add(a, b):
    return newFps(lambda n: extract(a, n) + extract(b, n))
",tests/rosetta/transpiler/Python/formal-power-series.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/four-is-magic.py,,1,6
survived,"def padFloat5(x, width):
    s = fmtF5(x)
    while len(s) < width:
        s = "" "" + s
    return s
",tests/rosetta/transpiler/Python/formal-power-series.py,,1,6
survived,"def sortPairs(xs):
    arr = xs
    i = 1
    while i < len(arr):
        j = i
        while j > 0 and (int(arr[j - 1].get(""count""))) < (int(arr[j].get(""count""))):
            tmp = arr[j - 1]
            arr[j - 1] = arr[j]
            arr[j] = tmp
            j = j - 1
        i = i + 1
    return arr
",tests/rosetta/transpiler/Python/function-frequency.py,,1,6
survived,"def _lambda2(n):
    b0 = extract(b, 0)
    if b0 == 0.0:
        return (0.0 / 0.0)
    s = extract(a, n)
    k = 1
    while k <= n:
        s = s - extract(b, k) * extract(q, n - k)
        k = k + 1
    return s // b0
",tests/rosetta/transpiler/Python/formal-power-series.py,,0,6
survived,"def one():
    return newFps(_lambda0)
",tests/rosetta/transpiler/Python/formal-power-series.py,,0,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    nums = [0, 4, 6, 11, 13, 75, 100, 337, -164, 9223372036854775807]
    for n in nums:
        print(fourIsMagic(n))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/four-is-magic.py,,1,7
survived,"def countLetters(s):
    cnt = 0
    i = 0
    while i < len(s):
        ch = s[i:i + 1]
        if ch >= ""A"" and ch <= ""Z"" or ch >= ""a"" and ch <= ""z"":
            cnt = cnt + 1
        i = i + 1
    return cnt
",tests/rosetta/transpiler/Python/four-is-the-number-of-letters-in-the-....py,,1,7
survived,"def configure() -> None:
    """"""Initialise tracing and metrics if the SDK is installed.""""""
    global tracer, meter
    if trace is None:
        return

    endpoint = os.getenv(""OTEL_EXPORTER_OTLP_ENDPOINT"")
    if endpoint:
        span_exporter = OTLPSpanExporter(endpoint=endpoint)
        metric_exporter = OTLPMetricExporter(endpoint=endpoint)
    else:
        span_exporter = ConsoleSpanExporter()
        metric_exporter = ConsoleMetricExporter()

    resource = Resource.create({""service.name"": ""alpha-insight""})
    provider = TracerProvider(resource=resource)
    provider.add_span_processor(BatchSpanProcessor(span_exporter))
    trace.set_tracer_provider(provider)
    tracer = trace.get_tracer(""alpha_insight"")

    meter_provider = MeterProvider(
        resource=resource,
        metric_readers=[PeriodicExportingMetricReader(metric_exporter)],
    )
    metrics.set_meter_provider(meter_provider)
    meter = metrics.get_meter(""alpha_insight"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/tracing.py,,1,7
survived,"    def test_missing_spec_skips_check(self) -> None:
        fake_mod = types.SimpleNamespace(__spec__=None)

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return importlib.import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            if name == ""agents"":
                return None
            return importlib.util.find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
        ):
            self.assertTrue(preflight.check_openai_agents_version())
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion,1,7
survived,"    async def step(self) -> None:
        await self.publish(""alpha.risk"", {""risk"": ""risk level nominal""})
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,AlphaRiskAgent,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_in_operator.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/closure.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/tail_recursion.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/left_join.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/user_type_literal.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/in_operator_extended.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/avg_builtin.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/typed_var.py,,1,7
survived,"def test_archive_ls_outputs_entries(tmp_path: Path) -> None:
    runner = CliRunner()
    from unittest.mock import patch

    class DummyArchive:
        def __init__(self, path: str) -> None:
            self.path = path

        def list_entries(self) -> list[tuple[int, str, str, int]]:
            return [(1, ""foo.tar"", ""deadbeef"", 1)]

    with patch.object(cli, ""HashArchive"", return_value=DummyArchive(""db"")):
        result = runner.invoke(cli.main, [""archive"", ""ls"", ""--db"", str(tmp_path / ""a.db"")])

    assert result.exit_code == 0
    assert ""deadbeef"" in result.output
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,,1,7
survived,"def test_service_worker_exists() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist""
    assert (dist / ""service-worker.js"").is_file()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_service_worker_present.py,,0,7
survived,"def download_with_retry(
    cid: str,
    path: Path,
    fallback: str | None = None,
    attempts: int = 3,
    label: str | None = None,
) -> None:
    last_exc: Exception | None = None
    lbl = label or str(path)
    for i in range(1, attempts + 1):
        try:
            download(cid, path, fallback)
            return
        except Exception as exc:  # noqa: PERF203
            last_exc = exc
            if i < attempts:
                print(f""Attempt {i} failed for {lbl}: {exc}, retrying..."")
            else:
                print(f""ERROR: could not fetch {lbl} after {attempts} attempts"")
    if last_exc:
        raise last_exc
",scripts/fetch_assets.py,,1,7
survived,"def test_sync_sqs(mock_get_attrs, mock_get_list, neo4j_session):
    boto3_session = MagicMock()
    create_test_account(neo4j_session, TEST_ACCOUNT_ID, TEST_UPDATE_TAG)

    sync(
        neo4j_session,
        boto3_session,
        [TEST_REGION],
        TEST_ACCOUNT_ID,
        TEST_UPDATE_TAG,
        {""UPDATE_TAG"": TEST_UPDATE_TAG, ""AWS_ID"": TEST_ACCOUNT_ID},
    )

    assert check_nodes(
        neo4j_session,
        ""SQSQueue"",
        [""arn""],
    ) == {
        (""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-1"",),
        (""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-2"",),
    }

    assert check_rels(
        neo4j_session,
        ""AWSAccount"",
        ""id"",
        ""SQSQueue"",
        ""arn"",
        ""RESOURCE"",
        rel_direction_right=True,
    ) == {
        (
            TEST_ACCOUNT_ID,
            ""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-1"",
        ),
        (
            TEST_ACCOUNT_ID,
            ""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-2"",
        ),
    }

    assert check_rels(
        neo4j_session,
        ""SQSQueue"",
        ""arn"",
        ""SQSQueue"",
        ""arn"",
        ""HAS_DEADLETTER_QUEUE"",
        rel_direction_right=True,
    ) == {
        (
            ""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-1"",
            ""arn:aws:secretsmanager:us-east-1:000000000000:test-queue-2"",
        ),
    }",tests/integration/cartography/intel/aws/test_sqs.py,,1,7
survived,"    async def run_cycle(self) -> None:
        """"""Generate and emit an ADK summary when data is available.""""""
        with span(""summariser.run_cycle""):
            if not self._records or not self.adk:
                return
            try:
                summary = self.adk.generate_text(""\n"".join(self._records))
            except Exception:
                return
            await self.emit(""strategy"", {""summary"": summary})
            self._records.clear()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_summariser_agent.py,ADKSummariserAgent,1,7
survived,"    async def skill_test(self, payload: dict) -> dict:
        """"""Execute a diagnostic skill test.

        Agents may override this method to provide custom behaviour.
        The default implementation returns ``{""ok"": True}``.
        """"""
        return {""ok"": True}
",alpha_factory_v1/backend/agents/base.py,AgentBase,1,7
survived,"    async def _skill_test(request: Request, name: str):
        payload = await request.json()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""skill_test""):
            raise HTTPException(501, ""Agent does not support skill_test"")
        return await inst.skill_test(payload)  # type: ignore[func-returns-value]
",alpha_factory_v1/backend/orchestrator.py,,1,7
survived,"def xor_checksum(address: int, sig: Signal, d: bytearray) -> int:
    checksum = 0
    checksum_byte = sig.start_bit // 8
    for i in range(len(d)):
        if i != checksum_byte:
            checksum ^= d[i]
    return checksum
",opendbc/can/packer.py,,1,7
survived,"    def test_no_cache_header_for_recent_content(self):
        recent_entry = EntryFactory(created=timezone.now())
        response = self.client.get(recent_entry.get_absolute_url())
        assert ""cache-control"" not in response.headers
",blog/tests.py,BlogTests,0,7
survived,"def create_scheduler(
    *, db_path: str | Path | None = None, out_file: str | Path = ""archive_root.json""
) -> Rocketry | None:
    """"""Return a ``Rocketry`` app publishing the archive root daily.""""""
    if Rocketry is None or daily is None:
        return None

    app = Rocketry(execution=""async"")

    @app.task(daily)
    def _job() -> None:  # pragma: no cover - Rocketry callback
        publish_root(db_path=db_path, out_file=out_file)

    return app
",src/archive/cron.py,,1,7
survived,"def _compute_root(hashes: Iterable[str]) -> str:
    nodes = [hashlib.sha256(h.encode()).digest() for h in sorted(hashes)]
    if not nodes:
        return """"
    while len(nodes) > 1:
        if len(nodes) % 2 == 1:
            nodes.append(nodes[-1])
        nodes = [
            hashlib.sha256(nodes[i] + nodes[i + 1]).digest()
            for i in range(0, len(nodes), 2)
        ]
    return nodes[0].hex()
",src/archive/archive.py,,1,7
survived,"    def uop(self, op):
        if isinstance(op, ast.USub):
            return ""-""
        if isinstance(op, ast.Not):
            return ""not ""
        return """"
",tools/any2mochi/py_simple.py,Conv,1,6
survived,"    def __init__(self, name: str, cycle_seconds: int, max_cycle_sec: int, publish: callable):
        self.name = name
        self.inst = get_agent(name)
        self.period = getattr(self.inst, ""CYCLE_SECONDS"", cycle_seconds)
        self.spec = getattr(self.inst, ""SCHED_SPEC"", None)
        self.next_ts = 0.0
        self.last_beat = time.time()
        self.task: Optional[asyncio.Task] = None
        self._max_cycle_sec = max_cycle_sec
        self._publish = publish
        self._calc_next()

        with contextlib.suppress(ModuleNotFoundError):
            from openai.agents import AgentContext  # type: ignore[attr-defined]

            if isinstance(self.inst, AgentContext):
                from .telemetry import tracer  # avoid circular import
                from openai.agents import AgentRuntime  # type: ignore[attr-defined]

                runtime = AgentRuntime()
                runtime.register(self.inst)
                atexit.register(runtime.close)
",alpha_factory_v1/backend/agent_manager.py,AgentRunner,1,7
survived,"async def maybe_await(fn, *a, **kw):  # type: ignore
    return await fn(*a, **kw) if asyncio.iscoroutinefunction(fn) else await asyncio.to_thread(fn, *a, **kw)
",alpha_factory_v1/backend/agent_manager.py,,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by.py,Person,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/join_multi.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_having.py,Person,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/left_join_multi.py,Order,1,6
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join_sort.py,Lineitem,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join.py,Supplier,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/sort_stable.py,Item,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/save_jsonl_stdout.py,Person,1,7
survived,"        def run(self) -> None:
            print(""OpenAI Agents bridge disabled."")
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AgentRuntime,1,7
survived,"    def __eq__(self, other):
        if isinstance(other, _UTCOffset):
            return self.minutes == other.minutes
        return NotImplemented
",hl7/datatypes.py,_UTCOffset,1,7
survived,"    def test_parse_negative_zero_offset(self):
        dt = parse_datetime(""201403111412-0030"")
        self.assertEqual(dt.tzinfo, _UTCOffset(-30))",tests/test_datetime.py,DatetimeTest,1,7
survived,"    async def run() -> None:
        bus.subscribe(""b"", lambda e: received.append(e))
        await bus.start()
        try:
            creds = grpc.ssl_channel_credentials(root_certificates=ca)
            async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                stub = ch.unary_unary(""/bus.Bus/Send"")
                payload = {
                    ""sender"": ""a"",
                    ""recipient"": ""b"",
                    ""payload"": {""v"": 1},
                    ""ts"": 0.0,
                    ""token"": token,
                }
                await stub(json.dumps(payload).encode())
            await asyncio.sleep(0.05)
        finally:
            await bus.stop()
            shutil.rmtree(tmp_path / ""certs"", ignore_errors=True)
",tests/test_bus_ssl_gen.py,,1,6
survived,"    def _install_from_url(dep: RuntimeDependency, logger: LanguageServerLogger, target_dir: str) -> None:
        if dep.archive_type == ""gz"" and dep.binary_name:
            dest = os.path.join(target_dir, dep.binary_name)
            FileUtils.download_and_extract_archive(logger, dep.url, dest, dep.archive_type)
        else:
            FileUtils.download_and_extract_archive(logger, dep.url, target_dir, dep.archive_type or ""zip"")",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection,1,6
survived,"    def _fail_net() -> bool:
        raise AssertionError(""has_network called"")
",tests/test_check_env_network.py,,1,6
survived,"def test_pdf_copied_after_build(tmp_path: Path) -> None:
    dist = BROWSER_DIR / ""dist""
    pdf = dist / ""insight_browser_quickstart.pdf""
    if pdf.exists():
        pdf.unlink()
    result = subprocess.run([
        ""npm"",
        ""run"",
        ""build"",
    ], cwd=BROWSER_DIR, capture_output=True, text=True)
    assert result.returncode == 0, result.stderr
    assert pdf.exists(), ""insight_browser_quickstart.pdf missing in dist""
",tests/test_build_quickstart_pdf.py,,1,7
survived,"def collect_entries() -> list[tuple[str, str, str]]:
    entries: list[tuple[str, str, str]] = []
    for page in sorted(DEMOS_DIR.glob(""*.md"")):
        entries.append(parse_page(page))
    return entries
",scripts/generate_gallery_html.py,,1,7
survived,"def test_git_manager_init_and_commit(tmp_path: Path) -> None:
    gm = GitManager(tmp_path)
    gm.init()
    (tmp_path / ""foo.txt"").write_text(""hi"")
    sha = gm.commit_all(""init"")

    assert (tmp_path / "".git"").exists()
    out = subprocess.check_output(
        [""git"", ""-C"", str(tmp_path), ""rev-parse"", ""HEAD""], text=True
    ).strip()
    assert out == sha
",tests/test_git_utils.py,,1,7
survived,"    def git_available() -> bool:
        """"""Return True if the ``git`` executable can be found.""""""
        return shutil.which(""git"") is not None
",src/meta_agent/git_utils.py,GitManager,1,7
survived,"    def init(self) -> None:
        """"""Initialize a new repository if one does not already exist.""""""
        if (self.repo_dir / "".git"").exists():
            return
        self.repo_dir.mkdir(parents=True, exist_ok=True)
        self._run(""init"")
        self._run(""config"", ""user.name"", ""meta-agent"")
        self._run(""config"", ""user.email"", ""meta-agent@example.com"")
        self._run(""branch"", ""-M"", ""main"")
",src/meta_agent/git_utils.py,GitManager,1,7
survived,"def test_git_manager_init_and_commit(tmp_path: Path) -> None:
    gm = GitManager(tmp_path)
    gm.init()
    (tmp_path / ""foo.txt"").write_text(""hi"")
    sha = gm.commit_all(""init"")

    assert (tmp_path / "".git"").exists()
    out = subprocess.check_output(
        [""git"", ""-C"", str(tmp_path), ""rev-parse"", ""HEAD""], text=True
    ).strip()
    assert out == sha
",tests/test_git_utils.py,,1,7
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def __init__(self, *args: Any, **kwargs: Any) -> None:
        self.chat = _Chat()",src/meta_agent/services/openai_stub.py,OpenAI,1,7
survived,"def dayUnique(b, list):
    c = 0
    for x in list:
        if x.day == b.day:
            c = c + 1
    return c == 1
",tests/rosetta/transpiler/Python/cheryls-birthday.py,,1,7
survived,"def sqrtApprox(x):
    guess = x
    i = 0
    while i < 20:
        guess = (guess + x / guess) / 2.0
        i = i + 1
    return guess
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,,1,8
survived,"def bigFromInt(x):
    if x == 0:
        return [0]
    digits = []
    n = x
    while n > 0:
        digits = digits + [n % 10]
        n = n // 10
    return digits
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,,1,6
survived,"def horiz(c1, c2, r):
    c = c1
    while c <= c2:
        n[r][c] = ""x""
        c = c + 1
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,1,6
survived,"def ccNumbers(start, end):
    n = start
    while n <= end:
        m = 1
        if n > 4:
            m = pow2(n - 4)
        while True:
            num = ccFactors(n, m)
            if len(num) > 0:
                print(""a("" + str(n) + "") = "" + bigToString(num))
                break
            if n <= 4:
                m = m + 1
            else:
                m = m + pow2(n - 4)
        n = n + 1
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,,1,6
survived,"def isPrime(n):
    if n < 2:
        return False
    if n % 2 == 0:
        return n == 2
    if n % 3 == 0:
        return n == 3
    d = 5
    while d * d <= n:
        if n % d == 0:
            return False
        d = d + 2
        if n % d == 0:
            return False
        d = d + 4
    return True
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,,1,7
survived,"def main():
    print(quibble([]))
    print(quibble([""ABC""]))
    print(quibble([""ABC"", ""DEF""]))
    print(quibble([""ABC"", ""DEF"", ""G"", ""H""]))
",tests/rosetta/transpiler/Python/comma-quibbling.py,,0,6
survived,"def monthUnique(b, list):
    c = 0
    for x in list:
        if x.month == b.month:
            c = c + 1
    return c == 1
",tests/rosetta/transpiler/Python/cheryls-birthday.py,,1,6
survived,"def sqrtApprox(x):
    g = x
    i = 0
    while i < 40:
        g = (g + x / g) / 2.0
        i = i + 1
    return g
",tests/rosetta/transpiler/Python/circles-of-given-radius-through-two-points.py,,1,7
survived,"def bigToString(a):
    s = """"
    i = len(a) - 1
    while i >= 0:
        s = s + str(a[i])
        i = i - 1
    return s
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,,1,6
survived,"def hullStr(h):
    s = ""[""
    i = 0
    while i < len(h):
        s = s + pointStr(h[i])
        if i < len(h) - 1:
            s = s + "" ""
        i = i + 1
    s = s + ""]""
    return s
",tests/rosetta/transpiler/Python/convex-hull.py,,1,6
survived,"def test_app_state_cache_management_and_stats():
    state = AppState()
    batch = ""b1""
    df = pd.DataFrame({
        ""metric_name"": [""m1"", ""m1"", ""m2"", ""m2""],
        ""metric_alert"": [1, 0, 0, 1],
        ""metric_score"": [0.5, 0.7, 0.3, 0.2],
        ""thumbsup_sum"": [1, 2, 3, 4],
        ""thumbsdown_sum"": [0, 1, 2, 3],
    })
    state.df_cache[batch] = df

    state.calculate_metric_stats(batch)

    stats = state.stats_cache[batch]
    assert len(stats) == 2
    assert stats[0][""metric_name""] == ""m1""
    assert stats[0][""anomaly_rate""] == pytest.approx(0.5)
    assert stats[0][""avg_score""] == pytest.approx(0.6)

    state.clear_batch_cache(batch)
    assert batch not in state.df_cache
    assert batch not in state.chart_cache
    assert batch not in state.stats_cache",tests/test_dashboard.py,,1,7
survived,"    def fail(*args, **kwargs):
        raise OSError(""boom"")
",tests/ux/test_cli_output.py,,0,7
survived,"    def error_suggestion(self, error_message: str) -> str | None:
        """"""Return a suggestion string for the given error message and output it.""""""
        suggestions = {
            ""failed to load"": ""Check that the file path exists and is readable."",
            ""network"": ""Ensure your internet connection is available."",
        }
        error_lower = error_message.lower()
        for token, suggestion in suggestions.items():
            if token in error_lower:
                self.cli_output.info(f""Suggestion: {suggestion}"")
                return suggestion
        return None
",src/meta_agent/ux/user_feedback.py,UserFeedback,1,7
survived,"    def refresh_metadata(self) -> None:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        self._metadata = BundleMetadata(**data)
",src/meta_agent/bundle.py,Bundle,1,7
survived,"def test_show_results_closes_ledger(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.__enter__.return_value = led
            led.__exit__.side_effect = lambda *_: led.close()
            led.tail.return_value = [{""ts"": 1.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            CliRunner().invoke(cli.main, [""show-results""])
        led.close.assert_called_once()
",tests/test_demo_cli.py,,1,7
survived,"        async def invoke_tool(self, name: str, args: dict[str, object] | None = None) -> object:
            args = args or {}
            self.called.append((name, args))
            return {""ok"": True}
",tests/test_adapters.py,StubMCP,1,6
survived,"def test_fuzz_envelope_blocks_malicious(sender: str, recipient: str, ts: float, payload: dict[str, object]) -> None:
    code = payload[""code""]
    assume(""import os"" in code)
    bus = DummyBus(config.Settings(bus_port=0))
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(sender, recipient, payload, ts)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_safety_guardian_property.py,,0,6
survived,"        async def send_transaction(self, tx: object, *args: object) -> None:
            captured[""data""] = tx.instructions[0].data.decode()
",tests/test_safety_guardian_property.py,DummyClient,0,6
survived,"        async def close(self) -> None:  # pragma: no cover - dummy
            pass
",tests/test_safety_guardian_property.py,DummyClient,0,7
survived,"def test_run_macro_demo_no_offline(tmp_path: Path) -> None:
    """"""`OPENAI_API_KEY` disables the offline profile.""""""
    docker_log, _ = _run_script(tmp_path, env={""OPENAI_API_KEY"": ""dummy-key""})
    assert ""--profile offline"" not in docker_log
",tests/test_macro_launcher.py,,1,7
survived,"            def __init__(self, *a, **kw):
                pass
",alpha_factory_v1/demos/aiga_meta_evolution/utils.py,OpenAIAgent,1,6
survived,"        def __init__(self) -> None:
            self.app = type(""app"", (), {""middleware"": lambda *_a, **_kw: lambda f: f})
",stubs/google_adk/__init__.py,Router,1,6
survived,"    async def run() -> None:
        await asyncio.gather(
            orch.evolve(""a"", fn, 1, experiment_id=""exp1"", population_size=2, generations=1),
            orch.evolve(""b"", fn, 1, experiment_id=""exp2"", population_size=2, generations=1),
        )
",tests/test_experiments.py,,1,6
survived,"def test_health() -> None:
    client = TestClient(app)
    response = client.get(""/health"")
    assert response.status_code == 200
    assert response.json() == {""status"": ""ok""}",backend/tests/test_main.py,,1,7
survived,"def _verify_wheel(path: Path) -> bool:
    """"""Return ``True`` if the wheel's signature is valid.""""""
    sig_path = path.with_suffix(path.suffix + "".sig"")
    if not sig_path.is_file():
        logger.error(""Missing .sig file for %s"", path.name)
        return False
    if ed25519 is None:
        logger.error(""cryptography library required for signature checks"")
        return False
    try:
        sig_b64 = sig_path.read_text().strip()
        expected = _WHEEL_SIGS.get(path.name)
        if expected and expected != sig_b64:
            logger.error(""Signature mismatch for %s"", path.name)
            return False
        pub_bytes = base64.b64decode(_WHEEL_PUBKEY)
        signature = base64.b64decode(sig_b64)
        ed25519.Ed25519PublicKey.from_public_bytes(pub_bytes).verify(signature, path.read_bytes())
        return True
    except InvalidSignature:
        logger.error(""Invalid signature for %s"", path.name)
    except Exception:
        logger.exception(""Signature verification failed for %s"", path.name)
    return False
",alpha_factory_v1/backend/agents/__init__.py,,1,7
survived,"    def tearDown(self):
        self._cm.__exit__(None, None, None)
",alpha_factory_v1/tests/test_memory_provider.py,MemoryFabricFallbackTest,1,7
survived,"def main(argv: List[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Process files with Attachments DSL"")
    parser.add_argument(""paths"", nargs=""*"", help=""Files, URLs or directories to process"")
    parser.add_argument(""-c"", ""--cwd"", help=""Change working directory before processing"")
    parser.add_argument(""-q"", ""--quiet"", action=""store_true"", help=""Silence verbose logs"")
    parser.add_argument(
        ""-y"",
        ""--copy"",
        action=""store_true"",
        help=""Copy result text to clipboard using to_clipboard_text"",
    )
    parser.add_argument(""--prompt"", default="""", help=""Prompt when copying to clipboard"")

    args, extra = parser.parse_known_args(argv)

    if args.cwd:
        os.chdir(args.cwd)

    set_verbose(not args.quiet)

    dsl_fragment = _build_dsl(extra)
    paths = args.paths or ["".""]
    paths_with_dsl = [p + dsl_fragment for p in paths]

    try:
        ctx = Attachments(*paths_with_dsl)
        if args.copy:
            adapt.to_clipboard_text(ctx, prompt=args.prompt)
        else:
            output = str(ctx)
            if output:
                print(output)
    except Exception as exc:
        print(f""Error running attachments CLI: {exc}"", file=sys.stderr)
        sys.exit(1)
",src/attachments/cli.py,,1,7
survived,"def test_simulate_save_plots(tmp_path: Path) -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            with runner.isolated_filesystem(temp_dir=tmp_path):
                res = runner.invoke(
                    cli.main,
                    [
                        ""simulate"",
                        ""--horizon"",
                        ""1"",
                        ""--offline"",
                        ""--sectors"",
                        ""1"",
                        ""--pop-size"",
                        ""1"",
                        ""--generations"",
                        ""1"",
                        ""--save-plots"",
                    ],
                )
    assert res.exit_code == 0
    assert Path(""pareto.png"").exists()
    assert Path(""pareto.json"").exists()",tests/test_demo_cli.py,,0,6
survived,"        def __init__(self, text):
            self.text = text
",no-ocr-api/tests/test_ingest_search.py,FakePage,1,7
survived,"def test_apply_diff_failure_returns_output():
    with tempfile.TemporaryDirectory() as repo:
        open(os.path.join(repo, ""file.txt""), ""w"").close()
        success, output = diff_utils.apply_diff(""bad diff"", repo_dir=repo)
        assert not success
        assert ""patch"" in output.lower()
",tests/test_diff_utils_apply.py,,1,7
survived,"        async def start(self) -> None:
            return None
",tests/test_bus_large_payloads_property.py,Prod,0,7
survived,"        async def stop(self) -> None:
            return None
",tests/test_bus_large_payloads_property.py,Prod,1,6
survived,"def test_workbox_integrity() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    dist = browser_dir / ""dist""
    index = dist / ""index.html""
    expected = sha384(dist / ""workbox-sw.js"")
    url = index.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        integrity = page.get_attribute(""script[src='workbox-sw.js']"", ""integrity"")
        assert integrity == expected
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_workbox_integrity.py,,0,6
survived,"def sha384(path: Path) -> str:
    digest = hashlib.sha384(path.read_bytes()).digest()
    return ""sha384-"" + base64.b64encode(digest).decode()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,7
survived,"def main(config: SampleLmConfig):
    levanter.initialize(config)
    tokenizer = load_tokenizer(config.tokenizer)

    vocab_size = len(tokenizer)
    Vocab = round_axis_for_partitioning(Axis(""vocab"", vocab_size), config.trainer.compute_axis_mapping)

    key = jrandom.PRNGKey(0)

    with config.trainer.device_mesh, hax.axis_mapping(config.trainer.parameter_axis_mapping):
        model = _load_model(config, Vocab, key=key)
        assert isinstance(model, LlamaLMHeadModel), ""Only LlamaLMHeadModel supported""

        sampler = Sampler(Vocab)

        prompt_ids = tokenizer.encode(config.prompt, add_special_tokens=False)
        prompt_axis = Axis(""position"", len(prompt_ids))
        prompt_tokens = hax.NamedArray(jnp.array(prompt_ids, dtype=jnp.int32), axes=(prompt_axis,))

        page_table = PageTable.init(
            max_pages=1,
            max_seqs=1,
            page_size=len(prompt_ids) + config.max_new_tokens,
            max_pages_per_seq=1,
        )
        page_table, seq_id = page_table.assign_seq_id_to_seq()
        cache = model.initial_cache(page_table, dtype=jnp.float32)

        page_table, binfo = page_table.allocate_for_seqs(
            updated_seqs=hax.named([seq_id], ""seq""),
            new_counts=hax.named([len(prompt_ids)], ""seq""),
            tokens=hax.named([seq_id] * len(prompt_ids), prompt_axis),
        )
        state = KvPageState.from_batch(binfo, cache)
        pos_ids = hax.arange(prompt_axis, dtype=jnp.int32)
        _, state = model.decode(prompt_tokens, state, pos_ids)

        generated = list(prompt_ids)
        temps = hax.full(Axis(""batch"", 1), config.temperature, dtype=jnp.float32)

        for i in range(config.max_new_tokens):
            page_table, binfo = page_table.allocate_for_seqs(
                updated_seqs=hax.named([seq_id], ""seq""),
                new_counts=hax.named([1], ""seq""),
                tokens=hax.named([seq_id], Axis(""position"", 1)),
            )
            state = KvPageState.from_batch(binfo, state.cache)
            pos_id = hax.arange(Axis(""position"", 1), start=len(generated))
            logits, state = model.decode(
                hax.NamedArray(jnp.array([generated[-1]], dtype=jnp.int32), axes=(Axis(""position"", 1),)),
                state,
                pos_id,
            )
            logits = logits[""position"", 0]
            tok, _ = sampler(logits, temps, key=jrandom.PRNGKey(i + 1))
            next_token = int(tok.array)
            generated.append(next_token)

        text = tokenizer.decode(generated, skip_special_tokens=True)
        print(text)
",src/levanter/main/sample_lm.py,,1,6
survived,"        def __init__(self) -> None:
            self.status_code = 200
",tests/test_start_alpha_business.py,Resp,1,7
survived,"def _call(
    method: str,
    url: str,
    *,
    params: dict | None = None,
    json: dict | None = None,
    data: dict | bytes | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    if params:
        query = _parse.urlencode(params, doseq=True)
        url += (""&"" if ""?"" in url else ""?"") + query

    body = None
    req_headers = {""User-Agent"": _UA, **(headers or {})}
    if json is not None:
        body = _json.dumps(json).encode()
        req_headers.setdefault(""Content-Type"", ""application/json"")
    elif data is not None:
        if isinstance(data, (bytes, bytearray)):
            body = data
        else:
            body = _parse.urlencode(data).encode()
            req_headers.setdefault(""Content-Type"", ""application/x-www-form-urlencoded"")

    req = _request.Request(url, data=body, headers=req_headers, method=method)
    try:
        with _request.urlopen(req, timeout=timeout) as resp:
            content = resp.read()
            resp_headers = dict(resp.headers.items())
            return Response(resp.getcode(), content, resp_headers, url)
    except _error.HTTPError as exc:
        content = exc.read()
        resp_headers = dict(exc.headers.items()) if hasattr(exc, ""headers"") else {}
        return Response(exc.code, content, resp_headers, url)
    except _error.URLError as exc:  # pragma: no cover - network issues
        if isinstance(getattr(exc, ""reason"", None), TimeoutError):
            raise Timeout(str(exc.reason))
        raise RequestException(str(exc))
",alpha_factory_v1/af_requests.py,,1,8
survived,"def test_versioning_diff_and_rollback(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    meta = _meta()
    reg.register(meta, ""hello {{name}}"", version=""0.1.0"")
    reg.register(meta, ""hi {{name}}"", version=""0.2.0"")

    diff = reg.diff(""greet"", ""0.1.0"", ""0.2.0"")
    assert ""-hello {{name}}"" in diff
    assert ""+hi {{name}}"" in diff

    reg.rollback(""greet"", ""0.1.0"")
    assert reg.list_templates()[0][""current_version""] == ""0.1.0""
    assert reg.load_template(""greet"") == ""hello {{name}}""",tests/test_template_registry.py,,1,7
survived,"def test_insight_helm_template_renders_env_vars() -> None:
    result = subprocess.run(
        [""helm"", ""template"", ""insight"", str(CHART_DIR), ""-f"", str(VALUES_FILE)],
        check=True,
        cwd=CHART_DIR,
        capture_output=True,
        text=True,
    )
    rendered = result.stdout
    assert ""OPENAI_API_KEY"" in rendered
    assert ""AGI_INSIGHT_OFFLINE"" in rendered
    assert ""AGI_INSIGHT_BUS_PORT"" in rendered
    assert ""AGI_INSIGHT_LEDGER_PATH"" in rendered",tests/test_insight_helm_template.py,,0,7
survived,"    async def restart(self, bus: messaging.A2ABus, ledger: Ledger) -> None:
        if self.task:
            self.task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self.task
        self.agent = self.agent.__class__(bus, ledger)
        self.start(bus, ledger)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,AgentRunner,1,7
survived,"    async def _on_orch(self, env: messaging.Envelope) -> None:
        if env.payload.get(""heartbeat"") and env.sender in self.runners:
            self.runners[env.sender].last_beat = env.ts
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator,1,7
survived,"    def test_rpc_auth(self) -> None:
        self.settings.bus_token = ""s3cr3t""
        bus = messaging.A2ABus(self.settings)

        class Ctx:
            def abort(self, *_a, **_kw):
                raise RuntimeError(""denied"")

        payload = {
            ""sender"": ""a"",
            ""recipient"": ""b"",
            ""payload"": {},
            ""ts"": 0.0,
            ""token"": ""s3cr3t"",
        }
        asyncio.run(bus._handle_rpc(json.dumps(payload).encode(), Ctx()))
",tests/test_insight_orchestrator_features.py,TestMessaging,1,6
survived,"def test_dtype_and_axes_annotation():
    def foo(x: f32[""batch embed""]):  # type: ignore  # noqa: F722
        pass

    ann = typing.get_args(typing.get_type_hints(foo, include_extras=True)[""x""])
    assert ann[0] is NamedArray
    spec = ann[1]
    assert spec.dtype == jnp.float32
    assert spec.before == (""batch"", ""embed"")
",tests/test_dtype_typing.py,,1,7
survived,"def _ensure_assets() -> None:
    placeholders = []
    for p in (bundle_path, workbox_path):
        if p.exists():
            data = p.read_text(errors=""ignore"")
            if ""placeholder"" in data.lower():
                placeholders.append(p)
        else:
            placeholders.append(p)
    if placeholders:
        print(""Fetching missing browser assets..."")
        subprocess.run(
            [sys.executable, str(repo_root / ""scripts/fetch_assets.py"")],
            check=True,
        )
        for p in placeholders:
            if not p.exists() or ""placeholder"" in p.read_text(errors=""ignore"").lower():
                sys.exit(f""Failed to download {p.relative_to(ROOT)}"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,7
survived,"def test_mutate_cleanup_nested(server: str) -> None:
    """"""Creating nested files should be cleaned up.""""""
    import io
    import tarfile

    before = set(evolution_worker.STORAGE_PATH.iterdir()) if evolution_worker.STORAGE_PATH.exists() else set()

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""dir1/dir2/file.txt"")
        data = b""nested""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""nested.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 200

    after = set(evolution_worker.STORAGE_PATH.iterdir()) if evolution_worker.STORAGE_PATH.exists() else set()
    assert before == after",tests/test_evolution_worker.py,,1,7
survived,"def convert_corpus(corpus, index):
    new_corpus = []
    for doc in corpus:
        new_corpus.append([index[w] for w in doc])
    return new_corpus
",scripts/bbc_demo.py,,1,6
survived,"def parse_requirements(path: Path) -> list[str]:
    """"""Parse a requirements file, resolving ``-r`` inclusions.""""""
    requirements: list[str] = []
    for line in path.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""-r ""):
            nested = (path.parent / line.split(maxsplit=1)[1]).resolve()
            requirements.extend(parse_requirements(nested))
        else:
            requirements.append(line)
    return requirements
",pioreactor/tests/test_requirements_sync.py,,1,7
survived,"def parse_setup_list(path: Path, name: str) -> list[str]:
    """"""Return list assigned to ``name`` in ``setup.py``.""""""
    tree = ast.parse(path.read_text())
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign):
            if any(isinstance(t, ast.Name) and t.id == name for t in node.targets):
                return [ast.literal_eval(elt) for elt in node.value.elts]
    raise AssertionError(f""{name} not found"")
",pioreactor/tests/test_requirements_sync.py,,1,6
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return int(s.getsockname()[1])
",tests/test_metrics.py,,1,7
survived,"def test_metrics_endpoint_subprocess() -> None:
    port = _free_port()
    proc = _start_server(port)
    url = f""http://127.0.0.1:{port}""
    try:
        _wait_ready(url)
        resp = httpx.get(f""{url}/metrics"")
        assert resp.status_code == 200
        text = resp.text
        assert ""api_requests_total"" in text
        assert ""api_request_duration_seconds"" in text
        assert text.startswith(""# HELP"")
    finally:
        proc.terminate()
        proc.wait(timeout=5)
",tests/test_metrics.py,,1,7
survived,"    def add_rule(self, rule: GuardrailRule) -> None:
        """"""Add a new rule to the configuration.""""""

        self.rules.append(rule)
",src/meta_agent/generators/guardrail_generator.py,GuardrailConfig,1,7
survived,"def test_guardrail_rule_validation():
    rule = GuardrailRule(name=""no-secrets"", pattern=r""secret"")
    assert rule.action is GuardrailAction.DENY

    with pytest.raises(ValueError):
        GuardrailRule(name=""bad"", pattern=""("")
",tests/test_guardrail_generator.py,,1,8
survived,"    def __init__(self):
        self.allowed_tags = [
            ""p"",""br"",""strong"",""em"",""h1"",""h2"",""h3"",""h4"",""h5"",""h6"",
            ""ul"",""ol"",""li"",""blockquote"",""code"",""pre"",
            ""a"",""img"",""hr"",""table"",""thead"",""tbody"",""tr"",""th"",""td""
        ]
        self.allowed_attributes = {
            ""a"": [""href"", ""title""],
            ""img"": [""src"", ""alt"", ""title"", ""width"", ""height""],
        }
        self.allowed_protocols = [""http"", ""https"", ""mailto""]
        self.md = Markdown(extras=[""fenced-code-blocks""])
",app/utils/markdown_renderer.py,SafeMarkdownRenderer,1,7
survived,"    async def run_cycle(self) -> None:
        if self.first:
            self.first = False
            raise RuntimeError(""boom"")
        await asyncio.sleep(0)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,BoomAgent,0,7
survived,"    def _record_restart(self, runner: AgentRunner) -> None:
        env = messaging.Envelope(
            ""orch"",
            ""system"",
            {""event"": ""restart"", ""agent"": runner.agent.name},
            time.time(),
        )
        self.ledger.log(env)
        self.bus.publish(""system"", env)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator,1,7
survived,"        def __init__(self, *_a, **_kw) -> None:
            pass
",tests/test_agents.py,DummyLedger,0,7
survived,"    def Field(default: Any, **_: Any) -> Any:  # type: ignore
        return default
",alpha_factory_v1/backend/memory_fabric.py,,1,6
survived,"def test_Q1_aggregates_revenue_and_quantity_by_returnflag___linestatus():
    assert result == [
        {
            ""returnflag"": ""N"",
            ""linestatus"": ""O"",
            ""sum_qty"": 53,
            ""sum_base_price"": 3000,
            ""sum_disc_price"": 950 + 1800,
            ""sum_charge"": 950 * 1.07 + 1800 * 1.05,
            ""avg_qty"": 26.5,
            ""avg_price"": 1500,
            ""avg_disc"": 0.07500000000000001,
            ""count_order"": 2,
        }
    ]
",tests/machine/x/python/q1.py,,0,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/machine/x/python/q2.py,,1,6
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Nation,1,6
survived,"def test_service_worker_checksum() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    text = (browser_dir / ""manual_build.py"").read_text()
    assert 'sha384(dist_dir / ""service-worker.js"")' in text",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_manual_build_size_limit.py,,1,7
survived,"def main() -> int:
    missing: list[str] = []
    for pkg in REQUIRED:
        if importlib.util.find_spec(pkg) is None:
            missing.append(pkg)
    if missing:
        print(""Missing packages:"", "", "".join(missing))
        print(""Run 'python check_env.py --auto-install' to install them."")
        return 1
    print(""All required packages available."")
    return 0
",scripts/check_python_deps.py,,1,7
survived,"        def __init__(self, reward: float) -> None:
            self.reward = reward
",tests/test_world_model_demo.py,DummyEnv,1,7
survived,"def test_pack_empty_tree():
    tree = {}
    offsets, packed = pack_pytree(tree, dtype=jnp.float32)
    assert packed.size == 0
    rebuilt = unpack_pytree(offsets, packed)
    assert rebuilt == tree",tests/test_pack_tree.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""17""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(17)",benchmarks/poly_mini/task_017.py,,1,6
survived,"def run() -> None:
    n = 11
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_011.py,,1,6
survived,"def run() -> None:
    n = 2
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_002.py,,1,7
survived,"def run() -> None:
    n = 13
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_013.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""7""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(7)",benchmarks/poly_mini/task_007.py,,1,6
survived,"def test_blocks_malicious_patch() -> None:
    diff = _read(""malicious_patch.diff"")
    assert not is_patch_safe(diff)
",tests/test_safety_filter.py,,1,7
survived,"def add(a: int, b: int) -> int:
    """"""Return the sum of a and b, intentionally broken.""""""
    return a - b",tests/fixtures/self_heal_repo/calc.py,,1,7
survived,"async def new_env() -> dict:
    resp = requests.post(
        ""http://localhost:7860/command"", json={""cmd"": ""new_env""}, timeout=5
    )
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_asi_world_model/openai_agents_bridge.py,,1,6
survived,"    async def run() -> None:
        async with bus, ledger:
            await chaos.run_cycle()
            await asyncio.sleep(0)
",tests/test_safety_block.py,,1,6
survived,"def test_improve_repo_applies_patch(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)

    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""patch.diff""
    patch_file.write_text(patch)
    log_file = tmp_path / ""log.json""

    delta, clone = self_improver.improve_repo(
        str(repo_dir), str(patch_file), ""metric.txt"", str(log_file), cleanup=False
    )

    assert (clone / ""metric.txt"").read_text().strip() == ""2""
    data = json.loads(log_file.read_text())
    assert isinstance(data[0][""delta""], (int, float))
    assert delta == data[0][""delta""]
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_self_improver.py,,1,7
survived,"def ema(prices: Sequence[float], span: int = 20) -> float:
    """"""Return the exponential moving average over ``span`` periods.""""""

    if span <= 0:
        raise ValueError(""span must be positive"")
    if not prices:
        return 0.0

    alpha = 2 / (span + 1)
    ema_val = float(prices[0])
    for p in prices[1:]:
        ema_val = (float(p) - ema_val) * alpha + ema_val
    return ema_val
",alpha_factory_v1/backend/alpha_model.py,,1,7
survived,"    def test_bollinger_bands(self):
        prices = [1, 2, 3, 4, 5]
        lower, upper = am.bollinger_bands(prices, window=4, num_std=1)
        self.assertLess(lower, upper)
",alpha_factory_v1/tests/test_alpha_model.py,AlphaModelTest,1,7
survived,"    def test_toy_optimal(self):
        genes = {""temperature"": 0.7, ""top_p"": 0.9, ""max_tokens"": 128}
        self.assertAlmostEqual(gt.toy_fitness(genes), 3.0, places=2)
",alpha_factory_v1/tests/test_genetic_tests.py,GeneticTestsTest,1,7
survived,"def _sanitise_genes(genes: Mapping[str, float]) -> Dict[str, float]:
    """"""Ensure required keys exist and cast values to the correct types.""""""

    required = (""temperature"", ""top_p"", ""max_tokens"")
    if not all(k in genes for k in required):
        missing = [k for k in required if k not in genes]
        raise KeyError(f""Missing gene(s): {', '.join(missing)}"")

    return {
        ""temperature"": float(genes[""temperature""]),
        ""top_p"": float(genes[""top_p""]),
        ""max_tokens"": int(genes[""max_tokens""]),
    }
",alpha_factory_v1/backend/genetic_tests.py,,1,7
survived,"async def healthcheck() -> str:
    """"""Simple liveness probe.""""""

    return ""ok""
",alpha_factory_v1/backend/rpc_server.py,,1,8
survived,"    def step(self, action: str) -> Tuple[float, float, bool]:
        """"""Execute ``action`` and return (price, reward, done).""""""
        self.price = self.sample_next_price(self.price)
        reward = 0.0
        done = False
        return self.price, reward, done
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv,1,7
survived,"    def __enter__(self) -> ""GraphMemory"":
        return self
",alpha_factory_v1/backend/memory_graph.py,GraphMemory,1,7
survived,"    def __init__(self, name: str = ""dummy""):
        self.name = name
        self.ran = False
",alpha_factory_v1/tests/test_planner_agent.py,DummyAgent,1,7
survived,"    def forward(self, h, a):
        x = torch.cat([h, a], -1)
        return self.r(x), torch.tanh(self.h(x))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Dyn,1,6
survived,"def emit_helm(dir_:Path=Path(""helm_chart"")):
    dir_.mkdir(exist_ok=True)
    (dir_/""values.yaml"").write_text(HELM_VALUES)
    (dir_/""Chart.yaml"").write_text(""apiVersion: v2\nname: alpha-asi-demo\nversion: 0.1.0\n"")
    print(""Helm chart â†’"",dir_)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,7
survived,"    def __init__(self, hidden: int, act_dim: int):
        super().__init__(); self.r = nn.Linear(hidden+act_dim, 1); self.h = nn.Linear(hidden+act_dim, hidden)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Dyn,1,7
survived,"    def _on(self, msg: dict):
        try:
            self.handle(msg)
        except Exception as exc:
            LOG.exception(""[%s] crash: %s"", self.name, exc)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Agent,1,6
survived,"    def handle(self, msg: dict):  # to be overridden
        raise NotImplementedError
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Agent,1,7
survived,"    def _parse(self, args):
        old = sys.argv
        sys.argv = [""edge_runner.py""] + args
        try:
            return edge_runner.parse_args()
        finally:
            sys.argv = old
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest,1,6
survived,"def join_domain(domain, admin_user, ou=None):
    cmd = [""realm"", ""join"", ""-v"", f""--user={admin_user}""]
    if ou:
        cmd.append(f""--computer-ou={ou}"")
    cmd.append(domain)
    run_cmd("" "".join(cmd))
",adconnection_app.py,,1,7
survived,"def test_run_macro_demo_passes_base_url(tmp_path: Path) -> None:
    """"""Custom OLLAMA_BASE_URL should reach docker compose.""""""
    env = {""OLLAMA_BASE_URL"": ""http://example.com/v1""}
    docker_log, _ = _run_script(tmp_path, env=env)
    assert ""OLLAMA_BASE_URL=http://example.com/v1"" in docker_log",tests/test_macro_launcher.py,,1,7
survived,"def list_available_examples() -> dict[str, str]:
    """"""Return a mapping of example name to app path.""""""
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
    examples: dict[str, str] = {}
    for entry in os.scandir(base_dir):
        if not entry.is_dir() or entry.name == ""openai_chat_agent"":
            continue
        app_path = os.path.join(entry.path, ""app.py"")
        if os.path.exists(app_path):
            examples[entry.name] = app_path
    return examples
",examples/openai_chat_agent/app.py,,1,7
survived,"def login_with_persistence() -> Client:
    """"""Return Client logged in using saved session settings.""""""
    cl = Client()
    if os.path.exists(SESSION_FILE):
        cl.load_settings(SESSION_FILE)
    cl.login(IG_USERNAME, IG_PASSWORD)
    cl.dump_settings(SESSION_FILE)
    return cl
",examples/session_login.py,,1,7
survived,"def list_and_download(username: str, amount: int = 10):
    """"""Download recent posts from the specified account.""""""
    cl = login_with_persistence()
    user_id = cl.user_id_from_username(username)
    for media in cl.user_medias(user_id, amount=amount):
        if media.media_type == 1:
            cl.photo_download(media.pk)
        elif media.media_type == 2:
            cl.video_download(media.pk)
        elif media.media_type == 8:
            cl.album_download(media.pk)
",examples/session_login.py,,0,7
survived,"def optimize_autovacuum(
    ctx: Context,
    dry_run: bool = typer.Option(False, help=""Print SQL commands only.""),
    rollback: bool = typer.Option(False, help=""Reset to defaults instead.""),
) -> None:
    qbe = qb.QueryBuilderEnvironment()
    if rollback:
        query = qbe.build_optimize_autovacuum_rollback_query()
    else:
        query = qbe.build_optimize_autovacuum_query()

    print(query)

    async def run() -> None:
        async with yield_queries(ctx, qb.DBSettings()) as q:
            await q.optimize_autovacuum(rollback=rollback)

    if not dry_run:
        asyncio_run(run())
",pgqueuer/cli.py,,1,6
survived,"def test_csp_hashes_match() -> None:
    html_path = Path(""docs/alpha_agi_insight_v1/index.html"")
    html = html_path.read_text()
    meta = re.search(r""<meta[^>]*Content-Security-Policy[^>]*content=\""([^\""]+)\"""", html)
    assert meta, ""CSP meta tag missing""
    csp = meta.group(1)
    match = re.search(r""script-src ([^;]+)"", csp)
    assert match, ""script-src missing in CSP""
    allowed_hashes = set(re.findall(r""'sha384-[^']+'"", match.group(1)))
    inline_scripts = re.findall(r""<script(?![^>]*src)[^>]*>([\s\S]*?)</script>"", html)
    computed = {_hash_snippet(s) for s in inline_scripts}
    assert computed <= allowed_hashes

    srcs = re.findall(r""<script[^>]*src=['\""]([^'\""]+)['\""]"", html)
    assert len(srcs) == len(set(srcs))",tests/security/test_csp.py,,1,7
survived,"def test_error_propagation(monkeypatch, caplog):
    def fail_secho(*args, **kwargs):
        raise OSError(""boom"")

    # Force CLI output to fail so CLIOutputError is raised
    monkeypatch.setattr(click, ""secho"", fail_secho)
    cli = CLIOutput()

    with pytest.raises(CLIOutputError) as exc:
        cli.info(""hi"")

    # Restore working output for error handling path
    monkeypatch.setattr(click, ""secho"", lambda *a, **k: None)
    handler = ErrorHandler(cli_output=cli, log=logging.getLogger(""test""))
    with caplog.at_level(logging.ERROR):
        handler.handle(exc.value)
    assert ""failed to write output"" in caplog.text

    # Diagram generation failure should propagate through ErrorHandler
    generator = DiagramGenerator()
    with caplog.at_level(logging.ERROR):
        try:
            generator.generate(None)  # type: ignore[arg-type]
        except DiagramGenerationError as dg_err:
            handler.handle(dg_err)
    assert ""spec must be a mapping"" in caplog.text

    feedback = UserFeedback(cli_output=cli)
    suggestion = feedback.error_suggestion(""Failed to load file"")
    assert suggestion and ""file path exists"" in suggestion",tests/integration/test_ux_interactions.py,,1,7
survived,"    def __call__(self, module: M_contra, carry: CarryT) -> CarryT:
        ...
",src/haliax/nn/scan.py,FoldFunction,1,6
survived,"def New():
    b = Box(Contents=""rabbit"", secret=1)
    return b
",tests/rosetta/transpiler/Python/call-an-object-method.py,,1,6
survived,"def sum_rec(n, acc):
    if (n == 0):
        return acc
    return sum_rec((n - 1), (acc + n))
",tests/transpiler/x/py/tail_recursion.py,,1,7
survived,"def sum_rec(n, acc):
    if n == 0:
        return acc
    return sum_rec(n - 1, acc + n)
",tests/transpiler/x/py/tail_recursion.py,,1,7
survived,"def boom(a, b):
    print(""boom"")
    return True
",tests/transpiler/x/py/short_circuit.py,,1,6
survived,"        async def get(self, url: str, **kwargs):
            return requests.get(url, **kwargs)
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AsyncClient,0,7
survived,"    async def simulate(req: SimRequest, _: None = Depends(verify_token)) -> SimStartResponse | JSONResponse:
        try:
            sim_id = secrets.token_hex(8)
            asyncio.create_task(_background_run(sim_id, req))
            return SimStartResponse(id=sim_id)
        except HTTPException as exc:
            return problem_response(exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def simulate(req: SimRequest, _: None = Depends(verify_token)) -> SimStartResponse:
        sim_id = secrets.token_hex(8)
        asyncio.create_task(_background_run(sim_id, req))
        return SimStartResponse(id=sim_id)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"def test_existing_results_dir_permissions(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Ensure permissions are tightened when directory already exists.""""""
    path = tmp_path / ""results""
    path.mkdir(mode=0o755)
    monkeypatch.setenv(""SIM_RESULTS_DIR"", str(path))

    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface import api_server

    api_server = importlib.reload(api_server)

    assert path.exists()
    assert (path.stat().st_mode & 0o777) == 0o700",tests/test_results_dir_permissions.py,,1,7
survived,"def compile_model_jax(sbml_dir: Path, test_id: str, model_dir: Path):
    model_dir.mkdir(parents=True, exist_ok=True)
    sbml_file = find_model_file(sbml_dir, test_id)
    sbml_importer = amici.SbmlImporter(sbml_file)
    model_name = f""SBMLTest{test_id}_jax""
    sbml_importer.sbml2jax(model_name, output_dir=model_dir)
    model_module = amici.import_model_module(model_dir.name, model_dir.parent)
    jax_model = model_module.Model()
    return jax_model, sbml_importer
",tests/testSBMLSuiteJax.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q1.py,Lineitem,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_sort.py,_Group,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by.py,_Group,1,8
survived,"        def fake_find_spec(name: str):
            if name in {""pytest"", ""prometheus_client""}:
                return object()
            return None
",tests/test_preflight_optional_missing.py,TestPreflightOptionalMissing,1,6
survived,"def test_lint(monkeypatch: pytest.MonkeyPatch) -> None:
    def fake_run(cmd, input, capture_output):
        return MagicMock(stdout=b""template.py:1:1 F401 unused import os\n"")

    monkeypatch.setattr(""subprocess.run"", fake_run)
    gov = TemplateGovernance(secret=""k"")
    issues = gov.lint(""import os\n"")
    assert issues and ""unused import"" in issues[0]
",tests/test_template_governance.py,,1,7
survived,"    def run_unsigned(self, code_dir: Path, command: List[str]) -> Tuple[int, str, str]:
        """"""Execute ``command`` in a sandbox for unsigned templates.""""""
        manager = SandboxManager()
        return manager.run_code_in_sandbox(code_dir, command)",src/meta_agent/template_governance.py,TemplateGovernance,1,7
survived,"    def test_notebook_valid(self) -> None:
        nb_path = Path(""alpha_factory_v1/demos/alpha_agi_marketplace_v1/colab_alpha_agi_marketplace_demo.ipynb"")
        self.assertTrue(nb_path.exists(), ""Notebook missing"")
        data = json.loads(nb_path.read_text(encoding=""utf-8""))
        self.assertIn(""cells"", data)
        self.assertIn(""nbformat"", data)
        self.assertGreaterEqual(data.get(""nbformat"", 0), 4)",tests/test_marketplace_notebook.py,TestMarketplaceNotebook,1,7
survived,"        def publish(self, *_a: object, **_kw: object) -> None:
            pass
",tests/test_agent_manager_consumer.py,DummyBus,0,7
survived,"    def setUp(self) -> None:
        self._reg_backup = agents.AGENT_REGISTRY.copy()
        agents.AGENT_REGISTRY.clear()
        self._fail_backup = discovery.FAILED_AGENTS.copy()
        discovery.FAILED_AGENTS.clear()
",tests/test_failed_agent_discovery.py,TestFailedAgentDiscovery,1,7
survived,"    def test_disabled_when_deps_missing(self) -> None:
        with mock.patch.object(mod, ""MetaEvolver"", None), \
             mock.patch.object(mod, ""CurriculumEnv"", None):
            agent = mod.AIGAEvolverAgent()
            self.assertIsNone(agent.evolver)
            asyncio.run(agent.step())
",tests/test_aiga_evolver_agent_logic.py,TestEvolverAgentLogic,1,7
survived,"            async def stop(self) -> None:
                events.append(""stop"")
",tests/test_message_bus.py,TestMessageBus.Prod,1,6
survived,"    def test_main_requires_fastapi(self) -> None:
        mod_name = ""alpha_factory_v1.demos.alpha_agi_insight_v0.api_server""
        with mock.patch.dict(sys.modules, {""fastapi"": None}):
            api = importlib.reload(importlib.import_module(mod_name))
            with self.assertRaises(SystemExit) as cm:
                api.main([])
            self.assertIn(""FastAPI"", str(cm.exception))
",tests/test_insight_api_server_no_fastapi.py,TestInsightAPIServerNoFastAPI,1,7
survived,"    async def root(request: Request) -> Response:
        """"""Return the project disclaimer.""""""

        accept = request.headers.get(""accept"", """").lower()
        if ""text/html"" in accept:
            return HTMLResponse(f""<p>{DISCLAIMER}</p>"")
        return PlainTextResponse(DISCLAIMER)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    def test_selects_long_bonds(self) -> None:
        signals = {
            ""yield_curve"": ""spread -0.5, consider LONG BONDS soon"",
            ""supply_chain"": ""all clear"",
        }
        self.assertEqual(alpha_report.best_alpha(signals), signals[""yield_curve""])
",tests/test_alpha_report.py,TestBestAlpha,1,7
survived,"def document_module(module: ModuleType, file: TextIO) -> None:
    file.write(f""# {module.__name__}\n\n"")
    if module.__doc__:
        file.write(inspect.getdoc(module))
        file.write(""\n\n"")
    for name, obj in inspect.getmembers(module):
        if name.startswith(""_""):
            continue
        if inspect.isfunction(obj) or inspect.isclass(obj):
            file.write(f""## {name}\n\n"")
            doc = inspect.getdoc(obj) or ""No documentation.""
            file.write(doc)
            file.write(""\n\n"")
",scripts/generate_interface_docs.py,,1,7
survived,"def dump_lmdb(path: Path, keys: Iterable[str] | None = None) -> None:
    """"""Print selected or all key-value pairs from the LMDB database.""""""
    env = lmdb.open(str(path), readonly=True, lock=False)
    with env.begin() as txn:
        if keys:
            records = _dump_selected(txn, keys)
        else:
            records = _dump_all(txn)
    env.close()

    print(orjson.dumps(records, option=orjson.OPT_INDENT_2).decode())
",scripts/dump_lmdb.py,,1,7
deleted,"    def _root_cond_fns(self, p):
        """"""Return root condition functions for discontinuities.""""""
        TPL_P_SYMS = p

        return TPL_ROOT_FUNS
",python/sdist/amici/jax/jax.template.py,JAXModel_TPL_MODEL_NAME,0,7
survived,"    def _root_cond_fns(
        self, p: jt.Float[jt.Array, ""np""]
    ) -> tuple[
        Callable[[float, jt.Float[jt.Array, ""nxs""], tuple], jt.Float], ...
    ]:
        """"""Return condition functions for implicit discontinuities.

        These functions are passed to :class:`diffrax.Event` and must evaluate
        to zero when a discontinuity is triggered.

        :param p:
            model parameters
        :return:
            tuple of callable root functions
        """"""
        ...
",python/sdist/amici/jax/model.py,JAXModel,1,6
survived,"async def _background_run(sim_id: str, cfg: SimRequest) -> None:
    secs = [sector.Sector(f""s{i:02d}"") for i in range(cfg.pop_size)]
    results = forecast.simulate_years(secs, cfg.horizon)
    logs: List[str] = []
    for r in results:
        logs.append(f""Year {r.year}: {len(r.affected)} affected"")
        _progress.setdefault(sim_id, []).append(logs[-1])
        await asyncio.sleep(0.05)

    pop = [mats.Individual([0.0, 0.0]) for _ in range(cfg.pop_size)]

    def eval_fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    for g in range(cfg.generations):
        pop = mats.nsga2_step(pop, eval_fn, mu=cfg.pop_size)
        _progress.setdefault(sim_id, []).append(f""Generation {g+1}"")
        await asyncio.sleep(0.05)

    _simulations[sim_id] = {
        ""forecast"": [{""year"": r.year, ""capability"": r.capability} for r in results],
        ""pareto"": [ind.genome for ind in pop if ind.rank == 0],
        ""logs"": logs,
    }
",src/interface/api_server.py,,1,7
survived,"def test_agents_status_lists_names() -> None:
    with patch.object(cli.orchestrator, ""Orchestrator"") as orch_cls:
        orch = orch_cls.return_value
        orch.agents = [type(""A"", (), {})()]
        orch.agents[0].__class__.__name__ = ""AgentX""
        result = CliRunner().invoke(cli.main, [""agents-status""])
        assert ""AgentX"" in result.output
",tests/test_cli.py,,1,7
survived,"def get_function(compiled_sol, fn_name: str):  # type: ignore
    try:
        assert hasattr(compiled_sol, fn_name)
        return getattr(compiled_sol, fn_name)
    except Exception as e:
        return
",scripts/utils/lcb_runner.py,,1,6
survived,"def test_archive_insert_and_load(tmp_path) -> None:
    db = tmp_path / ""a.db""
    arch = Archive(db)
    arch.add({""name"": ""a""}, 0.1)
    arch.add({""name"": ""b""}, 0.2)
    rows = arch.all()
    assert len(rows) == 2
    assert rows[0].meta[""name""] == ""a""
",tests/test_archive.py,,1,7
survived,"    def function_tool(*_dargs, **_dkwargs):
        def _wrap(func):
            return func

        return _wrap
",src/self_edit/tools.py,,1,6
survived,"def test_evolve_stops_on_cost_cap():
    arch = InMemoryArchive()
    asyncio.run(arch.accept(Candidate(0.0, fitness=0.0, novelty=1.0)))

    async def run():
        await evolve(_op, _eval, arch, max_cost=0.1)

    asyncio.run(run())
    # seed + at least two children added
    assert len(arch.all()) >= 3",tests/test_evolve.py,,1,7
survived,"async def _dummy_evaluate(genome: Any) -> tuple[float, float]:
    await asyncio.sleep(0)
    return random.random(), 0.01
",src/evolve.py,,1,6
survived,"    def setUp(self) -> None:
        self.orig_pub = agents._WHEEL_PUBKEY
        self.orig_sigs = agents._WHEEL_SIGS.copy()
        agents._WHEEL_PUBKEY = PUB_KEY_B64
        sig = SIG_PATH.read_text().strip()
        agents._WHEEL_SIGS = {WHEEL_PATH.name: sig}
",tests/test_verify_wheel.py,VerifyWheelTests,1,7
survived,"    def _get_tasks(self, project_id: int) -> List[Dict]:
        ls = label_studio_sdk.Client(self.LABEL_STUDIO_HOST, self.LABEL_STUDIO_API_KEY)
        project = ls.get_project(id=project_id)
        return project.get_labeled_tasks()
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter,1,6
survived,"def bool_expr(prev_token_index, next_token_index, tok, change_end_line, change_end_char):
    if (not (prev_token_index is None or next_token_index is None)) and (tok[0] > change_end_line or (tok[0] == change_end_line and tok[1] > change_end_char)):
        return True
    return False",jac/jaclang/tests/fixtures/py_bool_expr.py,,1,6
survived,"def append_json(data, path):
    cur = load_json(path)
    cur.extend(data)
    save_json(cur, path)
",convert_missing.py,,1,6
survived,"def extract_title(readme: Path) -> str:
    """"""Return a reasonable title for the given README.""""""
    lines = readme.read_text(encoding=""utf-8"").splitlines()
    # Search the first 50 lines for a level-one heading
    for line in lines[:50]:
        m = TITLE_RE.match(line.strip())
        if m:
            return m.group(1).strip()
    # Fallback to folder name if no heading found early in the file
    return readme.parent.name.replace(""_"", "" "").title()
",scripts/generate_demo_docs.py,,1,7
survived,"    def test_happy_path(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            path = Path(tmpdir) / 'dash.json'
            path.write_text(json.dumps({'title': 't'}))
            called = {}

            class Resp:
                text = 'ok'

                def raise_for_status(self):
                    called['raised'] = True

            with mock.patch.dict(os.environ, {'GRAFANA_TOKEN': 'x'}):
                with mock.patch('alpha_factory_v1.scripts.import_dashboard.post', return_value=Resp()) as post:
                    with mock.patch.object(import_dashboard.sys, 'argv', ['imp.py', str(path)]):
                        import_dashboard.main()
                    post.assert_called_once()
                    self.assertTrue(called.get('raised'))
",alpha_factory_v1/tests/test_scripts_import_dashboard.py,ImportDashboardScriptTest,1,7
survived,"def _have_opencl():
    try:
        out = subprocess.run([""clinfo""], check=False, capture_output=True, text=True)
    except FileNotFoundError:
        return False
    return ""Number of platforms                               0"" not in out.stdout
",tests/test_solver_logs.py,,1,6
survived,"    def mocker():
        return MockerFixture()
",src/pytest_mock/__init__.py,,1,6
survived,"def expo(*_args: Any, **_kwargs: Any) -> float:
    return 0.0
",src/backoff/__init__.py,,0,7
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,8
survived,"def test_purge_old(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(1, 0.01, 0.1, 0)
    # update timestamp to old date
    old_ts = ""2000-01-01T00:00:00""
    db.conn.execute(""UPDATE telemetry SET timestamp=?"", (old_ts,))
    db.conn.commit()
    db.purge_old()
    assert db.fetch_all() == []
    db.close()
",tests/unit/test_telemetry_db.py,,1,8
survived,"    def load_dotenv(*_args, **_kwargs) -> None:
        return None
",src/meta_agent/cli/main.py,,0,7
survived,"    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """"""
            CREATE TABLE IF NOT EXISTS telemetry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                tokens INTEGER,
                cost REAL,
                latency REAL,
                guardrail_hits INTEGER
            )
            """"""
        )
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def test_export_json_and_csv(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path)
    # Create two records with known timestamps
    now = datetime.utcnow()
    db.record(5, 0.02, 0.3, 1)
    db.conn.execute(
        ""UPDATE telemetry SET timestamp=? WHERE id=1"",
        ((now - timedelta(days=1)).isoformat(),),
    )
    db.record(10, 0.05, 0.6, 2)

    json_path = tmp_path / ""export.json""
    csv_path = tmp_path / ""export.csv""

    db.export_json(json_path, start=now.isoformat())
    with open(json_path, ""r"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert len(data) == 1
    assert data[0][""tokens""] == 10

    db.export_csv(csv_path, metrics=[""tokens""], start=now.isoformat())
    with open(csv_path, newline="""", encoding=""utf-8"") as f:
        reader = csv.reader(f)
        rows = list(reader)
    assert rows[0] == [""timestamp"", ""tokens""]
    assert len(rows) == 2
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"def top_tags(request):
    """"""Display recent headlines for the 10 most popular tags.""""""
    tags = (
        Tag.objects.annotate(
            entry_count=models.Count(
                ""entry"", filter=models.Q(entry__is_draft=False), distinct=True
            ),
            blogmark_count=models.Count(
                ""blogmark"", filter=models.Q(blogmark__is_draft=False), distinct=True
            ),
            quotation_count=models.Count(
                ""quotation"", filter=models.Q(quotation__is_draft=False), distinct=True
            ),
            note_count=models.Count(
                ""note"", filter=models.Q(note__is_draft=False), distinct=True
            ),
        )
        .annotate(
            total=models.F(""entry_count"")
            + models.F(""blogmark_count"")
            + models.F(""quotation_count"")
            + models.F(""note_count"")
        )
        .order_by(""-total"")[:10]
    )
    tags_info = [
        {
            ""tag"": tag,
            ""total"": tag.total,
            ""recent_entries"": tag.entry_set.filter(is_draft=False)
            .order_by(""-created"")[:5],
        }
        for tag in tags
    ]
    return render(request, ""top_tags.html"", {""tags_info"": tags_info})
",blog/views.py,,1,6
survived,"def test_download_invocation(monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
    calls: list[tuple[str, Path]] = []

    def fake_download(url: str, dest: Path) -> None:
        calls.append((url, dest))
        dest.write_text(""stub"")

    monkeypatch.setattr(dg, ""_download"", fake_download)
    dg.download_openai_gpt2(""117M"", dest=tmp_path)
    assert len(calls) == len(dg._FILE_LIST)
    assert calls[0][0] == dg.model_urls(""117M"")[0]",tests/test_download_openai_gpt2.py,,1,7
survived,"def discover_adk() -> None:
    """"""Pull remote agent wheels via Google ADK if ``$ADK_MESH`` is set.""""""
    if adk is None or not os.getenv(""ADK_MESH""):
        return
    try:
        client = adk.Client()
        for pkg in client.list_remote_packages():
            if pkg.name in AGENT_REGISTRY:
                continue
            wheel_path = client.download_package(pkg.name)
            try:
                sig_path = client.download_package(pkg.name + "".sig"")
            except Exception:
                sig_path = None
            _HOT_DIR.mkdir(parents=True, exist_ok=True)
            dest = _HOT_DIR / wheel_path.name
            dest.write_bytes(wheel_path.read_bytes())
            if sig_path:
                (dest.with_suffix(dest.suffix + "".sig"")).write_bytes(sig_path.read_bytes())
            if not verify_wheel(dest):
                logger.error(""Discarding unverified wheel from ADK: %s"", pkg.name)
                dest.unlink(missing_ok=True)
                if sig_path:
                    dest.with_suffix(dest.suffix + "".sig"").unlink(missing_ok=True)
                continue
            logger.info(""Pulled %s from ADK mesh"", pkg.name)
        discover_hot_dir()
    except Exception:  # noqa: BLE001
        logger.exception(""ADK discovery failed"")",alpha_factory_v1/backend/agents/discovery.py,,1,7
survived,"def test_memory_agent_persists_records(tmp_path):
    mem_file = tmp_path / ""mem.log""
    cfg = config.Settings(bus_port=0, memory_path=str(mem_file))
    bus = messaging.A2ABus(cfg)
    led = logging.Ledger(str(tmp_path / ""ledger.db""))
    agent = memory_agent.MemoryAgent(bus, led, str(mem_file))
    env = messaging.Envelope(""a"", ""memory"", {""v"": 1}, 0.0)
    asyncio.run(agent.handle(env))
    agent2 = memory_agent.MemoryAgent(bus, led, str(mem_file))
    assert agent2.records and agent2.records[0][""v""] == 1
    asyncio.run(bus.stop())
    led.close()",tests/test_memory_agent_persistence.py,,1,7
survived,"def _set_seed(val: int) -> None:
    global _SEED
    _SEED = val
    random.seed(val)
    np.random.seed(val)
    torch.manual_seed(val)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,7
survived,"            def _forward(batch):
                tokens = tokenizer(
                    batch[""text""],
                    truncation=True,
                    padding=True,
                    max_length=cfg.max_length,
                    return_tensors=""pt"",
                )
                tokens = {k: v.to(device) for k, v in tokens.items()}
                with torch.no_grad():
                    outputs = model(**tokens)
                xm.mark_step()
                batch[""logits""] = outputs.logits.cpu().tolist()
                return batch
",marin/generation/logits.py,,1,7
survived,"def compute_logits(config: TextLogitsConfig) -> None:
    """"""Run a model forward pass and store logits for each example on TPU.""""""

    logger.info(
        f""Computing logits for {config.input_path} using {config.model_name}""
    )

    @ray.remote(
        memory=config.memory_gb * 1024 * 1024 * 1024,
        resources={""TPU"": 4, ""TPU-v4-8-head"": 1},
    )
    @remove_tpu_lockfile_on_exit
    def run(cfg: TextLogitsConfig):
        import torch_xla.core.xla_model as xm
        import torch_xla.distributed.xla_multiprocessing as xmp

        def _mp_fn(index: int, cfg: TextLogitsConfig, tmp_dir: str):
            dataset = read_dataset(cfg.input_path)
            dataset = dataset.shard(xmp.xrt_world_size(), index)

            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)
            model = AutoModelForCausalLM.from_pretrained(cfg.model_name)
            device = xm.xla_device()
            model.to(device)
            model.eval()

            def _forward(batch):
                tokens = tokenizer(
                    batch[""text""],
                    truncation=True,
                    padding=True,
                    max_length=cfg.max_length,
                    return_tensors=""pt"",
                )
                tokens = {k: v.to(device) for k, v in tokens.items()}
                with torch.no_grad():
                    outputs = model(**tokens)
                xm.mark_step()
                batch[""logits""] = outputs.logits.cpu().tolist()
                return batch

            dataset = dataset.map(
                _forward, batched=True, batch_size=cfg.batch_size
            )

            shard_path = os.path.join(tmp_dir, f""logits_{index}.jsonl.gz"")
            write_dataset(dataset, shard_path)

        with tempfile.TemporaryDirectory() as tmp_dir:
            xmp.spawn(_mp_fn, args=(cfg, tmp_dir))
            import glob
            import datasets

            shard_files = sorted(glob.glob(os.path.join(tmp_dir, ""logits_*.jsonl.gz"")))
            shards = [read_dataset(p) for p in shard_files]
            combined = datasets.concatenate_datasets(shards)
            write_dataset(combined, cfg.output_path)

    ray.get(run.remote(config))
",marin/generation/logits.py,,1,6
survived,"        def __init__(self) -> None:
            self.coro: Any | None = None
",tests/test_alpha_agi_business_3_v1.py,DummyLoop,1,6
survived,"def boom():
    print(""boom"")
    return True
",tests/transpiler/x/py/bool_chain.py,,1,6
survived,"def sum_tree(t):
    return (0 if t == Leaf else (sum_tree(t.left) + t.value + sum_tree(t.right) if t != None else None))
",tests/transpiler/x/py/tree_sum.py,,1,6
survived,"def improve_repo(repo_url: str, patch_file: str, metric_file: str, log_file: str) -> Tuple[float, Path]:
    """"""Clone ``repo_url``, apply ``patch_file`` and log score delta.

    Returns the score delta and path to the cloned repository.
    """"""
    if git is None:
        raise RuntimeError(""GitPython is required"")
    repo_dir = Path(tempfile.mkdtemp(prefix=""selfimprover-""))
    repo = git.Repo.clone_from(repo_url, repo_dir)
    baseline = _evaluate(repo_dir, metric_file)
    repo.git.apply(patch_file)
    repo.index.add([metric_file])
    repo.index.commit(""apply patch"")
    new_score = _evaluate(repo_dir, metric_file)
    delta = new_score - baseline
    _log_delta(delta, Path(log_file))
    return delta, repo_dir",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_improver.py,,1,7
survived,"def test_improve_repo(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)

    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""patch.diff""
    patch_file.write_text(patch)
    log_file = tmp_path / ""log.json""

    delta, clone = self_improver.improve_repo(str(repo_dir), str(patch_file), ""metric.txt"", str(log_file))

    assert delta == 1
    assert (clone / ""metric.txt"").read_text().strip() == ""2""
    data = json.loads(log_file.read_text())
    assert data and data[0][""delta""] == 1",tests/test_self_improver.py,,1,7
survived,"    def test_generate_association_rules_no_results(self):
        """"""High confidence threshold yields no rules.""""""
        patterns = find_frequent_patterns(self.transactions, self.support_threshold)
        rules = generate_association_rules(patterns, 1.1)
        self.assertEqual(rules, {})
",tests/test_pyfpgrowth.py,FPGrowthTests,1,7
survived,"def test_namedarray_runtime_check():
    Batch = Axis(""batch"", 2)
    Embed = Axis(""embed"", 3)
    arr = NamedArray(jnp.zeros((Batch.size, Embed.size)), (Batch, Embed))
    assert arr.matches_axes(NamedArray[""batch"", ""embed""])
    assert arr.matches_axes(NamedArray[""batch embed""])
    assert arr.matches_axes(NamedArray[""batch embed ...""])
    assert arr.matches_axes(NamedArray[{""batch"", ""embed""}])
    assert arr.matches_axes(NamedArray[{""batch"", ""embed"", ...}])
    assert not arr.matches_axes(NamedArray[""embed batch""])
    assert not arr.matches_axes(NamedArray[{""batch"", ""foo"", ...}])",tests/test_namedarray_typing.py,,1,7
survived,"    def matches_axes(self, spec: NamedArrayAxesSpec) -> bool:
        """"""Check whether this NamedArray conforms to the given `NamedArray` type.

        Parameters
        ----------
        spec : NamedArrayAxesSpec
            The specification to check against. It can be produced via the
            ``NamedArray[...]`` syntax or passed directly as a string or
            sequence of axis names.
        """"""

        ann = _parse_namedarray_axes(spec)
        names = tuple(ax.name for ax in self.axes)
        if ann.ordered:
            if not ann.subset:
                return names == ann.before
            if len(names) < len(ann.before) + len(ann.after):
                return False
            if names[: len(ann.before)] != ann.before:
                return False
            if ann.after and names[-len(ann.after) :] != ann.after:
                return False
            return True
        else:
            name_set = set(names)
            spec_set = set(ann.before)
            if ann.subset:
                return spec_set.issubset(name_set)
            else:
                return name_set == spec_set
",src/haliax/core.py,NamedArray,1,7
survived,"def _parse_namedarray_axes(item: NamedArrayAxesSpec | typing.Annotated[""NamedArray"", NamedArrayAxes]) -> NamedArrayAxes:
    origin = typing.get_origin(item)
    if origin is typing.Annotated:
        args = typing.get_args(item)
        if len(args) >= 2:
            item = args[1]
    if isinstance(item, NamedArrayAxes):
        return item
    if isinstance(item, str):
        parts = item.split()
        if parts.count(""..."") > 1:
            raise TypeError(""Only one ellipsis allowed in NamedArray typing spec"")
        if ""..."" in parts:
            idx = parts.index(""..."")
            before = tuple(parts[:idx])
            after = tuple(parts[idx + 1 :])
            return NamedArrayAxes(before, after, ordered=True, subset=True)
        else:
            return NamedArrayAxes(tuple(parts), (), ordered=True, subset=False)
    if isinstance(item, set) or isinstance(item, frozenset):
        subset = False
        names: List[str] = []
        for part in item:
            if part is Ellipsis:
                if subset:
                    raise TypeError(""Only one ellipsis allowed in NamedArray typing spec"")
                subset = True
            else:
                if not isinstance(part, str):
                    raise TypeError(f""Invalid axis spec: {part}"")
                names.append(part)
        return NamedArrayAxes(tuple(names), (), ordered=False, subset=subset)
    if isinstance(item, (tuple, list)):
        subset = False
        before: List[str] = []
        after: List[str] = []
        cur = before
        for part in item:
            if part is Ellipsis:
                if subset:
                    raise TypeError(""Only one ellipsis allowed in NamedArray typing spec"")
                subset = True
                cur = after
            else:
                if not isinstance(part, str):
                    raise TypeError(f""Invalid axis spec: {part}"")
                cur.append(part)
        if subset:
            return NamedArrayAxes(tuple(before), tuple(after), ordered=True, subset=True)
        else:
            return NamedArrayAxes(tuple(before), (), ordered=True, subset=False)
    raise TypeError(f""Invalid NamedArray typing spec: {item}"")
",src/haliax/core.py,,1,7
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,,1,7
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_index.py,,1,7
survived,"    def search(
        self,
        query: str,
        *,
        category: str | None = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """"""Search the index using a simple token overlap ranking.""""""
        if not self._index:
            self.ensure_up_to_date()
        tokens = [t.lower() for t in query.split() if t]
        results = []
        for item in self._index:
            meta = item.get(""metadata"", {})
            if category and meta.get(""category"") != category:
                continue
            if tags and not all(t in meta.get(""tags"", []) for t in tags):
                continue
            haystack = "" "".join(
                [
                    item.get(""content"", """"),
                    meta.get(""title"", """"),
                    meta.get(""description"", """"),
                    meta.get(""slug"", """"),
                    "" "".join(meta.get(""tags"", [])),
                ]
            ).lower()
            score = sum(1 for tok in tokens if tok in haystack)
            if score:
                results.append({**item, ""score"": float(score)})
        results.sort(key=lambda r: r[""score""], reverse=True)
        return results[:limit]",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def needs_rebuild(self) -> bool:
        """"""Return True if stored checksums differ from source files.""""""
        if not self.index_path.exists():
            return True
        if not self._index:
            self.load()
        for item in self._index:
            template_path = self.registry.templates_dir / item[""path""]
            try:
                content = template_path.read_text(encoding=""utf-8"")
            except OSError:  # file removed
                return True
            checksum = sha256(content.encode(""utf-8"")).hexdigest()
            if checksum != item.get(""checksum""):
                return True
        # check for new templates not in index
        seen = {(i[""slug""], i[""version""]) for i in self._index}
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                if (slug, version_info[""version""]) not in seen:
                    return True
        return False
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.index_path = self.registry.templates_dir / self.INDEX_FILE_NAME
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"        def __init__(self, url: str) -> None:
            captured[""url""] = url
",tests/test_archive.py,DummyClient,0,7
survived,"    async def stop_merkle_task(self) -> None:
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:  # pragma: no cover - expected
                pass
            self._task = None
",src/archive/service.py,ArchiveService,1,7
survived,"def _sync_chat(prompt: str) -> str:
    """"""Synchronously invoke the async chat helper.""""""
    from alpha_factory_v1.backend.llm_provider import chat

    async def _call() -> str:
        return await chat(prompt, max_tokens=512)

    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(_call())

    result: list[str] = []

    def _worker() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        task = loop.create_task(_call())
        try:
            result.append(loop.run_until_complete(task))
        finally:
            loop.close()

    t = threading.Thread(target=_worker)
    t.start()
    t.join()
    return result[0]
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,,1,7
survived,"    def score(self, context: str, response: str) -> Dict[str, Any]:
        logic = self.logic_score(context, response)
        feas, cites = self.feasibility_score(response)
        reasons = []
        if logic < 0.5:
            reasons.append(""response not supported by context"")
        if feas < 0.5:
            reasons.append(""low similarity to known facts"")
        if not reasons:
            reasons.append(""looks good"")
        return {
            ""logic"": logic,
            ""feas"": feas,
            ""reasons"": reasons,
            ""citations"": cites,
        }
",src/critics/dual_critic_service.py,DualCriticService,1,6
survived,"    async def start_grpc(self, port: int) -> None:
        if grpc is None:
            raise RuntimeError(""grpc not installed"")
        server = grpc.aio.server()
        method = grpc.unary_unary_rpc_method_handler(
            self._handle_rpc,
            request_deserializer=lambda b: b,
            response_serializer=lambda b: b,
        )
        service = grpc.method_handlers_generic_handler(""critics.Critic"", {""Score"": method})
        server.add_generic_rpc_handlers((service,))
        server.add_insecure_port(f""[::]:{port}"")
        await server.start()
        self._server = server
",src/critics/dual_critic_service.py,DualCriticService,1,7
survived,"def __getattr__(name: str) -> Any:  # pragma: no cover - thin wrapper
    """"""Lazily import topâ€‘level modules.

    This keeps ``import alpha_factory_v1`` fast and avoids importing heavy
    dependencies until actually needed.
    """"""

    if name in {""backend"", ""demos"", ""ui"", ""run""}:
        return importlib.import_module(f"".{name}"", __name__)
    raise AttributeError(f""module {__name__!r} has no attribute {name}"")
",alpha_factory_v1/__init__.py,,1,7
survived,"def __dir__() -> list[str]:  # pragma: no cover - environment driven
    """"""Return module attributes for ``dir()`` calls.""""""

    return sorted(list(globals().keys()) + __all__)",alpha_factory_v1/__init__.py,,1,7
survived,"            def __init__(self) -> None:
                self.instructions = []
",tests/test_insight_orchestrator_features.py,TestLedger.DummyTx,1,6
survived,"async def main() -> None:
    example_path = Path(__file__).parent / ""app.py""

    config = {
        ""mcpServers"": {
            ""hello"": {
                ""command"": sys.executable,
                ""args"": [str(example_path)],
            }
        }
    }

    client = MCPClient(config=config)
    session = await client.create_session(""hello"")
    result = await session.connector.call_tool(""hello_world"", {})
    print(result.content[0].text)

    await client.close_all_sessions()
",examples/hello_world/client.py,,1,7
survived,"def get_kill_after_minutes() -> int:
    """"""Return minutes after which to kill a running task.""""""
    return _load_config_timeout_minutes()
",anomstack/sensors/timeout.py,,1,7
survived,"def test_authorize_button_success(monkeypatch):
    st.session_state.clear()
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)

    # Mock async client methods
    monkeypatch.setattr(oauth.client, ""get_authorization_url"", AsyncMock(return_value=""http://auth""))
    monkeypatch.setattr(oauth.client, ""get_access_token"", AsyncMock(return_value={""access_token"": ""tok""}))

    # Force deterministic state and component output
    monkeypatch.setattr(""streamlit_oauth._generate_state"", lambda key=None: ""STATE"")
    monkeypatch.setattr(""streamlit_oauth._authorize_button"", lambda **kwargs: {""code"": ""CODE"", ""state"": ""STATE""})

    result = oauth.authorize_button(""Login"", ""http://cb"", ""scope"", key=""k"")
    assert result[""token""][""access_token""] == ""tok""
    assert f""state-k"" not in st.session_state
",tests/test_oauth_component.py,,1,7
survived,"def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)
",src/model/u2tokenizer/rope.py,,1,7
survived,"def forecast_disruptions(
    sectors: Iterable[Sector],
    horizon: int,
    curve: str = ""logistic"",
    *,
    k: float | None = None,
    x0: float | None = None,
    pop_size: int = 6,
    generations: int = 1,
    seed: int | None = None,
    mut_rate: float = 0.1,
    xover_rate: float = 0.5,
) -> List[TrajectoryPoint]:
    """"""Simulate sector trajectories and disruption events.

    Args:
        sectors: Iterable of sectors to simulate.
        horizon: Number of years to simulate.
        curve: Name of the capability growth curve.
        k: Optional curve steepness parameter.
        x0: Optional curve midpoint shift.
        pop_size: Population size for the evolutionary search.
        generations: Number of evolution steps.
        seed: Random seed for deterministic behaviour.
        mut_rate: Probability of mutating a gene.
        xover_rate: Probability of performing crossover.

    Returns:
        List of trajectory points for each simulated year.
    """"""

    secs = list(sectors)
    results: List[TrajectoryPoint] = []
    for year in range(1, horizon + 1):
        t = year / horizon
        cap = capability_growth(t, curve, k=k, x0=x0)
        affected: List[Sector] = []
        for sec in secs:
            if not sec.disrupted:
                sec.energy *= 1.0 + sec.growth
                if thermodynamic_trigger(sec, cap):
                    sec.disrupted = True
                    sec.energy += _innovation_gain(
                        pop_size,
                        generations,
                        seed=seed,
                        mut_rate=mut_rate,
                        xover_rate=xover_rate,
                    )
                    affected.append(sec)
        snapshot = [Sector(s.name, s.energy, s.entropy, s.growth, s.disrupted) for s in secs]
        results.append(TrajectoryPoint(year, cap, snapshot))
    return results
",alpha_factory_v1/core/simulation/forecast.py,,1,7
survived,"def _evaluate(repo_path: Path, metric_file: str) -> float:
    """"""Return the numeric metric stored in ``metric_file`` inside ``repo_path``.""""""
    return float((repo_path / metric_file).read_text().strip())
",alpha_factory_v1/core/self_evolution/self_improver.py,,1,7
survived,"    def fn(genome: list[float]) -> tuple[float, float, float, float]:
        x, y = genome
        effectiveness = x**2
        negative_evar = y**2
        complexity = (x + y) ** 2
        history = [1.0, 1.0, 1.0]
        base = lead_time._arima_baseline(history, 3)
        forecast_series = [b + x + y for b in base]
        lead_impr = lead_time.lead_signal_improvement(history, forecast_series, months=3, threshold=1.1)
        lead_penalty = 1.0 - lead_impr
        return effectiveness, negative_evar, complexity, lead_penalty
",alpha_factory_v1/core/simulation/forecast.py,,1,6
survived,"    def first_cross(seq: Sequence[float]) -> int:
        for i, v in enumerate(seq, 1):
            if v >= thr:
                return i
        return months + 1
",alpha_factory_v1/core/evaluators/lead_time.py,,1,6
survived,"def _noop(*_a: Any, **_kw: Any) -> Any:
    class _N:
        def labels(self, *_a: Any, **_kw: Any) -> ""_N"":
            return self

        def observe(self, *_a: Any) -> None:
            ...

        def inc(self, *_a: Any) -> None:
            ...

    return _N()
",alpha_factory_v1/core/utils/tracing.py,,1,7
survived,"    async def evolve(
        self,
        scenario_hash: str,
        fn: Callable[[list[float]], tuple[float, ...]],
        genome_length: int,
        *,
        sector: str = ""generic"",
        approach: str = ""ga"",
        experiment_id: str = ""default"",
        **kwargs: object,
    ) -> object:
        pops = self.experiment_pops.setdefault(experiment_id, {})
        pop = await asyncio.to_thread(
            fn,
            [0.0] * genome_length,
            scenario_hash=scenario_hash,
            populations=pops,
            **kwargs,
        )
        pops[scenario_hash] = pop
        for ind in pop:
            self.solution_archive.add(sector, approach, ind.score, {""genome"": ind.genome})
            self.archive.insert_entry({""experiment_id"": experiment_id, ""genome"": ind.genome}, {""score"": ind.score})
        return pop
",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator,1,7
survived,"    def verify_merkle_root(self, expected: str, agent_id: str) -> None:
        actual = self.ledger.compute_merkle_root()
        if actual != expected:
            self.slash(agent_id)
",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator,1,7
survived,"    def _generate_basic_tool_code(name: str) -> str:
        """"""Return a very small tool implementation used as a fallback.""""""
        return f""""""
import logging

logger_tool = logging.getLogger(__name__)

class {name}Tool:
    def __init__(self, salutation: str = 'Hello'):
        self.salutation = salutation
        logger_tool.info(f'{name}Tool initialized with {{self.salutation}}')

    def run(self, name: str) -> str:
        logger_tool.info(f'{name}Tool.run called with {{name}}')
        return f'{{self.salutation}}, {{name}} from {name}Tool!'

def get_tool_instance():
    logger_tool.info('get_tool_instance called')
    return {name}Tool()
""""""
",src/meta_agent/sub_agent_manager.py,SubAgentManager,1,7
survived,"def test_workbox_replaced():
    path = pathlib.Path(""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/lib/workbox-sw.js"")
    data = path.read_text(errors=""ignore"")
    assert ""Placeholder"" not in data",tests/test_assets_replaced.py,,1,7
survived,"def test_non_dict_returns_zero() -> None:
    _reset_ledger()
    value = eb.reward(None, None, ""bad"")
    assert value == 0.0",tests/test_energy_balance_reward.py,,1,7
survived,"    def __init__(self, settings: config.Settings) -> None:
        self.settings = settings
        self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_safety_guardian_property.py,DummyBus,1,7
survived,"def test_blocks_import_os(code: str) -> None:
    assume(""import os"" in code)
    bus = DummyBus(config.Settings(bus_port=0))
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(""codegen"", ""safety"", {""code"": code}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_safety_guardian_property.py,,1,7
survived,"def test_allows_safe_code(code: str) -> None:
    assume(""import os"" not in code)
    bus = DummyBus(config.Settings(bus_port=0))
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(""codegen"", ""safety"", {""code"": code}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""ok""",tests/test_safety_guardian_property.py,,1,6
survived,"def _timeline_df(traj: list[Any]) -> ""pd.DataFrame"":
    """"""Convert trajectory data into a pandas DataFrame.""""""
    import pandas as pd

    rows = []
    for point in traj:
        for sec in point.sectors:
            rows.append(
                {
                    ""year"": point.year,
                    ""sector"": sec.name,
                    ""energy"": sec.energy,
                    ""disrupted"": sec.disrupted,
                }
            )
    return pd.DataFrame(rows)
",src/interface/minimal_ui.py,,1,7
survived,"def test_get_github_api_url():
    url = get_github_api_url('user/repo', 'src', 'file.sol', 'abc123')
    assert url == 'https://api.github.com/repos/user/repo/contents/src/file.sol?ref=abc123'
",tests/test_github_utils.py,,1,7
survived,"def test_percent_str_call(state: State):
    s_in = """"""'%s' % str(var)""""""
    s_expected = """"""f'{var!s}'""""""

    s_out, count = code_editor.fstringify_code_by_line(s_in, state)
    assert s_out == s_expected
",test/test_edits.py,,1,7
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/cyber_threat_agent.py,CyberThreatAgent,1,7
survived,"    def __init__(self, path: str) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Ledger,1,7
survived,"    async def handle(self, env):
        pass",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/strategy_agent.py,StrategyAgent,0,7
survived,"def simulate_years(sectors: Iterable[Sector], horizon: int) -> List[ForecastPoint]:
    results: List[ForecastPoint] = []
    for year in range(1, horizon + 1):
        cap = logistic_curve(year / horizon * 10.0)
        affected = [s for s in sectors if thermodynamic_trigger(s, cap)]
        results.append(ForecastPoint(year, cap, affected))
    return results",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/forecast.py,,1,7
survived,"async def _main() -> None:  # pragma: no cover - CLI helper
    orch = Orchestrator()
    await orch.run_forever()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,,1,6
survived,"    async def start(self) -> None:
        if not self.settings.bus_port or grpc is None:
            return
        server = grpc.aio.server()
        method = grpc.unary_unary_rpc_method_handler(
            self._handle_rpc,
            request_deserializer=lambda b: b,
            response_serializer=lambda b: b,
        )
        service = grpc.method_handlers_generic_handler(""bus.Bus"", {""Send"": method})
        server.add_generic_rpc_handlers((service,))
        creds = None
        if creds:
            server.add_secure_port(f""[::]:{self.settings.bus_port}"", creds)
        else:
            server.add_insecure_port(f""[::]:{self.settings.bus_port}"")
        await server.start()
        self._server = server
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus,1,6
survived,"async def test_agents_cycle() -> None:
    settings = config.Settings()
    bus = messaging.A2ABus(settings)
    ledger = Ledger(settings.ledger_path)
    agents = [
        planning_agent.PlanningAgent(bus, ledger),
        research_agent.ResearchAgent(bus, ledger),
        strategy_agent.StrategyAgent(bus, ledger),
        market_agent.MarketAgent(bus, ledger),
        codegen_agent.CodeGenAgent(bus, ledger),
        safety_agent.SafetyGuardianAgent(bus, ledger),
        memory_agent.MemoryAgent(bus, ledger),
    ]
    for a in agents:
        await a.run_cycle()",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_agents.py,,1,7
survived,"        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime,1,6
survived,"    async def process_conversation(messages: list[Message], tempdir: Path | None = None):
        """"""
        Process the entire conversation and return a formatted string and list of
        files. The last message is assumed to be the assistant's response.
        """"""
        conversation: list[str] = []
        files: list[Path | str] = []

        for msg in messages:
            input_part, files_part = await GeminiClientWrapper.process_message(msg, tempdir)
            conversation.append(input_part)
            files.extend(files_part)

        # Left with the last message as the assistant's response
        conversation.append(add_tag(""assistant"", """", unclose=True))

        return ""\n"".join(conversation), files
",app/services/client.py,GeminiClientWrapper,1,7
survived,"    def clients(self) -> List[GeminiClientWrapper]:
        """"""Return managed clients.""""""
        return self._clients
",app/services/pool.py,GeminiClientPool,1,7
survived,"    def test_has_version(self) -> None:
        mod = importlib.import_module(""alpha_factory_v1.demos.cross_industry_alpha_factory"")
        self.assertTrue(hasattr(mod, ""__version__""))
",tests/test_cross_industry_version.py,TestCrossIndustryVersion,1,7
survived,"def pytest_configure(config):
    """"""Register custom markers for test categorization.""""""
    config.addinivalue_line(""markers"", ""unit: Unit tests"")
    config.addinivalue_line(""markers"", ""integration: Integration tests"")
    config.addinivalue_line(""markers"", ""slow: Slow running tests"")
    config.addinivalue_line(""markers"", ""parser: Parser tests"")
    config.addinivalue_line(""markers"", ""transformer: Transformer tests"")
    config.addinivalue_line(""markers"", ""loader: Loader tests"")
    config.addinivalue_line(""markers"", ""ranker: Ranker tests"")",tests/conftest.py,,1,8
survived,"    def stop(self):
        '''
        Stop capturing thread
        '''
        self.is_terminated = True
        logger.info(""[GameWindowCapturor] Terminated"")
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor,1,7
survived,"    def is_near_edge(self):
        '''
        is_near_edge
        '''
        if self.cfg.is_use_minimap:
            x0, y0 = self.loc_player_minimap
            h, w = self.img_route.shape[:2]
            x_min = max(0, x0 - self.cfg.edge_teleport_minimap_box_width//2)
            x_max = min(w, x0 + self.cfg.edge_teleport_minimap_box_width//2)
            y_min = max(0, y0 - self.cfg.edge_teleport_minimap_box_height//2)
            y_max = min(h, y0 + self.cfg.edge_teleport_minimap_box_height//2)
        else:
            x0, y0 = self.loc_player_global
            h, w = self.img_route.shape[:2]
            x_min = max(0, x0 - self.cfg.edge_teleport_box_width//2)
            x_max = min(w, x0 + self.cfg.edge_teleport_box_width//2)
            y_min = max(0, y0 - self.cfg.edge_teleport_box_height//2)
            y_max = min(h, y0 + self.cfg.edge_teleport_box_height//2)

        # Debug: draw search box
        draw_rectangle(
            self.img_route_debug,
            (x_min, y_min),
            (y_max - y_min, x_max - x_min),
            (0, 0, 255), ""Edge Check""
        )

        # Find mask of matching pixels
        roi = self.img_route[y_min:y_max, x_min:x_max]
        mask = np.all(roi == self.cfg.edge_teleport_color_code, axis=2)
        coords = np.column_stack(np.where(mask))

        # No edge pixel
        if coords.size == 0:
            return """"

        # Calculate mean position of matching pixels
        mean_x = np.mean(coords[:, 1])

        # Compare to roi center
        if mean_x < x0:
            return ""edge on left""
        else:
            return ""edge on right""
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"    def is_rune_near_player(self):
        '''
        is_rune_near_player
        '''
        # Calculate bounding box
        h, w = self.img_frame.shape[:2]
        x0 = max(0, self.loc_player[0] - self.cfg.rune_detect_box_width // 2)
        y0 = max(0, self.loc_player[1] - self.cfg.rune_detect_box_height)
        x1 = min(w, self.loc_player[0] + self.cfg.rune_detect_box_width // 2)
        y1 = min(h, self.loc_player[1])

        # Debug
        draw_rectangle(
            self.img_frame_debug, (x0, y0), (y1-y0, x1-x0),
            (255, 0, 0), ""Rune Detection Range""
        )

        # Find rune icon near player
        if  (x1 - x0) < self.img_rune.shape[1] or \
            (y1 - y0) < self.img_rune.shape[0]:
            return False # Skip check if box is out of range
        else:
            img_roi = self.img_frame[y0:y1, x0:x1]
            loc_rune, score, _ = find_pattern_sqdiff(
                            img_roi,
                            self.img_rune,
                            mask=get_mask(self.img_rune, (0, 255, 0)))
            # # Draw rectangle for debug
            # draw_rectangle(
            #     self.img_frame_debug,
            #     (x0 + loc_rune[0], y0 + loc_rune[1]),
            #     self.img_rune.shape,
            #     (255, 0, 255),  # purple in BGR
            #     f""Rune,{round(score, 2)}""
            # )
            detect_thres = self.cfg.rune_detect_diff_thres + self.rune_detect_level*self.cfg.rune_detect_level_coef
            if score < detect_thres:
                logger.info(f""[Rune Detect] Found rune near player with score({score}),"" + \
                            f""level({self.rune_detect_level}),threshold({detect_thres})"")
                # Draw rectangle for debug
                draw_rectangle(
                    self.img_frame_debug,
                    (x0 + loc_rune[0], y0 + loc_rune[1]),
                    self.img_rune.shape,
                    (255, 0, 255),  # purple in BGR
                    f""Rune,{round(score, 2)}""
                )
                screenshot(self.img_frame_debug, ""rune_detected"")

                return True
            else:
                return False
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"    def get_nearest_color_code(self):
        '''
        get_nearest_color_code
        '''
        x0, y0 = self.loc_player_global
        h, w = self.img_route.shape[:2]
        x_min = max(0, x0 - self.cfg.color_code_search_range)
        x_max = min(w, x0 + self.cfg.color_code_search_range)
        y_min = max(0, y0 - self.cfg.color_code_search_range)
        y_max = min(h, y0 + self.cfg.color_code_search_range)

        nearest = None
        min_dist = float('inf')
        for y in range(y_min, y_max):
            for x in range(x_min, x_max):
                pixel = tuple(self.img_route[y, x])  # (R, G, B)
                if pixel in self.cfg.color_code:
                    dist = abs(x - x0) + abs(y - y0)
                    if dist < min_dist:
                        min_dist = dist
                        nearest = {
                            ""pixel"": (x, y),
                            ""color"": pixel,
                            ""action"": self.cfg.color_code[pixel],
                            ""distance"": dist
                        }

        # Debug
        draw_rectangle(
            self.img_route_debug,
            (x_min, y_min),
            (self.cfg.color_code_search_range*2, self.cfg.color_code_search_range*2),
            (0, 0, 255), ""Color Search Range""
        )
        # Draw a straigt line from map_loc_player to color_code[""pixel""]
        if nearest is not None:
            cv2.line(
                self.img_route_debug,
                self.loc_player_global, # start point
                nearest[""pixel""],       # end point
                (0, 255, 0),            # green line
                1                       # thickness
            )

            # Print color code on debug image
            cv2.putText(
                self.img_frame_debug,
                f""Route Action: {nearest['action']}"",
                (720, 90),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),
                2, cv2.LINE_AA
            )
            cv2.putText(
                self.img_frame_debug, f""Route Index: {self.idx_routes}"",
                (720, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),
                2, cv2.LINE_AA
            )

        return nearest  # if not found return none
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"    def get_hp_mp_exp(self):
        '''
        get_hp_mp_exp
        '''
        # HP crop
        hp_bar = self.img_frame[self.cfg.hp_bar_top_left[1]:self.cfg.hp_bar_bottom_right[1]+1,
                                self.cfg.hp_bar_top_left[0]:self.cfg.hp_bar_bottom_right[0]+1]
        # MP crop
        mp_bar = self.img_frame[self.cfg.mp_bar_top_left[1]:self.cfg.mp_bar_bottom_right[1]+1,
                                self.cfg.mp_bar_top_left[0]:self.cfg.mp_bar_bottom_right[0]+1]
        # EXP crop
        exp_bar = self.img_frame[self.cfg.exp_bar_top_left[1]:self.cfg.exp_bar_bottom_right[1]+1,
                                self.cfg.exp_bar_top_left[0]:self.cfg.exp_bar_bottom_right[0]+1]
        # HP Detection (detect empty part)
        empty_mask_hp = (hp_bar[:,:,0] == hp_bar[:,:,1]) & (hp_bar[:,:,0] == hp_bar[:,:,2])
        empty_pixels_hp = np.count_nonzero(empty_mask_hp)-6 # 6 pixel always be white
        total_pixels_hp = hp_bar.shape[0] * hp_bar.shape[1] - 6
        hp_ratio = 1 - (empty_pixels_hp / total_pixels_hp)

        # MP Detection (detect empty part)
        empty_mask_mp = (mp_bar[:,:,0] == mp_bar[:,:,1]) & (mp_bar[:,:,0] == mp_bar[:,:,2])
        empty_pixels_mp = np.count_nonzero(empty_mask_mp)-6 # 6 pixel always be white
        total_pixels_mp = mp_bar.shape[0] * mp_bar.shape[1] - 6
        mp_ratio = 1 - (empty_pixels_mp / total_pixels_mp)

        # EXP Detection (detect eexpty part)
        empty_mask_exp = (exp_bar[:,:,0] == exp_bar[:,:,1]) & (exp_bar[:,:,0] == exp_bar[:,:,2])
        eexpty_pixels_exp = np.count_nonzero(empty_mask_exp)-6 # 6 pixel always be white
        total_pixels_exp = exp_bar.shape[0] * exp_bar.shape[1] - 6
        exp_ratio = 1 - (eexpty_pixels_exp / total_pixels_exp)

        # Compute original bar dimensions
        hp_h, hp_w = hp_bar.shape[:2]
        mp_h, mp_w = mp_bar.shape[:2]
        exp_h, exp_w = exp_bar.shape[:2]

        # Overlay HP/MP/EXP text
        x_start, y_start = (250, 90)
        cv2.putText(self.img_frame_debug, f""HP: {hp_ratio*100:.1f}%"", (x_start, y_start),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)
        cv2.putText(self.img_frame_debug, f""MP: {mp_ratio*100:.1f}%"", (x_start, y_start+30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)
        cv2.putText(self.img_frame_debug, f""EXP: {exp_ratio*100:.1f}%"", (x_start, y_start+60),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)

        # Paste HP/MP/EXP bar on img_frame_debug
        x_start, y_start = (410, 73)
        self.img_frame_debug[y_start:y_start+hp_h, x_start:x_start+hp_w] = hp_bar
        self.img_frame_debug[y_start+30:y_start+30+mp_h, x_start:x_start+mp_w] = mp_bar
        self.img_frame_debug[y_start+60:y_start+60+exp_h, x_start:x_start+exp_w] = exp_bar

        return hp_ratio, mp_ratio, exp_ratio
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,7
survived,"def _remote_available(url: str) -> bool:
    try:
        req = Request(url, method=""HEAD"")
        with urlopen(req, timeout=3) as resp:
            status = getattr(resp, ""status"", None)
        return bool(status and 200 <= int(status) < 300)
    except Exception:
        return False
",scripts/launch_gallery.py,,1,6
survived,"def _gallery_url() -> str:
    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"").removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    return f""https://{org}.github.io/{repo}/alpha_factory_v1/demos/""
",scripts/launch_gallery.py,,0,6
survived,"    def test_anorm(self):
        vec = np.array([3.0, 4.0])
        self.assertAlmostEqual(common.anorm(vec), 5.0)
        self.assertAlmostEqual(common.anorm2(vec), 25.0)
",tests/test_common.py,TestCommonFunctions,1,7
survived,"    def __init__(self):
        Assimilator.__init__(self)
",sched/testasm.py,TestAssimilator,0,6
survived,"    def test_agents_must_be_positive(self) -> None:
        with self.assertRaises(ValueError):
            run_sim(agents=0, rounds=10, delta=0.5, stake=1.0)
        with self.assertRaises(ValueError):
            run_sim(agents=-1, rounds=10, delta=0.5, stake=1.0)
",tests/test_governance_sim.py,TestGovernanceSim,1,8
survived,"def test_service_worker_registration_failure_toast() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.add_init_script(
            ""navigator.serviceWorker.register = () => Promise.reject('fail')""
        )
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(
            ""document.getElementById('toast').textContent.includes('offline mode disabled')""
        )
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py,,0,6
survived,"def test_gpt2_cli_help() -> None:
    result = subprocess.run([sys.executable, str(SCRIPT), ""--help""], capture_output=True, text=True)
    assert result.returncode == 0
    assert ""usage"" in result.stdout.lower()
",tests/test_gpt2_cli_demo.py,,1,7
survived,"def compose_stack() -> None:
    subprocess.run([""docker"", ""compose"", ""-f"", str(COMPOSE_FILE), ""up"", ""-d""], check=True)
    try:
        yield
    finally:
        subprocess.run([""docker"", ""compose"", ""-f"", str(COMPOSE_FILE), ""down"", ""-v""], check=False)
",tests/test_compose_health.py,,1,7
survived,"def _insert_after_tool(ctx: RunContextWrapper | dict, path: str, anchor: str, code: str) -> str:
    insert_after(path, anchor, code)
    return ""ok""
",src/self_edit/tools.py,,1,7
survived,"def test_undo_idempotent(temp_path: Path) -> None:
    temp_path.write_text(""a\nb\nc\n"")
    insert_after(temp_path, ""b"", ""x"")
    assert ""x"" in temp_path.read_text()
    assert undo_last_edit() is True
    assert temp_path.read_text() == ""a\nb\nc\n""
    assert undo_last_edit() is False
    assert temp_path.read_text() == ""a\nb\nc\n""
",tests/test_tools_undo.py,,1,7
survived,"def _undo_tool(ctx: RunContextWrapper | dict) -> bool:
    return undo_last_edit()
",src/self_edit/tools.py,,1,6
survived,"def _ensure_db(path: Path) -> None:
    with sqlite3.connect(path) as cx:
        cx.execute(
            ""CREATE TABLE IF NOT EXISTS tarballs(id INTEGER PRIMARY KEY AUTOINCREMENT,path TEXT,cid TEXT,pinned INTEGER,ts REAL)""
        )
        cx.execute(
            ""CREATE TABLE IF NOT EXISTS merkle(date TEXT PRIMARY KEY,root TEXT)""
        )
",src/archive/hash_archive.py,,1,7
survived,"    def _select_job(self) -> Job:
        samples: Dict[Job, float] = {}
        for job in self._active_jobs:
            suc, fail = self._stats.get(job, (1, 1))
            samples[job] = random.betavariate(suc, fail)
        best = max(samples, key=samples.get)
        return best
",src/scheduler.py,SelfImprovementScheduler,1,7
survived,"    def _finalize_first_round(self) -> None:
        deltas = list(self._results.values())
        if not deltas:
            self._first_round_done = True
            return
        threshold = sorted(deltas)[len(deltas) // 4]
        for job, delta in self._results.items():
            if delta > threshold:
                self._active_jobs.append(job)
                self._stats[job] = (1 if delta > 0 else 0, 0 if delta > 0 else 1)
        self._first_round_done = True
",src/scheduler.py,SelfImprovementScheduler,1,7
survived,"def _convert_scalar(value: str) -> Any:
    """"""Convert a YAML scalar to a Python type.""""""

    lowered = value.lower()
    if lowered == ""true"":
        return True
    if lowered == ""false"":
        return False
    try:
        if ""."" in value:
            return float(value)
        return int(value)
    except ValueError:
        return value
",src/yaml/__init__.py,,1,6
survived,"def _dump_yaml(obj: Any, indent: int = 0) -> List[str]:
    lines: List[str] = []
    prefix = "" "" * indent
    if isinstance(obj, dict):
        for k, v in obj.items():
            if isinstance(v, (dict, list)):
                lines.append(f""{prefix}{k}:"")
                lines.extend(_dump_yaml(v, indent + 2))
            else:
                lines.append(f""{prefix}{k}: {v}"")
    elif isinstance(obj, list):
        for item in obj:
            if isinstance(item, (dict, list)):
                lines.append(f""{prefix}-"")
                lines.extend(_dump_yaml(item, indent + 2))
            else:
                lines.append(f""{prefix}- {item}"")
    else:
        lines.append(f""{prefix}{obj}"")
    return lines
",src/yaml/__init__.py,,1,6
survived,"        def _model_dump_json(self: BaseModel, *args: Any, **kwargs: Any) -> str:
            return self.json(*args, **kwargs)
",src/meta_agent/__init__.py,,0,7
survived,"    def test_default_ledger_path(self) -> None:
        ledger = Path.home() / "".aiga"" / ""alpha_conversion_log.json""
        if ledger.exists():
            ledger.unlink()
        result = subprocess.run(
            [sys.executable, STUB, ""--alpha"", ""test opportunity""],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertTrue(ledger.exists())
        ledger.unlink()
",tests/test_alpha_conversion_stub.py,TestAlphaConversionStub,1,7
survived,"async def main(args):
    df = pd.read_json(args.JSONLFile, lines=True)

    pbar = tqdm(total=len(df))
    dropped_records = []

    for i, row in df.iterrows():
        question = row[""question""]
        answer = row[""answer""]
        report = row[""report""]
        content = FILTER_TEMPLATE.format(question=question, answer=answer, report=report)
        response, _ = await query(content, enable_thinking=False)
        if not response.startswith(""Yes""):
            print(f""Index {i} è¢«è®¤ä¸ºæ˜¯ä¸åˆé€‚çš„é—®é¢˜æˆ–ç­”æ¡ˆï¼Œå°†å‰”é™¤ï¼Œå“åº”ä¸º {response}"")
            df.drop(index=i, inplace=True)
            dropped_records.append(i)
        pbar.update(1)
    pbar.close()

    if dropped_records:
        with open(f""{args.JSONLFile.replace('.jsonl','')}_dropped.txt"", ""w"") as f:
            f.write(repr(dropped_records))
            f.flush()

    EDIT_TEMPLATE = """"""
    Help me edit the narrative below:
    - If the narrative refers to a report, you change it as if you see it from the radiology image
    - Edit only the places mentioned above, leave all other text the same 
    - Do not add/remove/change any other information
    - Directly output the result text

    **The narrative:**
    ```
    {report}
    ```
    """""".strip()

    results = []
    for index, row in df.iterrows():
        result, _ = await query(EDIT_TEMPLATE.format(report=row[""thinking""]), enable_thinking=False)
        result = result.strip('`\n')
        df.loc[index, ""refined_thinking""] = result
    
    # df.to_json(f""{args.JSONLFile}_refined.jsonl"", orient='records', lines=True)
    with open(f""{args.JSONLFile}_refined.jsonl"", ""w"") as f:
        for index, row in df.iterrows():
            f.write(json.dumps({
                ""image"": row[""image""],
                ""report"": row[""report""],
                ""question"": row[""question""],
                ""thinking"": row[""thinking""],
                ""refined_thinking"": row[""refined_thinking""],
                ""answer"": row[""answer""]
            }, ensure_ascii=False) + ""\n"")
            f.flush()
    THINKING_TEMPLATE = """"""
    You are a radiology medicine expert. Now you are looking at a radiology image.
    Here is your self talk when viewing the image:
    ```
    {thinking_before}
    ```

    Please paraphrase the self talk text and output it as **thinking progress**. Remember:
    - Do not add/remove/alter any information
    - Mind the coherence and fluence of output
    - Deductions are prefered
    - Directly output the result text

    Your output:
    """""".strip()

    groups = df.groupby(""image"")
    pbar = tqdm(total=len(groups))
    full_report = []
    for image, group in groups:
        report = group[""report""].iloc[0]
        thinking_before = """"
        for i, row in group.iterrows():
            thinking_before += row[""question""] + row[""refined_thinking""] + row[""answer""]
        content = THINKING_TEMPLATE.format(report=report, thinking_before=thinking_before)
        thinking_after, _ = await query(content, enable_thinking=False)
        thinking_after = re.sub(r""^(?:\*+)?Thinking Progress:(?:\*+)?"", """", thinking_after, flags=re.IGNORECASE).strip('`\n')
        full_report.append(json.dumps({
            ""image"": image,
            ""report"": report,
            ""question"": questions[random.randint(0, len(questions) - 1)].format(""findings""),
            ""thinking_before"": thinking_before,
            ""thinking_after"": thinking_after,
        }, ensure_ascii=False))
        pbar.update(1)
    pbar.close()

    with open(f""{args.JSONLFile.replace('.jsonl','')}_report_thinking.jsonl"", ""w"") as f:
        for item in full_report:
            f.write(item + ""\n"")
            f.flush()
",src/preprocess/thinking_data_synthesis_refine_and_translation.py,,1,7
survived,"    def _convert_messages(
        self, messages: str | list[str | SamplingMessage]
    ) -> list[SamplingMessage]:
        """"""Convert plain strings to ``SamplingMessage`` objects.""""""

        if isinstance(messages, str):
            messages = [messages]

        converted: list[SamplingMessage] = []
        for msg in messages:
            if isinstance(msg, SamplingMessage):
                converted.append(msg)
            elif isinstance(msg, str):
                converted.append(
                    SamplingMessage(
                        role=""user"",
                        content=TextContent(type=""text"", text=msg),
                    )
                )
            else:
                raise TypeError(""messages must be str or SamplingMessage"")
        return converted
",src/enrichmcp/context.py,EnrichContext,1,7
survived,"        def predict(self, x: int) -> int:
            return x
",tests/trace/test_serialize.py,MyObj,1,8
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,,1,7
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_index.py,,1,7
survived,"    def search(
        self,
        query: str,
        *,
        category: str | None = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """"""Search the index using a simple token overlap ranking.""""""
        if not self._index:
            self.ensure_up_to_date()
        tokens = [t.lower() for t in query.split() if t]
        results = []
        for item in self._index:
            meta = item.get(""metadata"", {})
            if category and meta.get(""category"") != category:
                continue
            if tags and not all(t in meta.get(""tags"", []) for t in tags):
                continue
            haystack = "" "".join(
                [
                    item.get(""content"", """"),
                    meta.get(""title"", """"),
                    meta.get(""description"", """"),
                    meta.get(""slug"", """"),
                    "" "".join(meta.get(""tags"", [])),
                ]
            ).lower()
            score = sum(1 for tok in tokens if tok in haystack)
            if score:
                results.append({**item, ""score"": float(score)})
        results.sort(key=lambda r: r[""score""], reverse=True)
        return results[:limit]",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.index_path = self.registry.templates_dir / self.INDEX_FILE_NAME
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"def tpus_per_node(tpu_type: str) -> int:
    """"""Return the number of TPU chips per node for a given TPU type.""""""
    if tpu_type in {""v4-8"", ""v5p-8""}:
        return 4
    match = re.search(r""-(\d+)$"", tpu_type)
    if not match:
        raise ValueError(f""Cannot parse TPU type: {tpu_type}"")
    chips = int(match.group(1))
    if chips > 8:
        raise ValueError(""Only single tpu nodes are supported with the CLI"")
    return chips
",marin/run/ray_run.py,,1,7
survived,"    def test_cli_invalid_port_error(self) -> None:
        with patch.dict(os.environ, {}, clear=True):
            with self.assertRaises(SystemExit):
                edge_runner.parse_args([""--port"", ""0""])
",tests/test_edge_runner_cli.py,TestParseArgs,1,7
survived,"def _env_int(name: str, default: int) -> int:
    """"""Return ``int`` environment value or ``default`` if conversion fails.""""""

    try:
        return int(ENV(name, default))
    except (TypeError, ValueError):
        return default
",alpha_factory_v1/backend/orchestrator.py,,1,7
survived,"    def __init__(self, execution_module: Optional[ExecutionModule] = None) -> None:
        self.execution_module = execution_module or ExecutionModule()
",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule,1,7
survived,"def zero():
    return Pt(x=0.0, y=0.0, inf=True)
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,,1,7
survived,"def pow10(n):
    r = 1.0
    i = 0
    while i < n:
        r = r * 10.0
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/element-wise-operations.py,,1,6
survived,"def elementWiseMM(m1, m2, f):
    z = []
    r = 0
    while r < len(m1):
        row = []
        c = 0
        while c < len(m1[r]):
            row = row + [f(m1[r][c], m2[r][c])]
            c = c + 1
        z = z + [row]
        r = r + 1
    return z
",tests/rosetta/transpiler/Python/element-wise-operations.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,,1,7
survived,"def entropy(data):
    if data == """":
        return 0.0
    counts = {}
    i = 0
    while i < len(data):
        ch = data[i:i + 1]
        if ch in counts:
            counts[ch] = counts[ch] + 1
        else:
            counts[ch] = 1
        i = i + 1
    e = 0.0
    l = float(len(data))
    for ch in counts:
        px = (float(counts[ch])) / l
        if px > 0.0:
            e = e - px * log2(px)
    return e
",tests/rosetta/transpiler/Python/entropy-narcissist.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/entropy-narcissist.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/empty-string-1.py,,1,6
survived,"def check(s):
    if len(s) == 0:
        print(""empty"")
    else:
        print(""not empty"")
",tests/rosetta/transpiler/Python/empty-string-2.py,,1,6
survived,"def neg(p):
    return Pt(x=p.x, y=-p.y, inf=p.inf)
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,,1,7
survived,"def cbrtApprox(x):
    guess = x
    i = 0
    while i < 40:
        guess = (2.0 * guess + x // (guess * guess)) / 3.0
        i = i + 1
    return guess
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,,1,7
survived,"def btoi(b):
    if b:
        return 1
    return 0
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/ekg-sequence-convergence.py,,1,6
survived,"def randInit(cells, seed):
    s = """"
    val = seed
    i = 0
    while i < cells:
        val = (val * 1664525 + 1013904223) % 2147483647
        if val % 2 == 0:
            s = s + ""0""
        else:
            s = s + ""1""
        i = i + 1
    return s
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,,1,6
survived,"        def _listen() -> None:
            ws_url = f""ws://127.0.0.1:{port}/ws/progress""
            with websockets.connect(ws_url, additional_headers=headers) as ws:
                try:
                    while True:
                        msg = ws.recv()
                        progress.append(msg)
                        if progress:
                            break
                except Exception:
                    pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_subprocess.py,,1,6
survived,"def test_fstringify_files_charcount(tmp_path, monkeypatch):
    source = ""'{}'.format(1)\n""
    f = tmp_path / ""a.py""
    f.write_text(source)

    captured = {}

    def fake_print_report(state, found_files, changed_files, total_cc_new, total_cc_original, total_expr, total_time):
        captured[""new""] = total_cc_new
        captured[""orig""] = total_cc_original

    monkeypatch.setattr(api, ""_print_report"", fake_print_report)

    state = State()
    api.fstringify_files([str(f)], state)

    assert captured[""orig""] == len(source)
    assert captured[""new""] == len(""f'{1}'\n"")",test/integration/test_api.py,,0,7
survived,"    def test_reset_batch_invalid_size(self):
        env = ce.CurriculumEnv(genome=ce.EnvGenome(max_steps=10), size=6)
        with self.assertRaises(ValueError):
            env.reset_batch(0)
",alpha_factory_v1/tests/test_aiga_meta_evolution.py,CurriculumEnvTest,1,7
survived,"    def test_entrypoint_compiles(self):
        py_compile.compile(ENTRYPOINT, doraise=True)
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry,1,8
survived,"def test_cli_overrides_env(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""CLI options should override preset environment variables.""""""
    if MODULE in sys.modules:
        del sys.modules[MODULE]
    mod = importlib.import_module(MODULE)

    monkeypatch.setattr(mod, ""check_env"", types.SimpleNamespace(main=lambda *_a, **_k: None), raising=False)

    monkeypatch.setenv(""OPENAI_API_KEY"", ""env-key"")
    monkeypatch.setenv(""ADK_HOST"", ""http://env-adk:8"")
    monkeypatch.setenv(""A2A_PORT"", ""1234"")
    monkeypatch.setenv(""A2A_HOST"", ""env-host"")
    monkeypatch.setenv(""LOCAL_LLM_URL"", ""http://env-llm"")
    monkeypatch.setenv(""LLAMA_MODEL_PATH"", ""/env/model.gguf"")
    monkeypatch.setenv(""LLAMA_N_CTX"", ""99"")

    captured: dict[str, Any] = {}

    async def _llm(_: float) -> str:
        captured[""api_key""] = os.getenv(""OPENAI_API_KEY"")
        captured[""local_llm_url""] = os.getenv(""LOCAL_LLM_URL"")
        captured[""llama_model_path""] = os.getenv(""LLAMA_MODEL_PATH"")
        captured[""llama_n_ctx""] = os.getenv(""LLAMA_N_CTX"")
        return ""ok""

    class DummyADK:
        def __init__(self, host: str) -> None:  # pragma: no cover - init only
            captured[""adk_host""] = host

    class DummySock:
        def __init__(self, host: str, port: int, app_id: str) -> None:
            captured[""a2a""] = f""{host}:{port}""  # pragma: no cover - record args

        def start(self) -> None:  # pragma: no cover - unused
            pass

        def stop(self) -> None:  # pragma: no cover - unused
            pass

        def sendjson(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - unused
            pass

    monkeypatch.setattr(mod, ""_llm_comment"", _llm)
    monkeypatch.setattr(mod, ""ADKClient"", DummyADK)
    monkeypatch.setattr(mod, ""A2ASocket"", DummySock)

    asyncio.run(
        mod.main(
            [
                ""--cycles"",
                ""1"",
                ""--interval"",
                ""0"",
                ""--openai-api-key"",
                ""cli-key"",
                ""--adk-host"",
                ""http://cli-adk:9"",
                ""--a2a-port"",
                ""7777"",
                ""--a2a-host"",
                ""cli-host"",
                ""--local-llm-url"",
                ""http://cli-llm"",
                ""--llama-model-path"",
                ""/cli/model.gguf"",
                ""--llama-n-ctx"",
                ""120"",
            ]
        )
    )

    assert captured[""api_key""] == ""cli-key""
    assert captured[""adk_host""] == ""http://cli-adk:9""
    assert captured[""a2a""] == ""cli-host:7777""
    assert captured[""local_llm_url""] == ""http://cli-llm""
    assert captured[""llama_model_path""] == ""/cli/model.gguf""
    assert captured[""llama_n_ctx""] == ""120""
",tests/test_alpha_agi_business_3_v1.py,,1,7
survived,"            async def send_transaction(self, tx: Any, *args: Any) -> None:
                captured[""root""] = tx.instructions[0].data.decode()
                raise RuntimeError(""fail"")
",tests/test_ledger_broadcast.py,DummyClient,1,6
survived,"def test_results_dir_permissions(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    path = tmp_path / ""results""
    monkeypatch.setenv(""SIM_RESULTS_DIR"", str(path))

    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface import api_server

    api_server = importlib.reload(api_server)

    assert path.exists()
    assert (path.stat().st_mode & 0o777) == 0o700",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_results_dir_permissions.py,,0,7
survived,"    def test_share_checksum(self):
        share = ""hearing echo academic acid deny bracelet playoff exact fancy various evidence standard adjust muscle parcel sled crucial amazing mansion losing""
        wallet = btcrseed.WalletSLIP39Seed.create_from_params()
        wallet.config_mnemonic(share)
        self.assertEqual(wallet.return_verified_password_or_false((btcrseed.mnemonic_ids_guess,)),
                         (btcrseed.mnemonic_ids_guess, 1))
",btcrecover/test/test_seeds.py,TestSLIP39Seed,1,6
survived,"    def convert(self, node):
        self.visit(node)
        if self.lines and self.lines[-1] != """":
            self.lines.append("""")
        return ""\n"".join(self.lines)
",tools/any2mochi/py_simple.py,Conv,1,6
survived,"def main() -> None:
    app.run()
",examples/mutable_crud/app.py,,1,7
survived,"    def update_note(self, note_id: str, patch: dict[str, object]) -> MemoryNote:
        note = self.get_note(note_id)
        if note is None:
            raise KeyError(note_id)
        updated = note.model_copy(update=patch)
        self.store.save(self.name, updated)
        return updated
",examples/basic_memory/memory.py,MemoryProject,1,8
survived,"    def __init__(self, name: str, store: MemoryStore) -> None:
        self.name = name
        self.store = store
",examples/basic_memory/memory.py,MemoryProject,1,8
survived,"def _have_torch():
    try:
        import torch  # noqa: F401

        return True
    except Exception:
        return False
",tests/test_torch_backend.py,,1,7
survived,"    def run_integrate(self):
        """"""Integrate positions and velocities using current acceleration.""""""
        dt = self.config[""time_step""]
        self.sorted_velocity += dt * self.acceleration
        self.sorted_position += dt * self.sorted_velocity
        inv = torch.argsort(self.particle_index_back)
        self.position = self.sorted_position[inv]
        self.velocity = self.sorted_velocity[inv]",pytorch_solver.py,PytorchSolver,1,8
survived,"    def _noop(*_a: Any, **_kw: Any) -> Any:
        class _N:
            def labels(self, *_a: Any, **_kw: Any) -> ""_N"":
                return self

            def observe(self, *_a: Any) -> None: ...

            def inc(self, *_a: Any) -> None: ...

        return _N()
",src/interface/api_server.py,,1,6
survived,"    async def _metrics() -> Response:
        if ""generate_latest"" not in globals():
            raise HTTPException(status_code=503, detail=""prometheus_client not installed"")
        return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)
",src/interface/api_server.py,,1,7
survived,"def test_planning_agent_handle_logs() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = planning_agent.PlanningAgent(bus, led)
    env = messaging.Envelope(""a"", ""planning"", {""plan"": ""x""}, 0.0)
    asyncio.run(agent.handle(env))
    assert led.logged and led.logged[0] is env
",tests/test_agent_handle_methods.py,,0,6
survived,"def test_market_agent_emits_codegen() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = market_agent.MarketAgent(bus, led)
    env = messaging.Envelope(""strategy"", ""market"", {""strategy"": ""foo""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][0] == ""codegen""
",tests/test_agent_handle_methods.py,,1,7
survived,"def test_bundle_metadata_custom_fields_preserved():
    data = {
        ""schema_version"": BUNDLE_SCHEMA_VERSION,
        ""meta_agent_version"": ""0.1.0"",
        ""foo"": ""bar"",
        ""custom"": {""x"": 1},
    }
    meta = BundleMetadata(**data)
    assert meta.meta_agent_version == ""0.1.0""
    assert meta.custom == {""x"": 1}
    assert getattr(meta, ""foo"") == ""bar""",tests/test_bundle_metadata.py,,1,7
survived,"def test_resolve_with_hashes():
    manager = DependencyManager()
    reqs, licenses, hashes = manager.resolve([""pydantic""], include_hashes=True)
    assert any(r.startswith(""pydantic=="") for r in reqs)
    assert isinstance(hashes, dict)
    assert ""pydantic"" in hashes
    assert re.fullmatch(r""[0-9a-f]{64}"", hashes[""pydantic""])",tests/test_dependency_manager.py,,0,6
survived,"    def force_garbage_collection(self):
        """"""
        å¼ºåˆ¶åžƒåœ¾å›žæ”¶å¹¶è¿”å›žæ¸…ç†çš„å¯¹è±¡æ•°é‡
        """"""
        try:
            collected = gc.collect()
            logger.info(f""å¼ºåˆ¶åžƒåœ¾å›žæ”¶å®Œæˆï¼Œæ¸…ç†äº† {collected} ä¸ªå¯¹è±¡"")
            return collected
        except Exception as e:
            logger.error(f""å¼ºåˆ¶åžƒåœ¾å›žæ”¶å¤±è´¥: {e}"")
            return 0
",app/helper/memory.py,MemoryHelper,1,7
survived,"    def get_memory_summary(self) -> Dict[str, float]:
        """"""
        èŽ·å–å†…å­˜ä½¿ç”¨æ‘˜è¦
        """"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            
            # èŽ·å–Pythonå¯¹è±¡æ€»å†…å­˜
            all_objects = muppy.get_objects()
            sum1 = summary.summarize(all_objects)
            
            python_total_mb = 0
            for line in summary.format_(sum1):
                if '|' in line and line.strip() and not line.startswith('=') and not line.startswith('-'):
                    parts = line.split('|')
                    if len(parts) >= 3:
                        try:
                            size_str = parts[2].strip()
                            if 'MB' in size_str:
                                size_mb = float(size_str.replace('MB', '').strip())
                                python_total_mb += size_mb
                        except:
                            pass
            
            total_memory = memory_info.rss / 1024 / 1024
            unaccounted = total_memory - python_total_mb
            
            return {
                'total_memory_mb': total_memory,
                'python_objects_mb': python_total_mb,
                'unaccounted_mb': unaccounted,
                'unaccounted_percent': (unaccounted / total_memory * 100) if total_memory > 0 else 0
            }
        except Exception as e:
            logger.error(f""èŽ·å–å†…å­˜æ‘˜è¦å¤±è´¥: {e}"")
            return {}
",app/helper/memory.py,MemoryHelper,1,7
deleted,"    def _canonicalize_url(self, url: str) -> str:
        """"""Canonicalize URL using transformer logic""""""
        from package_managers.pkgx.transformer import PkgxTransformer

        temp_transformer = PkgxTransformer(self.config, None)
        return temp_transformer.canonicalize(url)
",package_managers/pkgx/diff.py,PkgxDiff,1,6
survived,"    def test_dependency_type_priority_no_change(self, mock_config, mock_logger):
        """"""Test case 1: p1 has runtime dependency to p2 in cache,
        p1 depends on p2 as both runtime and build in parsed data.
        Expect no change (runtime has priority).""""""

        # Setup existing package and dependencies
        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""pkgx/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""pkgx/p2"", name=""p2"", import_id=""p2"")

        # Existing runtime dependency in cache
        existing_runtime_dep = LegacyDependency(
            package_id=p1_id,
            dependency_id=p2_id,
            dependency_type_id=mock_config.dependency_types.runtime,
        )

        cache = Cache(
            package_map={""p1"": p1_pkg, ""p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={p1_id: {existing_runtime_dep}},
        )

        # Parsed data has p2 as both runtime and build dependency
        new_pkg_data = create_pkgx_package(
            dependencies=[""p2""],  # runtime
            build_deps=[""p2""],  # build
        )

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""p1"", new_pkg_data)

        # Should have no changes - runtime priority means no change needed
        assert len(new_deps) == 0
        assert len(removed_deps) == 0
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading,1,7
survived,"        def process_deps(dependencies: list[DependencyBlock], dep_type: UUID) -> None:
            """"""Helper to process dependencies of a given type with priority""""""
            for dep in dependencies:
                for dep_obj in dep.dependencies:
                    if not dep_obj.name:
                        continue

                    # Get the dependency package from cache
                    dependency = self.caches.package_map.get(dep_obj.name)
                    if not dependency:
                        self.logger.warn(
                            f""{dep_obj.name}, dep of {import_id} is not in cache""
                        )
                        continue

                    # If this dependency already exists in our map, choose higher priority
                    if dep_obj.name in dependency_map:
                        existing_priority = priority_order.get(
                            dependency_map[dep_obj.name], 999
                        )
                        new_priority = priority_order.get(dep_type, 999)

                        if (
                            new_priority < existing_priority
                        ):  # Lower number = higher priority
                            old_type_id = dependency_map[dep_obj.name]
                            dependency_map[dep_obj.name] = dep_type
                            self.logger.debug(
                                f""Updated dependency type for {dep_obj.name} from ""
                                f""{old_type_id} to {dep_type} (higher priority)""
                            )
                    else:
                        dependency_map[dep_obj.name] = dep_type
",package_managers/pkgx/diff.py,PkgxDiff,1,7
survived,"def test_reasoning_effort_parameter_mapping():
    """"""Test that reasoning_effort parameter is correctly mapped""""""
    print(""Testing reasoning_effort parameter mapping..."")
    
    os.environ[""LITELLM_LOCAL_MODEL_COST_MAP""] = ""True""
    litellm.model_cost = litellm.get_model_cost_map(url="""")
    
    model = ""perplexity/sonar-reasoning""
    reasoning_effort = ""high""
    
    # Get provider and optional params
    _, provider, _, _ = litellm.get_llm_provider(model=model)
    
    optional_params = get_optional_params(
        model=model,
        custom_llm_provider=provider,
        reasoning_effort=reasoning_effort,
    )
    
    print(f""Provider: {provider}"")
    print(f""Optional params: {optional_params}"")
    
    # Verify that reasoning_effort is preserved in optional_params for Perplexity
    assert ""reasoning_effort"" in optional_params, ""reasoning_effort should be in optional_params""
    assert optional_params[""reasoning_effort""] == reasoning_effort, f""reasoning_effort should be {reasoning_effort}""
    
    print(""âœ“ Reasoning effort parameter mapping test passed!\n"")
",verify_perplexity_reasoning.py,,1,7
survived,"    def column_rename_mapping(self) -> Dict[str, str]:
        """"""Get column rename mapping from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_rename_mapping""
        ]
        if not config:
            return {}
        return config[0].get(""value"", {})
",keep/api/models/db/preset.py,PresetDto,1,7
survived,"def test_semantic_unnest_list():
    """"""Test semantic unnest operation with list values.""""""
    df = pd.DataFrame({
        ""id"": [1, 2],
        ""tags"": [[""python"", ""pandas"", ""data""], [""ml"", ""ai""]]
    })
    
    result = df.semantic.unnest(unnest_key=""tags"")
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) == 5  # 3 + 2 tags
    assert all(result.columns == [""id"", ""tags""])
    
    # Check that each tag becomes a separate row
    expected_tags = [""python"", ""pandas"", ""data"", ""ml"", ""ai""]
    actual_tags = result[""tags""].tolist()
    assert set(actual_tags) == set(expected_tags)
    
    # Check that original data is preserved
    python_rows = result[result[""tags""] == ""python""]
    assert len(python_rows) == 1
    assert python_rows.iloc[0][""id""] == 1
",tests/test_pandas_accessors.py,,1,7
survived,"    def test_status_utils_with_invalid_resources(self):
        """"""Test status utility functions handle invalid resources gracefully.""""""
        # Create a mock cluster record with problematic resources
        mock_record = {
            'status': None,
            'num_nodes': 1,
            'resources': None,  # Problematic: None resources
            'total_cost': 0.0
        }
        
        # These should not crash even with None resources
        try:
            status_utils._get_resources_for_cost_report(mock_record, truncate=True)
            status_utils._get_price_for_cost_report(mock_record, truncate=True)
            status_utils._get_estimated_cost_for_cost_report(mock_record, truncate=True)
        except (AttributeError, TypeError):
            # Expected - the functions might fail gracefully, but shouldn't crash the whole system
            pass
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness,1,6
survived,"    def test_cost_report_cli_function_calls(self, mock_get_total, mock_show_table,
                                          mock_sdk_cost_report, mock_sdk_get):
        """"""Test cost-report CLI function calls with mocking.""""""
        from sky.client.cli import command
        
        mock_sdk_cost_report.return_value = 'request_id'
        mock_sdk_get.return_value = []
        mock_get_total.return_value = 0.0
        
        # Test the internal logic by calling the CLI function directly
        # Use mock context to simulate different days parameter
        with mock.patch('sys.argv', ['sky', 'cost-report', '--days', '7']):
            # Mock click context
            with mock.patch('click.get_current_context') as mock_ctx:
                mock_ctx.return_value.params = {'all': False, 'days': 7}
                
                # Test that the function would call SDK with correct params
                # This tests the core logic without click dependency
                command.cost_report(all=False, days=7)
                
                mock_sdk_cost_report.assert_called_once_with(days=7)
",tests/unit_tests/test_sky_cost_report.py,TestCostReportCLI,1,7
survived,"def test_severity_greater_than_info_bug_fix(
    db_session, workflow_manager, create_workflow, create_alert
):
    """"""
    Test the specific bug case from GitHub issue #5086:
    severity > 'info' should match 'warning', 'high', and 'critical' severities
    
    Before fix: This would fail because 'high' < 'info' lexicographically (h < i)
    After fix: This works because high (4) > info (2) numerically
    """"""
    # Create a workflow with the exact CEL expression from the bug report
    workflow = create_workflow(
        ""test-severity-gt-info-bug"", 
        ""severity > 'info' && source.contains('prometheus')""
    )

    # These alerts should match (severity > info)
    high_alert = create_alert(
        severity=AlertSeverity.HIGH, 
        fingerprint=""fp-high""
    )
    critical_alert = create_alert(
        severity=AlertSeverity.CRITICAL, 
        fingerprint=""fp-critical""
    )
    warning_alert = create_alert(
        severity=AlertSeverity.WARNING, 
        fingerprint=""fp-warning""
    )

    # These alerts should NOT match
    info_alert = create_alert(
        severity=AlertSeverity.INFO, 
        fingerprint=""fp-info""
    )
    low_alert = create_alert(
        severity=AlertSeverity.LOW, 
        fingerprint=""fp-low""
    )

    # Test high severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [high_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test critical severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [critical_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test warning severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [warning_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test info severity alert (should NOT match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [info_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before

    # Test low severity alert (should NOT match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [low_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before
",tests/test_workflow_severity_comparisons.py,,1,7
survived,"async def async_rollout_tau_bench_task(
    model: art.Model[TauBenchPolicyConfig],
    task_index: int,
) -> art.Trajectory:
    """"""
    Async wrapper for rollout_tau_bench_task using asyncio.to_thread().
    This allows the sync tau-bench infrastructure to work with the async ART framework.
    """"""
    return await asyncio.to_thread(rollout_tau_bench_task, model, task_index)
",dev/tau-bench/run_rl.py,,1,7
survived,"    def test_missing_dependency_handling(self, mock_config, mock_logger, mock_db):
        """"""Tests the case that we DON'T add dependencies for new packages""""""

        existing_pkg_id = uuid4()
        existing_package = Package(
            id=existing_pkg_id,
            derived_id=""debian/missing-dep-pkg"",
            name=""missing-dep-pkg"",
            import_id=""missing-dep-pkg"",
        )

        cache = Cache(
            package_map={""missing-dep-pkg"": existing_package},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Create package with dependency that doesn't exist in cache
        pkg_data = create_debian_package(
            package=""missing-dep-pkg"", depends=[""non-existent-dep""]
        )

        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""missing-dep-pkg"", pkg_data)

        # Should handle gracefully - no deps added for missing packages
        assert len(new_deps) == 0
        assert len(removed_deps) == 0
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading,1,7
survived,"def diff(
    data: list[DebianData], config: Config, cache: Cache, db: DebianDB, logger: Logger
) -> DiffResult:
    # Keeps track of all the new packages we're adding
    seen: dict[str, UUID] = {}
    seen_new_pkg_urls: set[tuple[UUID, UUID]] = set()

    # Objects that we will return
    new_packages: list[Package] = []
    new_urls: dict[URLKey, URL] = {}
    new_package_urls: list[PackageURL] = []
    updated_packages: list[dict[str, UUID | str | datetime]] = []
    updated_package_urls: list[dict[str, UUID | datetime]] = []
    new_deps: list[LegacyDependency] = []
    removed_deps: list[LegacyDependency] = []

    # Create diff processor
    diff = DebianDiff(config, cache, db, logger)

    # Process each enriched package
    for i, debian_data in enumerate(data):
        import_id = f""debian/{debian_data.package}""
        if not import_id:
            logger.warn(f""Skipping package with empty name at index {i}"")
            continue

        # Diff the package
        pkg_id, pkg_obj, update_payload = diff.diff_pkg(import_id, debian_data)

        # Guard: if pkg_obj is not None, that means it's a new package
        # If it's new, **and** we have seen it before, set the ID to what is seen
        # So, duplicates absorb all URLs & Dependencies under one umbrella
        resolved_pkg_id = seen.get(pkg_obj.import_id, pkg_id) if pkg_obj else pkg_id

        if pkg_obj and pkg_obj.import_id not in seen:
            logger.debug(f""New package: {pkg_obj.name}"")
            new_packages.append(pkg_obj)
            seen[pkg_obj.import_id] = resolved_pkg_id
        if update_payload:
            logger.debug(f""Updated package: {update_payload['id']}"")
            updated_packages.append(update_payload)

        # Diff URLs (resolved_urls is map of url types to final URL ID)
        resolved_urls = diff.diff_url(import_id, debian_data, new_urls)

        # Diff package URLs
        new_links, updated_links = diff.diff_pkg_url(resolved_pkg_id, resolved_urls)
        if new_links:
            logger.debug(f""New package URLs: {len(new_links)}"")

            # guard: only add truly new links
            for link in new_links:
                if (link.package_id, link.url_id) not in seen_new_pkg_urls:
                    new_package_urls.append(link)
                    seen_new_pkg_urls.add((link.package_id, link.url_id))

        if updated_links:
            updated_package_urls.extend(updated_links)

        # Diff dependencies
        new_dependencies, removed_dependencies = diff.diff_deps(import_id, debian_data)
        if new_dependencies:
            logger.debug(f""New dependencies: {len(new_dependencies)}"")
            new_deps.extend(new_dependencies)
        if removed_dependencies:
            logger.debug(f""Removed dependencies: {len(removed_dependencies)}"")
            removed_deps.extend(removed_dependencies)

        if config.exec_config.test and i > 2:
            break

    return DiffResult(
        new_packages,
        new_urls,
        new_package_urls,
        updated_packages,
        updated_package_urls,
        new_deps,
        removed_deps,
    )
",package_managers/debian/main.py,,1,7
survived,"    def test_package_exists_url_update(self, mock_config, mock_logger, mock_db):
        """"""Tests that Diff updates URLs when the package exists and the URL changes""""""

        # Setup existing package and URL
        existing_pkg_id = uuid4()
        existing_url_id = uuid4()
        existing_package_url_id = uuid4()

        existing_package = Package(
            id=existing_pkg_id,
            derived_id=""debian/url-pkg"",
            name=""url-pkg"",
            package_manager_id=mock_config.pm_config.pm_id,
            import_id=""url-pkg"",
            readme=""Test package"",
        )

        existing_url = URL(
            id=existing_url_id,
            url=""https://old-homepage.com"",
            url_type_id=mock_config.url_types.homepage,
        )

        existing_package_url = PackageURL(
            id=existing_package_url_id,
            package_id=existing_pkg_id,
            url_id=existing_url_id,
        )

        # Create cache
        cache = Cache(
            package_map={""url-pkg"": existing_package},
            url_map={
                URLKey(
                    ""https://old-homepage.com"", mock_config.url_types.homepage
                ): existing_url
            },
            package_urls={existing_pkg_id: {existing_package_url}},
            dependencies={},
        )

        # Create package data with new URL
        new_pkg_data = create_debian_package(
            package=""url-pkg"",
            homepage=""https://new-homepage.com"",
        )
        new_urls = {}  # this tracks all the new URLs we've created so far

        # Test the diff
        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        resolved_urls = diff.diff_url(""url-pkg"", new_pkg_data, new_urls)
        new_links, _ = diff.diff_pkg_url(existing_pkg_id, resolved_urls)

        # Assertions
        assert len(new_links) == 1  # New URL should be created
        new_link = new_links[0]
        assert new_link.package_id == existing_pkg_id

        # The URL should be created in new_urls dict and the link should reference it
        assert len(new_urls) == 1  # One new URL should be created
        new_url_key = next(iter(new_urls.keys()))
        new_url = new_urls[new_url_key]
        assert new_link.url_id == new_url.id  # Link should reference the new URL
        assert new_url_key.url == ""https://new-homepage.com""
        assert new_url_key.url_type_id == mock_config.url_types.homepage
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading,1,7
survived,"def ping():
    """"""Health check endpoint""""""
    return jsonify({
        'status': 'success',
        'message': 'pong',
        'timestamp': time.time()
    })
",server/health.py,,1,8
survived,"def get_task_status(task_id):
    """"""Get the status of a specific task""""""
    if task_id not in tasks:
        logger.warning(f""ðŸ” Frontend polling for unknown task: {task_id}"")
        return jsonify({'error': 'Task not found'}), 404
    
    task = tasks[task_id]
    logger.info(f""ðŸ“Š Frontend polling task {task_id}: status={task['status']}"")
    
    return jsonify({
        'status': 'success',
        'task': {
            'id': task['id'],
            'status': task['status'],
            'prompt': task['prompt'],
            'repo_url': task['repo_url'],
            'branch': task['branch'],
            'model': task.get('model', 'claude'),  # Include model in response
            'commit_hash': task.get('commit_hash'),
            'changed_files': task.get('changed_files', []),
            'error': task.get('error'),
            'created_at': task['created_at']
        }
    })
",server/tasks.py,,1,7
survived,"def save_tasks():
    """"""Save tasks to file for persistence""""""
    try:
        with open(TASKS_FILE, 'w') as f:
            json.dump(tasks, f, indent=2, default=str)
        logger.info(f""ðŸ’¾ Saved {len(tasks)} tasks to {TASKS_FILE}"")
    except Exception as e:
        logger.warning(f""âš ï¸ Failed to save tasks: {e}"")
",server/utils.py,,1,7
survived,"def home():
    """"""Root endpoint""""""
    return jsonify({
        'status': 'success',
        'message': 'Claude Code Automation API',
        'endpoints': ['/ping', '/start-task', '/task-status', '/git-diff', '/create-pr']
    })",server/health.py,,1,7
survived,"def update_project(project_id):
    """"""Update a project""""""
    try:
        data = request.get_json()
        user_id = request.headers.get('X-User-ID')
        
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        # If repo_url is being updated, parse it
        if 'repo_url' in data:
            try:
                repo_owner, repo_name = parse_github_url(data['repo_url'])
                data['repo_owner'] = repo_owner
                data['repo_name'] = repo_name
            except ValueError as e:
                return jsonify({'error': str(e)}), 400
        
        project = DatabaseOperations.update_project(project_id, user_id, data)
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        return jsonify({
            'status': 'success',
            'project': project
        })
        
    except Exception as e:
        logger.error(f""Error updating project {project_id}: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/projects.py,,1,7
survived,"    def create_task(user_id: str, project_id: int = None, repo_url: str = None, 
                   target_branch: str = 'main', agent: str = 'claude', 
                   chat_messages: List[Dict] = None) -> Dict:
        """"""Create a new task""""""
        try:
            task_data = {
                'user_id': user_id,
                'project_id': project_id,
                'repo_url': repo_url,
                'target_branch': target_branch,
                'agent': agent,
                'status': 'pending',
                'chat_messages': chat_messages or [],
                'execution_metadata': {}
            }
            
            result = supabase.table('tasks').insert(task_data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error creating task: {e}"")
            raise
",server/database.py,DatabaseOperations,1,7
survived,"    def get_user_tasks(user_id: str, project_id: int = None) -> List[Dict]:
        """"""Get all tasks for a user, optionally filtered by project""""""
        try:
            query = supabase.table('tasks').select('*').eq('user_id', user_id)
            if project_id:
                query = query.eq('project_id', project_id)
            result = query.order('created_at', desc=True).execute()
            return result.data or []
        except Exception as e:
            logger.error(f""Error fetching user tasks: {e}"")
            raise
",server/database.py,DatabaseOperations,1,7
survived,"def internal_error(error):
    return jsonify({'error': 'Internal server error'}), 500
",server/main.py,,1,6
survived,"def sanitize_filename(filename: str) -> str:
    """"""Sanitize filename to prevent path traversal attacks""""""
    # Remove directory separators and other dangerous characters
    filename = os.path.basename(filename)
    # Remove any remaining path separators that might exist
    filename = filename.replace("".."", """").replace(""/"", """").replace(""\\"", """")
    # Ensure the filename is not empty after sanitization
    if not filename or filename.startswith('.'):
        filename = ""uploaded_file""
    return filename
",server/utils/file.py,,1,7
survived,"    def test_mcp_no_api_key_required(self, runner, temp_python_script):
        """"""Test that MCP mode doesn't require LANGFLOW_API_KEY.""""""
        # Ensure no API key is set
        with patch.dict(""os.environ"", {}, clear=True):
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--verbose""
            ])
            
            # Should not fail due to missing API key
            # The validation message should show MCP is enabled
            assert ""MCP mode enabled"" in result.output or result.exit_code in [0, 1]
            # Should not show API key validation error
            assert ""LANGFLOW_API_KEY"" not in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand,1,7
survived,"    def test_flow_input_model(self):
        """"""Test FlowInput model validation.""""""
        # Valid input
        flow_input = FlowInput(input_value=""test input"")
        assert flow_input.input_value == ""test input""
        assert flow_input.tweaks is None

        # With tweaks
        flow_input_with_tweaks = FlowInput(
            input_value=""test input"",
            tweaks={""param1"": ""value1""}
        )
        assert flow_input_with_tweaks.tweaks == {""param1"": ""value1""}
",src/backend/tests/unit/test_mcp_server.py,TestFlowModels,1,7
survived,"    def test_mcp_server_creation_folder(self, mock_run_mcp, mock_create_mcp, runner, tmp_path):
        """"""Test MCP server creation for folder with multiple flows.""""""
        # Create test JSON files
        flow1 = tmp_path / ""flow1.json""
        flow2 = tmp_path / ""flow2.json""
        
        # Create minimal valid JSON flow structure
        flow_content = {
            ""data"": {
                ""nodes"": [],
                ""edges"": []
            }
        }
        
        flow1.write_text(json.dumps(flow_content))
        flow2.write_text(json.dumps(flow_content))
        
        # Mock the graph loading to avoid complex flow parsing
        with patch(""langflow.cli.commands.load_graph_from_path"") as mock_load_graph:
            mock_graph = MagicMock()
            mock_graph.flow_id = ""test_flow""
            mock_load_graph.return_value = mock_graph
            
            # Mock MCP server components
            mock_mcp_server = MagicMock()
            mock_create_mcp.return_value = mock_mcp_server
            mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(tmp_path),
                ""--mcp"", ""--mcp-transport"", ""sse"",
                ""--port"", ""8001"",
                ""--verbose""
            ])
            
            # Should find both JSON files and try to load them
            assert mock_load_graph.call_count == 2
            
            # Verify MCP server was created
            mock_create_mcp.assert_called_once()
            
            # Verify MCP server was run with correct transport and port
            mock_run_mcp.assert_called_once()
            run_args = mock_run_mcp.call_args
            assert run_args[1][""transport""] == ""sse""
            assert run_args[1][""port""] == 8001
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand,0,6
survived,"    def mock_meta(self):
        """"""Create a mock meta object.""""""
        meta = MagicMock()
        meta.title = ""Test Flow""
        meta.description = ""Test flow description""
        return meta
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerCreation,1,6
survived,"    def test_mcp_folder_no_json_files(self, runner, tmp_path):
        """"""Test MCP mode with folder containing no JSON files.""""""
        # Create a folder with no JSON files
        (tmp_path / ""not_a_flow.txt"").write_text(""This is not a flow"")
        
        result = runner.invoke(app, [
            ""serve"", str(tmp_path),
            ""--mcp"", ""--verbose""
        ])
        
        assert result.exit_code == 1
        assert ""No .json flow files found"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand,0,6
survived,"async def trace_transfer_fixture(db: DbSessionFactory) -> dict[str, int]:
    async with db() as session:
        source_project_id = await session.scalar(
            insert(models.Project).values(name=""source-project"").returning(models.Project.id)
        )
        assert source_project_id is not None

        dest_project_id = await session.scalar(
            insert(models.Project).values(name=""dest-project"").returning(models.Project.id)
        )
        assert dest_project_id is not None

        other_project_id = await session.scalar(
            insert(models.Project).values(name=""other-project"").returning(models.Project.id)
        )
        assert other_project_id is not None

        session_id = await session.scalar(
            insert(models.ProjectSession)
            .values(
                session_id=""test-session-1"",
                project_id=source_project_id,
                start_time=datetime.fromisoformat(""2021-01-01T00:00:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:02:00.000+00:00""),
            )
            .returning(models.ProjectSession.id)
        )
        assert session_id is not None

        trace1_id = await session.scalar(
            insert(models.Trace)
            .values(
                trace_id=""test-trace-id-1"",
                project_rowid=source_project_id,
                project_session_rowid=session_id,
                start_time=datetime.fromisoformat(""2021-01-01T00:00:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
            )
            .returning(models.Trace.id)
        )
        assert trace1_id is not None

        span1_id = await session.scalar(
            insert(models.Span)
            .values(
                trace_rowid=trace1_id,
                span_id=""test-span-id-1"",
                parent_id=None,
                name=""test span 1"",
                span_kind=""CHAIN"",
                start_time=datetime.fromisoformat(""2021-01-01T00:00:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
                attributes={
                    ""input"": {""value"": ""test-input"", ""mime_type"": ""text/plain""},
                    ""output"": {""value"": ""test-output"", ""mime_type"": ""text/plain""},
                },
                events=[],
                status_code=""OK"",
                status_message=""okay"",
                cumulative_error_count=0,
                cumulative_llm_token_count_prompt=0,
                cumulative_llm_token_count_completion=0,
            )
            .returning(models.Span.id)
        )
        assert span1_id is not None

        span_cost1_id = await session.scalar(
            insert(models.SpanCost)
            .values(
                span_rowid=span1_id,
                trace_rowid=trace1_id,
                span_start_time=datetime.fromisoformat(""2021-01-01T00:00:00.000+00:00""),
                total_cost=1.50,
                total_tokens=100,
                prompt_cost=1.00,
                prompt_tokens=80,
                completion_cost=0.50,
                completion_tokens=20,
            )
            .returning(models.SpanCost.id)
        )
        assert span_cost1_id is not None

        trace_annotation1_id = await session.scalar(
            insert(models.TraceAnnotation)
            .values(
                trace_rowid=trace1_id,
                name=""test-annotation-1"",
                label=""good"",
                score=0.9,
                explanation=""This is a good trace"",
                metadata_={},
                annotator_kind=""HUMAN"",
                identifier=""test-1"",
                source=""APP"",
            )
            .returning(models.TraceAnnotation.id)
        )
        assert trace_annotation1_id is not None

        trace2_id = await session.scalar(
            insert(models.Trace)
            .values(
                trace_id=""test-trace-id-2"",
                project_rowid=source_project_id,
                project_session_rowid=session_id,
                start_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:02:00.000+00:00""),
            )
            .returning(models.Trace.id)
        )
        assert trace2_id is not None

        span2_id = await session.scalar(
            insert(models.Span)
            .values(
                trace_rowid=trace2_id,
                span_id=""test-span-id-2"",
                parent_id=None,
                name=""test span 2"",
                span_kind=""CHAIN"",
                start_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:02:00.000+00:00""),
                attributes={
                    ""input"": {""value"": ""test-input-2"", ""mime_type"": ""text/plain""},
                    ""output"": {""value"": ""test-output-2"", ""mime_type"": ""text/plain""},
                },
                events=[],
                status_code=""OK"",
                status_message=""okay"",
                cumulative_error_count=0,
                cumulative_llm_token_count_prompt=0,
                cumulative_llm_token_count_completion=0,
            )
            .returning(models.Span.id)
        )
        assert span2_id is not None

        span_cost2_id = await session.scalar(
            insert(models.SpanCost)
            .values(
                span_rowid=span2_id,
                trace_rowid=trace2_id,
                span_start_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
                total_cost=2.00,
                total_tokens=150,
                prompt_cost=1.50,
                prompt_tokens=120,
                completion_cost=0.50,
                completion_tokens=30,
            )
            .returning(models.SpanCost.id)
        )
        assert span_cost2_id is not None

        trace_annotation2_id = await session.scalar(
            insert(models.TraceAnnotation)
            .values(
                trace_rowid=trace2_id,
                name=""test-annotation-2"",
                label=""excellent"",
                score=0.95,
                explanation=""This is an excellent trace"",
                metadata_={},
                annotator_kind=""HUMAN"",
                identifier=""test-2"",
                source=""APP"",
            )
            .returning(models.TraceAnnotation.id)
        )
        assert trace_annotation2_id is not None

        other_trace_id = await session.scalar(
            insert(models.Trace)
            .values(
                trace_id=""test-trace-id-other"",
                project_rowid=other_project_id,
                start_time=datetime.fromisoformat(""2021-01-01T00:00:00.000+00:00""),
                end_time=datetime.fromisoformat(""2021-01-01T00:01:00.000+00:00""),
            )
            .returning(models.Trace.id)
        )
        assert other_trace_id is not None

        return {
            ""source_project_id"": source_project_id,
            ""dest_project_id"": dest_project_id,
            ""other_project_id"": other_project_id,
            ""trace1_id"": trace1_id,
            ""trace2_id"": trace2_id,
            ""other_trace_id"": other_trace_id,
        }",tests/unit/server/api/mutations/test_trace_transfer_mutations.py,,1,7
survived,"    async def test_transfer_traces_to_project_success(
        self,
        gql_client: AsyncGraphQLClient,
        trace_transfer_fixture: dict[str, int],
        db: DbSessionFactory,
    ) -> None:
        source_project_id = trace_transfer_fixture[""source_project_id""]
        dest_project_id = trace_transfer_fixture[""dest_project_id""]
        trace1_id = trace_transfer_fixture[""trace1_id""]
        trace2_id = trace_transfer_fixture[""trace2_id""]

        async with db() as session:
            traces = (
                await session.scalars(
                    select(models.Trace).where(models.Trace.id.in_([trace1_id, trace2_id]))
                )
            ).all()
            assert len(traces) == 2
            assert all(trace.project_rowid == source_project_id for trace in traces)

            trace_annotations = (
                await session.scalars(
                    select(models.TraceAnnotation).where(
                        models.TraceAnnotation.trace_rowid.in_([trace1_id, trace2_id])
                    )
                )
            ).all()
            assert len(trace_annotations) == 2

            span_costs = (
                await session.scalars(
                    select(models.SpanCost).where(
                        models.SpanCost.trace_rowid.in_([trace1_id, trace2_id])
                    )
                )
            ).all()
            assert len(span_costs) == 2

        result = await gql_client.execute(
            self.TRANSFER_TRACES_MUTATION,
            variables={
                ""traceIds"": [
                    str(GlobalID(""Trace"", str(trace1_id))),
                    str(GlobalID(""Trace"", str(trace2_id))),
                ],
                ""projectId"": str(GlobalID(""Project"", str(dest_project_id))),
            },
        )
        assert not result.errors

        async with db() as session:
            traces = (
                await session.scalars(
                    select(models.Trace).where(models.Trace.id.in_([trace1_id, trace2_id]))
                )
            ).all()
            assert len(traces) == 2
            assert all(trace.project_rowid == dest_project_id for trace in traces)

            trace_annotations = (
                await session.scalars(
                    select(models.TraceAnnotation).where(
                        models.TraceAnnotation.trace_rowid.in_([trace1_id, trace2_id])
                    )
                )
            ).all()
            assert len(trace_annotations) == 2

            span_costs = (
                await session.scalars(
                    select(models.SpanCost).where(
                        models.SpanCost.trace_rowid.in_([trace1_id, trace2_id])
                    )
                )
            ).all()
            assert len(span_costs) == 2
",tests/unit/server/api/mutations/test_trace_transfer_mutations.py,TestTraceTransferMutationMixin,1,7
survived,"    def test_csharp_local_functions_and_nested(self):
        patch = """"""
@@ -152,10 +152,6 @@ void OuterMethod()
{
    void InnerFunction()
    {
        // local function inside method
    }
}

@@ -165,15 +165,15 @@ static int Calculate(int x)

@@ -175,18 +175,18 @@ async Task<string> ProcessDataAsync()

@@ -185,20 +185,20 @@ T GenericLocalFunction<T>(T input)

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""OuterMethod"",
            ""InnerFunction"",
            ""Calculate"",
            ""ProcessDataAsync"",
            ""GenericLocalFunction"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase,0,7
survived,"    def test_csharp_edge_cases(self):
        patch = """"""
@@ -152,10 +152,6 @@ public unsafe void* GetPointer()

@@ -152,10 +152,6 @@ public extern static void ExternalMethod();

@@ -152,10 +152,6 @@ [Obsolete(""Use NewMethod instead"")]
public void OldMethod()

@@ -152,10 +152,6 @@ public partial void PartialMethod();

@@ -152,10 +152,6 @@ public virtual async Task<IEnumerable<T>> ComplexMethod<T>()

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""GetPointer"",
            ""ExternalMethod"",
            ""OldMethod"",
            ""PartialMethod"",
            ""ComplexMethod"",
        }",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase,1,6
survived,"def get_patch_parsers_for_organization(organization=None):
    """"""
    Returns the appropriate patch parsers based on feature flags.
    Falls back to the standard parsers if no organization is provided.
    """"""
    if organization and features.has(""organizations:csharp-open-pr-comments"", organization):
        # Merge stable and beta parsers when feature flag is enabled
        return {**PATCH_PARSERS, **BETA_PATCH_PARSERS}
    else:
        # Return only stable parsers when feature flag is disabled or no organization context
        return {k: v for k, v in PATCH_PARSERS.items() if k not in BETA_PATCH_PARSERS}",src/sentry/integrations/source_code_management/language_parsers.py,,1,7
survived,"async def train(model: art.TrainableModel[TauBenchPolicyConfig]):
    """"""Main training loop adapted from art-e example""""""
    config = model.config.run_config
    training_config = model.config.training_config
    
    if training_config is None:
        raise ValueError(""Training config is not set"")
    
    with LocalBackend() as backend:
        # Setup model with backend
        await model.register(backend)
        
        print(""Loading training tasks..."")
        # Get environment to access tasks
        env = get_env(
            config.env,
            user_strategy=config.user_strategy,
            user_model=config.user_model,
            user_provider=config.user_model_provider,
            task_split=config.task_split,
        )
        
        # Create list of task indices for training
        end_index = min(config.end_index, len(env.tasks)) if config.end_index != -1 else len(env.tasks)
        if config.task_ids:
            train_task_indices = config.task_ids
        else:
            train_task_indices = list(range(config.start_index, min(end_index, training_config.training_dataset_size)))
        
        # Validation task indices
        val_task_indices = list(range(len(train_task_indices), len(train_task_indices) + training_config.val_set_size))
        
        print(f""Training on {len(train_task_indices)} tasks"")
        print(f""Validation on {len(val_task_indices)} tasks"")
        
        # Training iterator
        train_iterator = iterate_dataset(
            train_task_indices,
            groups_per_step=training_config.groups_per_step,
            num_epochs=training_config.num_epochs,
            initial_step=await model.get_step(),
        )
        
        for batch, epoch, global_step, epoch_step in train_iterator:
            print(f""\n--- Training Step {global_step} (Epoch {epoch}, Step {epoch_step}) ---"")
            
            # Evaluation
            if global_step % training_config.eval_steps == 0:
                print(f""\n--- Evaluating at Step {global_step} ---"")
                await evaluate_model(model, config, num_eval_tasks=min(50, len(val_task_indices)))
                await model.delete_checkpoints()
            
            # Generate trajectory groups
            print(f""Generating trajectories for {len(batch)} tasks..."")
            groups = await art.gather_trajectory_groups(
                (
                    art.TrajectoryGroup(
                        (
                            rollout_tau_bench_task(model, task_index)
                            for _ in range(training_config.trajectories_per_group)
                        )
                    )
                    for task_index in batch
                )
            )
            
            # Training step
            print(f""Training on {len(groups)} trajectory groups..."")
            await model.train(
                groups,
                config=art.TrainConfig(
                    learning_rate=training_config.learning_rate
                ),
            )
            
            # Log progress
            total_reward = sum(
                sum(traj.reward for traj in group.trajectories) 
                for group in groups
            )
            num_trajectories = sum(len(group.trajectories) for group in groups)
            avg_reward = total_reward / num_trajectories if num_trajectories > 0 else 0
            print(f""Step {global_step}: Average training reward = {avg_reward}"")
        
        # Final evaluation
        print(""\n--- Final Evaluation ---"")
        final_reward = await evaluate_model(model, config, num_eval_tasks=len(val_task_indices))
        print(f""Final average reward: {final_reward}"")
        
        print(""Training completed!"")
",dev/tau-bench/run_rl.py,,1,7
survived,"    async def test_score_rollouts_with_apply_weights(self):
        """"""Test scoring rollouts with apply_weights parameter.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        rubric = Rubric(funcs=[func1, func2], weights=[2.0, 3.0])
        
        prompts = [""test""]
        completions = [""test""]
        answers = [""test""]
        states = [{}]
        tasks = [""test""]
        infos = [{}]
        
        # Test with apply_weights=True (default)
        results_weighted = await rubric.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            apply_weights=True
        )
        
        assert results_weighted[""reward""][0] == 1.0 * 2.0 + 0.5 * 3.0  # 2.0 + 1.5 = 3.5
        
        # Test with apply_weights=False (should not be used, but test anyway)
        results_unweighted = await rubric.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            apply_weights=False
        )
        
        # When apply_weights=False, only individual scores are returned, no weighted sum
        assert results_unweighted[""reward""][0] == 1.0 * 2.0 + 0.5 * 3.0  # Still weighted
",tests/test_rubric.py,TestRubric,0,7
survived,"    def test_env_group_dataset_concatenation(self, mock_openai_client):
        """"""Test that EnvGroup properly concatenates datasets with task labels.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1"", ""q2""], ""answer"": [""a1"", ""a2""]}),
            rubric=Rubric()
        )
        
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q3""], ""answer"": [""a3""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1, env2], env_names=[""math"", ""code""])
        
        # Check concatenated dataset
        dataset = env_group.get_dataset()
        assert len(dataset) == 3
        assert ""task"" in dataset.column_names
        
        # Check task labels
        tasks = dataset[""task""]
        assert tasks[0] == ""math""
        assert tasks[1] == ""math""
        assert tasks[2] == ""code""
",tests/test_env_group.py,TestEnvGroup,1,7
survived,"def mcp_json(request):
    if settings.SENTRY_MODE == SentryMode.SELF_HOSTED:
        return HttpResponse(status=404)

    return HttpResponse(json.dumps(MCP_CONFIG), content_type=""application/json"")
",src/sentry/web/api.py,,1,6
deleted,"    def _make_structured_output_call(
        self, 
        model: str, 
        messages: List[Dict[str, str]], 
        output_schema: Dict[str, Any], 
        scratchpad: Optional[str],
        extra_kwargs: Dict[str, Any]
    ) -> Any:
        """"""Make a structured output call.""""""
        schema = OutputSchemaBuilder.build_structured_output_schema(output_schema, scratchpad)
        
        try:
            return completion(
                model=model,
                messages=messages,
                response_format=schema,
                **extra_kwargs,
            )
        except Exception as e:
            self._handle_model_error(model, e)
",docetl/operations/utils/api.py,LLMCallHandler,1,7
survived,"    async def test_score_rollout_single(self):
        """"""Test scoring a single rollout.""""""
        def func1(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0
        
        def func2(completion, **kwargs):
            return len(completion) * 0.1
        
        rubric = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])
        
        result = await rubric.score_rollout(
            prompt=""test prompt"",
            completion=""test"",
            answer=""test"",
            state={},
            task=""test_task"",
            info={}
        )
        
        assert ""func1"" in result
        assert ""func2"" in result
        assert ""reward"" in result
        assert result[""func1""] == 1.0  # completion == answer
        assert result[""func2""] == 0.4  # len(""test"") * 0.1
        assert result[""reward""] == 1.0 * 1.0 + 0.4 * 0.5  # Weighted sum
",tests/test_rubric.py,TestRubric,1,7
survived,"            def env_response(self, messages, state, **kwargs):
                # This should never be called due to immediate completion
                return {""role"": ""user"", ""content"": ""Should not appear""}, state
",tests/test_multiturn_env.py,TestMultiTurnEnv.ImmediateCompletionEnv,1,6
survived,"    def test_rubric_with_custom_parser(self):
        """"""Test Rubric with custom parser.""""""
        custom_parser = Parser()
        rubric = Rubric(funcs=[], weights=[], parser=custom_parser)
        
        assert rubric.parser is custom_parser",tests/test_rubric.py,TestRubric,1,8
survived,"def think_parser():
    """"""Return a ThinkParser instance.""""""
    return ThinkParser()
",tests/conftest.py,,1,7
survived,"    def test_add_multiple_reward_funcs(self):
        """"""Test adding multiple reward functions.""""""
        # Create fresh rubric to avoid test isolation issues
        rubric = Rubric(funcs=[], weights=[])
        
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        rubric.add_reward_func(func1, weight=1.0)
        rubric.add_reward_func(func2, weight=0.3)
        
        assert len(rubric.reward_funcs) == 2
        assert rubric.get_reward_func_names() == [""func1"", ""func2""]
        assert rubric.reward_weights == [1.0, 0.3]
",tests/test_rubric.py,TestRubric,1,7
survived,"        def kwargs_func(completion, **kwargs):
            return len(kwargs)
",tests/test_rubric.py,TestRubric,1,6
survived,"def mock_openai_client():
    """"""Return a mocked AsyncOpenAI client with input-output mapping.""""""
    return MockAsyncOpenAI()
",tests/conftest.py,,1,6
survived,"    async def test_sampling_args_passed_through(self, mock_multiturn_env):
        """"""Test that sampling arguments are passed to model calls.""""""
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Test sampling""}],
            response=""Quick DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Test sampling""}]
        sampling_args = {""temperature"": 0.8, ""max_tokens"": 50}
        
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""test_answer"",
            sampling_args=sampling_args
        )
        
        # Verify sampling args were passed
        call_args = mock_multiturn_env.client.chat.completions.create.call_args
        assert ""temperature"" in call_args.kwargs
        assert ""max_tokens"" in call_args.kwargs
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    async def test_non_list_prompt_assertion(self, mock_multiturn_env):
        """"""Test that non-list prompts raise AssertionError.""""""
        with pytest.raises(AssertionError):
            await mock_multiturn_env.rollout(
                client=mock_multiturn_env.client,
                model=""test-model"",
                prompt=""String prompt not allowed"",  # Should be list
                answer=""test_answer""
            )
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    async def test_error_handling_stops_rollout(self, mock_multiturn_env):
        """"""Test that errors stop the rollout immediately.""""""
        # Set up the mock to return an error response for the expected conversation
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Start conversation""}],
            response=""[ERROR] Something went wrong""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Start conversation""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""target_answer""
        )
        
        # Should stop immediately after error
        assert len(completion) == 1
        assert completion[0][""content""] == ""[ERROR] Something went wrong""
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    def add_chat_response(self, messages, response, finish_reason=""stop""):
        """"""Add a mapped response for specific messages.""""""
        # Convert messages to a hashable key
        key = self._messages_to_key(messages)
        self.chat_completions[key] = {
            ""content"": response,
            ""finish_reason"": finish_reason
        }
",tests/conftest.py,MockAsyncOpenAI,1,7
survived,"    def test_parse_answer_integration(self, think_parser):
        """"""Test parse_answer method inherited from Parser.""""""
        completion = [
            {""role"": ""user"", ""content"": ""What is 2+2?""},
            {""role"": ""assistant"", ""content"": ""<think>Let me calculate</think>The answer is 4""}
        ]
        result = think_parser.parse_answer(completion)
        assert result == ""The answer is 4""
",tests/test_think_parser.py,TestThinkParser,1,7
survived,"    async def test_get_model_response_chat(self, mock_openai_client):
        """"""Test get_model_response with chat format.""""""
        env = TestEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""test""], ""answer"": [""test""]}),
            parser=Parser(),
            rubric=Rubric()
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Hello""}]
        response = await env.get_model_response(
            prompt=prompt,
            client=mock_openai_client,
            model=""test-model"",
            message_type=""chat""
        )
        
        assert response == ""This is a test response""
        mock_openai_client.chat.completions.create.assert_called_once()
",tests/test_environment.py,TestEnvironmentBase,1,7
survived,"    def test_rubric_group_score_rollouts_duplicate_names(self):
        """"""Test that duplicate reward function names are summed up.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup,0,8
survived,"    def test_parse_whitespace_handling(self, think_parser):
        """"""Test that whitespace is properly stripped.""""""
        text = """"""<think>
        Thinking process here.
        </think>
        
        Answer with spaces around it.
        
        """"""
        result = think_parser.parse(text)
        assert result == ""Answer with spaces around it.""
",tests/test_think_parser.py,TestThinkParser,1,7
survived,"def packages(package_ids):
    """"""Fixture providing test packages.""""""
    return {
        ""foo"": Package(
            id=package_ids[""foo""],
            name=""foo"",
            package_manager_id=1,
            import_id=""foo"",
            created_at=datetime.now(),
            updated_at=datetime.now(),
        ),
        ""bar"": Package(
            id=package_ids[""bar""],
            name=""bar"",
            package_manager_id=1,
            import_id=""bar"",
            created_at=datetime.now(),
            updated_at=datetime.now(),
        ),
        ""baz"": Package(
            id=package_ids[""baz""],
            name=""baz"",
            package_manager_id=1,
            import_id=""baz"",
            created_at=datetime.now(),
            updated_at=datetime.now(),
        ),
        ""qux"": Package(
            id=package_ids[""qux""],
            name=""qux"",
            package_manager_id=1,
            import_id=""qux"",
            created_at=datetime.now(),
            updated_at=datetime.now(),
        ),
    }
",tests/package_managers/homebrew/test_diff_dep.py,,1,7
survived,"def crate_with_dependencies():
    """"""
    Factory fixture to create Crate objects with specified dependencies.

    Returns a function that creates Crate objects.
    """"""

    def create_crate(crate_id=""1048221"", dependencies=None):
        latest_version = CrateLatestVersion(
            id=9337571,
            checksum=""some-checksum"",
            downloads=1000,
            license=""MIT"",
            num=""1.0.0"",
            published_by=None,
            published_at=""2023-01-01"",
        )

        if dependencies:
            latest_version.dependencies = dependencies
        else:
            latest_version.dependencies = []

        crate = Crate(
            id=int(crate_id),
            name=""main_pkg"",
            readme=""Test readme"",
            homepage="""",
            repository="""",
            documentation="""",
            source=None,
        )
        crate.latest_version = latest_version

        return crate

    return create_crate
",tests/package_managers/crates/test_diff_deps.py,,1,6
survived,"    def chaos(self) -> None:
        """"""Run chaos-client to discover subdomains from Chaos database.""""""
        if self.htb or self.is_ip_address(self.site):
            return
        
        if not self.pdcp_api_key:
            self.print(""Chaos"", ""PDCP_API_KEY not found in environment, skipping"", Colors.WARNING)
            return
        
        self.print(""Chaos"", ""Querying ProjectDiscovery Chaos database"")
        
        chaos_cmd = (
            f""chaos -d {self.site} ""
            f""-key {self.pdcp_api_key} ""
            f""{'-silent' if not self.verbose else ''} ""
            f""-o {self.dir_path}/chaos_subdomains.txt""
        )
        self.cmd(chaos_cmd)
        
        self.ask_to_add(self.read(""chaos_subdomains.txt""))
",main.py,HaxUnit,1,6
survived,"    def test_no_encryption(self, mock_smtp_class, context_manager):
        """"""Test SMTP without encryption.""""""
        # Create provider with no encryption config
        no_enc_config = ProviderConfig(
            description=""Test SMTP Provider"",
            authentication={
                ""smtp_server"": ""smtp.example.com"",
                ""smtp_port"": 25,
                ""encryption"": ""None"",
                ""smtp_username"": """",
                ""smtp_password"": """",
            },
        )
        smtp_provider = SmtpProvider(
            context_manager=context_manager,
            provider_id=""test_smtp_provider"",
            config=no_enc_config,
        )

        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send email
        smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test No Encryption"",
            body=""No encryption test"",
        )

        # Verify SMTP was used without TLS
        mock_smtp_class.assert_called_once_with(""smtp.example.com"", 25)
        mock_smtp.starttls.assert_not_called()
        mock_smtp.login.assert_not_called()  # No credentials provided
        mock_smtp.sendmail.assert_called_once()
",tests/test_smtp_provider.py,TestSmtpProvider,1,7
survived,"    def workflow_share(self, workflow_id: int,
                       share_title: str, share_comment: str, share_user: str) -> Tuple[bool, str]:
        """"""
        åˆ†äº«å·¥ä½œæµ
        """"""
        if not settings.WORKFLOW_STATISTIC_SHARE:  # ä½¿ç”¨ç‹¬ç«‹çš„å·¥ä½œæµåˆ†äº«å¼€å…³
            return False, ""å½“å‰æ²¡æœ‰å¼€å¯å·¥ä½œæµæ•°æ®å…±äº«åŠŸèƒ½""
        
        # èŽ·å–å·¥ä½œæµä¿¡æ¯
        workflow = WorkflowOper().get(workflow_id)
        if not workflow:
            return False, ""å·¥ä½œæµä¸å­˜åœ¨""
        
        workflow_dict = workflow.to_dict()
        workflow_dict.pop(""id"")
        
        # æ¸…é™¤ç¼“å­˜
        cache_backend.clear(region=self._shares_cache_region)
        
        # å‘é€åˆ†äº«è¯·æ±‚
        res = RequestUtils(proxies=settings.PROXY or {}, content_type=""application/json"",
                           timeout=10).post(self._workflow_share,
                                            json={
                                                ""share_title"": share_title,
                                                ""share_comment"": share_comment,
                                                ""share_user"": share_user,
                                                ""share_uid"": self._share_user_id,
                                                **workflow_dict
                                            })
        if res is None:
            return False, ""è¿žæŽ¥MoviePilotæœåŠ¡å™¨å¤±è´¥""
        if res.ok:
            # æ¸…é™¤ get_shares çš„ç¼“å­˜ï¼Œä»¥ä¾¿å®žæ—¶çœ‹åˆ°ç»“æžœ
            cache_backend.clear(region=self._shares_cache_region)
            return True, """"
        else:
            return False, res.json().get(""message"")
",app/helper/workflow.py,WorkflowHelper,1,7
deleted,"    def __test_connection_via_rest_api(self):
        """"""
        Test connection to OpenShift using REST API instead of CLI.
        This is more reliable as it doesn't depend on oc CLI being installed.
        """"""
        try:
            # Suppress SSL warnings if insecure is True
            if self.authentication_config.insecure:
                # Suppress SSL verification warnings
                warnings.filterwarnings('ignore', message='Unverified HTTPS request')
            
            # Test API connectivity by hitting the /version endpoint
            headers = {
                'Authorization': f'Bearer {self.authentication_config.token}',
                'Accept': 'application/json'
            }
            
            verify_ssl = not self.authentication_config.insecure
            
            # Try to get cluster version info
            response = requests.get(
                f""{self.authentication_config.api_server}/version"",
                headers=headers,
                verify=verify_ssl,
                timeout=30
            )
            
            if response.status_code == 200:
                self.logger.info(""Successfully connected to OpenShift cluster via REST API"")
                return True, None
            else:
                error_msg = f""API returned status code {response.status_code}: {response.text}""
                self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
                return False, error_msg
                
        except requests.exceptions.RequestException as e:
            error_msg = f""Connection error: {str(e)}""
            self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
            return False, error_msg
        except Exception as e:
            error_msg = f""Unexpected error: {str(e)}""
            self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
            return False, error_msg
",keep/providers/openshift_provider/openshift_provider.py,OpenshiftProvider,1,7
survived,"    def format_cost(cls, cost: float) -> str:
        """"""
        ã‚³ã‚¹ãƒˆã‚’è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã™ã‚‹

        Args:
            cost: ã‚³ã‚¹ãƒˆå€¤

        Returns:
            str: ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã•ã‚ŒãŸã‚³ã‚¹ãƒˆæ–‡å­—åˆ—
        """"""
        return f""${cost:.4f}""",server/src/services/llm_pricing.py,LLMPricing,1,7
survived,"    def supports_chain(self, chain) -> bool:
        return True
",python/src/plugins/opensea/goat_plugins/opensea/__init__.py,OpenSeaPlugin,1,6
survived,"    def __init__(self, api_key: Optional[str] = None, api_root: str = ""https://api.upshot.xyz/v2/allora""):
        self.api_key = api_key
        self.api_root = api_root.rstrip('/')  # Remove trailing slash if present
",python/src/plugins/allora/goat_plugins/allora/service.py,AlloraService,1,8
survived,"    async def get_aggregated_balances(self, parameters: dict):
        """"""Get token balances and allowances for a wallet address on a specific chain.""""""
        wallet_address = parameters.get(""wallet_address"")
        if not wallet_address:
            raise ValueError(""wallet_address is required"")
            
        chain_id = parameters.get(""chain_id"", 1)  # Default to Ethereum mainnet

        url = f""{self.base_url}/balance/v1.2/{chain_id}/balances/{wallet_address}""

        headers = {
            ""Accept"": ""application/json""
        }
        if self.api_key:
            headers[""Authorization""] = f""Bearer {self.api_key}""

        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers) as response:
                if not response.ok:
                    raise Exception(f""Failed to fetch balances: {response.status} {await response.text()}"")
                return await response.json()",python/src/plugins/1inch/goat_plugins/oneinch/service.py,OneInchService,1,6
deleted,"    def description(self) -> str:
        return ""Validates that the connector version was incremented if files were modified.""
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionIncrementCheck,1,7
survived,"    def _get_master_connector_version(self, connector: Connector) -> semver.Version:
        """"""Get the version from the master branch.""""""
        metadata = self._get_master_metadata(connector)
        if not metadata:
            return semver.Version.parse(""0.0.0"")
        
        return semver.Version.parse(str(metadata[""data""][""dockerImageTag""]))
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck,1,7
deleted,"    def category(self) -> CheckCategory:
        return CheckCategory.METADATA
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck,1,7
deleted,"def connector():
    connector = MagicMock(spec=Connector)
    connector.technical_name = ""source-test""
    connector.metadata = {""dockerImageTag"": ""1.0.0""}
    connector.is_released = False
    return connector
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,,1,6
deleted,"    def test_run_should_not_run(self, mock_should_run, version_increment_check, connector):
        mock_should_run.return_value = False
        
        result = version_increment_check._run(connector)
        
        assert result.status == CheckStatus.SKIPPED
        assert ""No modified files required a version bump"" in result.message",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,TestVersionIncrementCheck,1,7
survived,"    async def completion(
        self,
        messages: List[common.Message],
        max_tokens: int,
        model: str | None = None,
        temperature: float = 1.0,
        tools: List[common.Tool] | None = None,
        tool_choice: str | None = None,
        system_prompt: str | None = None,
        *args,
        **kwargs,
    ) -> common.Completion:
        chosen_model = model or self.default_model
        ollama_messages = self._messages_into(messages)
        
        if system_prompt:
            ollama_messages.insert(0, {""role"": ""system"", ""content"": system_prompt})
        
        request_params = {
            ""model"": chosen_model,
            ""messages"": ollama_messages,
            ""options"": {
                ""temperature"": temperature,
                ""num_predict"": max_tokens,
            }
        }
        
        ollama_tools = self._tools_into(tools)
        if ollama_tools:
            request_params[""tools""] = ollama_tools
        
        try:
            response = await self.client.chat(**request_params)
            return self._completion_into(response, input_tokens=0)
        except Exception as e:
            logger.error(f""Ollama API error: {e}"")
            raise",agent/llm/ollama_client.py,OllamaLLM,1,7
survived,"def plot_pose_cube(img, yaw, pitch, roll, tdx=None, tdy=None, size=150.):
    p = pitch * np.pi / 180
    y = -(yaw * np.pi / 180)
    r = roll * np.pi / 180
    if tdx != None and tdy != None:
        face_x = tdx - 0.50 * size
        face_y = tdy - 0.50 * size

    else:
        height, width = img.shape[:2]
        face_x = width / 2 - 0.5 * size
        face_y = height / 2 - 0.5 * size

    x1 = size * (cos(y) * cos(r)) + face_x
    y1 = size * (cos(p) * sin(r) + cos(r) * sin(p) * sin(y)) + face_y
    x2 = size * (-cos(y) * sin(r)) + face_x
    y2 = size * (cos(p) * cos(r) - sin(p) * sin(y) * sin(r)) + face_y
    x3 = size * (sin(y)) + face_x
    y3 = size * (-cos(y) * sin(p)) + face_y

    cv2.line(img, (int(face_x), int(face_y)), (int(x1), int(y1)), (0, 0, 255), 3)
    cv2.line(img, (int(face_x), int(face_y)), (int(x2), int(y2)), (0, 0, 255), 3)
    cv2.line(img, (int(x2), int(y2)), (int(x2 + x1 - face_x), int(y2 + y1 - face_y)), (0, 0, 255), 3)
    cv2.line(img, (int(x1), int(y1)), (int(x1 + x2 - face_x), int(y1 + y2 - face_y)), (0, 0, 255), 3)
    cv2.line(img, (int(face_x), int(face_y)), (int(x3), int(y3)), (255, 0, 0), 2)
    cv2.line(img, (int(x1), int(y1)), (int(x1 + x3 - face_x), int(y1 + y3 - face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x2), int(y2)), (int(x2 + x3 - face_x), int(y2 + y3 - face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x2 + x1 - face_x), int(y2 + y1 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x3 + x1 - face_x), int(y3 + y1 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x2 + x3 - face_x), int(y2 + y3 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x3), int(y3)), (int(x3 + x1 - face_x), int(y3 + y1 - face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x3), int(y3)), (int(x3 + x2 - face_x), int(y3 + y2 - face_y)), (0, 255, 0), 2)

    return img
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,,1,7
survived,"def get_R(x, y, z):
    Rx = np.array([[1, 0, 0],
                   [0, np.cos(x), -np.sin(x)],
                   [0, np.sin(x), np.cos(x)]])
    Ry = np.array([[np.cos(y), 0, np.sin(y)],
                   [0, 1, 0],
                   [-np.sin(y), 0, np.cos(y)]])
    Rz = np.array([[np.cos(z), -np.sin(z), 0],
                   [np.sin(z), np.cos(z), 0],
                   [0, 0, 1]])

    R = Rz.dot(Ry.dot(Rx))
    return R
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,,1,7
survived,"def py_cpu_nms(dets, thresh):
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,,1,7
survived,"def test_cohere_integration():
    """"""Integration test demonstrating all four Cohere call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Get API key with error handling
    api_key = os.getenv(""COHERE_API_KEY"")
    if not api_key:
        raise ValueError(""COHERE_API_KEY environment variable is required"")
    print(""\nInitializing test with Cohere API key..."")

    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()
    print(f""Started new session with ID: {session.session_id}"")

    # Initialize client and provider with error handling
    try:
        co = cohere.Client(api_key=api_key)
        aco = cohere.AsyncClient(api_key=api_key)
        from agentops.llms.providers.cohere import CohereProvider
        provider = CohereProvider(co)
        provider.client = session  # Pass session to provider before override
        provider.override()  # This will handle both sync and async clients
        
        # Set up async client with the same session
        aco.session = session
        # Ensure the async client's provider also has the session
        aco.provider = provider
        print(""Successfully initialized Cohere clients and provider"")
        print(f""Provider session ID: {provider.client.session_id}"")
    except Exception as e:
        print(f""Error initializing Cohere clients: {str(e)}"")
        raise

    def sync_no_stream():
        try:
            print(""\nExecuting sync_no_stream..."")
            response = co.chat(message=""Hello from sync no stream"", model=""command"", session=session)
            print(f""sync_no_stream completed successfully with response: {response.text}"")
        except Exception as e:
            print(f""Error in sync_no_stream: {str(e)}"")
            raise

    def sync_stream():
        try:
            print(""\nExecuting sync_stream..."")
            stream = co.chat_stream(message=""Hello from sync streaming"", model=""command"", session=session)
            completion = """"
            for chunk in stream:
                if hasattr(chunk, 'text'):
                    completion += chunk.text
                print(f""Received sync chunk: {chunk}"")
            print(f""sync_stream completed successfully with completion: {completion}"")
        except Exception as e:
            print(f""Error in sync_stream: {str(e)}"")
            raise

    async def async_no_stream():
        try:
            print(""\nExecuting async_no_stream..."")
            async with asyncio.timeout(30):
                response = await aco.chat(message=""Hello from async no stream"", model=""command"", session=session)
                print(f""async_no_stream completed successfully with response: {response.text}"")
        except asyncio.TimeoutError:
            print(""Warning: async_no_stream timed out"")
            raise
        except Exception as e: 
            print(f""Error in async_no_stream: {str(e)}"")
            raise

    async def async_stream(provider, session):
        try:
            print(""\nStarting async_stream call..."")
            async with asyncio.timeout(30):  # Add timeout to prevent hanging
                # Ensure provider has the current session
                provider.client = session
                # Create a new stream with the provider to ensure proper event tracking
                stream = await aco.chat_stream(
                    message=""Hello from async streaming"",
                    model=""command"",
                    session=session
                )
                print(""Stream created, starting iteration..."")
                async for chunk in stream:
                    print(f""Received async chunk: {chunk}"")
                print(""Stream completed successfully"")
        except asyncio.TimeoutError:
            print(""Warning: Async stream timed out"")
            raise
        except Exception as e:
            print(f""Error in async_stream: {str(e)}"")
            raise

    async def run_async_tests():
        print(""\nRunning async tests..."")
        print(""Starting async_no_stream..."")
        await async_no_stream()
        print(""Completed async_no_stream"")
        
        print(""\nStarting first async_stream..."")
        await async_stream(provider, session)
        print(""Completed first async_stream"")
        
        print(""\nStarting second async_stream..."")
        await async_stream(provider, session)  # Run twice to ensure we get all LLM calls
        print(""Completed second async_stream"")
        
        print(""\nStarting third async_stream..."")
        await async_stream(provider, session)  # Run thrice to ensure we get all LLM calls
        print(""Completed third async_stream"")
        
        print(""\nAll async tests completed successfully"")
        
        # End session and verify analytics after all tests
        session.end_session(""Success"")
        analytics = session.get_analytics()
        print(f""\nAnalytics: {analytics}"")
        assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during Cohere test: {str(e)}"")
        raise

    print(""\nTest completed successfully"")
",tests/core_manual_tests/providers/cohere_canary.py,,0,7
survived,"    async def run_async_tests():
        await async_no_stream()
        await async_stream()
",tests/core_manual_tests/providers/litellm_canary.py,,1,6
survived,"    async def async_stream():
        async_stream_response = await async_chat_client.create(
            model=""jamba-instruct"",
            system=""You are a helpful AI assistant"",
            messages=async_stream_messages,
            maxTokens=10,
            stream=True
        )
        async for chunk in async_stream_response:
            _ = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta, 'content') else ''
",tests/core_manual_tests/providers/ai21_canary.py,,1,6
survived,"    async def async_no_stream():
        await async_anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from async no stream"",
                }
            ],
            session=session
        )
",tests/core_manual_tests/providers/anthropic_canary.py,,1,7
survived,"def determine_if_files_are_relevant(reasoning: str, file_paths: List[str]) -> Dict[str, Any]:
    """"""Determines if files are relevant to the prompt using parallelism.
    
    Args:
        reasoning: Explanation of why we're determining relevance
        file_paths: List of file paths to check
        
    Returns:
        Dictionary with results for each file
    """"""
    try:
        console.log(f""[blue]Determine If Files Are Relevant Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Checking {len(file_paths)} files in batches of {BATCH_SIZE}[/dim]"")
        
        # Initialize Anthropic client
        client = Anthropic(api_key=os.getenv(""ANTHROPIC_API_KEY""))
        
        results = {}
        
        # Process files in batches
        for i in range(0, len(file_paths), BATCH_SIZE):
            batch = file_paths[i:i+BATCH_SIZE]
            console.log(f""[dim]Processing batch {i//BATCH_SIZE + 1}/{(len(file_paths) + BATCH_SIZE - 1)//BATCH_SIZE}[/dim]"")
            
            # Process batch in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
                future_to_file = {
                    executor.submit(determine_if_file_is_relevant, USER_PROMPT, file_path, client): file_path
                    for file_path in batch
                }
                
                for future in concurrent.futures.as_completed(future_to_file):
                    file_path = future_to_file[future]
                    try:
                        result = future.result()
                        results[file_path] = result
                        relevance = ""Relevant"" if result[""is_relevant""] else ""Not relevant""
                        console.log(f""[dim]{file_path}: {relevance}[/dim]"")
                    except Exception as e:
                        console.log(f""[red]Error processing {file_path}: {str(e)}[/red]"")
        
        return results
    except Exception as e:
        console.log(f""[red]Error determining file relevance: {str(e)}[/red]"")
        return {}
",sfa_codebase_context_agent_v3.py,,1,7
survived,"def git_list_files(reasoning: str, directory: str = os.getcwd(), globs: List[str] = [], extensions: List[str] = []) -> List[str]:
    """"""Returns a list of files in the repository, respecting gitignore.

    Args:
        reasoning: Explanation of why we're listing files
        directory: Directory to search in (defaults to current working directory)
        globs: List of glob patterns to filter files (optional)
        extensions: List of file extensions to filter files (optional)

    Returns:
        List of file paths as strings
    """"""
    try:
        console.log(f""[blue]Git List Files Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Directory: {directory}, Globs: {globs}, Extensions: {extensions}[/dim]"")
        
        # Change to the specified directory
        original_dir = os.getcwd()
        os.chdir(directory)
        
        # Get all files tracked by git
        result = subprocess.run(
            ""git ls-files"",
            shell=True,
            text=True,
            capture_output=True,
        )
        
        files = result.stdout.strip().split(""\n"")
        
        # Filter by globs if provided
        if globs:
            filtered_files = []
            for pattern in globs:
                for file in files:
                    if fnmatch.fnmatch(file, pattern):
                        filtered_files.append(file)
            files = filtered_files
        
        # Filter by extensions if provided
        if extensions:
            files = [file for file in files if any(file.endswith(f"".{ext}"") for ext in extensions)]
        
        # Change back to the original directory
        os.chdir(original_dir)
        
        # Convert to absolute paths
        files = [os.path.join(directory, file) for file in files]
        
        console.log(f""[dim]Found {len(files)} files[/dim]"")
        return files
    except Exception as e:
        console.log(f""[red]Error listing files: {str(e)}[/red]"")
        return []
",sfa_codebase_context_agent_v3.py,,0,7
survived,"    def is_primitive(value: Any) -> bool:
        return isinstance(
            value,
            (
                str,
                int,
                float,
                bool,
                type(None),
                datetime.datetime,
                datetime.date,
            ),
        )
",tests/_plugins/ui/_impl/tables/test_narwhals.py,,1,7
survived,"            def stream_handler(stream):
                for chunk in stream:
                    handle_stream_chunk(chunk)
                    yield chunk
",agentops/llms/providers/gemini.py,GeminiProvider,1,7
survived,"    def test_change_tracking_options(self, mock_post):
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            'success': True,
            'data': {
                'markdown': 'Test markdown content',
                'changeTracking': {
                    'previousScrapeAt': '2023-01-01T00:00:00Z',
                    'changeStatus': 'changed',
                    'visibility': 'visible',
                    'diff': {
                        'text': '@@ -1,1 +1,1 @@\n-old content\n+new content',
                        'json': {
                            'files': [{
                                'from': None,
                                'to': None,
                                'chunks': [{
                                    'content': '@@ -1,1 +1,1 @@',
                                    'changes': [{
                                        'type': 'del',
                                        'content': '-old content',
                                        'del': True,
                                        'ln': 1
                                    }, {
                                        'type': 'add',
                                        'content': '+new content',
                                        'add': True,
                                        'ln': 1
                                    }]
                                }]
                            }]
                        }
                    },
                    'json': {
                        'title': {
                            'previous': 'Old Title',
                            'current': 'New Title'
                        }
                    }
                }
            }
        }
        mock_post.return_value = mock_response

        app = FirecrawlApp(api_key=os.environ.get('FIRECRAWL_API_KEY', 'dummy-api-key-for-testing'))
        result = app.scrape_url('https://example.com', {
            'formats': ['markdown', 'changeTracking'],
            'changeTrackingOptions': {
                'modes': ['git-diff', 'json'],
                'schema': {'type': 'object', 'properties': {'title': {'type': 'string'}}}
            }
        })

        args, kwargs = mock_post.call_args
        self.assertEqual(kwargs['json']['formats'], ['markdown', 'changeTracking'])
        self.assertEqual(kwargs['json']['changeTrackingOptions']['modes'], ['git-diff', 'json'])
        
        self.assertEqual(result['changeTracking']['diff']['text'], '@@ -1,1 +1,1 @@\n-old content\n+new content')
        self.assertEqual(result['changeTracking']['json']['title']['previous'], 'Old Title')
        self.assertEqual(result['changeTracking']['json']['title']['current'], 'New Title')",apps/python-sdk/tests/test_change_tracking.py,TestChangeTracking,1,7
survived,"def _update_json_loaders(
    connector_path: Path,
    data: dict,
    streams: dict[str, JsonStream],
    loaders: List[JsonLoaderNode],
) -> None:
    logger = main_logger
    for loader in loaders:
        if ""{{"" in loader.file_path:
            # remove templated paths and their references
            (f""    Removing reference: {loader.ref}"")
            _remove_reference(data, None, loader, [])
            continue
        else:
            # direct pointer to a file. update.
            file_path = Path(os.path.abspath(os.path.join(connector_path, loader.file_path)))
            if not file_path.is_file():
                logger.info(f""    JsonFileSchemaLoader not found: {file_path}"")
                continue
            schema_loader = _load_reference(data, loader.ref)
            if not schema_loader:
                logger.info(f""    JsonFileSchemaLoader reference not found: {loader.ref}"")
                continue
            _update_inline_schema(schema_loader, streams, file_path.stem)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,,1,6
survived,"def _load_reference(data: dict, ref: str) -> dict | None:
    yaml_stream = data
    path = ref.split(""/"")
    for p in path:
        if p == ""#"":
            continue
        if p.startswith(""Array[""):
            i = int(p[6:-1])
            if not isinstance(yaml_stream, list) or len(yaml_stream) <= i:
                return None
            yaml_stream = yaml_stream[i]
            continue
        if p not in yaml_stream:
            return None
        yaml_stream = yaml_stream[p]
    return yaml_stream
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,,1,7
survived,"    def __init__(self, context: ConnectorContext) -> None:
        super().__init__(context)
        self.manifest_path = context.connector.manifest_path
        self.original_manifest = None
        if self.manifest_path.is_file():
            self.original_manifest = self.manifest_path.read_text()

        self.schemas_path = context.connector.python_source_dir_path / SCHEMAS_DIR_NAME
        self.backup_schema_path = None
        if self.schemas_path.is_dir():
            self.backup_schema_path = Path(tempfile.mkdtemp())
            copy_directory(self.schemas_path, self.backup_schema_path)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RestoreInlineState,1,7
survived,"    async def _run(self) -> StepResult:
        connector = self.context.connector
        connector_path = connector.code_directory
        manifest_path = connector.manifest_path
        python_path = connector.python_source_dir_path
        logger = self.logger

        json_streams = _parse_json_streams(python_path)
        if len(json_streams) == 0:
            return StepResult(step=self, status=StepStatus.SKIPPED, stderr=""No JSON streams found."")

        data = read_yaml(manifest_path)
        if ""streams"" not in data:
            return StepResult(
                step=self,
                status=StepStatus.SKIPPED,
                stderr=""No manifest streams found."",
            )

        # find the explit ones and remove or udpate
        json_loaders = _find_json_loaders(data, [])
        for loader in json_loaders:
            logger.info(f""     JSON loader ref: {loader.ref} -> {loader.file_path}"")

        _update_json_loaders(connector_path, data, json_streams, json_loaders)

        # go through the declared streams and update the inline schemas
        for stream in data[""streams""]:
            if isinstance(stream, str):
                # see if reference
                if stream.startswith(""#""):
                    yaml_stream = _load_reference(data, stream)
                    if not yaml_stream:
                        logger.info(f""    Stream reference not found: {stream}"")
                        continue
                    if not _get_stream_name(yaml_stream):
                        logger.info(f""    Stream reference name not found: {stream}"")
                        continue
                else:
                    logger.info(f""    Stream reference unknown: {stream}"")
                    continue
            else:
                yaml_stream = stream

            if not yaml_stream:
                logger.info(f""    !! Yaml stream not found: {stream}"")
                continue

            stream_name = _get_stream_name(yaml_stream)
            if not stream_name:
                logger.info(f""    !! Stream name not found: {stream}"")
                continue
            if yaml_stream.get(""schema_loader"") and yaml_stream[""schema_loader""].get(""type"") == ""InlineSchemaLoader"":
                continue

            yaml_stream[""schema_loader""] = {}
            schema_loader = yaml_stream[""schema_loader""]
            _update_inline_schema(schema_loader, json_streams, stream_name)

        write_yaml(data, manifest_path)
        # await format_prettier([manifest_path], logger=logger)

        for json_stream in json_streams.values():
            logger.info(f""     !! JSON schema not found: {json_stream.name}"")

        return StepResult(step=self, status=StepStatus.SUCCESS)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,InlineSchemas,1,6
survived,"def test_default_bucket():
    # Verify DEFAULT_BUCKET covers appropriate ranges for both fast APIs and LLM workloads
    from bentoml._internal.utils.metrics import DEFAULT_BUCKET

    # Test smallest bucket
    assert DEFAULT_BUCKET[0] == 0.005  # 5ms for fast APIs

    # Test largest finite bucket
    assert DEFAULT_BUCKET[-2] == 180.0  # 180s for LLM workloads

    # Test infinity is present
    assert DEFAULT_BUCKET[-1] == float(""inf"")

    # Test monotonic increase
    for i in range(len(DEFAULT_BUCKET) - 1):
        assert DEFAULT_BUCKET[i] < DEFAULT_BUCKET[i + 1]
",tests/unit/_internal/utils/test_metrics.py,,1,7
survived,"def env_vars_page():
    """"""Generate the environment variables documentation page.
    
    Returns:
        A Reflex component containing the documentation.
    """"""
    return rx.box(
        h1_comp(text=""Environment Variables""),
        rx.code(""reflex.config.EnvironmentVariables"", class_name=""code-style text-[18px]""),
        rx.divider(),
        markdown(
            """"""
            Reflex provides a number of environment variables that can be used to configure the behavior of your application.
            These environment variables can be set in your shell environment or in a `.env` file.
            
            This page documents all available environment variables in Reflex.
            """"""
        ),
        h2_comp(text=""Environment Variables""),
        EnvVarDocs.generate_env_var_table(include_internal=False),
    )
",pcweb/pages/docs/env_vars.py,,1,7
survived,"    def test_pause_live_updates_when_already_paused(self):
        """"""Test pausing when already paused does nothing.""""""
        formatter = ConsoleFormatter()
        
        mock_live = MagicMock(spec=Live)
        formatter._live = mock_live
        formatter._live_paused = True
        
        formatter.pause_live_updates()
        
        mock_live.stop.assert_not_called()
        assert formatter._live_paused
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume,1,7
survived,"    def test_human_input_pauses_flow_updates(self, mock_input):
        """"""Test that human input pauses Flow status updates.""""""
        from crewai.agents.agent_builder.base_agent_executor_mixin import CrewAgentExecutorMixin
        
        executor = CrewAgentExecutorMixin()
        executor.crew = MagicMock()
        executor.crew._train = False
        executor._printer = MagicMock()
        
        formatter = event_listener.formatter
        
        original_paused_state = formatter._live_paused
        
        try:
            formatter._live_paused = False
            
            with patch.object(formatter, 'pause_live_updates') as mock_pause, \
                 patch.object(formatter, 'resume_live_updates') as mock_resume:
                
                result = executor._ask_human_input(""Test result"")
                
                mock_pause.assert_called_once()
                mock_resume.assert_called_once()
                mock_input.assert_called_once()
                assert result == ''
        finally:
            formatter._live_paused = original_paused_state
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration,0,7
survived,"    def generate(cls, seed: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None) -> 'Fingerprint':
        """"""
        Static factory method to create a new Fingerprint.

        Args:
            seed (Optional[str]): A string to use as seed for the UUID generation.
                If None, a random UUID is generated.
            metadata (Optional[Dict[str, Any]]): Additional metadata to store with the fingerprint.

        Returns:
            Fingerprint: A new Fingerprint instance
        """"""
        fingerprint = cls(metadata=metadata or {})
        if seed:
            # For seed-based generation, we need to manually set the uuid_str after creation
            object.__setattr__(fingerprint, 'uuid_str', cls._generate_uuid(seed))
        return fingerprint
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def __str__(self) -> str:
        """"""String representation of the fingerprint (the UUID).""""""
        return self.uuid_str
",src/crewai/security/fingerprint.py,Fingerprint,1,8
survived,"    def fingerprint(self) -> Fingerprint:
        """"""
        Get the crew's fingerprint.

        Returns:
            Fingerprint: The crew's fingerprint
        """"""
        return self.security_config.fingerprint
",src/crewai/crew.py,Crew,1,7
survived,"    def validate_fingerprint(cls, values):
        """"""Ensure fingerprint is properly initialized.""""""
        if isinstance(values, dict):
            # Handle case where fingerprint is not provided or is None
            if 'fingerprint' not in values or values['fingerprint'] is None:
                values['fingerprint'] = Fingerprint()
            # Handle case where fingerprint is a string (seed)
            elif isinstance(values['fingerprint'], str):
                if not values['fingerprint'].strip():
                    raise ValueError(""Fingerprint seed cannot be empty"")
                values['fingerprint'] = Fingerprint.generate(seed=values['fingerprint'])
        return values
",src/crewai/security/security_config.py,SecurityConfig,1,7
survived,"    def uuid(self) -> uuid.UUID:
        """"""Get the UUID object for this fingerprint.""""""
        return uuid.UUID(self.uuid_str)
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def is_compatible(self, min_version: str) -> bool:
        """"""
        Check if this security configuration is compatible with the minimum required version.
        
        Args:
            min_version (str): Minimum required version in semver format (e.g., ""1.0.0"")
            
        Returns:
            bool: True if this configuration is compatible, False otherwise
        """"""
        # Simple version comparison (can be enhanced with packaging.version if needed)
        current = [int(x) for x in self.version.split(""."")]
        minimum = [int(x) for x in min_version.split(""."")]
        
        # Compare major, minor, patch versions
        for c, m in zip(current, minimum):
            if c > m:
                return True
            if c < m:
                return False
        return True
",src/crewai/security/security_config.py,SecurityConfig,1,7
survived,"    async def initialize(self) -> None:
        @self.route('', methods=['GET', 'POST'])
        async def _() -> str:
            if quart.request.method == 'GET':
                return self.success(data={'models': await self.ap.embeddings_models_service.get_embeddings_models()})
            elif quart.request.method == 'POST':
                json_data = await quart.request.json

                model_uuid = await self.ap.embeddings_models_service.create_embeddings_model(json_data)

                return self.success(data={'uuid': model_uuid})

        @self.route('/<model_uuid>', methods=['GET', 'PUT', 'DELETE'])
        async def _(model_uuid: str) -> str:
            if quart.request.method == 'GET':
                model = await self.ap.embeddings_models_service.get_embeddings_model(model_uuid)

                if model is None:
                    return self.http_status(404, -1, 'model not found')

                return self.success(data={'model': model})
            elif quart.request.method == 'PUT':
                json_data = await quart.request.json

                await self.ap.embeddings_models_service.update_embeddings_model(model_uuid, json_data)

                return self.success()
            elif quart.request.method == 'DELETE':
                await self.ap.embeddings_models_service.delete_embeddings_model(model_uuid)

                return self.success()

        @self.route('/<model_uuid>/test', methods=['POST'])
        async def _(model_uuid: str) -> str:
            json_data = await quart.request.json

            await self.ap.embeddings_models_service.test_embeddings_model(model_uuid, json_data)

            return self.success()",pkg/api/http/controller/groups/provider/models.py,EmbeddingsModelsRouterGroup,1,7
deleted,"    async def invoke_embeddings(
        self,
        query: core_entities.Query,
        model: RuntimeEmbeddingsModel,
        input_text: str,
        extra_args: dict[str, typing.Any] = {},
    ) -> list[float]:
        """"""è°ƒç”¨ Embeddings API

        Args:
            query (core_entities.Query): è¯·æ±‚ä¸Šä¸‹æ–‡
            model (RuntimeEmbeddingsModel): ä½¿ç”¨çš„æ¨¡åž‹ä¿¡æ¯
            input_text (str): è¾“å…¥æ–‡æœ¬
            extra_args (dict[str, typing.Any], optional): é¢å¤–çš„å‚æ•°. Defaults to {}.

        Returns:
            list[float]: è¿”å›žçš„ embedding å‘é‡
        """"""
        pass",pkg/provider/modelmgr/requester.py,LLMAPIRequester,0,7
deleted,"    async def get_embeddings_model_by_uuid(self, uuid: str) -> requester.RuntimeEmbeddingsModel:
        """"""é€šè¿‡uuidèŽ·å– Embeddings æ¨¡åž‹""""""
        for model in self.embeddings_models:
            if model.model_entity.uuid == uuid:
                return model
        raise ValueError(f'Embeddings model {uuid} not found')
",pkg/provider/modelmgr/modelmgr.py,ModelManager,1,7
survived,"def consume(config):
    connection = create_sink_connection(config=config)

    while True:
        # Consume transformed event from the pipeline
        res = connection.consume()

        if res.status_code == 200:
            record = res.event()
            assert record[""data""] == TEST_MESSAGE
            assert record[""stream""] == TEST_STREAM
            assert record[""namespace""] == TEST_NAMESPACE
            assert ""emitted_at"" in record
            break
",airbyte-integrations/connectors/destination-glassflow/integration_tests/integration_test.py,,1,6
survived,"def _state() -> AirbyteMessage:
    return AirbyteMessage(type=Type.STATE, state=AirbyteStateMessage(data={}))
",airbyte-integrations/connectors/destination-glassflow/integration_tests/integration_test.py,,1,7
survived,"def test_get_configured_catalog_with_overrides(mock_catalog, mock_stream):
    """"""Test that get_configured_catalog correctly applies overrides.""""""
    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(
            executor=Mock(),
            name=""test-source"",
            cursor_key_overrides={""test_stream"": ""custom_cursor""},
            primary_key_overrides={""test_stream"": ""custom_pk""},
        )

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.cursor_field == [""custom_cursor""]
        assert configured_stream.primary_key == [[""custom_pk""]]
",tests/unit_tests/sources/test_source_key_overrides.py,,1,7
survived,"def test_get_configured_catalog_composite_primary_key(mock_catalog, mock_stream):
    """"""Test that get_configured_catalog correctly handles composite primary keys.""""""
    mock_stream.source_defined_primary_key = [[""pk1"", ""pk2""]]
    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(
            executor=Mock(),
            name=""test-source"",
            primary_key_overrides={""test_stream"": [""custom_pk1"", ""custom_pk2""]},
        )

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.primary_key == [[""custom_pk1"", ""custom_pk2""]]
",tests/unit_tests/sources/test_source_key_overrides.py,,1,7
survived,"def test_source_init_with_overrides():
    """"""Test that overrides are set when provided to the constructor.""""""
    cursor_overrides = {""stream1"": ""cursor1"", ""stream2"": ""cursor2""}
    pk_overrides = {""stream1"": ""pk1"", ""stream2"": [""pk2a"", ""pk2b""]}

    with patch.object(Source, ""_discover"", return_value=Mock()):
        with patch.object(Source, ""set_cursor_keys"") as mock_set_cursor:
            with patch.object(Source, ""set_primary_keys"") as mock_set_pk:
                source = Source(
                    executor=Mock(),
                    name=""test-source"",
                    cursor_key_overrides=cursor_overrides,
                    primary_key_overrides=pk_overrides,
                )

                mock_set_cursor.assert_called_once_with(kwargs=cursor_overrides)
                mock_set_pk.assert_called_once_with(kwargs=pk_overrides)
",tests/unit_tests/sources/test_source_key_overrides.py,,1,7
survived,"def test_set_primary_key(input_key, expected_output):
    """"""Test that set_primary_key properly converts and updates a single primary key override.""""""
    with patch.object(Source, ""_discover"", return_value=Mock()):
        source = Source(executor=Mock(), name=""test-source"")

        source.set_primary_key(""stream1"", input_key)

        assert source._primary_key_overrides == {""stream1"": expected_output}
",tests/unit_tests/sources/test_source_key_overrides.py,,1,7
survived,"    def test_cache_attempt_hit(self) -> None:
        """"""Test cache attempt with a hit.""""""
        loader = MockLoader(""test"")
        defs = {""var1""}
        stateful_refs: Set[str] = set()
        
        # Create and save a cache
        original_cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            stateful_refs,
            ""Pure"",
            True,
            {""version"": 1}
        )
        loader.saved_caches[""Pure_hash1""] = original_cache
        
        # Attempt to load the cache
        cache = loader.cache_attempt(defs, ""hash1"", stateful_refs, ""Pure"")
        
        assert cache.hash == ""hash1""
        assert cache.hit is True
        assert cache.cache_type == ""Pure""
        assert cache.defs[""var1""] == ""value1""
        assert cache.meta == {""version"": 1}
",tests/_save/loaders/test_loader.py,TestLoader,1,8
survived,"    def test_init(self) -> None:
        """"""Test initialization.""""""
        loader = PickleLoader(""test"", self.save_path)
        assert loader.name == ""test""
        assert loader.suffix == ""pickle""
        assert str(loader.save_path).endswith(""/test"")
        
        # Check that the directory was created
        assert os.path.exists(os.path.join(self.save_path, ""test""))
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader,1,7
survived,"    def save_cache(self, cache: Cache) -> None:
        key = f""{cache.cache_type}_{cache.hash}""
        self.saved_caches[key] = cache
",tests/_save/loaders/test_loader.py,MockPersistenceLoader,1,7
survived,"    def __init__(self, name: str, config_value: str = ""default"") -> None:
        super().__init__(name)
        self.config_value = config_value
        self.saved_caches: Dict[str, Cache] = {}
",tests/_save/loaders/test_loader.py,MockLoader,1,8
survived,"    def test_load_cache(self) -> None:
        """"""Test the load_cache method.""""""
        loader = JsonLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        
        # Create a valid JSON cache
        cache_dict = {
            ""defs"": {""var1"": ""value1""},
            ""hash"": ""hash1"",
            ""stateful_refs"": [],
            ""cache_type"": ""Pure"",
            ""hit"": True,
            ""meta"": {}
        }
        
        with open(cache_path, ""w"") as f:
            json.dump(cache_dict, f)
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")
",tests/_save/loaders/test_json_loader.py,TestJsonLoader,1,7
survived,"    def __init__(self, name: str, save_path: str) -> None:
        super().__init__(name, ""mock"", save_path)
        self.saved_caches: Dict[str, Cache] = {}
",tests/_save/loaders/test_loader.py,MockPersistenceLoader,1,7
survived,"    def test_load_cache(self) -> None:
        """"""Test loading a cache.""""""
        loader = MemoryLoader(""test"")
        
        # Create and save a cache
        # Use string directly instead of Name constructor
        stateful_refs = set()
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            stateful_refs,
            ""Pure"",
            True,
            {}
        )
        loader.save_cache(cache)
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader,1,7
survived,"    def test_init_with_custom_cache(self) -> None:
        """"""Test initialization with a custom cache.""""""
        custom_cache = OrderedDict()
        loader = MemoryLoader(""test"", cache=custom_cache)
        assert id(loader._cache) != id(custom_cache)  # Should be a copy, not the same instance
        assert len(loader._cache) == 0
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader,1,7
survived,"    def test_init(self) -> None:
        """"""Test initialization.""""""
        partial = LoaderPartial(MockLoader, config_value=""custom"")
        assert partial.loader_type == MockLoader
        assert partial.kwargs == {""config_value"": ""custom""}
",tests/_save/loaders/test_loader.py,TestLoaderPartial,1,7
survived,"    def validate_metadata(cls, v):
        """"""Validate that metadata is a dictionary with string keys.""""""
        if not isinstance(v, dict):
            raise ValueError(""Metadata must be a dictionary"")
        
        # Validate that all keys are strings
        for key, value in v.items():
            if not isinstance(key, str):
                raise ValueError(f""Metadata keys must be strings, got {type(key)}"")
        return v
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"def test_special_token_handling():
    """"""Test that special tokens like <|endoftext|> are handled correctly in token estimation.""""""
    config = OnlineRequestProcessorConfig(model=""gpt-4"")
    processor = OpenAIOnlineRequestProcessor(config)
    
    # Test message containing special token
    messages = [{""role"": ""user"", ""content"": ""Testing <|endoftext|> token""}]
    
    try:
        total_tokens = processor.estimate_total_tokens(messages)
        assert total_tokens > 0, ""Token estimation should return a positive number""
    except ValueError as e:
        if ""<|endoftext|>"" in str(e):
            pytest.fail(""Special token <|endoftext|> should not raise ValueError"")
        raise  # Re-raise if it's a different ValueError",tests/test_openai_online_request_processor.py,,0,6
survived,"def setup_test_environment():
    """"""Set up test environment with a temporary directory for SQLite storage.""""""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Create the directory with proper permissions
        storage_dir = Path(temp_dir) / ""crewai_test_storage""
        storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Set environment variable to point to the test storage directory
        os.environ[""CREWAI_STORAGE_DIR""] = str(storage_dir)
        
        yield
",tests/conftest.py,,1,6
survived,"def test_tool_initialization():
    tool = SeleniumScrapingTool()

    assert tool.website_url is None
    assert tool.css_element is None
    assert tool.cookie is None
    assert tool.wait_time == 3
    assert tool.return_html is False
",tests/tools/selenium_scraping_tool_test.py,,1,7
survived,"    def generate(cls, seed: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None) -> 'Fingerprint':
        """"""
        Static factory method to create a new Fingerprint.

        Args:
            seed (Optional[str]): A string to use as seed for the UUID generation.
                If None, a random UUID is generated.
            metadata (Optional[Dict[str, Any]]): Additional metadata to store with the fingerprint.

        Returns:
            Fingerprint: A new Fingerprint instance
        """"""
        fingerprint = cls(metadata=metadata or {})
        if seed:
            # For seed-based generation, we need to manually set the uuid_str after creation
            object.__setattr__(fingerprint, 'uuid_str', cls._generate_uuid(seed))
        return fingerprint
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def __init__(self, **data):
        """"""Initialize a Fingerprint with auto-generated uuid_str and created_at.""""""
        # Remove uuid_str and created_at from data to ensure they're auto-generated
        if 'uuid_str' in data:
            data.pop('uuid_str')
        if 'created_at' in data:
            data.pop('created_at')

        # Call the parent constructor with the modified data
        super().__init__(**data)
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def _generate_uuid(cls, seed: str) -> str:
        """"""
        Generate a deterministic UUID based on a seed string.

        Args:
            seed (str): The seed string to use for UUID generation

        Returns:
            str: A string representation of the UUID consistently generated from the seed
        """"""
        if not isinstance(seed, str):
            raise ValueError(""Seed must be a string"")
        
        if not seed.strip():
            raise ValueError(""Seed cannot be empty or whitespace"")
            
        # Create a deterministic UUID using v5 (SHA-1)
        # Custom namespace for CrewAI to enhance security
        CREW_AI_NAMESPACE = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')
        return str(uuid.uuid5(CREW_AI_NAMESPACE, seed))
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the fingerprint to a dictionary representation.

        Returns:
            Dict[str, Any]: Dictionary representation of the fingerprint
        """"""
        return {
            ""uuid_str"": self.uuid_str,
            ""created_at"": self.created_at.isoformat(),
            ""metadata"": self.metadata
        }
",src/crewai/security/fingerprint.py,Fingerprint,1,8
deleted,"    def test_sanitize_collection_name_none(self):
        """"""Test sanitizing a None value.""""""
        sanitized = sanitize_collection_name(None)
        self.assertEqual(sanitized, ""default_collection"")
",tests/utilities/test_string_utils.py,TestStringUtils,1,7
survived,"def test_telemetry_disable_with_multiple_instances():
    """"""Test that multiple telemetry instances respect dynamically changed env vars.""""""
    Telemetry._instance = None
    
    with patch.dict(os.environ, {}, clear=True):
        with patch(""crewai.telemetry.telemetry.TracerProvider""):
            telemetry1 = Telemetry()
            assert telemetry1.ready is True
            
            os.environ['CREWAI_DISABLE_TELEMETRY'] = 'true'
            
            telemetry2 = Telemetry()
            assert telemetry2 is telemetry1
            assert telemetry2.ready is True
            
            mock_operation = MagicMock()
            telemetry2._safe_telemetry_operation(mock_operation)
            mock_operation.assert_not_called()
",tests/telemetry/test_telemetry_disable.py,,0,7
survived,"    def test_require_api_key_missing(self, mock_get_context: MagicMock) -> None:
        """"""Test _require_api_key with missing key.""""""
        mock_context = MagicMock()
        mock_context.marimo_config = {""ai"": {""open_ai"": {""api_key"": """"}}}
        mock_get_context.return_value = mock_context

        model = openai(""gpt-4"")
        with pytest.raises(ValueError):
            _ = model._require_api_key
",tests/_ai/llm/_impl.py,TestOpenAI,1,7
survived,"    def test_require_api_key_env(self) -> None:
        """"""Test _require_api_key with environment variable.""""""
        model = google(""gemini-pro"")
        assert model._require_api_key == ""env-key""
",tests/_ai/llm/_impl.py,TestGoogle,0,7
survived,"    def test_call(
        self, mock_groq_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the groq class.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_groq_class.return_value = mock_client
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = ""Test response""
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response

        model = groq(""llama3-70b-8192"")
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig(
                max_tokens=100,
                temperature=0.7,
                top_p=0.9,
            )

            result = model(messages, config)
            assert result == ""Test response""

            mock_groq_class.assert_called_once_with(
                api_key=""test-key"", base_url=None
            )
        mock_client.chat.completions.create.assert_called_once()
        call_args = mock_client.chat.completions.create.call_args[1]
        assert call_args[""model""] == ""llama3-70b-8192""
        assert len(call_args[""messages""]) == 2
        assert call_args[""messages""][0][""role""] == ""system""
        assert call_args[""messages""][0][""content""] == DEFAULT_SYSTEM_MESSAGE
        assert call_args[""messages""][1][""role""] == ""user""
        assert call_args[""messages""][1][""content""] == ""Test prompt""
        assert call_args[""max_tokens""] == 100
        assert call_args[""temperature""] == 0.7
        assert call_args[""top_p""] == 0.9
        assert call_args[""stop""] is None
        assert call_args[""stream""] is False
",tests/_ai/llm/_impl.py,TestGroq,1,8
survived,"    def test_call_empty_content(
        self, mock_anthropic_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the anthropic class with empty content.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_anthropic_class.return_value = mock_client
        mock_response = MagicMock()
        mock_response.content = []
        mock_client.messages.create.return_value = mock_response

        model = anthropic(""claude-3-opus-20240229"")
        messages = [ChatMessage(role=""user"", content=""Test prompt"")]
        config = ChatModelConfig()

        result = model(messages, config)
        assert result == """"
",tests/_ai/llm/_impl.py,TestAnthropic,1,6
survived,"    def test_init(self) -> None:
        """"""Test initialization of the anthropic class.""""""
        model = anthropic(""claude-3-opus-20240229"")
        assert model.model == ""claude-3-opus-20240229""
        assert model.system_message == DEFAULT_SYSTEM_MESSAGE
        assert model.api_key is None
        assert model.base_url is None

        model = anthropic(
            ""claude-3-opus-20240229"",
            system_message=""Custom system message"",
            api_key=""test-key"",
            base_url=""https://example.com"",
        )
        assert model.model == ""claude-3-opus-20240229""
        assert model.system_message == ""Custom system message""
        assert model.api_key == ""test-key""
        assert model.base_url == ""https://example.com""
",tests/_ai/llm/_impl.py,TestAnthropic,1,8
survived,"    def test_call(
        self,
        mock_generative_model: MagicMock,
        mock_configure: MagicMock,
        mock_require_api_key: MagicMock,
    ) -> None:
        """"""Test calling the google class.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_generative_model.return_value = mock_client
        mock_response = MagicMock()
        mock_response.text = ""Test response""
        mock_client.generate_content.return_value = mock_response

        model = google(""gemini-pro"")
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig(
                max_tokens=100,
                temperature=0.7,
                top_p=0.9,
                top_k=10,
                frequency_penalty=0.5,
                presence_penalty=0.5,
            )

            result = model(messages, config)
            assert result == ""Test response""

            mock_configure.assert_called_once_with(api_key=""test-key"")
        mock_generative_model.assert_called_once()
        call_args = mock_generative_model.call_args[1]
        assert call_args[""model_name""] == ""gemini-pro""
        generation_config = call_args[""generation_config""]
        assert generation_config.max_output_tokens == 100
        assert generation_config.temperature == 0.7
        assert generation_config.top_p == 0.9
        assert generation_config.top_k == 10
        assert generation_config.frequency_penalty == 0.5
        assert generation_config.presence_penalty == 0.5

        mock_client.generate_content.assert_called_once()
",tests/_ai/llm/_impl.py,TestGoogle,1,7
survived,"    def test_is_file_path_with_nonexistent_file(self) -> None:
        # Test with nonexistent file
        with pytest.raises(click.BadParameter) as excinfo:
            is_file_path(None, None, ""nonexistent_file.txt"")
        assert ""File does not exist: nonexistent_file.txt"" in str(excinfo.value)
",tests/_cli/test_cli_validators.py,TestIsFilePath,1,6
survived,"    def test_base_url_with_none_or_empty(self) -> None:
        # Test with None or empty string
        assert base_url(None, None, None) == """"
        assert base_url(None, None, """") == """"
",tests/_cli/test_cli_validators.py,TestBaseUrl,1,7
survived,"    def test_highlight_traceback(self) -> None:
        # Test that _highlight_traceback adds HTML formatting
        traceback = ""Traceback (most recent call last):\n  File \""<stdin>\"", line 1, in <module>\nValueError: invalid value""

        highlighted = _highlight_traceback(traceback)

        # Should contain HTML formatting
        assert ""<span class=\""codehilite\"">"" in highlighted
        assert ""</span>"" in highlighted

        # Should contain the original traceback text
        assert ""Traceback"" in highlighted
        # The ValueError text is present but with HTML tags around it
        assert ""ValueError"" in highlighted
        assert ""invalid value"" in highlighted
",tests/_messaging/test_tracebacks.py,TestTracebacks,1,6
deleted,"    def test_is_sensitive_error(self) -> None:
        # These errors are not sensitive
        assert not is_sensitive_error(MarimoAncestorPreventedError(
            msg="""", raising_cell=""cell1"", blamed_cell=None
        ))
        assert not is_sensitive_error(MarimoAncestorStoppedError(
            msg="""", raising_cell=""cell1""
        ))
        assert not is_sensitive_error(MarimoInternalError(error_id=""""))

        # These errors are sensitive
        assert is_sensitive_error(MarimoExceptionRaisedError(
            msg="""", exception_type="""", raising_cell=None
        ))
        assert is_sensitive_error(MarimoSyntaxError(msg=""""))
        assert is_sensitive_error(UnknownError(msg=""""))",tests/_messaging/test_errors.py,TestErrorUtilityFunctions,1,7
deleted,"    def test_http_request_context_manager_with_request(self) -> None:
        # Test that http_request_context sets and unsets the HTTP request
        request = HTTPRequest(
            url={""path"": ""/test"", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            base_url={""path"": """", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            headers={},
            query_params={},
            path_params={},
            cookies={},
            meta={},
            user=None,
        )

        with http_request_context(request):
            # Request should be set within the context
            ctx_request = HTTP_REQUEST_CTX.get()
            assert ctx_request is not None
            assert ctx_request is request
            assert ctx_request.url[""path""] == ""/test""

        # Request should be unset outside the context
        with pytest.raises(LookupError):
            HTTP_REQUEST_CTX.get()
",tests/_messaging/test_context.py,TestHTTPRequestContext,1,7
deleted,"    def test_multiple_definition_error(self) -> None:
        error = MultipleDefinitionError(
            name=""test_var"", cells=(""cell1"", ""cell2"")
        )

        # Test properties
        assert error.type == ""multiple-defs""
        assert ""test_var"" in error.describe()
        assert ""defined by another cell"" in error.describe()
",tests/_messaging/test_errors.py,TestErrorClasses,1,7
survived,"    def test_mime_bundle(self) -> None:
        # Test that MimeBundle can be used as a type annotation
        def accepts_mime_bundle(bundle: MimeBundle) -> MimeBundle:
            return bundle

        # Create a valid mime bundle
        bundle: MimeBundle = {
            ""text/plain"": ""Hello, world!"",
            ""text/html"": ""<h1>Hello, world!</h1>"",
        }

        assert accepts_mime_bundle(bundle) == bundle

        # Test with empty bundle
        empty_bundle: MimeBundle = {}
        assert accepts_mime_bundle(empty_bundle) == empty_bundle
",tests/_messaging/test_mimetypes.py,TestMimeTypes,1,7
survived,"        def accepts_kernel_message(message: KernelMessage) -> KernelMessage:
            return message
",tests/_messaging/test_types.py,TestKernelMessage,1,6
survived,"    def test_stderr_write(self) -> None:
        stderr = self.MockStderr()

        # Test write method
        result = stderr.write(""Error message"")

        # Should return the length of the string
        assert result == 13

        # Should call _write_with_mimetype with text/plain mimetype
        assert len(stderr.written_data) == 1
        assert stderr.written_data[0] == (""Error message"", ""text/plain"")
",tests/_messaging/test_types.py,TestStdoutStderr,1,7
deleted,"    def test_delete_nonlocal_error(self) -> None:
        error = DeleteNonlocalError(
            name=""test_var"", cells=(""cell1"", ""cell2"")
        )

        # Test properties
        assert error.type == ""delete-nonlocal""
        assert ""test_var"" in error.describe()
        assert ""can't be deleted"" in error.describe()
",tests/_messaging/test_errors.py,TestErrorClasses,1,7
survived,"def test_print_shutdown() -> None:
    """"""Test the print_shutdown function.""""""
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.return_value = ""UTF8_EMOJI""
                print_shutdown()
                mock_print.assert_called()
                mock_print_tabbed.assert_called_once()
",tests/_server/test_print.py,,1,7
survived,"def test_read_toml_valid() -> None:
    with NamedTemporaryFile(mode=""w"", suffix="".toml"", delete=False) as f:
        f.write('value = ""test""')
        f.flush()

        reader = ConfigReader(f.name)
        fallback = TestConfig(value=""fallback"")
        result = reader.read_toml(TestConfig, fallback=fallback)
        assert result == TestConfig(value=""test"")

    os.unlink(f.name)",tests/_utils/config/test_config_reader.py,,1,7
survived,"def test_read_toml_invalid_syntax() -> None:
    with NamedTemporaryFile(mode=""w"", suffix="".toml"", delete=False) as f:
        # Write invalid TOML content (missing value after comma)
        f.write(""key = 'value',"")
        f.flush()

        reader = ConfigReader(f.name)
        fallback = TestConfig(value=""fallback"")
        result = reader.read_toml(TestConfig, fallback=fallback)
        assert result == fallback

    os.unlink(f.name)
",tests/_utils/config/test_config_reader.py,,1,7
survived,"def test_plugin_instantiation():
    """"""Test that the plugin can be instantiated without errors.""""""
    api_key = os.environ.get(""UNISWAP_API_KEY"")
    assert api_key is not None, ""UNISWAP_API_KEY environment variable is required""
    
    options = UniswapPluginOptions(api_key=api_key)
    plugin = uniswap(options)
    
    assert plugin is not None
    assert plugin.name == ""uniswap""
    
    # Test chain support
    ethereum_chain: EvmChain = {""type"": ""evm"", ""id"": 1}  # Ethereum mainnet
    polygon_chain: EvmChain = {""type"": ""evm"", ""id"": 137}  # Polygon
    solana_chain: SolanaChain = {""type"": ""solana""}  # Solana chain
    unknown_chain: EvmChain = {""type"": ""evm"", ""id"": 999}  # Unknown EVM chain
    
    assert plugin.supports_chain(ethereum_chain)
    assert plugin.supports_chain(polygon_chain)
    assert not plugin.supports_chain(solana_chain)
    assert not plugin.supports_chain(unknown_chain)",python/src/plugins/uniswap/tests/test_uniswap_plugin.py,,0,7
survived,"    def condition_func(task_output: TaskOutput) -> bool:
        return ""success"" in task_output.raw.lower()
",tests/crew_test.py,,1,7
survived,"def compare_transaction_responses(py_response: Dict[str, Any], ts_response: Dict[str, Any]) -> None:
    """"""Compare transaction responses between implementations.
    
    Args:
        py_response: Response from Python implementation
        ts_response: Response from TypeScript implementation
        
    Raises:
        AssertionError: If responses don't match
    """"""
    assert py_response[""status""] == ts_response[""status""], ""Transaction status doesn't match""
    assert py_response.get(""hash"") == ts_response.get(""hash""), ""Transaction hash doesn't match""
    
    # Compare onChain data if present
    if ""onChain"" in py_response or ""onChain"" in ts_response:
        py_onchain = py_response.get(""onChain"", {})
        ts_onchain = ts_response.get(""onChain"", {})
        assert py_onchain.get(""txId"") == ts_onchain.get(""txId""), ""Transaction IDs don't match""
",python/src/wallets/crossmint/tests/utils/helpers.py,,1,6
survived,"def compare_wallet_responses(py_response: Dict[str, Any], ts_response: Dict[str, Any]) -> None:
    """"""Compare wallet responses between Python and TypeScript implementations.
    
    Args:
        py_response: Response from Python implementation
        ts_response: Response from TypeScript implementation
        
    Raises:
        AssertionError: If responses don't match
    """"""
    assert py_response[""address""] == ts_response[""address""], ""Wallet addresses don't match""
    assert py_response[""type""] == ts_response[""type""], ""Wallet types don't match""
    
    # Compare optional fields if present
    if ""linkedUser"" in py_response or ""linkedUser"" in ts_response:
        assert py_response.get(""linkedUser"") == ts_response.get(""linkedUser""), ""Linked users don't match""
",python/src/wallets/crossmint/tests/utils/helpers.py,,1,6
survived,"def test_error_handling_invalid_payload(custodial_api):
    """"""Test error handling with invalid request payload.""""""
    with pytest.raises(Exception) as exc:
        custodial_api._request(
            ""/wallets"",
            method=""POST"",
            json={""invalid"": ""payload""}
        )
    assert ""Error"" in str(exc.value)
    assert ""400"" in str(exc.value)
",python/src/wallets/crossmint/tests/test_api_client.py,,1,6
survived,"def test_smart_wallet_message_signing(smart_api, test_wallet_options, test_message, test_keypair):
    """"""Test message signing with smart wallet.""""""
    # Create wallet and client
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        test_wallet_options[""chain""],
        test_keypair,
        test_wallet_options[""provider""],
        test_wallet_options[""options""][""ensProvider""]
    )
    
    # Sign message
    signature = client.sign_message(test_message)
    assert signature[""signature""].startswith(""0x"")
    
    # Verify signature format
    assert len(signature[""signature""]) > 130  # Valid EVM signature length
",python/src/wallets/crossmint/tests/test_smart_wallet.py,,1,7
survived,"    def get_json_schema(self):
        return get_generic_json_schema()
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder,0,7
survived,"def box_folder_ai_extract(
    client: BoxClient, folder_id: str, prompt: str, is_recursive: bool = False, by_pass_text_extraction: bool = False
) -> Iterable[BoxFileExtended]:
    # folder items iterator
    for item in client.folders.get_folder_items(folder_id).entries:
        if item.type == ""file"":
            file = box_file_get_by_id(client=client, file_id=item.id)
            if not by_pass_text_extraction:
                text_representation = box_file_ai_extract(client=client, file_id=item.id, prompt=prompt)
            else:
                text_representation = """"
            yield BoxFileExtended(file=file, text_representation=text_representation)
        elif item.type == ""folder"" and is_recursive:
            yield from box_folder_ai_extract(
                client=client, folder_id=item.id, prompt=prompt, is_recursive=is_recursive, by_pass_text_extraction=by_pass_text_extraction
            )
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,,1,7
survived,"def box_file_ai_extract(client: BoxClient, file_id: str, prompt: str) -> str:
    ai_item = AiItemBase(id=file_id, type=AiItemBaseTypeField.FILE)
    response = client.ai.create_ai_extract(prompt=prompt, items=[ai_item])
    return response.answer
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,,1,6
survived,"    def __init__(self, client: BoxClient, folder_id: str, is_recursive: bool = False):
        self.client = client
        self.folder_id = folder_id
        self.is_recursive = is_recursive
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamTextRepresentationFolder,1,7
survived,"    def get_json_schema(self):
        return get_generic_json_schema()
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractFolder,0,6
survived,"    def get_json_schema(self):
        return get_generic_json_schema()
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIAskFolder,0,7
survived,"    def __init__(self, client: BoxClient, folder_id: str, prompt: str, is_recursive: bool = False):
        self.client = client
        self.folder_id = folder_id
        self.is_recursive = is_recursive
        self.prompt = prompt
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractFolder,1,8
survived,"def main():
    """"""Main function to parse arguments and run the agent with custom tools.""""""
    parser = argparse.ArgumentParser(description=""Agent with Custom Tools Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the agent and get response
        response = asyncio.run(run_custom_tool_agent(args.prompt))
        
        # Display the response
        console.print(Panel(response, title=""Financial Assistant Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/06_agent_with_custom_tools.py,,1,7
survived,"def test_create_basic_agent():
    """"""Test that the agent is created with the correct configuration.""""""
    agent = create_basic_agent(""Test instructions"")
    assert agent.name == ""BasicAssistant""
    assert agent.instructions == ""Test instructions""
    assert agent.model == ""gpt-4o-mini""
",openai-agents-examples/01_basic_agent.py,,1,8
survived,"def main():
    """"""Main function to parse arguments and run the agent with tracing.""""""
    parser = argparse.ArgumentParser(description=""Agent with Tracing Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Set up tracing
        tracer = setup_tracing()
        
        # Run the agent with tracing and get response
        response = asyncio.run(run_traced_agent(args.prompt, tracer))
        
        # Display the response
        console.print(Panel(response, title=""Agent Response with Tracing"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/04_agent_with_tracing.py,,1,6
survived,"def create_coordinator_agent(specialists: List[Agent]) -> Agent:
    """"""
    Create a coordinator agent that manages the research and blog writing process.
    
    Args:
        specialists: List of specialist agents to coordinate
        
    Returns:
        An Agent instance that coordinates the content creation process
    """"""
    instructions = """"""
    You are a content coordinator who manages the process of creating research-based blog posts.
    Your task is to:
    1. Understand the blog topic request
    2. Delegate research to the Research Specialist
    3. Provide the research to the Blog Specialist to create a blog post
    4. Ensure the final blog post is comprehensive, engaging, and based on solid research
    5. Deliver the final markdown blog post
    
    Manage the workflow efficiently and ensure each specialist has the information they need.
    """"""
    
    # Create handoffs to specialist agents
    handoffs = [handoff(agent) for agent in specialists]
    
    return Agent(
        name=""ContentCoordinator"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoffs=handoffs
    )
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"def analyze_topic(topic: str) -> str:
    """"""
    Analyze a topic to identify key aspects for research.
    
    Args:
        topic: The topic to analyze
        
    Returns:
        A string containing analysis of the topic with key aspects to research
    """"""
    # This is a mock implementation - in a real application, this would be more sophisticated
    topic_analyses = {
        ""artificial intelligence ethics"": """"""
            Topic Analysis: Artificial Intelligence Ethics
            
            Key aspects to research:
            1. Ethical frameworks and principles for AI development
            2. Bias and fairness in AI systems
            3. Privacy implications of AI technologies
            4. Accountability and transparency in AI decision-making
            5. Regulatory approaches and governance models
            6. Economic and social impacts of AI deployment
            7. Case studies of ethical failures and successes
            8. Future challenges and emerging ethical concerns
        """""",
        
        ""climate change solutions"": """"""
            Topic Analysis: Climate Change Solutions
            
            Key aspects to research:
            1. Renewable energy technologies and implementation
            2. Carbon capture and sequestration approaches
            3. Policy mechanisms (carbon pricing, regulations, incentives)
            4. Adaptation strategies for vulnerable communities
            5. Agricultural and land use changes
            6. Behavioral and lifestyle modifications
            7. Economic considerations and just transition
            8. International cooperation frameworks
        """""",
        
        ""quantum computing"": """"""
            Topic Analysis: Quantum Computing
            
            Key aspects to research:
            1. Fundamental quantum mechanics principles relevant to computing
            2. Quantum computing architectures and hardware approaches
            3. Quantum algorithms and computational advantages
            4. Potential applications across industries
            5. Current state of development and key players
            6. Challenges and limitations of quantum systems
            7. Quantum programming languages and software tools
            8. Timeline and roadmap for practical quantum computing
        """"""
    }
    
    # Find the most relevant analysis
    for key, value in topic_analyses.items():
        if any(word in topic.lower() for word in key.split()):
            return value.strip()
    
    # Default analysis if no match found
    return f""""""
        Topic Analysis: {topic}
        
        Key aspects to research:
        1. Historical context and development
        2. Current state and major concepts
        3. Key stakeholders and perspectives
        4. Challenges and controversies
        5. Future trends and developments
        6. Practical applications and implications
        7. Related fields and intersections
        8. Resources for further learning
    """""".strip()
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"def main():
    """"""Main function to parse arguments and run the protected agent.""""""
    parser = argparse.ArgumentParser(description=""Agent with Guardrails Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the protected agent and get response
        response = asyncio.run(run_protected_agent(args.prompt))
        
        # Display the response
        if ""rejected"" in response.lower():
            console.print(Panel(response, title=""Input Rejected"", border_style=""red""))
        else:
            console.print(Panel(response, title=""Protected Agent Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/10_agent_with_guardrails.py,,1,7
survived,"def get_current_time(location: str) -> str:
    """"""
    Get the current time in a given location.
    
    Args:
        location: The location to get the time for. Currently only supports ""UTC"".
        
    Returns:
        A string containing the current time information.
    """"""
    # In a real implementation, you would use a timezone library
    current_time = datetime.utcnow()
    formatted_time = current_time.strftime(""%Y-%m-%d %H:%M:%S"")
    
    return f""The current time in {location} is {formatted_time}.""
",openai-agents-examples/05_agent_with_function_tools.py,,1,7
survived,"def test_run_sync_agent():
    """"""Test that the agent can run synchronously and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a simple test query
    response = run_sync_agent(""What are some quick exercises I can do at my desk?"")
    
    # Verify we got a non-empty response
    assert response
    assert len(response) > 0
    # The response should contain relevant terms
    assert any(term in response.lower() for term in [""exercise"", ""stretch"", ""desk"", ""movement""])
",openai-agents-examples/03_sync_agent.py,,0,7
survived,"def format_blog_as_markdown(title: str, content: str) -> str:
    """"""
    Format a blog post as markdown.
    
    Args:
        title: The blog post title
        content: The blog post content
        
    Returns:
        A string containing the formatted markdown
    """"""
    # Ensure the content has proper markdown formatting
    if not content.startswith('#'):
        content = f""# {title}\n\n{content}""
    
    # Add metadata
    markdown = f""""""---
title: ""{title}""
date: ""{datetime.now().strftime('%Y-%m-%d')}""
author: ""AI Research & Blog System""
tags: [""ai"", ""research"", ""blog""]
---

{content}
""""""
    
    return markdown
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"    def filter(self, input_str: str) -> Optional[str]:
        """"""
        Filter the input string for potentially harmful content.
        
        Args:
            input_str: The input string to filter
            
        Returns:
            The filtered string if it passes, or None if it should be rejected
        """"""
        # Convert to lowercase for case-insensitive matching
        lower_input = input_str.lower()
        
        # Check for filtered terms
        for term in self.filtered_terms:
            if term in lower_input:
                return None  # Reject the input
        
        return input_str  # Accept the input
",openai-agents-examples/10_agent_with_guardrails.py,ContentModerationGuardrail,1,7
survived,"def setup_tracing():
    """"""Set up OpenTelemetry tracing with console exporter.""""""
    # Create a tracer provider
    provider = TracerProvider()
    
    # Add a console exporter to see spans in the console
    console_exporter = ConsoleSpanExporter()
    processor = SimpleSpanProcessor(console_exporter)
    provider.add_span_processor(processor)
    
    # Set the global tracer provider
    trace.set_tracer_provider(provider)
    
    # Get a tracer
    return trace.get_tracer(""agent_tracer"")
",openai-agents-examples/04_agent_with_tracing.py,,1,7
survived,"def test_create_travel_assistant():
    """"""Test that the travel assistant agent is created with the correct configuration.""""""
    agent = create_travel_assistant()
    assert agent.name == ""TravelAssistant""
    assert ""travel assistant"" in agent.instructions.lower()
    assert len(agent.tools) == 3
",openai-agents-examples/05_agent_with_function_tools.py,,1,7
survived,"def main():
    """"""Main function to parse arguments and run the conversation agent.""""""
    parser = argparse.ArgumentParser(description=""Agent with Context Management Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    parser.add_argument(""--follow-up"", ""-f"", type=str, nargs=""*"", default=[],
                        help=""Optional follow-up prompts to simulate a conversation"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Simulate a conversation with the provided prompts
        responses = simulate_conversation(args.prompt, args.follow_up)
        
        # Display the initial response
        console.print(Panel(responses[0], title=f""Response to: {args.prompt}"", border_style=""green""))
        
        # Display follow-up responses if any
        for i, response in enumerate(responses[1:]):
            console.print(Panel(response, title=f""Response to: {args.follow_up[i]}"", border_style=""blue""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/09_agent_with_context_management.py,,1,7
survived,"def test_run_basic_agent():
    """"""Test that the agent can run and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a simple test query
    import asyncio
    response = asyncio.run(run_basic_agent(""What is 2+2?""))
    
    # Verify we got a non-empty response
    assert response
    assert len(response) > 0
    # The response should contain ""4"" somewhere
    assert ""4"" in response
",openai-agents-examples/01_basic_agent.py,,0,7
survived,"    def unregister_listener(
        self,
        event_type: typing.Type[platform_events.Event],
        func: typing.Callable[[platform_events.Event, msadapter.MessagePlatformAdapter], typing.Awaitable[None]],
    ):
        """"""å–æ¶ˆæ³¨å†Œäº‹ä»¶ç›‘å¬å™¨""""""
        pass
",pkg/platform/sources/webchat.py,WebChatAdapter,0,7
survived,"    async def initialize(self) -> None:
        @self.route('/send', methods=['POST'])
        async def send_message() -> str:
            """"""å‘é€è°ƒè¯•æ¶ˆæ¯åˆ°æµæ°´çº¿""""""
            try:
                data = await quart.request.get_json()
                session_type = data.get('session_type', 'person')
                content = data.get('content', '')
                
                if not content:
                    return self.http_status(400, -1, 'content is required')
                
                if session_type not in ['person', 'group']:
                    return self.http_status(400, -1, 'session_type must be person or group')
                
                webchat_adapter = None
                for bot in self.ap.platform_mgr.bots:
                    if hasattr(bot.adapter, '__class__') and bot.adapter.__class__.__name__ == 'WebChatAdapter':
                        webchat_adapter = bot.adapter
                        break
                
                if not webchat_adapter:
                    return self.http_status(404, -1, 'WebChat adapter not found')
                
                result = await webchat_adapter.send_debug_message(session_type, content)
                
                return self.success(data=result)
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')

        @self.route('/messages/<session_type>', methods=['GET'])
        async def get_messages(session_type: str) -> str:
            """"""èŽ·å–è°ƒè¯•æ¶ˆæ¯åŽ†å²""""""
            try:
                if session_type not in ['person', 'group']:
                    return self.http_status(400, -1, 'session_type must be person or group')
                
                webchat_adapter = None
                for bot in self.ap.platform_mgr.bots:
                    if hasattr(bot.adapter, '__class__') and bot.adapter.__class__.__name__ == 'WebChatAdapter':
                        webchat_adapter = bot.adapter
                        break
                
                if not webchat_adapter:
                    return self.http_status(404, -1, 'WebChat adapter not found')
                
                messages = webchat_adapter.get_debug_messages(session_type)
                
                return self.success(data={'messages': messages})
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')

        @self.route('/reset/<session_type>', methods=['POST'])
        async def reset_session(session_type: str) -> str:
            """"""é‡ç½®è°ƒè¯•ä¼šè¯""""""
            try:
                if session_type not in ['person', 'group']:
                    return self.http_status(400, -1, 'session_type must be person or group')
                
                webchat_adapter = None
                for bot in self.ap.platform_mgr.bots:
                    if hasattr(bot.adapter, '__class__') and bot.adapter.__class__.__name__ == 'WebChatAdapter':
                        webchat_adapter = bot.adapter
                        break
                
                if not webchat_adapter:
                    return self.http_status(404, -1, 'WebChat adapter not found')
                
                webchat_adapter.reset_debug_session(session_type)
                
                return self.success(data={'message': 'Session reset successfully'})
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')

        @self.route('/pipelines', methods=['GET'])
        async def get_pipelines() -> str:
            """"""èŽ·å–å¯ç”¨çš„æµæ°´çº¿åˆ—è¡¨""""""
            try:
                pipelines = await self.ap.pipeline_mgr.get_pipelines()
                
                pipeline_list = []
                for pipeline in pipelines:
                    pipeline_list.append({
                        'id': pipeline.uuid,
                        'name': pipeline.name,
                        'description': pipeline.description,
                        'is_default': pipeline.is_default
                    })
                
                return self.success(data={'pipelines': pipeline_list})
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')",pkg/api/http/controller/groups/debug/webchat.py,WebChatDebugRouterGroup,1,6
survived,"    async def send_debug_message(self, session_type: str, content: str) -> dict:
        """"""å‘é€è°ƒè¯•æ¶ˆæ¯åˆ°æµæ°´çº¿""""""
        session_key = f'webchat{session_type}'
        
        if session_key not in self.debug_messages:
            self.debug_messages[session_key] = []
            
        message_chain = platform_message.MessageChain([
            platform_message.Plain(content)
        ])
        
        user_message = {
            'id': len(self.debug_messages[session_key]) + 1,
            'type': 'user',
            'content': content,
            'timestamp': datetime.now().isoformat(),
            'message_chain': [{'type': 'Plain', 'text': content}]
        }
        
        self.debug_messages[session_key].append(user_message)
        
        if session_type == 'person':
            sender = platform_entities.Friend(id='webchatperson', nickname='è°ƒè¯•ç”¨æˆ·')
            event = platform_events.FriendMessage(
                sender=sender,
                message_chain=message_chain,
                time=datetime.now().timestamp()
            )
            launcher_type = core_entities.LauncherTypes.PERSON
            launcher_id = 'webchatperson'
        else:
            group = platform_entities.Group(id='webchatgroup', name='è°ƒè¯•ç¾¤èŠ')
            sender = platform_entities.GroupMember(id='webchatperson', nickname='è°ƒè¯•ç”¨æˆ·', group=group)
            event = platform_events.GroupMessage(
                sender=sender,
                message_chain=message_chain,
                time=datetime.now().timestamp()
            )
            launcher_type = core_entities.LauncherTypes.GROUP
            launcher_id = 'webchatgroup'
        
        await self.ap.query_pool.add_query(
            bot_uuid='webchat-debug',
            launcher_type=launcher_type,
            launcher_id=launcher_id,
            sender_id='webchatperson',
            message_event=event,
            message_chain=message_chain,
            adapter=self,
        )
        
        return {'success': True, 'message_id': user_message['id']}
",pkg/platform/sources/webchat.py,WebChatAdapter,1,7
survived,"def search_result_item(result: rx.Var) -> rx.Component:
    """"""Render a single search result item.""""""
    return rx.box(
        rx.vstack(
            rx.text(
                result['title'],
                font_weight=""600"",
                color=""var(--c-slate-12)"",
                font_size=""14px"",
                margin_bottom=""4px""
            ),
            rx.text(
                result['content'],
                color=""var(--c-slate-11)"",
                font_size=""13px"",
                line_height=""1.4"",
                margin_bottom=""4px""
            ),
            rx.text(
                result['path'],
                color=""var(--c-slate-9)"",
                font_size=""12px""
            ),
            align_items=""start"",
            spacing=""1""
        ),
        padding=""12px"",
        border_bottom=""1px solid var(--c-slate-3)"",
        cursor=""pointer"",
        _hover={""background_color"": ""var(--c-slate-2)""},
        on_click=lambda: TypesenseSearchState.navigate_to_result(result['url'])
    )
",pcweb/components/docpage/navbar/typesense.py,,1,6
survived,"    def clean_content(self, content: str) -> str:
        """"""Clean markdown content for indexing.""""""
        content = re.sub(r'```[^`]*```', '', content, flags=re.DOTALL)
        
        content = re.sub(r'`[^`]*`', '', content)
        
        content = re.sub(r'\[([^\]]*)\]\([^)]*\)', r'\1', content)
        
        content = re.sub(r'[*_~`]', '', content)
        
        content = re.sub(r'<[^>]*>', '', content)
        
        content = re.sub(r'^---.*?---', '', content, flags=re.DOTALL | re.MULTILINE)
        
        content = re.sub(r'```python exec.*?```', '', content, flags=re.DOTALL)
        
        content = re.sub(r'```python demo.*?```', '', content, flags=re.DOTALL)
        
        content = re.sub(r'```python eval.*?```', '', content, flags=re.DOTALL)
        
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = content.strip()
        
        return content
",scripts/typesense_indexer.py,MarkdownProcessor,1,7
deleted,"def typesense_search_with_styles() -> rx.Component:
    """"""Create the Typesense search component with styles.""""""
    return rx.fragment(
        rx.html(search_styles),
        typesense_search()
    )",pcweb/components/docpage/navbar/typesense.py,,1,6
survived,"def test_solana_smart_wallet_with_user_id(smart_api, test_user_id, test_solana_wallet_options):
    """"""Test Solana smart wallet creation with user ID.""""""
    wallet = smart_api.create_wallet(
        wallet_type=WalletType.SOLANA_SMART_WALLET,
        linked_user=f""userId:{test_user_id}""
    )
    assert wallet[""type""] == ""solana-smart-wallet""
    assert ""linkedUser"" in wallet",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,,1,7
survived,"    def get_final_result(self):
        """"""
        Get the result from the final stage of the pipeline.
        
        Returns:
            dict: Result from the final stage
        """"""
        if not self.stages:
            return None
        
        final_stage_name = self.stages[-1][""name""]
        if final_stage_name in self.results:
            return self.results[final_stage_name]
        
        return None
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager,1,7
survived,"    def update_product(product_id, name=None, price=None, category_id=None, description=None, sku=None):
        """"""Update a product.""""""
        try:
            product = ProductService.update_product(product_id, name, price, category_id, description, sku)
            if not product:
                return {
                    ""success"": False,
                    ""message"": f""Product with ID {product_id} not found""
                }
            return {
                ""success"": True,
                ""message"": ""Product updated successfully"",
                ""data"": product
            }
        except ValueError as e:
            Logger.warning(app_logger, f""Validation error in update_product: {str(e)}"")
            return {
                ""success"": False,
                ""message"": str(e)
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in update_product: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while updating the product""
            }
",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI,1,6
survived,"    def to_dict(self):
        """"""Convert product to dictionary.""""""
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""price"": self.price,
            ""category_id"": self.category_id,
            ""description"": self.description,
            ""sku"": self.sku,
            ""created_at"": self.created_at,
            ""updated_at"": self.updated_at
        }
",codebase-architectures/layered-architecture/models/product.py,Product,1,8
survived,"    def create_product(name, price, category_id=None, description=None, sku=None):
        """"""Create a new product.""""""
        try:
            # Validate product data
            if not name or not isinstance(name, str):
                raise ValueError(""Product name is required and must be a string"")
            
            try:
                price = float(price)
                if price < 0:
                    raise ValueError()
            except (ValueError, TypeError):
                raise ValueError(""Price must be a positive number"")
            
            # Validate category if provided
            if category_id:
                category = db.get(""categories"", category_id)
                if not category:
                    raise ValueError(f""Category with ID {category_id} not found"")
            
            # Validate SKU if provided
            if sku:
                existing_products = db.query(""products"", lambda p: p[""sku""] == sku)
                if existing_products:
                    raise ValueError(f""Product with SKU '{sku}' already exists"")
            
            # Create and save product
            product = Product(
                name=name,
                price=price,
                category_id=category_id,
                description=description,
                sku=sku
            )
            saved_product = db.insert(""products"", product.to_dict())
            Logger.info(app_logger, f""Created product: {name}"")
            return saved_product
        except Exception as e:
            Logger.error(app_logger, f""Error creating product: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/product_service.py,ProductService,1,7
survived,"    def get_logger(name):
        """"""Get a logger instance for the given name.""""""
        return logging.getLogger(name)
",codebase-architectures/layered-architecture/utils/logger.py,Logger,1,7
survived,"def revoke_token(token: str) -> bool:
    """"""
    Revoke an authentication token.
    
    Args:
        token: The token to revoke
        
    Returns:
        True if the token was revoked, False if it didn't exist
    """"""
    if token in TOKEN_STORE:
        del TOKEN_STORE[token]
        return True
    return False
",codebase-architectures/atomic-composable-architecture/modules/auth.py,,1,6
survived,"def send_email_notification(email: str, subject: str, message: str) -> bool:
    """"""
    Send an email notification (mock implementation).
    
    Args:
        email: The recipient's email address
        subject: The email subject
        message: The email message
        
    Returns:
        True if the email was sent successfully (always True in this mock)
    """"""
    # In a real application, this would send an actual email
    print(f""[EMAIL] To: {email}, Subject: {subject}"")
    print(f""[EMAIL] Message: {message}"")
    return True
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,,1,7
survived,"def verify_password(password: str, hashed_password: str, salt: str) -> bool:
    """"""
    Verify a password against a stored hash.
    
    Args:
        password: The password to verify
        hashed_password: The stored hashed password
        salt: The salt used for hashing
        
    Returns:
        True if the password matches, False otherwise
    """"""
    calculated_hash, _ = hash_password(password, salt)
    return calculated_hash == hashed_password
",codebase-architectures/atomic-composable-architecture/modules/auth.py,,1,7
survived,"    def validate_data(self, schema=None, required_fields=None):
        """"""
        Validate the loaded data against a schema or required fields.
        
        Args:
            schema: Schema definition for validation
            required_fields: List of required field names
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data loaded to validate"")
            return self._create_result()
        
        try:
            validation_errors = []
            
            # Validate required fields if specified
            if required_fields:
                if isinstance(self.data, list):
                    for i, item in enumerate(self.data):
                        try:
                            validate_required_fields(item, required_fields)
                        except ValueError as e:
                            validation_errors.append(f""Record {i}: {str(e)}"")
                else:
                    try:
                        validate_required_fields(self.data, required_fields)
                    except ValueError as e:
                        validation_errors.append(str(e))
            
            # Update metadata based on validation results
            if validation_errors:
                self.metadata[""status""] = ""validation_failed""
                self.metadata[""errors""].extend(validation_errors)
            else:
                self.metadata[""status""] = ""validated""
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Validation error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/input_stage.py,InputStage,1,7
survived,"    def update(self, table_name, item_id, item):
        """"""Update an item in a table.""""""
        if table_name not in self.data or item_id not in self.data[table_name]:
            Logger.warning(self.logger, f""Cannot update: Item with ID {item_id} not found in '{table_name}'"")
            return None
        
        # Ensure ID remains the same
        item[""id""] = item_id
        self.data[table_name][item_id] = item
        Logger.info(self.logger, f""Updated item with ID {item_id} in '{table_name}'"")
        return item
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase,1,7
survived,"def validate_required_fields(data, required_fields):
    """"""Validate that all required fields are present in the data.""""""
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        raise ValueError(f""Missing required fields: {', '.join(missing_fields)}"")
    return True",codebase-architectures/vertical-slice-architecture/shared/utils.py,,1,7
survived,"def create_alert(user_id: str, message: str, level: str = ""info"", 
                data: Optional[Dict] = None) -> Dict:
    """"""
    Create an alert notification.
    
    Args:
        user_id: The ID of the user to alert
        message: The alert message
        level: Alert level (info, warning, error)
        data: Additional data for the alert
        
    Returns:
        The created notification
    """"""
    if data is None:
        data = {}
    
    data[""message""] = message
    
    notification = create_notification(
        user_id=user_id,
        notification_type=""alert"",
        data={
            **data,
            ""level"": level
        }
    )
    
    return notification",codebase-architectures/atomic-composable-architecture/modules/notifications.py,,1,7
survived,"    def get_category(category_id):
        """"""Get a category by ID.""""""
        try:
            category_data = db.get(""categories"", category_id)
            if not category_data:
                Logger.warning(app_logger, f""Category not found: {category_id}"")
                return None
            return category_data
        except Exception as e:
            Logger.error(app_logger, f""Error getting category: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/category_service.py,CategoryService,1,7
survived,"    def update(self, collection_name, id, item):
        """"""Update an item in a collection.""""""
        if collection_name not in self.data or id not in self.data[collection_name]:
            return False
        self.data[collection_name][id] = item
        return True
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB,1,7
survived,"    def get_products_by_category(category_id):
        """"""Get all products in a category.""""""
        try:
            products = ProductService.get_products_by_category(category_id)
            return {
                ""success"": True,
                ""data"": products
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in get_products_by_category: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while retrieving products""
            }
",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI,1,7
survived,"    def get_all_products():
        """"""Get all products.""""""
        try:
            products = db.get_all(""products"")
            Logger.info(app_logger, f""Retrieved {len(products)} products"")
            return products
        except Exception as e:
            Logger.error(app_logger, f""Error getting all products: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/product_service.py,ProductService,1,7
survived,"    def _execute_stage(self, stage_instance, previous_result):
        """"""Execute a stage with the result from the previous stage.""""""
        # Determine which stage we're executing based on the instance type
        if hasattr(stage_instance, ""process""):
            # Processing stage
            result = stage_instance.process(previous_result)
            
            # Execute additional processing methods if configured
            if hasattr(self, ""processing_config""):
                config = self.processing_config
                
                # Calculate statistics if configured
                if config.get(""calculate_statistics""):
                    result = stage_instance.calculate_statistics(
                        numeric_fields=config.get(""numeric_fields"")
                    )
                
                # Apply filters if configured
                if ""filters"" in config:
                    for filter_config in config[""filters""]:
                        result = stage_instance.filter_data(
                            filter_config[""filter_func""],
                            filter_config.get(""description"")
                        )
                
                # Apply transformations if configured
                if ""transformations"" in config:
                    result = stage_instance.transform_fields(
                        config[""transformations""],
                        config.get(""transformation_description"")
                    )
            
            # Finalize the processing stage
            result = stage_instance.finalize()
            
        elif hasattr(stage_instance, ""prepare""):
            # Output stage
            result = stage_instance.prepare(previous_result)
            
            # Execute additional output methods if configured
            if hasattr(self, ""output_config""):
                config = self.output_config
                
                # Format as summary if configured
                if config.get(""format_summary"", False):
                    result = stage_instance.format_as_summary()
                
                # Format as detailed report if configured
                if config.get(""format_detailed"", False):
                    result = stage_instance.format_as_detailed_report()
                
                # Save to file if configured
                if ""save_to_file"" in config:
                    for save_config in config[""save_to_file""]:
                        result = stage_instance.save_to_file(
                            output_format=save_config.get(""format"", ""json""),
                            output_dir=save_config.get(""dir"", ""./output""),
                            filename=save_config.get(""filename"")
                        )
                
                # Print results if configured
                if config.get(""print_results""):
                    result = stage_instance.print_results(
                        output_type=config.get(""print_output_type"", ""summary"")
                    )
            
            # Finalize the output stage
            result = stage_instance.finalize()
            
        else:
            # Unknown stage type
            raise ValueError(f""Unknown stage type: {type(stage_instance).__name__}"")
        
        return result
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,DataProcessingPipeline,1,7
survived,"    def __init__(self):
        """"""Initialize the output stage.""""""
        self.data = None
        self.analysis = None
        self.metadata = {
            ""stage"": ""output"",
            ""status"": ""initialized"",
            ""errors"": [],
            ""output_formats"": []
        }
",codebase-architectures/pipeline-architecture/pipeline/output_stage.py,OutputStage,1,7
survived,"    def add_stage(self, stage_name, stage_instance):
        """"""
        Add a stage to the pipeline.
        
        Args:
            stage_name: Name of the stage
            stage_instance: Instance of the stage class
        """"""
        self.stages.append({
            ""name"": stage_name,
            ""instance"": stage_instance,
            ""status"": ""pending""
        })
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager,1,7
survived,"    def create_user(user_data):
        """"""Create a new user.""""""
        validate_required_fields(user_data, [""username"", ""email""])
        
        # Check if username already exists
        all_users = db.get_all(""users"")
        if any(user[""username""] == user_data[""username""] for user in all_users):
            raise ValueError(f""Username '{user_data['username']}' already exists"")
        
        user = User(**user_data)
        db.insert(""users"", user.id, user.to_dict())
        return user.to_dict()
",codebase-architectures/vertical-slice-architecture/features/users/service.py,UserService,1,7
survived,"    def get_by_username(username):
        """"""Get a user by username.""""""
        user = UserService.get_by_username(username)
        if not user:
            return {""error"": f""User with username '{username}' not found""}
        return user
",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI,1,6
survived,"def delete_user_alert(user_id: str, notification_id: str) -> bool:
    """"""
    Delete an alert.
    
    Args:
        user_id: The ID of the user
        notification_id: The ID of the notification
        
    Returns:
        True if the alert was deleted, False otherwise
    """"""
    return delete_notification(user_id, notification_id)
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,,0,7
survived,"def get_user_notifications(user_id: str, unread_only: bool = False) -> List[Dict]:
    """"""
    Get notifications for a user.
    
    Args:
        user_id: The ID of the user
        unread_only: Whether to return only unread notifications
        
    Returns:
        List of notifications
    """"""
    if user_id not in NOTIFICATION_STORE:
        return []
    
    if unread_only:
        return [n for n in NOTIFICATION_STORE[user_id] if not n[""is_read""]]
    
    return NOTIFICATION_STORE[user_id]
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,,1,7
survived,"def main():
    """"""Main entry point for the application.""""""
    # Set up argument parser
    parser = argparse.ArgumentParser(description=""Claude 3.7 File Editor Agent"")
    parser.add_argument(
        ""--prompt"",
        ""-p"",
        required=True,
        help=""The prompt for what file operations to perform"",
    )
    parser.add_argument(
        ""--max-loops"",
        ""-l"",
        type=int,
        default=15,
        help=""Maximum number of tool use loops (default: 15)"",
    )
    parser.add_argument(
        ""--thinking"",
        ""-t"",
        type=int,
        default=DEFAULT_THINKING_TOKENS,
        help=f""Maximum thinking tokens (default: {DEFAULT_THINKING_TOKENS})"",
    )
    parser.add_argument(
        ""--efficiency"",
        ""-e"",
        action=""store_true"",
        help=""Enable token-efficient tool use (beta feature)"",
    )
    args = parser.parse_args()

    console.print(Panel.fit(""Claude 3.7 File Editor Agent (Atomic/Composable Architecture)""))
    console.print(f""\n[bold]Prompt:[/bold] {args.prompt}\n"")
    console.print(f""[dim]Thinking tokens: {args.thinking}[/dim]"")
    console.print(f""[dim]Max loops: {args.max_loops}[/dim]"")
    
    if args.efficiency:
        console.print(f""[dim]Token-efficient tools: Enabled[/dim]\n"")
    else:
        console.print(f""[dim]Token-efficient tools: Disabled[/dim]\n"")

    # For testing purposes, we'll just print a success message
    console.print(""[green]Successfully loaded the Atomic/Composable Architecture implementation![/green]"")
    console.print(""[yellow]This is a mock implementation for testing the architecture structure.[/yellow]"")
    console.print(""[yellow]In a real implementation, this would connect to the Claude API.[/yellow]"")

    # Display mock token usage
    display_token_usage(1000, 500)
",example-agent-codebase-arch/atomic-composable-architecture/main.py,,1,7
survived,"    def _str_replace(self, path: str, old_str: str, new_str: str) -> FileOperationResult:
        """"""
        Replace a specific string in a file.

        Args:
            path: The path to the file to modify
            old_str: The text to replace
            new_str: The new text to insert

        Returns:
            FileOperationResult with result or error message
        """"""
        try:
            # Normalize the path
            path = normalize_path(path)

            if not os.path.exists(path):
                error_msg = f""File {path} does not exist""
                console.log(f""[str_replace] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            with open(path, ""r"") as f:
                content = f.read()

            if old_str not in content:
                error_msg = f""The specified string was not found in the file {path}""
                console.log(f""[str_replace] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            new_content = content.replace(old_str, new_str, 1)

            with open(path, ""w"") as f:
                f.write(new_content)

            console.print(f""[green]Successfully replaced text in {path}[/green]"")
            console.log(f""[str_replace] Successfully replaced text in {path}"")
            return FileOperationResult(True, f""Successfully replaced text in {path}"")
        except Exception as e:
            error_msg = f""Error replacing text: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[str_replace] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/pipeline-architecture/steps/processing_stage.py,ProcessingStage,1,7
survived,"def display_token_usage(input_tokens: int, output_tokens: int) -> None:
    """"""
    Display token usage in a table.

    Args:
        input_tokens: Number of input tokens used
        output_tokens: Number of output tokens used
    """"""
    table = Table(title=""Token Usage"")
    table.add_column(""Type"", style=""cyan"")
    table.add_column(""Count"", style=""green"")
    
    table.add_row(""Input Tokens"", str(input_tokens))
    table.add_row(""Output Tokens"", str(output_tokens))
    table.add_row(""Total Tokens"", str(input_tokens + output_tokens))
    
    console.print(table)",example-agent-codebase-arch/vertical-slice-architecture/shared/utils.py,,1,7
survived,"def display_file_content(path: str, content: str) -> None:
    """"""
    Display file content with syntax highlighting

    Args:
        path: Path to the file
        content: Content of the file
    """"""
    file_extension = os.path.splitext(path)[1][1:]  # Get extension without the dot
    syntax = Syntax(content, file_extension or ""text"", line_numbers=True)
    console.print(Panel(syntax, title=f""File: {path}""))
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,,1,7
survived,"def normalize_path(path: str) -> str:
    """"""
    Normalize file paths to handle various formats (absolute, relative, Windows paths, etc.)

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """"""
    if not path:
        return path

    # Handle Windows backslash paths if provided
    path = path.replace(""\\"", os.sep)

    is_windows_path = False
    if os.name == ""nt"" and len(path) > 1 and path[1] == "":"":
        is_windows_path = True

    # Handle /repo/ paths from Claude (tool use convention)
    if path.startswith(""/repo/""):
        path = os.path.join(os.getcwd(), path[6:])
        return path

    if path.startswith(""/""):
        # Handle case when Claude provides paths with leading slash
        if path == ""/"" or path == ""/."":
            # Special case for root directory
            path = os.getcwd()
        else:
            # Replace leading slash with current working directory
            path = os.path.join(os.getcwd(), path[1:])
    elif path.startswith(""./""):
        # Handle relative paths starting with ./
        path = os.path.join(os.getcwd(), path[2:])
    elif not os.path.isabs(path) and not is_windows_path:
        # For non-absolute paths that aren't Windows paths either
        path = os.path.join(os.getcwd(), path)

    return path
",example-agent-codebase-arch/layered-architecture/utils/path_utils.py,,1,7
survived,"def test_create_folder_structure_normal_name_unchanged():
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        folder_path, folder_name, class_name = create_folder_structure(""hello"")
        
        assert folder_name == ""hello""
        assert class_name == ""Hello""
        assert folder_path.name == ""hello""
        assert folder_path.exists()
",tests/cli/test_create_crew.py,,1,7
survived,"def test_create_crew_normal_name_still_works(mock_load_env, mock_write_env, mock_copy_template, temp_dir):
    mock_load_env.return_value = {}
    
    with tempfile.TemporaryDirectory() as work_dir:
        os.chdir(work_dir)
        create_crew(""normal-project"", skip_provider=True)
        
        project_path = Path(work_dir) / ""normal_project""
        assert project_path.exists()
        assert (project_path / ""src"" / ""normal_project"").exists()
",tests/cli/test_create_crew.py,,1,7
survived,"        def set_message(self):
            self.state.message = ""Original message""
            self.state.counter = 42
",tests/test_flow_persistence.py,RestorableFlow,1,6
survived,"    def __init__(self, db_path: Optional[str] = None):
        """"""Initialize SQLite persistence.
        
        Args:
            db_path: Path to the SQLite database file. If not provided, uses
                    CREWAI_FLOW_DB_PATH environment variable or falls back to
                    a temporary database.
        """"""
        self.db_path = db_path or os.getenv(
            ""CREWAI_FLOW_DB_PATH"",
            os.path.join(tempfile.gettempdir(), ""crewai_flows.db"")
        )
        self.init_db()
",src/crewai/flow/persistence/sqlite.py,SQLiteFlowPersistence,1,7
survived,"        def step_2(self):
            self.state.counter = 2
            self.state.message = ""Step 2""
",tests/test_flow_persistence.py,MultiStepFlow,1,6
survived,"    def call(
        self,
        messages: Union[str, List[Dict[str, str]]],
        tools: Optional[List[dict]] = None,
        callbacks: Optional[List[Any]] = None,
        available_functions: Optional[Dict[str, Any]] = None,
    ) -> Union[str, Any]:
        self.calls.append({
            ""messages"": messages, 
            ""tools"": tools,
            ""callbacks"": callbacks,
            ""available_functions"": available_functions
        })
        # In a real implementation, this would use the JWT token to authenticate
        # with an external service
        return ""Response from JWT-authenticated LLM""
",tests/custom_llm_test.py,JWTAuthLLM,1,6
survived,"    def test_get_content_with_delta_content(self) -> None:
        # Create a mock response with choices and delta.content
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].delta = Mock()
        mock_response.choices[0].delta.content = ""Test content""

        # Call get_content with the mock response
        result = get_content(mock_response)

        # Assert that the result is the expected content
        self.assertEqual(result, ""Test content"")",tests/_server/test_ai.py,TestGetContent,1,7
