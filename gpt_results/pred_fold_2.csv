status,method,filepath,class_name,predict,confidence
survived,"def test_no_changes():
    old_code = ""def func(a, b=1): pass""
    new_code = ""def func(a, b=1): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 0
",tests/dev/test_check_function_signatures.py,,1,7
survived,"def test_optional_positional_became_required():
    old_code = ""def func(a, b=1): pass""
    new_code = ""def func(a, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""Optional positional param 'b' became required.""
    assert errors[0].param_name == ""b""
",tests/dev/test_check_function_signatures.py,,1,7
deleted,"def precompute_c_values(op_data):
    input_data = op_data[""input_data""]
    other_data = op_data[""other_data""]
    rows, inner_dim = input_data.shape
    cols = other_data.shape[1]

    precomputed = {}
    for i in range(rows):
        for j in range(cols):
            precomputed[(i, j)] = [0] * (inner_dim + 1)
            for k in range(1, inner_dim + 1):
                precomputed[(i, j)][k] = torch.dot(
                    input_data[i, :k], other_data[:k, j]
                ).item()

    return precomputed
",triton_viz/visualizer/interface.py,,0,7
survived,"def move_nancorrmatrix(a, window, min_count, out):
    """"""
    Moving window correlation matrix gufunc.

    For 2D input, correlates variables (rows) across observations (columns in the window).
    """"""
    n_vars = a.shape[0]
    n_obs = a.shape[1]
    min_count = max(min_count, 1)

    # Initialize running statistics
    sums = np.zeros(n_vars, dtype=a.dtype)
    sums_sq = np.zeros(n_vars, dtype=a.dtype)
    counts = np.zeros(n_vars, dtype=np.int64)

    # Initialize pairwise statistics
    prods = np.zeros((n_vars, n_vars), dtype=a.dtype)
    pair_counts = np.zeros((n_vars, n_vars), dtype=np.int64)

    for t in range(n_obs):
        # Remove old values when window slides
        if t >= window:
            for i in range(n_vars):
                old_val = a[i, t - window]
                if not np.isnan(old_val):
                    sums[i] -= old_val
                    sums_sq[i] -= old_val * old_val
                    counts[i] -= 1

                    # Update pairwise products
                    for j in range(n_vars):
                        old_val_j = a[j, t - window]
                        if not np.isnan(old_val_j):
                            prods[i, j] -= old_val * old_val_j
                            pair_counts[i, j] -= 1

        # Add new values
        for i in range(n_vars):
            new_val = a[i, t]
            if not np.isnan(new_val):
                sums[i] += new_val
                sums_sq[i] += new_val * new_val
                counts[i] += 1

                # Update pairwise products
                for j in range(n_vars):
                    new_val_j = a[j, t]
                    if not np.isnan(new_val_j):
                        prods[i, j] += new_val * new_val_j
                        pair_counts[i, j] += 1

        # Compute correlation matrix for current window
        for i in range(n_vars):
            for j in range(n_vars):
                if i == j:
                    # Diagonal is 1 when we have enough data for correlation
                    # Correlation requires at least 2 observations to compute variance
                    if counts[i] >= max(min_count, 2):
                        out[t, i, j] = 1.0
                    else:
                        out[t, i, j] = np.nan
                else:
                    # Compute correlation
                    n = pair_counts[i, j]
                    # Need at least 2 observations for correlation (to compute variance)
                    if n >= max(min_count, 2) and counts[i] >= 2 and counts[j] >= 2:
                        mean_i = sums[i] / counts[i]
                        mean_j = sums[j] / counts[j]

                        # Compute variances
                        var_i = sums_sq[i] / counts[i] - mean_i * mean_i
                        var_j = sums_sq[j] / counts[j] - mean_j * mean_j

                        # Compute covariance
                        cov = prods[i, j] / n - (sums[i] / counts[i]) * (
                            sums[j] / counts[j]
                        )

                        # Compute correlation
                        if var_i > 0 and var_j > 0:
                            corr = cov / np.sqrt(var_i * var_j)
                            # Clamp to [-1, 1] for numerical stability
                            out[t, i, j] = max(-1.0, min(1.0, corr))
                        else:
                            out[t, i, j] = np.nan
                    else:
                        out[t, i, j] = np.nan
",numbagg/moving_matrix.py,,1,6
survived,"    def test_extreme_alpha_values(self):
        """"""Test behavior with alpha values very close to 0 and 1.""""""
        # Use data that's not perfectly correlated to see differences
        np.random.seed(42)
        data = np.random.randn(2, 20)
        data[1] = 0.7 * data[0] + 0.5 * np.random.randn(20)  # Partial correlation

        # Test with alpha very close to 0 (almost no decay)
        result_low = move_exp_nancorrmatrix(data, alpha=1e-6)

        # Test with alpha close to 1 (very fast decay)
        result_high = move_exp_nancorrmatrix(data, alpha=0.99)

        # Both should produce valid results
        assert not np.all(np.isnan(result_low[-1]))
        assert not np.all(np.isnan(result_high[-1]))

        # With different correlation patterns, results should differ
        # Check the off-diagonal correlation values
        assert not np.allclose(result_low[-5:, 0, 1], result_high[-5:, 0, 1], rtol=1e-2)
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced,1,7
survived,"    def test_single_valid_observation(self):
        """"""Test with only one valid observation per variable.""""""
        data = np.array(
            [[1.0, np.nan, np.nan, np.nan], [np.nan, 2.0, np.nan, np.nan]],
            dtype=np.float64,
        )

        result_corr = move_exp_nancorrmatrix(data, alpha=0.5)
        result_cov = move_exp_nancovmatrix(data, alpha=0.5)

        # Should have shape (4, 2, 2)
        assert result_corr.shape == (4, 2, 2)
        assert result_cov.shape == (4, 2, 2)

        # Most values should be NaN since we need at least 2 observations for correlation/covariance
        assert np.sum(np.isnan(result_corr)) > np.sum(np.isfinite(result_corr))
        assert np.sum(np.isnan(result_cov)) > np.sum(np.isfinite(result_cov))",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions,1,7
survived,"    def test_search_by_model_name(self, temp_model_dir):
        """"""Test searching for models by name""""""
        scanner = ModelScanner(temp_model_dir)
        
        # Test with exact name
        results = scanner.search_by_model_name(""gpt-4"")
        assert results['model_name'] == ""gpt-4""
        assert results['total_files'] > 0
        assert len(results['categories']) > 0
        
        # Test with prefix stripping
        results = scanner.search_by_model_name(""claude-3-opus"")
        assert results['normalized_name'] == ""3-opus""  # ""claude-"" prefix stripped
        assert results['total_files'] > 0
",tests/test_scan/test_scanner.py,TestModelScanner,1,7
survived,"    def _compare_use_cases(self, model_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """"""Compare model use cases""""""
        use_cases = {}
        
        use_case_patterns = {
            'chatbot': ['chat', 'conversation', 'dialogue'],
            'content_generation': ['generate', 'create', 'write'],
            'code_assistance': ['code', 'programming', 'development'],
            'translation': ['translate', 'multilingual'],
            'analysis': ['analyze', 'analysis', 'insight'],
            'summarization': ['summary', 'summarize'],
            'question_answering': ['qa', 'question', 'answer'],
            'research': ['research', 'academic', 'scientific']
        }
        
        for model, data in model_data.items():
            model_use_cases = set()
            
            # Check all text content for use case patterns
            all_text = []
            if data.get('config'):
                all_text.append(json.dumps(data['config']))
            
            for file_info in data.get('files', []):
                all_text.append(file_info['path'])
            
            combined_text = ' '.join(all_text).lower()
            
            for use_case, patterns in use_case_patterns.items():
                if any(pattern in combined_text for pattern in patterns):
                    model_use_cases.add(use_case)
            
            use_cases[model] = list(model_use_cases)
        
        return use_cases
",src/haconiwa/scan/comparator.py,ModelComparator,1,6
survived,"    def generate_project_wide(self,
                            action: str,
                            file_pattern: str = ""*.py"",
                            exclude_patterns: Optional[List[str]] = None) -> Dict[str, Any]:
        """"""Generate YAML for project-wide changes""""""
        
        from .scanner import ModelScanner
        
        scanner = ModelScanner(self.base_path)
        files = []
        
        # Find all matching files
        for file_path in scanner._iter_files([file_pattern]):
            if exclude_patterns:
                skip = False
                for pattern in exclude_patterns:
                    if pattern in str(file_path):
                        skip = True
                        break
                if skip:
                    continue
            
            files.append(str(file_path.relative_to(self.base_path)))
        
        # Generate tasks
        tasks = []
        for file_path in files[:50]:  # Limit to 50 files for safety
            prompt = self._get_action_prompt(action)
            tasks.append({
                'file': file_path,
                'prompt': prompt
            })
        
        config = {
            'provider': 'claude',
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'source': 'haconiwa scan generate-parallel-config',
                'action': action,
                'file_pattern': file_pattern,
                'total_tasks': len(tasks)
            },
            'tasks': tasks,
            'options': {
                'max_concurrent': 5,
                'timeout': 120,
                'allowed_tools': ['Read', 'Write', 'Edit', 'MultiEdit'],
                'permission_mode': 'confirmEach',
                'output_dir': f'./project-wide-{action}'
            }
        }
        
        return config
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator,1,7
survived,"    def test_scan_analyze_command(self, runner, temp_model_dir):
        """"""Test the scan analyze command""""""
        result = runner.invoke(
            scan_app,
            [""analyze"", ""--path"", str(temp_model_dir), ""--format"", ""summary""]
        )
        
        assert result.exit_code == 0
        assert ""Model Analysis Summary"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def _categorize_size(self, size_gb: float) -> str:
        """"""Categorize model size""""""
        if size_gb < 0.1:
            return 'Tiny'
        elif size_gb < 1:
            return 'Small'
        elif size_gb < 10:
            return 'Medium'
        elif size_gb < 50:
            return 'Large'
        else:
            return 'Very Large'
",src/haconiwa/scan/comparator.py,ModelComparator,1,7
survived,"    def format(self, data: Any, output_format: str) -> str:
        """"""Format data according to specified output format""""""
        handler = self.format_handlers.get(output_format, self._format_text)
        return handler(data)
",src/haconiwa/scan/formatter.py,OutputFormatter,1,7
survived,"    def _should_ignore(self, path: Path) -> bool:
        """"""Check if path should be ignored""""""
        path_str = str(path)
        
        # Check whitelist first
        if self.whitelist:
            whitelisted = any(
                fnmatch.fnmatch(path_str, pattern) or 
                pattern in path_str 
                for pattern in self.whitelist
            )
            if not whitelisted:
                return True
        
        # Check ignore patterns
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(path.name, pattern):
                return True
            if pattern in path_str:
                return True
        
        return False
",src/haconiwa/scan/scanner.py,ModelScanner,1,7
survived,"    def _format_size(self, size: int) -> str:
        """"""Format file size in human-readable format""""""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024.0:
                return f""{size:.1f} {unit}""
            size /= 1024.0
        return f""{size:.1f} PB""",src/haconiwa/scan/formatter.py,OutputFormatter,1,7
survived,"    def test_scan_model_command(self, runner, temp_model_dir):
        """"""Test the scan model command""""""
        result = runner.invoke(
            scan_app,
            [""model"", ""o1-mini"", ""--path"", str(temp_model_dir), ""--format"", ""json""]
        )
        
        assert result.exit_code == 0
        output = json.loads(result.stdout)
        assert output['model_name'] == ""o1-mini""
        assert output['total_files'] > 0
",tests/test_scan/test_cli.py,TestScanCLI,1,7
survived,"    def generate_for_model_migration(self,
                                   old_model: str,
                                   new_model: str,
                                   files: List[str]) -> Dict[str, Any]:
        """"""Generate YAML for model migration tasks""""""
        
        tasks = []
        
        for file_path in files:
            prompt = f""Migrate code from {old_model} to {new_model}. Update import statements, "" \
                    f""API calls, method names, and parameters. Ensure compatibility with {new_model} "" \
                    f""while maintaining existing functionality. Add migration comments where significant changes are made.""
            
            tasks.append({
                'file': file_path,
                'prompt': prompt
            })
        
        config = {
            'provider': 'claude',
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'source': 'haconiwa scan generate-parallel-config',
                'migration': f""{old_model} -> {new_model}"",
                'total_tasks': len(tasks)
            },
            'tasks': tasks,
            'options': {
                'max_concurrent': 3,
                'timeout': 180,  # 3 minutes for migration tasks
                'allowed_tools': ['Read', 'Write', 'Edit', 'MultiEdit'],
                'permission_mode': 'confirmEach',
                'output_dir': f'./migration-{old_model}-to-{new_model}'
            }
        }
        
        return config
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator,1,7
survived,"    def __init__(self):
        self.format_handlers = {
            'text': self._format_text,
            'json': self._format_json,
            'yaml': self._format_yaml,
            'summary': self._format_summary,
            'table': self._format_table,
            'tree': self._format_tree
        }
",src/haconiwa/scan/formatter.py,OutputFormatter,1,7
survived,"    def _generate_prompt_for_file(self, 
                                file_path: str, 
                                action: str,
                                custom_prompts: Optional[Dict[str, str]] = None) -> str:
        """"""Generate appropriate prompt based on file type and action""""""
        
        # Check custom prompts first
        if custom_prompts and file_path in custom_prompts:
            return custom_prompts[file_path]
        
        # Determine file category
        file_path_lower = file_path.lower()
        
        if 'model' in file_path_lower:
            category = 'model'
        elif 'api' in file_path_lower or 'route' in file_path_lower:
            category = 'api'
        elif 'util' in file_path_lower or 'helper' in file_path_lower:
            category = 'utils'
        elif 'config' in file_path_lower or 'setting' in file_path_lower:
            category = 'config'
        elif 'service' in file_path_lower:
            category = 'service'
        else:
            # Default based on action
            return self._get_action_prompt(action)
        
        # Get category-specific prompt
        if category in self.default_prompts and action in self.default_prompts[category]:
            return self.default_prompts[category][action]
        
        # Fallback to general action prompt
        return self._get_action_prompt(action)
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator,1,7
survived,"def _transform_for_matrix_function(a):
    """"""Transform array for STATIC matrix functions expecting (..., vars, obs) convention.

    Input convention (from benchmark): (..., obs, vars) - batch dims at front
    Output convention (for static funcs): (..., vars, obs) - swap last two dimensions
    Moving functions use input as-is since they expect (..., obs, vars).
    """"""
    return a.swapaxes(-2, -1)
",numbagg/test/conftest.py,,1,7
survived,"    def test_rolling_zero_variance_windows(self, move_func, expected_diag):
        """"""Test rolling windows with zero variance.""""""
        # Moving functions expect (obs, vars) format
        data = np.array(
            [[1, 2], [1, 2], [1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float64
        )
        result = move_func(data, window=3, min_count=2)

        # First full window has constant values
        if move_func == move_nancorrmatrix:
            # Correlation undefined for zero variance
            assert np.isnan(result[2, 0, 1])
        else:
            # Covariance should be 0
            assert result[2, 0, 0] == 0.0
            assert result[2, 1, 1] == 0.0
            assert result[2, 0, 1] == 0.0

        # Later windows have variance
        assert not np.all(np.isnan(result[5]))
",numbagg/test/test_matrix_functions.py,TestMovingMatrices,1,7
survived,"    def test_correlation_matrix_properties(self):
        """"""Test mathematical properties of correlation matrices.""""""
        np.random.seed(123)
        # Exponential moving functions expect (obs, vars) format
        data = np.random.randn(30, 3)

        result = move_exp_nancorrmatrix(data, alpha=0.4)

        for t in range(result.shape[0]):
            corr_matrix = result[t]
            if not np.any(np.isnan(corr_matrix)):
                # 1. Diagonal should be 1.0
                assert_allclose(np.diag(corr_matrix), 1.0, rtol=1e-12)

                # 2. Matrix should be symmetric
                assert_allclose(corr_matrix, corr_matrix.T, rtol=1e-12)

                # 3. All values should be very close to [-1, 1] (allowing for floating-point precision)
                assert np.all(corr_matrix >= -1.0 - 1e-10)
                assert np.all(corr_matrix <= 1.0 + 1e-10)

                # 4. Should be positive semi-definite
                eigenvals = np.linalg.eigvals(corr_matrix)
                assert np.all(eigenvals >= -1e-10), (
                    f""Correlation matrix not PSD at time {t}""
                )
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices,1,7
survived,"    def test_correlation_covariance_relationship(self):
        """"""Test relationship between correlation and covariance.""""""
        np.random.seed(42)
        data = np.random.randn(4, 50)

        cov_matrix = nancovmatrix(data)
        corr_matrix = nancorrmatrix(data)

        # Correlation = Covariance / (std_i * std_j)
        stds = np.sqrt(np.diag(cov_matrix))
        expected_corr = cov_matrix / np.outer(stds, stds)

        assert_allclose(corr_matrix, expected_corr, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices,1,8
survived,"    async def test_unix_process_termination(self):
        """"""Test Unix-specific process termination for comparison.""""""
        # Skip this test on actual Windows systems since Unix functions don't exist
        import platform

        if platform.system() == ""Windows"":
            pytest.skip(""Unix-specific test not applicable on Windows"")

        # Also skip if os.getpgid doesn't exist (Windows)
        if not hasattr(__import__(""os""), ""getpgid""):
            pytest.skip(""os.getpgid not available on this platform"")

        # Only run this test on actual Unix systems where os.killpg exists
        if not hasattr(__import__(""os""), ""killpg""):
            pytest.skip(""os.killpg not available on this platform"")

        with patch(""platform.system"", return_value=""Linux""), patch(
            ""os.killpg""
        ) as mock_killpg, patch(""os.getpgid"", return_value=1234):

            # Mock process that needs termination
            mock_process = AsyncMock()
            mock_process.pid = 1234
            mock_process.returncode = None  # Process still running
            mock_process.terminate.return_value = None
            mock_process.wait.side_effect = asyncio.TimeoutError()
            mock_process.kill.return_value = None

            # Test that os.killpg is called on Unix
            await _process_manager._terminate_process(mock_process)

            mock_killpg.assert_called_with(1234, signal.SIGKILL)
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility,0,6
survived,"    def __del__(self) -> None:
        """"""Destructor to ensure database connections are closed.

        Automatically called when the ContextManager object is garbage collected.
        This provides a safety net to ensure SQLite connections are closed even
        if explicit cleanup is not performed.
        """"""
        try:
            self.close_all_connections()
        except Exception:  # nosec B110
            # Ignore errors during destructor
            pass
",ocode_python/core/context_manager.py,ContextManager,1,7
survived,"    def test_command_sanitizer_unix_only(self, mock_platform):
        """"""Test that Windows patterns are not applied on Unix.""""""
        sanitizer = CommandSanitizer()

        # Windows-specific patterns should not be in Unix sanitizer
        assert (
            len([p for p in sanitizer.forbidden_patterns if ""format.*[cC]:"" in p]) == 0
        )
        assert len([p for p in sanitizer.forbidden_patterns if ""taskkill"" in p]) == 0
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility,1,7
survived,"    def test_windows_drive_letter_paths(self):
        """"""Test that Windows drive letter paths are allowed.""""""
        with patch(""platform.system"", return_value=""Windows""):
            validator = PathValidator()

            # Valid Windows paths
            valid_paths = [
                ""C:\\"",
                ""C:\\temp"",
                ""C:\\Users\\test\\file.txt"",
                ""D:\\project\\src"",
                ""C:/temp/file.txt"",  # Forward slashes also valid
            ]

            for path in valid_paths:
                is_valid, error, _ = validator.validate_path(path, check_exists=False)
                assert is_valid, f""Path '{path}' should be valid on Windows: {error}""
",tests/unit/test_windows_compatibility.py,TestWindowsPathValidation,1,7
survived,"def notify_sentry_deployment(cfg: Config, release: Release) -> None:
    """"""Notify Sentry about a deployment. Failures are logged but don't stop deployment.""""""
    try:
        print(""Marking as a release in sentry..."")
        token = get_ssm_param(""/compiler-explorer/sentryAuthToken"")
        result = requests.post(
            f""https://sentry.io/api/0/organizations/compiler-explorer/releases/{release.version}/deploys/"",
            data=dict(environment=cfg.env.value),
            headers=dict(Authorization=f""Bearer {token}""),
            timeout=30,
        )
        if not result.ok:
            print(f""Warning: Failed to notify sentry: {result.status_code}"")
            # Don't fail deployment for sentry notification failure
        else:
            print(""...done"")
    except Exception as e:
        print(f""Warning: Failed to notify sentry: {e}"")
",bin/lib/builds_core.py,,1,6
survived,"    def test_run_with_uv_basic(self, mock_run):
        """"""Test basic run_with_uv execution.""""""
        mock_run.return_value = Mock(returncode=0)

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(""server.py"")

        assert exc_info.value.code == 0

        # Check the command that was called
        mock_run.assert_called_once()
        cmd = mock_run.call_args[0][0]

        expected = [""uv"", ""run"", ""--with"", ""fastmcp"", ""fastmcp"", ""run"", ""server.py""]
        assert cmd == expected
",tests/cli/test_run_with_uv.py,TestRunWithUv,1,7
survived,"    def __init__(self, checkpoint_dir: str, *, enabled: bool = True):
        """"""Initialize checkpoint manager.
        
        Args:
            checkpoint_dir: Directory for saving checkpoints
            enabled: Whether checkpointing is enabled
        """"""
        self.checkpoint_dir = checkpoint_dir
        self.enabled = enabled
        
        if self.enabled:
            self.setup_checkpoint_dir()
",kura/v1/kura.py,CheckpointManager,1,7
survived,"    def embeddable_text(self) -> str:
        return f""Summary: {self.summary}\nRequest: {self.request}\nTask: {self.task}\nLanguages: {self.languages}\nAssistant Errors: {self.assistant_errors}""
",kura/types/summarisation.py,GeneratedSummary,1,7
survived,"def visualise_clusters(
    clusters: Optional[List[Cluster]] = None,
    *,
    checkpoint_path: Optional[Union[str, Path]] = None
) -> None:
    """"""Print a hierarchical visualization of clusters to the terminal.
    
    This function loads clusters either from the provided list or from a checkpoint file,
    builds a tree representation, and prints it to the console.
    The visualization shows the hierarchical relationship between clusters
    with indentation and tree structure symbols.
    
    Args:
        clusters: List of clusters to visualize. If None, loads from checkpoint_path
        checkpoint_path: Path to checkpoint file to load clusters from
        
    Raises:
        ValueError: If neither clusters nor checkpoint_path is provided
        FileNotFoundError: If checkpoint file doesn't exist
        
    Example output:
        â• â•â• Compare and improve Flutter and React state management (45 conversations)
        â•‘   â•šâ•â• Improve and compare Flutter and React state management (32 conversations)
        â•‘       â• â•â• Improve React TypeScript application (15 conversations)
        â•‘       â•šâ•â• Compare and select Flutter state management solutions (17 conversations)
        â• â•â• Optimize blog posts for SEO and improved user engagement (28 conversations)
    """"""
    # Load clusters
    if clusters is None:
        if checkpoint_path is None:
            raise ValueError(""Either clusters or checkpoint_path must be provided"")
        clusters = _load_clusters_from_checkpoint(checkpoint_path)
    
    logger.info(f""Visualizing {len(clusters)} clusters"")
    
    # Build tree structure
    node_id_to_cluster = _build_cluster_tree(clusters)

    # Find root nodes and build the tree
    root_nodes = [
        node_id_to_cluster[cluster.id] for cluster in clusters if not cluster.parent_id
    ]

    total_conversations = sum(node.count for node in root_nodes)
    fake_root = ClusterTreeNode(
        id=""root"",
        name=""Clusters"",
        description=""All clusters"",
        count=total_conversations,
        children=[node.id for node in root_nodes],
    )

    tree_output = _build_tree_structure(fake_root, node_id_to_cluster, 0, False)
    print(tree_output)
",kura/v1/visualization.py,,1,6
survived,"    def test_ab_testing_scenario(self):
        """"""Test A/B testing scenario with preprocessing.""""""
        # Create treatment arms
        arms = make_arms([""control"", ""treatment""])
        agent = Agent(arms, EpsilonGreedy(epsilon=0.1), random_seed=42)

        # No preprocessing needed for A/B test
        pipeline = AgentPipeline([], agent)

        # Run A/B test
        n_experiments = 100
        rewards = []

        for _ in range(n_experiments):
            assignment = pipeline.pull()[0]

            # Simulate reward based on assignment
            if assignment == ""treatment"":
                reward = np.random.normal(0.12, 0.1)  # Better performance
            else:
                reward = np.random.normal(0.10, 0.1)  # Baseline

            rewards.append(reward)
            pipeline.update(np.array([reward]))

        # Check that we collected data
        assert len(rewards) == n_experiments
",tests/test_agent_pipeline.py,TestIntegrationScenarios,1,7
survived,"    def decay(self, decay_rate: Optional[float] = None) -> None:
        """"""Decay all arms of the wrapped agent.

        Parameters
        ----------
        decay_rate : Optional[float], default=None
            Decay rate to use for decaying the arms.
        """"""
        self._agent.decay(decay_rate=decay_rate)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,0,6
survived,"    def pull(
        self, *, top_k: Optional[int] = None
    ) -> Union[List[TokenType], List[List[TokenType]]]:
        """"""Choose arm(s) and pull.

        Parameters
        ----------
        top_k : int, optional
            Number of arms to select. If None (default), selects single
            best arm.

        Returns
        -------
        List[TokenType] or List[List[TokenType]]
            If top_k is None: List containing single action token
            If top_k is int: List containing list of action tokens
        """"""
        if top_k is None:
            return self._agent.pull()
        else:
            return self._agent.pull(top_k=top_k)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,7
survived,"    def update(
        self,
        X: Any,
        y: NDArray[np.float64],
        sample_weight: Optional[NDArray[np.float64]] = None,
    ) -> None:
        """"""Update the wrapped agent with context(s) and reward(s).

        Parameters
        ----------
        X : Any
            Input data to transform and use for updating the arm.
            Will be transformed through the pipeline steps to ContextType.
        y : NDArray[np.float64]
            Reward(s) to use for updating the arm.
        sample_weight : Optional[NDArray[np.float64]], default=None
            Sample weights to use for updating the arm.
        """"""
        X_transformed = self.transform(X)
        self._agent.update(X_transformed, y, sample_weight=sample_weight)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,1,7
survived,"    def test_with_arm_and_lipschitz_agent(self):
        """"""Test LearnerPipeline used as learner in LipschitzContextualAgent.""""""
        from bayesianbandits.featurizers import FunctionArmFeaturizer

        # Create a numeric-only arm featurizer to avoid string conversion issues
        def numeric_arm_featurizer(X, action_tokens):
            """"""Add numeric arm features instead of string tokens.""""""
            n_contexts, n_features = X.shape
            n_arms = len(action_tokens)

            # Create 3D array: (n_contexts, n_features + 1, n_arms)
            result = np.zeros((n_contexts, n_features + 1, n_arms))

            for i, token in enumerate(action_tokens):
                result[:, :-1, i] = X  # Original features
                result[:, -1, i] = int(token.split(""_"")[1])  # Numeric arm ID

            return result

        # Create shared learner pipeline that works with numeric data
        # Pre-fit the scaler
        scaler = StandardScaler()
        scaler.fit(
            np.random.randn(50, 11)
        )  # Fit on dummy data (10 context + 1 arm feature)

        shared_learner: LearnerPipeline[NDArray[np.float64]] = LearnerPipeline(
            steps=[(""scale"", scaler)],  # Pre-fitted scaler
            learner=NormalRegressor(alpha=1.0, beta=1.0)
        )

        # Create arms that all share this learner
        arms = [Arm(f""item_{i}"", learner=shared_learner) for i in range(5)]

        # Create agent with numeric arm featurizer
        agent = LipschitzContextualAgent(
            arms=arms,
            policy=ThompsonSampling(),
            arm_featurizer=FunctionArmFeaturizer(numeric_arm_featurizer),
            learner=shared_learner,
        )

        # Generate context (will be enriched by ArmFeaturizer)
        context = np.random.randn(3, 10)  # 3 contexts, 10 features each

        # Pull recommendations
        recommendations = agent.pull(context)
        assert len(recommendations) == 3

        # Update with rewards
        rewards = np.array([1.0, 0.5, 0.8])
        agent.update(context, rewards)
",tests/test_learner_pipeline.py,TestLearnerPipelineIntegration,1,7
survived,"    def __len__(self) -> int:
        """"""Number of steps in the pipeline.""""""
        return len(self.steps)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline,1,8
survived,"    def test_update(self):
        """"""Test update method.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling(), random_seed=42)
        steps = []

        pipeline = NonContextualAgentPipeline(steps, agent)

        y = np.array([1.0, 2.0])

        # Pull to set arm_to_update
        pipeline.pull()

        # Should not raise
        pipeline.update(y)
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline,1,7
survived,"    def __len__(self) -> int:
        """"""Number of steps in the pipeline.""""""
        return len(self.steps)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,1,7
survived,"    def transform(self, X: Any) -> Any:
        """"""Apply all transformers to input data.""""""
        return _transform_data(X, self.steps)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline,1,7
survived,"    def test_no_transformers(self):
        """"""Test pipeline with no transformer steps (only learner).""""""
        mock_learner = MockLearner()
        pipeline = LearnerPipeline(steps=[], learner=mock_learner)

        X = np.array([[1, 2], [3, 4]])
        y = np.array([1, 2])

        # Should pass data through unchanged when no transformers
        pipeline.partial_fit(X, y)

        # Check that data was passed through unchanged
        received_X, received_y, _ = mock_learner.partial_fit_calls[0]
        np.testing.assert_array_equal(received_X, X)
        np.testing.assert_array_equal(received_y, y)

        # Test other methods also pass data through unchanged
        pipeline.sample(X, size=2)
        sample_X, size = mock_learner.sample_calls[0]
        np.testing.assert_array_equal(sample_X, X)
        assert size == 2

        pipeline.predict(X)
        predict_X = mock_learner.predict_calls[0]
        np.testing.assert_array_equal(predict_X, X)

        pipeline.decay(X, decay_rate=0.9)
        decay_X, decay_rate = mock_learner.decay_calls[0]
        np.testing.assert_array_equal(decay_X, X)
        assert decay_rate == 0.9
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers,1,8
survived,"    def partial_fit(self, X, y, sample_weight=None):
        self.partial_fit_calls.append((X, y, sample_weight))
        return self
",tests/test_learner_pipeline.py,MockLearner,1,6
survived,"    def test_steps_property(self):
        """"""Test steps property.""""""
        scaler = StandardScaler()
        learner = MockLearner()
        pipeline = LearnerPipeline(steps=[(""scale"", scaler)], learner=learner)

        steps = pipeline.steps
        assert len(steps) == 1  # Only transformer steps
        assert steps[0] == (""scale"", scaler)
",tests/test_learner_pipeline.py,TestLearnerPipelineProperties,1,7
survived,"    def test_uncovered_functionality(self):
        """"""Test functionality to improve code coverage.""""""
        arms = make_arms(range(3))

        # Test NonContextualAgentPipeline with non-empty steps (edge case)
        agent = Agent(arms, ThompsonSampling(), random_seed=42)
        steps = [(""identity"", FunctionTransformer())]
        pipeline = NonContextualAgentPipeline(steps, agent)

        # Test all delegation methods on NonContextualAgentPipeline
        assert len(pipeline.arms) == 3
        assert pipeline.arm(0) is not None
        pipeline.select_for_update(1)
        assert pipeline.arm_to_update is not None

        # Test add/remove arm
        new_arm = Arm(99, learner=NormalRegressor(alpha=1.0, beta=1.0))
        pipeline.add_arm(new_arm)
        pipeline.remove_arm(99)

        # Test indexing on NonContextualAgentPipeline
        assert pipeline[""identity""] is not None
        assert pipeline[0] == (""identity"", steps[0][1])

        # Test with invalid string key
        with pytest.raises(KeyError):
            _ = pipeline[""nonexistent""]

        # Test with invalid index
        with pytest.raises(IndexError):
            _ = pipeline[10]",tests/test_agent_pipeline.py,TestCoverage,1,6
survived,"    def test_recommendation_system_scenario(self, policy_class):
        """"""Test realistic recommendation system scenario.""""""
        # Create product arms
        product_arms = make_arms([f""product_{i}"" for i in range(5)])

        # Create agent with preprocessing
        agent = ContextualAgent(product_arms, policy_class(), random_seed=42)

        # Pre-fit scaler on historical user data
        scaler = StandardScaler()
        historical_users = np.random.randn(1000, 3)  # user features
        scaler.fit(historical_users)

        # Create pipeline with preprocessing
        steps = [(""user_scaler"", scaler)]
        pipeline = AgentPipeline(steps, agent)

        # Simulate user interactions
        n_users = 20
        user_contexts = np.random.randn(n_users, 3)

        # Pull recommendations
        recommendations = pipeline.pull(user_contexts)
        assert len(recommendations) == n_users
        assert all(rec.startswith(""product_"") for rec in recommendations)

        # Simulate rewards and update
        rewards = np.random.beta(2, 5, size=n_users)  # Realistic reward distribution
        pipeline.update(user_contexts, rewards)

        # Verify models were updated
        for arm in pipeline.arms:
            assert hasattr(arm.learner, ""coef_"")
",tests/test_agent_pipeline.py,TestIntegrationScenarios,1,7
survived,"    def fit(self, X, y=None):
        return self
",tests/test_agent_pipeline.py,MockTransformer,1,7
survived,"def test_export_from_pylock_not_empty(core, pdm):
    """"""Test that exporting from pylock.toml produces non-empty output (fixes issue #3573).""""""
    project = core.create_project(FIXTURES / ""projects/demo"")

    # Export from pylock.toml to requirements format
    with cd(project.root):
        result = pdm([""export"", ""-f"", ""requirements"", ""-L"", ""pylock.toml"", ""--no-hashes""], obj=project, strict=True)
        assert result.exit_code == 0

    # The output should not be empty (this was the original bug)
    output_lines = [
        line.strip() for line in result.stdout.strip().split(""\n"") if line.strip() and not line.strip().startswith(""#"")
    ]
    assert len(output_lines) > 0, ""Export from pylock.toml should not be empty""

    # Should contain expected packages
    output = result.stdout
    assert any(pkg in output for pkg in [""chardet"", ""idna""]), ""Expected at least some packages in output""",tests/test_formats.py,,1,7
survived,"    def _analyze_architecture(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze the overall architecture patterns.""""""
        architecture = {""primary_pattern"": ""mvc"", ""layers"": [], ""components"": []}
        # Implementation would analyze architectural patterns
        return architecture
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,0,7
survived,"def test_context_engineering_methods():
    """"""Test Context Engineering specific methods.""""""
    print(""\nðŸ§ª Testing Context Engineering Methods..."")
    
    try:
        from praisonaiagents import create_context_agent
        
        context_agent = create_context_agent()
        
        # Test that all Context Engineering methods exist
        required_methods = [
            'analyze_codebase_patterns',
            'generate_context_document',
            'create_validation_loop',
            'enhance_prompt_with_context',
            'generate_prp',
            'extract_documentation_patterns',
            'analyze_test_patterns',
            'create_implementation_blueprint'
        ]
        
        for method_name in required_methods:
            assert hasattr(context_agent, method_name), f""Missing method: {method_name}""
            assert callable(getattr(context_agent, method_name)), f""Method {method_name} is not callable""
        
        print(f""âœ… All {len(required_methods)} Context Engineering methods are available"")
        
        return True
        
    except Exception as e:
        print(f""âŒ Method availability test failed: {e}"")
        return False
",test_context_agent.py,,0,7
survived,"    def _generate_failure_actions(self, criterion: str) -> List[str]:
        """"""Generate actions to take if criterion fails.""""""
        return [f""Review implementation for: {criterion}"", ""Fix issues"", ""Re-test""]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def extract_documentation_patterns(self, project_path: str) -> Dict[str, Any]:
        """"""Extract documentation patterns and conventions from the project.""""""
        doc_patterns = {
            ""readme_style"": self._analyze_readme_style(project_path),
            ""code_comments"": self._analyze_comment_patterns(project_path),
            ""docstring_format"": self._analyze_docstring_format(project_path),
            ""api_documentation"": self._analyze_api_docs(project_path)
        }
        return doc_patterns
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,7
survived,"def demonstrate_context_engineering_benefits():
    """"""
    Demonstrate the benefits of Context Engineering methodology.
    """"""
    print(""\nðŸ“Š Context Engineering vs Traditional Approaches"")
    print(""="" * 60)
    
    # Traditional approach simulation
    print(""\nâŒ Traditional AI Coding Approach:"")
    print(""   1. Write basic prompt: 'Implement user authentication'"")
    print(""   2. AI uses general knowledge to implement"")
    print(""   3. Result often doesn't fit existing codebase patterns"")
    print(""   4. Multiple iterations needed to fix integration issues"")
    print(""   5. Success rate: ~30-40% first-try success"")
    
    # Context Engineering approach
    print(""\nâœ… Context Engineering Approach:"")
    print(""   1. Analyze codebase patterns and architecture"")
    print(""   2. Generate comprehensive context document"")
    print(""   3. Create validation framework with success criteria"")
    print(""   4. Enhance prompts with rich contextual information"")
    print(""   5. Generate PRP (Product Requirements Prompt)"")
    print(""   6. AI implements using complete context"")
    print(""   7. Success rate: ~90%+ first-try success"")
    
    print(""\nðŸ“ˆ Context Engineering Advantages:"")
    print(""   â€¢ 10x better than prompt engineering (context vs clever wording)"")
    print(""   â€¢ 100x better than vibe coding (structured vs ad-hoc)"")
    print(""   â€¢ Enables first-try implementation success"")
    print(""   â€¢ Reduces development iteration cycles"")
    print(""   â€¢ Ensures consistency with existing codebase"")
    print(""   â€¢ Provides built-in quality validation"")
",examples/python/concepts/context-engineering-workflow.py,,1,7
survived,"    def _extract_quality_guidance(self, context_data: Dict[str, Any]) -> str:
        """"""Extract quality guidance from context data.""""""
        return ""Maintain code quality standards identified in the codebase.""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    def _analyze_readme_style(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze README style and structure.""""""
        return {""style"": ""standard"", ""sections"": []}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent,1,6
survived,"    async def test_ai_text_generator_block_tracks_stats(self):
        """"""Test that AITextGeneratorBlock correctly tracks stats through delegation.""""""
        import backend.blocks.llm as llm

        block = llm.AITextGeneratorBlock()

        # Mock the underlying structured response block
        async def mock_llm_call(input_data, credentials):
            # Simulate the structured block setting stats
            block.execution_stats = NodeExecutionStats(
                input_token_count=30,
                output_token_count=40,
                llm_call_count=1,
            )
            return ""Generated text""  # AITextGeneratorBlock.llm_call returns a string

        block.llm_call = mock_llm_call  # type: ignore

        # Run the block
        input_data = llm.AITextGeneratorBlock.Input(
            prompt=""Generate text"",
            model=llm.LlmModel.GPT4O,
            credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore
        )

        outputs = {}
        async for output_name, output_data in block.run(
            input_data, credentials=llm.TEST_CREDENTIALS
        ):
            outputs[output_name] = output_data

        # Check stats
        assert block.execution_stats.input_token_count == 30
        assert block.execution_stats.output_token_count == 40
        assert block.execution_stats.llm_call_count == 1

        # Check output - AITextGeneratorBlock returns the response directly, not in a dict
        assert outputs[""response""] == ""Generated text""
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking,1,7
survived,"    async def test_stats_accumulation_with_retries(self):
        """"""Test that stats correctly accumulate across retries.""""""
        import backend.blocks.llm as llm

        block = llm.AIStructuredResponseGeneratorBlock()

        # Counter to track calls
        call_count = 0

        async def mock_llm_call(*args, **kwargs):
            nonlocal call_count
            call_count += 1

            # First call returns invalid format
            if call_count == 1:
                return llm.LLMResponse(
                    raw_response="""",
                    prompt=[],
                    response='{""wrong"": ""format""}',
                    tool_calls=None,
                    prompt_tokens=10,
                    completion_tokens=15,
                    reasoning=None,
                )
            # Second call returns correct format
            else:
                return llm.LLMResponse(
                    raw_response="""",
                    prompt=[],
                    response='{""key1"": ""value1"", ""key2"": ""value2""}',
                    tool_calls=None,
                    prompt_tokens=20,
                    completion_tokens=25,
                    reasoning=None,
                )

        block.llm_call = mock_llm_call  # type: ignore

        # Run the block with retry
        input_data = llm.AIStructuredResponseGeneratorBlock.Input(
            prompt=""Test prompt"",
            expected_format={""key1"": ""desc1"", ""key2"": ""desc2""},
            model=llm.LlmModel.GPT4O,
            credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore
            retry=2,
        )

        outputs = {}
        async for output_name, output_data in block.run(
            input_data, credentials=llm.TEST_CREDENTIALS
        ):
            outputs[output_name] = output_data

        # Check stats - should accumulate both calls
        # For 2 attempts: attempt 1 (failed) + attempt 2 (success) = 2 total
        # but llm_call_count is only set on success, so it shows 1 for the final successful attempt
        assert block.execution_stats.input_token_count == 30  # 10 + 20
        assert block.execution_stats.output_token_count == 40  # 15 + 25
        assert block.execution_stats.llm_call_count == 2  # retry_count + 1 = 1 + 1 = 2
        assert block.execution_stats.llm_retry_count == 1
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking,0,7
survived,"    def _message(self) -> str:
        return ""Empty notebook cell. Remove it or add some content.""",dev/clint/src/clint/rules/empty_notebook_cell.py,EmptyNotebookCell,1,6
survived,"    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        # Only generate ID for concrete classes
        if not inspect.isabstract(cls):
            id_ = next(cls._id_counter)
            cls._generated_id = f""MLF{id_:04d}""
",dev/clint/src/clint/rules/base.py,Rule,1,8
survived,"    def _is_optional(ann: ast.AST) -> bool:
        """"""
        Returns True if `ann` looks like `Optional[...]`.
        """"""
        return (
            isinstance(ann, ast.Subscript)
            and isinstance(ann.value, ast.Name)
            and ann.value.id == ""Optional""
        )
",dev/clint/src/clint/rules/implicit_optional.py,ImplicitOptional,1,6
survived,"    def check(node: ast.AnnAssign) -> bool:
        """"""
        Returns True if the value to assign is `None` but the type annotation is
        not `Optional[...]` or `... | None`. For example: `a: int = None`.
        """"""
        return ImplicitOptional._is_none(node.value) and not (
            ImplicitOptional._is_optional(node.annotation)
            or ImplicitOptional._is_bitor_none(node.annotation)
        )
",dev/clint/src/clint/rules/implicit_optional.py,ImplicitOptional,1,7
survived,"    def _message(self) -> str:
        """"""
        Return a message that explains this rule.
        """"""
",dev/clint/src/clint/rules/base.py,Rule,1,6
survived,"    def __init__(self, rules: set[str]) -> None:
        self.rules = rules
",dev/clint/src/clint/rules/do_not_disable.py,DoNotDisable,1,6
survived,"    def _message(self) -> str:
        return (
            f""Importing module `{self.module}` at the top level is not allowed ""
            ""in this file. Use lazy import instead.""
        )",dev/clint/src/clint/rules/forbidden_top_level_import.py,ForbiddenTopLevelImport,1,7
survived,"    def _message(self) -> str:
        return (
            ""@pytest.mark.repeat decorator should not be committed. ""
            ""This decorator is meant for local testing only to check for flaky tests.""
        )
",dev/clint/src/clint/rules/pytest_mark_repeat.py,PytestMarkRepeat,1,7
survived,"    def test_webhook(
        self, webhook_id: str, event: Optional[WebhookEvent] = None
    ) -> WebhookTestResult:
        """"""
        Test the webhook by sending a test event to the specified URL.

        Args:
            webhook_id: The ID of the webhook to test.
            event: Optional event type to test. If not specified, uses the first event from webhook.

        Returns:
            WebhookTestResult indicating success/failure and response details
        """"""
        req_body = message_to_json(TestWebhook(event=event.to_proto() if event else None))
        response_proto = self._call_webhook_endpoint(TestWebhook, req_body, webhook_id=webhook_id)
        return WebhookTestResult.from_proto(response_proto.result)",mlflow/store/model_registry/rest_store.py,RestStore,1,7
survived,"    def test_OpImpl_with_args(self):
        mod = self.compile(
        """"""
        from operator import OpImpl, OpArg

        def bar(x: i32) -> i32:
            return x * 2

        @blue
        def foo() -> OpImpl:
            # Create an OpImpl with an argument list
            arg = OpArg('blue', i32, 42)
            return OpImpl(bar, [arg])
        """""")
        w_opimpl = mod.foo(unwrap=False)
        assert isinstance(w_opimpl, W_OpImpl)
        assert not w_opimpl.is_simple()
        assert w_opimpl._args_wop is not None
        assert len(w_opimpl._args_wop) == 1

        # Check the OpArg stored in the arguments list
        wop = w_opimpl._args_wop[0]
        assert isinstance(wop, W_OpArg)
        assert wop.color == 'blue'
        assert wop.w_static_type is B.w_i32
        assert wop.is_blue()
        assert wop._w_val is not None
        assert self.vm.unwrap_i32(wop._w_val) == 42
",spy/tests/compiler/test_opimpl.py,TestOpImpl,1,7
survived,"    def test__NEW__(self):
        # ========== EXT module for this test ==========
        EXT = ModuleRegistry('ext')

        @EXT.builtin_type('Point')
        class W_Point(W_Object):
            w_x: Annotated[W_I32, Member('x')]
            w_y: Annotated[W_I32, Member('y')]

            def __init__(self, w_x: W_I32, w_y: W_I32) -> None:
                self.w_x = w_x
                self.w_y = w_y

            @builtin_method('__NEW__', color='blue')
            @staticmethod
            def w_NEW(vm: 'SPyVM', wop_cls: W_OpArg,
                     *args_wop: W_OpArg) -> W_OpImpl:
                # Support overloading based on argument count
                if len(args_wop) == 1:
                    # Point(x) -> Point(x, x)
                    @builtin_func('ext', 'new_point_single')
                    def w_new(vm: 'SPyVM', w_cls: W_Type, w_x: W_I32) -> W_Point:
                        return W_Point(w_x, w_x)
                    return W_OpImpl(w_new)
                else:
                    # Normal Point(x, y)
                    @builtin_func('ext', 'new_point')
                    def w_new(vm: 'SPyVM', w_cls: W_Type,
                              w_x: W_I32, w_y: W_I32) -> W_Point:
                        return W_Point(w_x, w_y)
                    return W_OpImpl(w_new)
        # ========== /EXT module for this test =========
        self.vm.make_module(EXT)
        mod = self.compile(""""""
        from ext import Point

        @blue
        def test_two_args(x: i32, y: i32) -> i32:
            p = Point(x, y)
            return p.x * 10 + p.y

        @blue
        def test_one_arg(x: i32) -> i32:
            p = Point(x)
            return p.x * 10 + p.y
        """""")

        # Test with two args
        res = mod.test_two_args(3, 6)
        assert res == 36

        # Test with one arg (x=7)
        # Should create Point(7, 7)
        res = mod.test_one_arg(7)
        assert res == 77  # 7*10 + 7 = 77
",spy/tests/compiler/test_operator_call.py,TestCallOp,1,7
survived,"    def w_spy_new(vm: 'SPyVM', w_cls: W_Type,
                 w_color: W_Object, w_static_type: W_Type,
                 w_val: W_Object) -> 'W_OpArg':
        """"""
        Create a new OpArg from SPy code:
        - color: 'red' or 'blue'
        - static_type: the static type of the argument
        - val: the value (optional for red OpArg, required for blue)
        """"""
        from spy.vm.str import W_Str
        # Check that w_color is a string
        w_type = vm.dynamic_type(w_color)
        if w_type is not B.w_str:
            raise SPyTypeError(f""OpArg color must be a string, got {w_type.fqn.human_name}"")

        color: Color = vm.unwrap_str(w_color)  # type: ignore
        if color not in ('red', 'blue'):
            raise SPyTypeError(f""OpArg color must be 'red' or 'blue', got '{color}'"")

        # Convert B.w_None to Python None
        if w_val is B.w_None:
            w_val2 = None
        else:
            w_val2 = w_val

        if color == 'blue' and w_val is None:
            raise SPyTypeError(""Blue OpArg requires a value"")

        loc = Loc.here(-2)  # approximate source location
        return W_OpArg(vm, color, w_static_type, w_val2, loc)
",spy/vm/opimpl.py,W_OpArg,0,7
survived,"        def w_get_color(vm: 'SPyVM', w_oparg: W_OpArg) -> W_Str:
            return vm.wrap(w_oparg.color)  # type: ignore
",spy/vm/opimpl.py,W_OpArg,1,7
survived,"    def definitions(
        self,
        curies: Iterable[CURIE],
        include_metadata=False,
        include_missing=False,
        lang: Optional[LANGUAGE_TAG] = None,
    ) -> Iterator[Tuple[CURIE, Optional[str], Dict]]:
        """"""
        Fetch definitions for multiple CURIEs from OLS.
        
        :param curies: The CURIEs to fetch definitions for
        :param include_metadata: Whether to include metadata (currently not supported)
        :param include_missing: Whether to include CURIEs with no definition
        :param lang: Optional language tag (not currently supported by this implementation)
        :return: Iterator of (CURIE, definition, metadata) tuples
        """"""
        for curie in curies:
            definition = self.definition(curie, lang)
            if definition is None and not include_missing:
                continue
            # Currently OLS doesn't provide metadata for definitions through the API
            # So we're just returning an empty dict
            yield curie, definition, {}
",src/oaklib/implementations/ols/ols_implementation.py,BaseOlsImplementation,1,6
survived,"    def test_all_nan_variable(self):
        # Test with a variable that is all NaN
        data = np.array(
            [[1, 2, 3, 4], [np.nan, np.nan, np.nan, np.nan], [2, 3, 4, 5]],
            dtype=np.float64,
        )
        result = nancorrmatrix(data)

        # Second row and column should be NaN
        assert np.all(np.isnan(result[1, :]))
        assert np.all(np.isnan(result[:, 1]))

        # Other correlations should still work
        assert result[0, 0] == 1.0
        assert result[2, 2] == 1.0
        assert not np.isnan(result[0, 2])
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix,1,7
survived,"    def test_zero_mean_variables(self):
        # Test with zero-mean variables
        data = np.array([[-1, 0, 1], [-2, 0, 2]], dtype=np.float64)
        result = nancovmatrix(data)

        expected = np.cov(data)
        assert_allclose(result, expected, rtol=1e-10)
",numbagg/test/test_nancovmatrix.py,TestNanCovMatrix,1,7
survived,"                def make(*args, **kwargs):
                    from ._core import FactorGraph

                    if ""factors"" in kwargs:
                        kwargs[""costs""] = kwargs.pop(""factors"")

                    warnings.warn(
                        ""`jaxls.FactorGraph` has been renamed `jaxls.FactorGraph`"",
                        DeprecationWarning,
                        stacklevel=2,
                    )

                    return FactorGraph(*args, **kwargs)
",src/jaxls/__init__.py,_FactorGraphDescriptor.FactorGraph,0,7
deleted,"    async def _list_tools(self, apply_middleware: bool = True) -> list[Tool]:
        """"""
        List all available tools.
        """"""

        if (tools := self._cache.get(""tools"")) is self._cache.NOT_FOUND:
            tools: list[Tool] = []

            # iterate such that new mounts overwrite older ones
            for mounted_server in self._mounted_servers:
                try:
                    if apply_middleware:
                        server_tools = (
                            await mounted_server.server._middleware_list_tools()
                        )
                    else:
                        server_tools = await mounted_server.server._list_tools()
                    # Apply prefix to each tool key if prefix exists and is not empty
                    if mounted_server.prefix:
                        for tool in server_tools:
                            tool = tool.with_key(f""{mounted_server.prefix}_{tool.key}"")
                            tools.append(tool)
                    else:
                        tools.extend(server_tools)
                except Exception as e:
                    logger.warning(
                        f""Failed to get tools from mounted server '{mounted_server.prefix}': {e}""
                    )
                    continue
            tools.extend(self._tool_manager.get_tools().values())
            self._cache.set(""tools"", tools)
        return tools
",src/fastmcp/server/server.py,FastMCP,1,7
survived,"    async def test_list_resource_templates_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.list_resource_templates()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(
            method=""resources/templates/list"", times=3
        )
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(
            hook=""on_list_resource_templates"", times=1
        )

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(
            method=""resources/templates/list"", times=3
        )
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(
            hook=""on_list_resource_templates"", times=1
        )",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks,1,6
deleted,"    async def _middleware_read_resource(
        self,
        uri: AnyUrl | str,
    ) -> list[ReadResourceContents]:
        """"""
        Read a resource with middleware.
        """"""

        async def _handler(
            context: MiddlewareContext[mcp.types.ReadResourceRequestParams],
        ) -> list[ReadResourceContents]:
            return await self._read_resource(
                uri=context.message.uri,
            )

        # Convert string URI to AnyUrl if needed
        if isinstance(uri, str):
            from pydantic import AnyUrl

            uri_param = AnyUrl(uri)
        else:
            uri_param = uri

        mw_context = MiddlewareContext(
            message=mcp.types.ReadResourceRequestParams(uri=uri_param),
            source=""client"",
            type=""request"",
            method=""resources/read"",
            fastmcp_context=fastmcp.server.dependencies.get_context(),
        )
        return await self._apply_middleware(mw_context, _handler)
",src/fastmcp/server/server.py,FastMCP,1,7
survived,"    def nested_middleware():
        return RecordingMiddleware(name=""nested_middleware"")
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks,1,6
survived,"    async def test_list_tools(
        self, mcp_server: FastMCP, recording_middleware: RecordingMiddleware
    ):
        async with Client(mcp_server) as client:
            await client.list_tools()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""tools/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_tools"", times=1)
",tests/server/middleware/test_middleware.py,TestMiddlewareHooks,1,6
survived,"    async def test_get_prompt(
        self, mcp_server: FastMCP, recording_middleware: RecordingMiddleware
    ):
        async with Client(mcp_server) as client:
            await client.get_prompt(""test_prompt"", {""x"": ""test""})

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""prompts/get"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_get_prompt"", times=1)
",tests/server/middleware/test_middleware.py,TestMiddlewareHooks,1,7
survived,"    async def test_read_resource_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.read_resource(""resource://nested/test"")

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/read"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_read_resource"", times=1)

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(method=""resources/read"", times=3)
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(hook=""on_read_resource"", times=1)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks,1,7
survived,"        async def sample_tool(context: Context) -> None:
            await context.sample(""hello"")
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks,1,6
deleted,"    def _collect_all_related_parameters(
        self, 
        interdeps: ""InterDependencies_"", 
        initial_params: set[ParamSpecBase], 
        result_dict: Mapping[ParamSpecBase, numpy.ndarray]
    ) -> set[ParamSpecBase]:
        """"""
        Transitively collect all parameters that are related to the initial set of parameters.
        This includes parameters that any parameter in the set is inferred from, and parameters
        that depend on or are inferred from those parameters, etc.
        
        Only includes parameters that are present in result_dict.
        """"""
        collected = set(initial_params)
        to_process = set(initial_params)
        
        while to_process:
            current = to_process.pop()
            
            # Add parameters that current parameter is inferred from
            inferred_from = set(interdeps.inferences.get(current, ()))
            new_inferred = inferred_from - collected
            # Only add if they're in result_dict
            new_inferred = new_inferred.intersection(result_dict.keys())
            collected.update(new_inferred)
            to_process.update(new_inferred)
            
            # Add parameters that depend on current parameter
            dependents = set(interdeps._dependencies_inv.get(current, ()))
            new_dependents = dependents - collected
            # Only add if they're in result_dict
            new_dependents = new_dependents.intersection(result_dict.keys())
            collected.update(new_dependents)
            to_process.update(new_dependents)
            
            # Add parameters that are inferred from current parameter
            infers = set(interdeps._inferences_inv.get(current, ()))
            new_infers = infers - collected
            # Only add if they're in result_dict
            new_infers = new_infers.intersection(result_dict.keys())
            collected.update(new_infers)
            to_process.update(new_infers)
        
        return collected
",src/qcodes/dataset/data_set.py,DataSet,1,7
survived,"def test_load_from_netcdf_non_completed_dataset(
    meas_with_registered_param, DMM, DAC, tmp_path
) -> None:
    """"""Test that non-completed datasets can be loaded from netcdf files.""""""
    with meas_with_registered_param.run(
        dataset_class=DataSetType.DataSetInMem
    ) as datasaver:
        for set_v in np.linspace(0, 25, 5):
            DAC.ch1.set(set_v)
            get_v = DMM.v1()
            datasaver.add_result((DAC.ch1, set_v), (DMM.v1, get_v))
    
    ds = datasaver.dataset
    # Note: do NOT call ds.mark_completed() to keep it non-completed
    
    # Verify that the dataset is not completed
    assert ds.completed_timestamp_raw is None
    assert not ds.completed
    
    # Export the non-completed dataset to NetCDF
    ds.export(export_type=""netcdf"", path=str(tmp_path))
    
    # Load the dataset from NetCDF 
    loaded_ds = DataSetInMem._load_from_netcdf(
        tmp_path / f""qcodes_{ds.captured_run_id}_{ds.guid}.nc""
    )
    
    # Verify that the loaded dataset is still non-completed
    assert isinstance(loaded_ds, DataSetInMem)
    assert loaded_ds.completed_timestamp_raw is None
    assert not loaded_ds.completed
    
    # Compare other properties
    assert loaded_ds.captured_run_id == ds.captured_run_id
    assert loaded_ds.guid == ds.guid
    assert loaded_ds.name == ds.name
    assert loaded_ds.run_timestamp_raw == ds.run_timestamp_raw
",tests/dataset/test_dataset_in_memory.py,,1,7
survived,"def test_export_datasets_and_create_metadata_db_basic(simple_dataset):
    """"""Test basic functionality of export_datasets_and_create_metadata_db""""""
    source_db_path, run_id = simple_dataset
    
    with tempfile.TemporaryDirectory() as temp_dir:
        target_db_path = Path(temp_dir) / ""target.db""
        export_path = Path(temp_dir) / ""exports""
        
        # Run the export function
        result = export_datasets_and_create_metadata_db(
            source_db_path=source_db_path,
            target_db_path=target_db_path,
            export_path=export_path,
        )
        
        # Check that the function returned a result
        assert isinstance(result, dict)
        assert run_id in result
        assert result[run_id] in [""exported"", ""copied_as_is""]
        
        # Check that target database was created
        assert target_db_path.exists()
        
        # Check that target database has the run
        target_conn = connect(target_db_path)
        target_runs = get_runs(target_conn)
        assert len(target_runs) == 1
        target_conn.close()
        
        # Check that NetCDF file was created if export was successful
        if result[run_id] == ""exported"":
            netcdf_files = list(export_path.glob(""*.nc""))
            assert len(netcdf_files) > 0
",tests/dataset/test_export_datasets_and_create_metadata_db.py,,1,7
survived,"    async def run(self, text: str) -> str:
        """"""Run all checks sequentially and return possibly modified text.""""""

        for check in self.checks:
            text = await check(text)
        return text
",src/meta_agent/policy.py,PolicyChecker,1,7
survived,"async def test_policy_checker_redact():
    config = GuardrailConfig(
        rules=[
            GuardrailRule(
                name=""secret"", pattern=""secret"", action=GuardrailAction.REDACT
            )
        ]
    )
    checker = PolicyChecker(config)

    result = await checker.run(""my secret is here"")
    assert result == ""my [REDACTED] is here""
",tests/test_policy_checker.py,,1,7
survived,"    def __init__(self, config: Optional[GuardrailConfig] = None) -> None:
        self.checks: List[Callable[[str], Awaitable[str]]] = []
        if config is not None:
            self.add_from_config(config)
",src/meta_agent/policy.py,PolicyChecker,1,7
survived,"def test_build_default_regex_config():
    config = build_default_regex_config()
    rule_names = {rule.name for rule in config.rules}
    assert set(DEFAULT_REGEX_PATTERNS.keys()) == rule_names
    # ensure patterns are preserved
    for rule in config.rules:
        assert isinstance(rule, GuardrailRule)
        assert DEFAULT_REGEX_PATTERNS[rule.name] == rule.pattern",tests/test_regex_patterns.py,,1,7
survived,"def test_root_serves_spa() -> None:
    client = make_client()
    r = client.get(""/"")
    assert r.status_code == 200
    assert ""<div id=\""root\"">"" in r.text",tests/test_api_server.py,,1,7
survived,"    def boom(*_: object) -> None:
        raise NotImplementedError
",test/windows/test_shutdown.py,,0,7
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.published.append((topic, env))
        super().publish(topic, env)
",tests/test_safety_agent.py,CaptureBus,1,7
survived,"    def read_and_refresh_token(self) -> Dict[str, Any]:
        """"""Read token from file and refresh if needed""""""
        path = self.get_auth_file()
        
        if path.is_file():
            with open(path, 'r') as f:
                auth_data = json.load(f)
            
            diff = time.time() - os.path.getmtime(path)
            expires_in = int(auth_data.get(""expiresIn""))
            
            if diff < expires_in:
                if diff > expires_in / 2:
                    auth_data[""idToken""], auth_data[""refreshToken""] = self.refresh_token(
                        auth_data.get(""refreshToken"")
                    )
                    with open(path, 'w') as f:
                        json.dump(auth_data, f)
                return auth_data
        
        # Create new token if file doesn't exist or token expired
        return self.create_token(path)
",webscout/Provider/TTI/aiarta.py,AIArtaImager,1,7
survived,"    def save(
        self,
        response: List[bytes],
        name: Optional[str] = None,
        dir: Optional[Union[str, Path]] = None,
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire generated images! ðŸ’¾

        Examples:
            >>> provider = AiForceimager()
            >>> images = provider.generate(""Cool art"")
            >>> # Save with default settings
            >>> paths = provider.save(images)
            >>> # Save with custom name and directory
            >>> paths = provider.save(
            ...     images,
            ...     name=""my_art"",
            ...     dir=""my_images"",
            ...     filenames_prefix=""test_""
            ... )

        Args:
            response (List[bytes]): Your generated images
            name (Optional[str]): Custom name for your images
            dir (Optional[Union[str, Path]]): Where to save the images (default: current directory)
            filenames_prefix (str): Prefix for your image files

        Returns:
            List[str]: Paths to your saved images
        """"""
        save_dir = dir if dir else os.getcwd()
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        name = self.prompt if name is None else name
        filenames = []
        count = 0

        for image in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(save_dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            with open(absolute_path_to_file, ""wb"") as fh:
                fh.write(image)

        return filenames
",webscout/Provider/TTI/aiforce.py,AiForceimager,1,7
survived,"    def _create_payload(self, prompt: str) -> Dict[str, Any]:
        """"""Create the API request payload ðŸ“¦

        Args:
            prompt (str): The image generation prompt

        Returns:
            Dict[str, Any]: API request payload
        """"""
        return {
            ""type"": ""image"",
            ""messagesHistory"": [
                {
                    ""id"": str(uuid.uuid4()),
                    ""from"": ""you"",
                    ""content"": prompt
                }
            ],
            ""settings"": {
                ""model"": ""gpt-4o-mini""  # Or another suitable model if available
            }
        }
",webscout/Provider/TTI/talkai.py,TalkaiImager,1,7
survived,"    def save(
        self,
        response: List[bytes],
        name: Optional[str] = None,
        dir: Optional[Union[str, Path]] = None,
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire generated images! ðŸ’¾

        Examples:
            >>> provider = PiclumenImager()
            >>> images = provider.generate(""Cool art"")
            >>> # Save with default settings
            >>> paths = provider.save(images)
            >>> # Save with custom name and directory
            >>> paths = provider.save(
            ...     images,
            ...     name=""my_art"",
            ...     dir=""my_images"",
            ...     filenames_prefix=""test_""
            ... )

        Args:
            response (List[bytes]): Your generated images
            name (Optional[str]): Custom name for your images
            dir (Optional[Union[str, Path]]): Where to save the images (default: current directory)
            filenames_prefix (str): Prefix for your image files

        Returns:
            List[str]: Paths to your saved images
        """"""
        save_dir = dir if dir else os.getcwd()
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        saved_paths = []
        timestamp = int(time.time())
        
        # Clean up name for filename use
        safe_name = """"
        if name:
            safe_name = """".join(c if c.isalnum() or c in ""_-"" else ""_"" for c in name)
            
        # Use prompt-based name if no name is provided
        if not safe_name and self.prompt:
            # Clean and truncate prompt for filename
            prompt_words = self.prompt.split()[:5]  # First 5 words
            safe_name = ""_"".join("""".join(c if c.isalnum() else ""_"" for c in word) for word in prompt_words).lower()
        
        for i, image_bytes in enumerate(response):
            if safe_name:
                filename = f""{filenames_prefix}{safe_name}_{i}.{self.image_extension}""
            else:
                filename = f""{filenames_prefix}piclumen_{timestamp}_{i}.{self.image_extension}""
            
            filepath = os.path.join(save_dir, filename)
            
            with open(filepath, ""wb"") as f:
                f.write(image_bytes)
            
            saved_paths.append(filepath)

        return saved_paths
",webscout/Provider/TTI/piclumen.py,PiclumenImager,1,7
survived,"            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager,1,7
survived,"    def __init__(
        self, 
        timeout: int = 60, 
        proxies: Optional[dict] = None
    ):
        """"""Initialize your ImgSys provider with custom settings

        Examples:
            >>> provider = ImgSys(timeout=30)
            >>> provider = ImgSys(proxies={""http"": ""http://proxy:8080""})

        Args:
            timeout (int): HTTP request timeout in seconds (default: 60)
            proxies (dict, optional): Proxy configuration for requests
        """"""
        self.request_id_endpoint = ""https://imgsys.org/api/initiate""
        self.image_response_endpoint = ""https://imgsys.org/api/get""
        self.image_provider_endpoint = ""https://imgsys.org/api/submit""
        
        self.headers = {
            ""Accept"": ""application/json"",
            ""Content-Type"": ""application/json"",
            ""User-Agent"": agent.random(),
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        if proxies:
            self.session.proxies.update(proxies)
            
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""jpeg""
",webscout/Provider/TTI/imgsys.py,ImgSys,1,8
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}):
        """"""Initialize your MagicStudio provider with custom settings! âš™ï¸""""""
        self.api_endpoint = ""https://ai-api.magicstudio.com/api/ai-art-generator""
        self.headers = {
            ""Accept"": ""application/json, text/plain, */*"",
            ""User-Agent"": agent.random(),
            ""Origin"": ""https://magicstudio.com"",
            ""Referer"": ""https://magicstudio.com/ai-art-generator/"",
            ""DNT"": ""1"",
            ""Sec-GPC"": ""1""
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""jpg""
",webscout/Provider/TTI/magicstudio.py,MagicStudioImager,1,8
survived,"    def generate(
        self, prompt: str, amount: int = 1,
        max_retries: int = 3, retry_delay: int = 5
    ) -> List[str]:
        """"""Generate some fire images from your prompt! ðŸŽ¨

        Args:
            prompt (str): Your creative prompt
            amount (int): How many images to generate
            max_retries (int): Max retry attempts if generation fails
            retry_delay (int): Seconds to wait between retries

        Returns:
            List[str]: List of image URLs
        """"""
        assert bool(prompt), ""Prompt cannot be empty.""
        assert isinstance(amount, int) and amount > 0, ""Amount must be a positive integer.""

        self.prompt = prompt
        image_urls = []

        if self.logging:
            logger.info(f""Generating {amount} images... ðŸŽ¨"")

        for _ in range(amount):
            for attempt in range(max_retries):
                try:
                    with self.session.post(
                        self.api_endpoint,
                        json=self._create_payload(prompt),
                        timeout=self.timeout
                    ) as response:
                        response.raise_for_status()
                        data = response.json()

                        if 'data' in data and len(data['data']) > 0 and 'url' in data['data'][0]:
                            image_urls.append(data['data'][0]['url'])
                            if self.logging:
                                logger.success(f""Generated image {len(image_urls)}/{amount}! ðŸŽ¨"")
                            break
                        else:
                            raise exceptions.InvalidResponseError(""No image URL found in API response."")

                except requests.exceptions.RequestException as e:
                    if attempt == max_retries - 1:
                        if self.logging:
                            logger.error(f""Error making API request: {e} ðŸ˜¢"")
                        raise exceptions.APIConnectionError(f""Error making API request: {e}"") from e
                    else:
                        if self.logging:
                            logger.warning(f""Attempt {attempt + 1} failed. Retrying in {retry_delay} seconds... ðŸ”„"")
                        import time
                        time.sleep(retry_delay)
                except json.JSONDecodeError as e:
                    if self.logging:
                        logger.error(f""Invalid JSON response: {e} ðŸ˜¢"")
                    raise exceptions.InvalidResponseError(f""Invalid JSON response: {e}"") from e
                except Exception as e:
                    if self.logging:
                        logger.error(f""An unexpected error occurred: {e} ðŸ˜¢"")
                    raise exceptions.FailedToGenerateResponseError(f""An unexpected error occurred: {e}"") from e

        if self.logging:
            logger.success(""All images generated successfully! ðŸŽ‰"")
        return image_urls
",webscout/Provider/TTI/talkai.py,TalkaiImager,1,7
survived,"    def test_array_grad(self):
        klong = KlongInterpreter()
        klong('x::Ë™!5')
        klong('loss::{+/x*x}')
        r = klong('x âˆ‡ loss')
        self.assertTrue(np.allclose(r, np.array([0,2,4,6,8]), atol=1e-3))
",tests/test_autograd.py,TestAutograd,1,7
survived,"    def test_scalar_grad_torch(self):
        self._check_scalar_square_grad(""torch"")
",tests/test_autograd.py,TestAutograd,1,6
survived,"        def func(v):
            klong[a] = v
            try:
                return klong.call(KGCall(b, [v], 1)) if isinstance(b, (KGSym, KGLambda, KGFn, KGCall)) else b(v)
            finally:
                klong[a] = orig
",klongpy/dyads.py,,1,6
survived,"async def test_send_http_error():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        with pytest.raises(ValueError):
            await client.send(""trace"", {""d"": 1})
        await client.close()
",tests/unit/test_telemetry_client.py,,1,7
survived,"    def __post_init__(self) -> None:
        if self.auth_token:
            self.headers[""Authorization""] = f""Bearer {self.auth_token}""
        self.headers.setdefault(""Content-Type"", ""application/json"")
",src/meta_agent/services/telemetry_client.py,EndpointConfig,1,7
survived,"        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()
",tests/unit/test_telemetry_client.py,FakeRunner,1,6
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        close_fn = getattr(self._session, ""close"", None)
        if close_fn is None:
            return
        if asyncio.iscoroutinefunction(close_fn):
            await close_fn()
        else:
            close_fn()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"async def test_attach_runner(monkeypatch):
    # Fake runner class
    class FakeRunner:
        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()

    client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
    send_mock = AsyncMock(return_value={""ok"": True})
    monkeypatch.setattr(client, ""send"", send_mock)

    client.attach_runner(FakeRunner, ""trace"")
    res = await FakeRunner().run(None)
    assert hasattr(res, ""span_graph"")
    send_mock.assert_awaited_once_with(""trace"", {""span"": 1})
    await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"async def test_attach_runner(monkeypatch):
    # Fake runner class
    class FakeRunner:
        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()

    client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
    send_mock = AsyncMock(return_value={""ok"": True})
    monkeypatch.setattr(client, ""send"", send_mock)

    client.attach_runner(FakeRunner, ""trace"")
    res = await FakeRunner().run(None)
    assert hasattr(res, ""span_graph"")
    send_mock.assert_awaited_once_with(""trace"", {""span"": 1})
    await client.close()",tests/unit/test_telemetry_client.py,,1,7
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        await self._session.close()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"        def __init__(self, *_, **__):
            pass
",src/meta_agent/services/telemetry_client.py,ClientSession,1,6
survived,"        async def json(self) -> dict:
            raise NotImplementedError(""aiohttp is required for network access"")
",src/meta_agent/services/telemetry_client.py,_DummyResponse,1,7
survived,"    def __init__(self, *_, **__):
        pass
",src/aiohttp/__init__.py,TCPConnector,1,6
survived,"    def info(self, message: str, *, level: int = 1) -> None:
        """"""Output an informational message.""""""
        self._echo(message, fg=""cyan"", level=level)
",src/meta_agent/ux/cli_output.py,CLIOutput,1,7
survived,"    def test_portfolio(self):
        p = finance_agent._Portfolio()
        p.update(""BTC"", 1.0)
        self.assertEqual(p.qty(""BTC""), 1.0)
        self.assertEqual(p.book(), {""BTC"": 1.0})
        self.assertEqual(p.value({""BTC"": 100.0}), 100.0)
        p.update(""BTC"", -1.0)
        self.assertEqual(p.qty(""BTC""), 0.0)
        self.assertEqual(p.book(), {})
",tests/test_finance_utils.py,TestFinanceUtils,1,7
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestVersionOverride.AgentV2,1,6
survived,"    def test_demo_shell_scripts(self) -> None:
        """"""All demo shell scripts should be executable and have shebangs.""""""
        base = Path(validate_demos.DEFAULT_DIR)
        for script in base.rglob(""*.sh""):
            with self.subTest(script=script.name):
                content = script.read_text(errors=""ignore"")
                self.assertTrue(content.startswith(""#!/""), f""{script} missing shebang"")
                self.assertTrue(script.stat().st_mode & 0o111, f""{script} not executable"")",tests/test_demos.py,TestDemos,1,7
survived,"    def test_cli_runs(self) -> None:
        result = subprocess.run(
            [sys.executable, '-m', 'alpha_factory_v1.demos.aiga_meta_evolution.meta_evolver', '--gens', '1'],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0)
        self.assertIn('Champion', result.stdout)
",tests/test_aiga_meta_cli.py,TestAigaMetaCLI,1,7
survived,"def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    """"""Return parsed CLI arguments.""""""

    parser = argparse.ArgumentParser(description=""Run the Î±â€‘AGI Business demo"")
    parser.add_argument(
        ""--loglevel"",
        default=os.getenv(""LOGLEVEL"", ""INFO""),
        help=""Logging verbosity (default: INFO)"",
    )
    return parser.parse_args(argv)
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,,1,7
survived,"    def __init__(self) -> None:
        self.committed = False
",tests/test_alpha_agi_business_3_v1.py,DummyModel,1,7
survived,"    def test_stream_macro_events_offline(self):
        async def get_one():
            it = data_feeds.stream_macro_events(live=False)
            return await anext(it)

        evt = asyncio.run(get_one())
        self.assertIn(""fed_speech"", evt)
        self.assertIn(""yield_10y"", evt)
        self.assertIn(""yield_3m"", evt)
        self.assertIn(""stable_flow"", evt)
        self.assertIn(""es_settle"", evt)
",tests/test_macro_sentinel.py,TestMacroSentinel,1,7
survived,"    def test_play_episode(self):
        frames, reward = play_episode(self.mu, render=False, max_steps=10)
        self.assertIsInstance(frames, list)
        self.assertIsInstance(reward, float)
        self.assertLessEqual(len(frames), 10)
",tests/test_muzero_planning.py,TestMiniMu,1,7
survived,"def _sqlalchemy_type_to_python(sa_type: TypeEngine) -> type:
    """"""
    Convert SQLAlchemy type to Python type.

    Args:
        sa_type: SQLAlchemy TypeEngine instance

    Returns:
        Corresponding Python type
    """"""
    # Import here to avoid circular dependencies
    from datetime import date, datetime, time

    from sqlalchemy import (
        JSON,
        Boolean,
        Date,
        DateTime,
        Float,
        Integer,
        LargeBinary,
        String,
        Text,
        Time,
    )

    type_map = {
        Integer: int,
        String: str,
        Text: str,
        Boolean: bool,
        Float: float,
        DateTime: datetime,
        Date: date,
        Time: time,
        JSON: dict,
        LargeBinary: bytes,
    }

    # Check for exact type matches first
    for sa_class, py_type in type_map.items():
        if type(sa_type) is sa_class:
            return py_type

    # Check for inheritance
    for sa_class, py_type in type_map.items():
        if isinstance(sa_type, sa_class):
            return py_type

    # Default to Any for unknown types
    return Any",src/enrichmcp/sqlalchemy/mixin.py,,1,7
survived,"async def get_order_item_product(
    order_item_id: int, ctx: EnrichContext
) -> Optional[""ProductEnrichModel""]:
    """"""Get the product for a specific order item.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        item = await session.get(OrderItem, order_item_id)
        if not item:
            return None

        # Load the product
        await session.refresh(item, [""product""])
        product = item.product

        return ProductEnrichModel(
            id=product.id,
            name=product.name,
            description=product.description,
            price=product.price,
            stock_quantity=product.stock_quantity,
            category=product.category,
            created_at=product.created_at,
        )
",examples/sqlalchemy_shop/app.py,,1,7
survived,"async def list_orders(
    ctx: EnrichContext, status: str | None = None, cursor: str | None = None, limit: int = 10
) -> CursorResult[OrderEnrichModel]:
    """"""List orders with cursor-based pagination.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        # Build query
        query = select(Order)

        # Apply status filter
        if status:
            query = query.where(Order.status == status)

        # Apply cursor (assuming cursor is the last order ID seen)
        if cursor:
            query = query.where(Order.id > int(cursor))

        # Order by ID for consistent cursor pagination
        query = query.order_by(Order.id).limit(limit + 1)

        result = await session.execute(query)
        orders = result.scalars().all()

        # Check if there are more results
        has_next = len(orders) > limit
        if has_next:
            orders = orders[:-1]  # Remove the extra item

        items = [
            OrderEnrichModel(
                id=order.id,
                order_number=order.order_number,
                user_id=order.user_id,
                status=order.status,
                total_amount=order.total_amount,
                created_at=order.created_at,
                updated_at=order.updated_at,
                shipping_address=order.shipping_address,
                notes=order.notes,
            )
            for order in orders
        ]

        next_cursor = str(orders[-1].id) if orders else None

        return CursorResult(
            items=items, next_cursor=next_cursor, page_size=limit, has_next=has_next
        )
",examples/sqlalchemy_shop/app.py,,1,7
survived,"    def _clip(self,v): return max(0, min(self.size-1, v))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,MiniWorld,1,7
survived,"    def forward(self, h): return self.v(h), torch.log_softmax(self.p(h), -1)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Pred,1,7
survived,"    def _parse(self, ep: str):
        if "":"" not in ep:
            raise ValueError(""Endpoint must be <backend>:<model>"")
        return ep.split("":"",1)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,LMClient,1,7
survived,"    def _risk_assess(self, prompt:str, response:str) -> float:
        # toy heuristic: long responses & code carry more risk
        return min(1.0, 0.1 + 0.9*(len(response)/4000))
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent,1,6
survived,"    def _estimate_cost(self, prompt_tokens:int, completion_tokens:int) -> float:
        price = float(os.getenv(""ALPHA_USD_PER_M"", 0.01)) # user override
        return ((prompt_tokens+completion_tokens)/1_000_000)*price
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent,1,7
survived,"def _inverse(value: float, ideal: float) -> float:
    """"""Higher score the *closer* ``value`` is to *below* ``ideal``.""""""
    if value <= ideal:
        return 1.0
    # penalise by percentage over ideal
    return ideal / (value + _EPS)
",alpha_factory_v1/demos/era_of_experience/reward_backends/fitness_reward.py,,1,7
survived,"    def __init__(self,
                 name: str,
                 role: str = ""autonomousâ€‘agent"",
                 provider: str | None = None,
                 objectives: Optional[ObjectiveWeights] = None,
                 lineage_dir: str | pathlib.Path = ""./lineage"",
                 rate_limit_tps: float = 3.0):
        self.name = name
        self.role = role
        self.id = f""{name}-{_sha(uuid.uuid4().hex)}""
        self.objectives = objectives or ObjectiveWeights()
        self.lm = LMClient(provider or os.getenv(""ALPHA_PROVIDER"", ""openai:gpt-4o""))
        self.tracer = LineageTracer(pathlib.Path(lineage_dir)/f""{self.id}.jsonl"")
        self.tracer.log(""init"", role=role, provider=self.lm.endpoint)
        GLOBAL_LIMITER._tps = rate_limit_tps
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent,1,8
survived,"def reward(state: Mapping | None, action, result: Mapping | None) -> float:
    """"""Compute *fitness* reward.""""""
    src = result or {}

    # Extract with graceful fall-backs (None â†’ target â†’ neutral score)
    steps = float(src.get(""steps"", _TARGET_STEPS))
    hr = float(src.get(""resting_hr"", _TARGET_REST_HR))
    sleep = float(src.get(""sleep_hours"", _TARGET_SLEEP))
    cal = float(src.get(""cal_intake"", _TARGET_CAL))

    scores = {
        ""steps"": _linear(steps, _TARGET_STEPS, cap=2 * _TARGET_STEPS),
        ""resting_hr"": _inverse(hr, _TARGET_REST_HR),
        ""sleep_hours"": _bell(sleep, _TARGET_SLEEP),
        ""cal_intake"": _bell(cal, _TARGET_CAL),
    }

    # Weighted average
    total_w = sum(_WEIGHTS.values())
    blended = sum(scores[k] * _WEIGHTS[k] for k in scores) / (total_w + _EPS)
    # Clamp for numerical safety
    return float(max(0.0, min(1.0, blended)))",alpha_factory_v1/demos/era_of_experience/reward_backends/fitness_reward.py,,1,7
survived,"    def __init__(self, tps: float = 3.0):
        self._tps = float(tps)
        self._allow = self._tps
        self._last = time.perf_counter()
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,RateLimiter,1,8
survived,"async def list_agents(): return list(AGENTS.keys())
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,,1,7
survived,"    def __init__(self,
                 endpoint: str = ""openai:gpt-4o"",
                 temperature: float = 0.2,
                 max_tokens: int = 2048,
                 context_len: int = 8192,
                 stream: bool = False,
                 timeout: int = 120,
                 **extra):
        self.endpoint = endpoint
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.context_len = context_len
        self.stream = stream
        self.timeout = timeout
        self.extra = extra
        self._backend, self._model = self._parse(endpoint)
        self._client = self._init_backend()
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,LMClient,1,7
survived,"        def handle(self, _msg):
            LOG.debug(""[Fallback%d] â† %s"", idx, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Fallback,1,6
survived,"    def __init__(self,
                 endpoint: str = ""openai:gpt-4o"",
                 temperature: float = 0.2,
                 max_tokens: int = 2048,
                 context_len: int = 8192,
                 stream: bool = False,
                 timeout: int = 120,
                 **extra):
        self.endpoint = endpoint
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.context_len = context_len
        self.stream = stream
        self.timeout = timeout
        self.extra = extra
        self._backend, self._model = self._parse(endpoint)
        self._client = self._init_backend()
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,LMClient,1,7
survived,"def test_csp_no_violations() -> None:
    dist = Path(__file__).resolve().parents[2] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            violations = []
            page.on(
                ""console"",
                lambda msg: violations.append(msg.text)
                if ""Content Security Policy"" in msg.text
                else None,
            )
            page.on(""pageerror"", lambda err: violations.append(str(err)))
            page.goto(url)
            page.wait_for_selector(""#controls"")
            assert not any(""Content Security Policy"" in v for v in violations)
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
",tests/security/test_csp.py,,0,6
survived,"def test_run_cycle_async_logs_delta_g(monkeypatch, caplog):
    """"""One cycle should log the computed Î”G value.""""""
    mod = importlib.import_module(MODULE)

    caplog.set_level(logging.INFO)
    monkeypatch.setattr(mod, ""_A2A"", None)
    monkeypatch.setattr(mod, ""_llm_comment"", lambda *_: ""ok"")

    asyncio.run(
        mod.run_cycle_async(
            mod.Orchestrator(),
            mod.AgentFin(),
            mod.AgentRes(),
            mod.AgentEne(),
            mod.AgentGdl(),
            mod.Model(),
        )
    )

    assert any(""Î”G=0.03"" in r.getMessage() for r in caplog.records)
",tests/test_alpha_agi_business_3_v1.py,,1,6
survived,"def test_log_to_section_training():
    """"""Test log_to_section writes to the training file""""""
    with tempfile.TemporaryDirectory() as tmpdir:
        logger = Logger(""test_module"", base_dir=tmpdir)
        data = {""foo"": ""bar""}
        logger.log_to_section(data, section=""training"")

        with open(logger.training_file, ""r"", encoding=""utf-8"") as f:
            lines = f.readlines()
            assert len(lines) == 1
            entry = json.loads(lines[0])
            assert entry[""foo""] == ""bar""
            assert isinstance(entry[""timestamp""], float)",tests/test_logger.py,,1,7
survived,"    def __init__(self, settings: config.Settings) -> None:
        self.settings = settings
        self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_codegen_safety.py,DummyBus,1,7
survived,"def test_blocks_insider_message() -> None:
    agent = _make_agent()
    env = messaging.Envelope(
        sender=""market"",
        recipient=""safety"",
        payload={""analysis"": ""buy AAPL tomorrow""},
        ts=0.0,
    )
    asyncio.run(agent.handle(env))
    assert agent.bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_codegen_safety.py,,1,7
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - interface
        pass
",tests/test_codegen_safety.py,DummyLedger,0,6
survived,"def _venv_pip(venv: Path) -> Path:
    """"""Return the path to the pip executable inside *venv*.""""""
    if os.name == ""nt"":
        return venv / ""Scripts"" / ""pip.exe""
    return venv / ""bin"" / ""pip""
",alpha_factory_v1/quickstart.py,,1,7
survived,"    def test_create_venv_runs_commands_when_missing(self):
        with mock.patch('subprocess.check_call') as cc:
            venv = Path('/tmp/qsvenv')
            if venv.exists():
                import shutil
                shutil.rmtree(venv)
            quickstart._create_venv(venv)
            pip = quickstart._venv_pip(venv)
            req = Path('alpha_factory_v1/requirements.lock')
            if not req.exists():
                req = Path('alpha_factory_v1/requirements.txt')
            self.assertEqual(cc.call_args_list[0].args[0][:3], [sys.executable, '-m', 'venv'])
            self.assertIn(str(venv), cc.call_args_list[0].args[0])
            called = [call.args[0] for call in cc.call_args_list]
            self.assertIn([str(pip), 'install', '-U', 'pip'], called)
            self.assertIn([str(pip), 'install', '-r', str(req)], called)
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest,0,7
survived,"        def recurrent(self, state, action):
            return None, 0.0, 0.0, None
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,MiniMuNet,0,7
survived,"def _select_child(node: Node) -> Tuple[int, Node]:
    """"""Pick child with highest UCB score.""""""
    best_score = -float(""inf"")
    best_action = 0
    best_child = None
    for action, child in node.children.items():
        ucb = child.value() + 1.5 * child.prior * math.sqrt(node.visit_count + 1) / (1 + child.visit_count)
        if ucb > best_score:
            best_score = ucb
            best_action = action
            best_child = child
    return best_action, best_child
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,,1,7
survived,"    def test_reset_produces_valid_grid(self):
        env = ce.CurriculumEnv(genome=ce.EnvGenome(max_steps=10), size=6)
        obs, info = env.reset()
        self.assertEqual(obs.shape[0], env.observation_space.shape[0])
        self.assertIn(""genome_id"", info)
        self.assertLessEqual(info[""difficulty""], 10)
",alpha_factory_v1/tests/test_aiga_meta_evolution.py,CurriculumEnvTest,1,7
survived,"    def __init__(self, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT) -> None:
        self.base_url = f""http://{host}:{port}""
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,MarketplaceClient,1,6
survived,"def load_job(path: str | Path) -> dict[str, Any]:
    """"""Load a job description from a JSON file.""""""
    return json.loads(Path(path).read_text())
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,,1,8
survived,"    def test_run_headless(self):
        if not dependencies_available:
            self.skipTest(""demo dependencies missing"")
        orch = run_headless(steps=10)
        time.sleep(0.5)
        orch.stop = True
        self.assertGreaterEqual(len(orch.learners), 1)
        self.assertGreater(len(orch.learners[0].buffer), 0)
",alpha_factory_v1/tests/test_alpha_asi_world_model.py,TestAlphaASIWorldModel,1,7
survived,"    def test_metrics_setup_with_stubs(self):
        class DummyMetric:
            def __init__(self, *a, **k):
                self.calls = []
            def inc(self, *a, **k):
                self.calls.append(""inc"")
            def set(self, *a, **k):
                self.calls.append(""set"")
            def observe(self, *a, **k):
                self.calls.append(""observe"")

        prom_stub = SimpleNamespace(Counter=DummyMetric, Gauge=DummyMetric, Histogram=DummyMetric)
        with mock.patch.object(ping_agent, ""_Prom"", prom_stub):
            ping_agent.PingAgent.__bases__ = (NewAgentBase,)
            agent = ping_agent.PingAgent()
            agent.orchestrator = self.orc
            asyncio.run(agent.setup())
            self.assertIsInstance(agent._prom_ping_total, DummyMetric)
            asyncio.run(agent.step())
            self.assertIn(""inc"", agent._prom_ping_total.calls)
            self.assertTrue(any(c == ""observe"" for c in agent._prom_cycle_hist.calls))
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest,0,7
survived,"    def test_check_docker_compose(self):
        with mock.patch('shutil.which', return_value=None):
            self.assertFalse(preflight.check_docker_compose())
        with mock.patch('shutil.which', return_value='/bin/docker'):
            with mock.patch('subprocess.run') as run:
                run.return_value = mock.Mock(returncode=0)
                self.assertTrue(preflight.check_docker_compose())
            with mock.patch('subprocess.run', side_effect=Exception):
                self.assertFalse(preflight.check_docker_compose())
",alpha_factory_v1/tests/test_preflight.py,PreflightTest,1,6
survived,"                def locate_file(self, path):
                    return init_file
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest.Dist,0,7
survived,"    def test_load_real_package_when_available(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            pkg = Path(tmpdir) / ""requests""
            pkg.mkdir()
            init_file = pkg / ""__init__.py""
            init_file.write_text(""value = 42\n"")

            class Dist:
                def locate_file(self, path):
                    return init_file

            original = im.distribution
            def fake_distribution(name):
                self.assertEqual(name, ""requests"")
                return Dist()
            im.distribution = fake_distribution
            sys.path.insert(0, tmpdir)
            try:
                mod = importlib.import_module(""requests"")
                self.assertEqual(getattr(mod, ""value"", None), 42)
                self.assertEqual(Path(mod.__file__).resolve(), init_file.resolve())
            finally:
                sys.path.remove(tmpdir)
                im.distribution = original
                sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest,1,7
survived,"    def test_alpha_factory_remote_chart(self):
        chart = HELM_DIR / ""alpha-factory-remote"" / ""Chart.yaml""
        values = HELM_DIR / ""alpha-factory-remote"" / ""values.yaml""
        helpers = HELM_DIR / ""alpha-factory-remote"" / ""templates"" / ""_helpers.tpl""
        self.check_chart_file(chart)
        self.assertTrue(values.is_file(), ""values.yaml missing for alpha-factory-remote"")
        self.assertTrue(helpers.is_file(), ""_helpers.tpl missing for alpha-factory-remote"")
",alpha_factory_v1/tests/test_helm_charts.py,HelmChartTests,1,6
survived,"    def test_graph_list_mode(self):
        self.assertEqual(self.fabric.graph._mode, ""list"")
        self.fabric.add_relation(""A"", ""rel"", ""B"")
        self.fabric.add_relation(""B"", ""rel"", ""C"")
        self.assertEqual(self.fabric.find_path(""A"", ""C""), [""A"", ""B"", ""C""])
",alpha_factory_v1/tests/test_memory_provider.py,MemoryFabricFallbackTest,1,7
survived,"    def test_stub_backend(self):
        provider = ModelProvider()
        self.assertEqual(provider.backend[0], ""stub"")
        out = provider.complete(""hello"")
        self.assertIsInstance(out, str)
        self.assertTrue(out)
",alpha_factory_v1/tests/test_memory_provider.py,ModelProviderStubTest,1,7
survived,"    def test_missing_optional_packages_ok(self) -> None:
        def fake_check_pkg(name: str) -> bool:
            # Required packages are always present
            if name in {""pytest"", ""prometheus_client""}:
                return True
            # Simulate all optional deps missing
            if name in preflight.OPTIONAL_DEPS:
                return False
            return True

        patches = [
            mock.patch.object(preflight, ""check_python"", return_value=True),
            mock.patch.object(preflight, ""check_cmd"", return_value=True),
            mock.patch.object(preflight, ""check_node"", return_value=True),
            mock.patch.object(preflight, ""check_docker_daemon"", return_value=True),
            mock.patch.object(preflight, ""check_docker_compose"", return_value=True),
            mock.patch.object(preflight, ""check_patch_in_sandbox"", return_value=True),
            mock.patch.object(preflight, ""check_pkg"", side_effect=fake_check_pkg),
            mock.patch.object(preflight, ""check_openai_agents_version"", return_value=True),
            mock.patch.object(preflight, ""ensure_dir"", return_value=None),
        ]
        with mock.patch.dict(os.environ, {""OPENAI_API_KEY"": """", ""ANTHROPIC_API_KEY"": """"}, clear=False):
            with contextlib.ExitStack() as stack:
                for p in patches:
                    stack.enter_context(p)
                # Should not raise SystemExit
                preflight.main([""--offline""])
",tests/test_preflight_optional_missing.py,TestPreflightOptionalMissing,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/concurrent-computing-2.py,,1,6
survived,"def nextState(f, x, y):
    count = 0
    dy = -1
    while dy <= 1:
        dx = -1
        while dx <= 1:
            if not (dx == 0 and dy == 0) and state(f, x + dx, y + dy):
                count = count + 1
            dx = dx + 1
        dy = dy + 1
    return count == 3 or (count == 2 and state(f, x, y))
",tests/rosetta/transpiler/Python/conways-game-of-life.py,,1,7
survived,"def example1(flag):
    if flag:
        None",tests/rosetta/transpiler/Python/conditional-structures-1.py,,0,7
survived,"def test_datamodelsummary_str_no_entities() -> None:
    model = ModelDescription(title=""M"", description="""", entities=[])
    summary = DataModelSummary(
        title=""Empty"",
        description=""A test"",
        entity_count=0,
        entities=[],
        model=str(model),
        usage_hint=""HINT"",
    )
    expected = ""\n"".join(
        [
            ""# Empty"",
            ""A test"",
            """",
            ""**Entity count:** 0"",
            """",
            str(model),
            """",
            ""HINT"",
        ]
    )
    assert str(summary) == expected
",tests/test_datamodel_summary.py,,1,7
survived,"    async def collect_trajectories(self, item):
        user_content = dict(item[0][0])[""content""]
        messages = []
        if self.config.system_prompt:
            messages.append({""role"": ""system"", ""content"": self.config.system_prompt})
        messages.append({""role"": ""user"", ""content"": user_content})
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        completions = await self.server.completion(
            prompt=prompt,
            n=self.config.group_size,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
        )
        trajectories = []
        for completion in completions.choices:
            completion_text = (
                completion.text if hasattr(completion, ""text"") else completion.message.content
            )
            msg_seq = []
            if self.config.system_prompt:
                msg_seq.append({""role"": ""system"", ""content"": self.config.system_prompt})
            msg_seq.append({""role"": ""user"", ""content"": user_content})
            msg_seq.append({""role"": ""assistant"", ""content"": completion_text})
            trajectories.append(msg_seq)
        return trajectories, []
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv,1,7
survived,"def load_scenario(name: str, directory: str | Path | None = None) -> Scenario:
    """"""Load a scenario definition by name.""""""

    dir_path = Path(directory or BASE_DIR)
    path = dir_path / f""{name}.yaml""
    if not path.exists():
        path = dir_path / f""{name}.yml""
    data = _load_yaml(path)

    secs: list[sector.Sector] = []
    for entry in data.get(""sectors"", []):
        if isinstance(entry, str):
            secs.append(sector.Sector(entry))
        elif isinstance(entry, dict):
            secs.append(
                sector.Sector(
                    entry.get(""name"", """"),
                    float(entry.get(""energy"", 1.0)),
                    float(entry.get(""entropy"", 1.0)),
                    float(entry.get(""growth"", 0.05)),
                    bool(entry.get(""disrupted"", False)),
                )
            )
        else:
            raise ValueError(f""Invalid sector entry: {entry!r}"")

    return Scenario(
        name=data.get(""name"", name),
        horizon=int(data.get(""horizon"", 1)),
        sectors=secs,
        curve=data.get(""curve"", ""logistic""),
        k=data.get(""k""),
        x0=data.get(""x0""),
        pop_size=int(data.get(""pop_size"", 6)),
        generations=int(data.get(""generations"", 1)),
    )
",src/simulation/replay.py,,1,7
survived,"def available_scenarios(directory: str | Path | None = None) -> list[str]:
    """"""Return the list of available scenario names.""""""

    dir_path = Path(directory or BASE_DIR)
    names = {p.stem for p in dir_path.glob(""*.yaml"")}
    names.update(p.stem for p in dir_path.glob(""*.yml""))
    return sorted(names)
",src/simulation/replay.py,,1,7
survived,"def test_cleanup_stale_entries(tmp_path):
    @cachier(
        cache_dir=tmp_path,
        stale_after=timedelta(seconds=1),
        cleanup_stale=True,
        cleanup_interval=timedelta(seconds=0),
    )
    def add(x):
        return x + 1

    add.clear_cache()
    add(1)
    add(2)
    fname = f"".{add.__module__}.{add.__qualname__}"".replace(""<"", ""_"").replace(
        "">"", ""_""
    )
    cache_path = os.path.join(add.cache_dpath(), fname)
    with open(cache_path, ""rb"") as fh:
        data = pickle.load(fh)
    assert len(data) == 2
    time.sleep(1.1)
    add(1)
    time.sleep(0.2)
    with open(cache_path, ""rb"") as fh:
        data = pickle.load(fh)
    assert len(data) == 1",tests/test_cleanup.py,,1,7
survived,"    async def get_position(self, symbol: str) -> float:
        """"""Return the signed position for ``symbol`` in shares.""""""
",alpha_factory_v1/backend/types.py,TradeBrokerProtocol,1,6
survived,"def projection_from_json(path: str | Path) -> Dict[str, Dict[str, Any]]:
    """"""Return discounted cash flow projections loaded from ``path``.

    Each top-level key in the JSON file should map to a sector. The value must be
    a mapping accepted by :func:`delta_sector_to_dcf` with keys ``delta_revenue``,
    ``margin``, ``discount_rate`` and ``years``.
    """"""
    data = json.loads(Path(path).read_text(encoding=""utf-8""))
    results: Dict[str, Dict[str, Any]] = {}
    for sector, vals in data.items():
        if not isinstance(vals, dict):
            continue
        results[sector] = delta_sector_to_dcf(vals)
    return results",alpha_factory_v1/core/finance/wealth_projection.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/user_type_literal.py,Person,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/order_by_map.py,Data,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Nation,1,6
survived,"            def _decorator(func):
                return func
",tests/test_macro_adk_integration.py,,0,6
survived,"    def Counter(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Counter, name, desc, labels)
",alpha_factory_v1/backend/agents/__init__.py,,1,7
survived,"    def convert_lambda(
        self,
        node: ast.Lambda,
        annotated_args: list[str] | None = None,
        ret_type: str | None = None,
    ) -> str:
        parts = []
        for i, a in enumerate(node.args.args):
            typ = (
                annotated_args[i]
                if annotated_args and i < len(annotated_args)
                else None
            )
            if typ:
                parts.append(f""{a.arg}: {typ}"")
            else:
                parts.append(a.arg)
        body = self.convert_expr(node.body)
        if ret_type:
            return f""fun({', '.join(parts)}): {ret_type} => {body}""
        return f""fun({', '.join(parts)}) => {body}""
",tools/any2mochi/py/py2mochi.py,Converter,1,7
survived,"    async def seed(session: AsyncSession) -> None:
        nonlocal seed_called
        session.add(User(id=1))
        seed_called = True
",tests/test_lifespan.py,,1,6
survived,"        async def act(self) -> str:
            return ""done""
",tests/test_agent_experience_entrypoint.py,DummyAgent,1,6
survived,"    def Button(self, *a, **k):
        return DummyButton()
",tests/test_agent_experience_entrypoint.py,DummyBlocks,1,6
survived,"    async def one_event():
        yield {""id"": 1, ""t"": ""0"", ""user"": ""a"", ""kind"": ""health"", ""payload"": {}}
",tests/test_agent_experience_entrypoint.py,,1,6
survived,"    def __enter__(self):
        return self
",tests/test_agent_experience_entrypoint.py,DummyBlocks,1,7
survived,"            async def step_once() -> None:
                evt = await queue.get()
                self.assertIsInstance(evt, dict)
",tests/test_era_experience.py,TestEraOfExperience,1,6
survived,"def test_simulator_loader_overlay() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-gen').value=1"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-pop').value=1"")
            page.click(""#simulator-panel #sim-start"")
            page.wait_for_selector(""#sim-loader"", state=""visible"")
            page.wait_for_function(""document.querySelector('#sim-status').textContent.includes('gen 1')"")
            page.wait_for_selector(""#sim-loader"", state=""hidden"")
            page.click(""#simulator-panel #sim-cancel"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",tests/test_simulator_loader.py,,0,7
survived,"def test_rejects_symlink(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(tmp_path, monkeypatch)

    info = tarfile.TarInfo(""link"")
    info.type = tarfile.SYMTYPE
    info.linkname = ""../evil""
    payload = _make_tar(info)

    resp = client.post(""/mutate"", files={""tar"": (""bad.tar"", payload, ""application/x-tar"")})
    assert resp.status_code == 400
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_evolution_worker.py,,1,7
survived,"def test_get_macros():
    context = Context(paths=[""examples/sushi""])
    lsp_context = LSPContext(context)

    file_path = next(key for key in lsp_context.map.keys() if key.name == ""active_customers.sql"")
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        file_content = f.read()

    file_uri = URI.from_path(file_path)
    completions = LSPContext.get_completions(lsp_context, file_uri, file_content)

    assert ""each"" in completions.macros
    assert ""add_one"" in completions.macros
",tests/lsp/test_completions.py,,1,7
survived,"def load_documents(data_dir: str):
    """"""Load and preprocess all text files under *data_dir*.""""""
    corpus = []
    for filename in sorted(glob.glob(os.path.join(data_dir, ""*.txt""))):
        with open(filename, ""r"", encoding=""utf-8"", errors=""ignore"") as f:
            text = f.read().lower()
        tokens = [t for t in TOKEN_RE.findall(text) if t not in STOPWORDS]
        corpus.append(tokens)
    return corpus
",scripts/run_hlda.py,,1,7
survived,"def build_vocab(corpus):
    vocab = sorted({word for doc in corpus for word in doc})
    index = {w: i for i, w in enumerate(vocab)}
    return vocab, index
",scripts/run_hlda.py,,1,7
survived,"    def add_child(self):
        ''' Adds a child to the next level of this node '''
        node = NCRPNode(
            self.num_levels,
            self.vocab,
            parent=self,
            level=self.level + 1,
            random_state=self.random_state,
        )
        self.children.append(node)
        NCRPNode.total_nodes += 1
        return node
",src/hlda/sampler.py,NCRPNode,1,7
survived,"def test_cross_industry_script_idempotent(tmp_path: Path) -> None:
    result = _run_script(tmp_path)
    first_services = result[""first""].get(""services"", {})
    second_services = result[""second""].get(""services"", {})
    assert first_services == second_services
    assert len(second_services) == len(set(second_services))",tests/test_cross_industry_patch.py,,1,7
survived,"def test_start_alpha_business_no_browser() -> None:
    script = Path(""alpha_factory_v1/demos/alpha_agi_business_v1/start_alpha_business.py"")
    port = _free_port()
    runtime_port = _free_port()
    env = os.environ.copy()
    env[""OPENAI_API_KEY""] = """"
    env[""AGENTS_RUNTIME_PORT""] = str(runtime_port)
    env[""PORT""] = str(port)
    env.setdefault(""API_TOKEN"", ""demo-token"")
    proc = subprocess.Popen([sys.executable, str(script), ""--no-browser""], env=env)
    try:
        time.sleep(2)
        assert proc.poll() is None
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_start_alpha_business.py,,0,6
survived,"def _jit_paged_decode(attn, x, pos_ids, cache):
    def _decode(a, b, c, d):
        return a.paged_decode(b, pos_ids=c, key=jrandom.PRNGKey(2), page_cache=d)

    if jax.default_backend() == ""tpu"":
        _decode_jit = equinox.filter_jit(_decode)
        return _decode_jit(attn, x, pos_ids, cache)
    else:
        return _decode(attn, x, pos_ids, cache)
",tests/test_attention.py,,1,7
survived,"    def test_qclid_set_get(self):
        klong = KlongInterpreter()
        klong('d::.qclid(4321)')
        d = klong('d')
        self.assertTrue(d.is_open())
        d.set(KGSym('a'), 42)
        self.assertEqual(d.connection.conn.queries[-1], 'a::42')
        r = d.get('x+y')
        self.assertEqual(r, 'EXEC:x+y')
        self.assertEqual(d.connection.conn.queries[-1], 'x+y')
        klong('.qclic(d)')
        self.assertFalse(d.is_open())
",tests/test_sys_fn_kdb.py,TestKdbIPC,1,7
survived,"    def setUp(self):
        self.patcher = unittest.mock.patch(
            'klongpy.sys_fn_kdb.qconnection',
            SimpleNamespace(QConnection=DummyQConnection, QFunction=type('QFunction', (), {}))
        )
        self.patcher.start()
",tests/test_sys_fn_kdb.py,TestKdbIPC,1,6
survived,"def test_apply_thresholds_selects_max():
    preds = np.array([[0.1, 0.2, 0.15]])
    thresholds = np.array([0.5, 0.5, 0.5])
    expected = np.array([[0, 1, 0]])
    result = apply_thresholds(preds, thresholds)
    assert np.array_equal(result, expected)",tests/test_utils.py,,1,7
survived,"def test_install_button_shows_on_event() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            assert page.is_hidden(""#install-btn"")
            page.evaluate(""window.dispatchEvent(new Event('beforeinstallprompt'))"")
            page.wait_for_selector(""#install-btn"", state=""visible"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
",tests/test_install_button.py,,0,7
survived,"    def __init__(self, inst: DummyAgent) -> None:
        self.inst = inst
        self.next_ts = 1
",tests/test_orchestrator_rest.py,DummyRunner,1,6
survived,"    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        seed = input_data.seed
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.model.api_name,
            prompt=input_data.prompt,
            input_image=input_data.input_image,
            aspect_ratio=input_data.aspect_ratio.value,
            seed=seed,
        )
        yield ""image_url"", result
",autogpt_platform/backend/backend/blocks/flux_kontext.py,AIImageEditorBlock,1,7
survived,"            def add(self, instr: object) -> ""DummyTx"":
                self.instructions.append(instr)
                return self
",tests/test_merkle_broadcast.py,TestMerkleBroadcast.DummyTx,1,7
survived,"    def bounds():
        cb = request.args.get(""callback"")
        data = rs.bounds(jsoncallback=cb)
        mimetype = ""application/javascript"" if cb else ""application/json""
        return Response(data, mimetype=mimetype)
",pygs/graphserver/ext/routeserver/routeserver.py,,1,6
survived,"    def add(self, text: str) -> None:
        vec = embed(text)
        if self.index is not None:
            self.index.add(vec)
        self.mean = (self.mean * self.count + vec[0]) / (self.count + 1)
        self.count += 1
",src/evaluators/novelty.py,NoveltyIndex,1,6
survived,"def test_impact_score() -> None:
    facts = CapsuleFacts(market_size=100, efficiency_gain=0.1, llm_score=0.6)
    scorer = ImpactScorer(llm_weight=0.5)
    score = scorer.score(facts, 0.2)
    assert score == 100 * 0.2 * (1 + 0.5 * 0.6)",tests/test_impact_scorer.py,,1,7
survived,"def test_download_file_success(tmp_path: Path, requests_mock: ""requests_mock.Mocker"") -> None:
    monkeypatch_file_list = [""dummy.txt""]
    url = dg.model_urls(""117M"")[0].replace(""checkpoint"", ""dummy.txt"")
    requests_mock.get(url, text=""ok"")

    dest_dir = tmp_path / ""models""
    with pytest.MonkeyPatch.context() as m:
        m.setattr(dg, ""_FILE_LIST"", monkeypatch_file_list)
        dg.download_openai_gpt2(""117M"", dest=dest_dir)

    assert (dest_dir / ""117M"" / ""dummy.txt"").read_text() == ""ok""
",tests/test_download_openai_gpt2.py,,1,7
survived,"def test_download_with_retry_fallback(tmp_path: Path, requests_mock: requests_mock.Mocker) -> None:
    path = tmp_path / ""out""
    monkeypatch = pytest.MonkeyPatch()
    monkeypatch.setattr(fa, ""FALLBACK_GATEWAYS"", [""https://alt.gateway/ipfs""])
    url_primary = f""{fa.GATEWAY}/CID""
    url_alt = ""https://alt.gateway/ipfs/CID""
    requests_mock.get(url_primary, status_code=500)
    requests_mock.get(url_alt, text=""data"")
    try:
        fa.download_with_retry(""CID"", path, attempts=1)
    finally:
        monkeypatch.undo()
    assert path.read_text() == ""data""
",tests/test_fetch_assets.py,,1,7
survived,"def test_env_required(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.delenv(""API_TOKEN"", raising=False)
    with pytest.raises(RuntimeError):
        orchestrator._build_rest({""dummy"": DummyRunner()})",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,,1,7
survived,"def _dir_checksum(path: Path) -> str:
    hasher = hashlib.sha256()
    for file in sorted(path.rglob(""*"")):
        if file.is_file():
            hasher.update(file.relative_to(path).as_posix().encode())
            hasher.update(file.read_bytes())
    return hasher.hexdigest()
",tests/test_checksum.py,,1,7
survived,"def _hamming_dist(a: bytes, b: bytes) -> int:
    diff = 0
    for x, y in zip(a, b):
        diff += (x ^ y).bit_count()
    diff += 8 * abs(len(a) - len(b))
    return diff
",tests/test_checksum.py,,1,7
survived,"    def project(self, vec: Sequence[float] | np.ndarray) -> np.ndarray:
        """"""Return ``vec`` multiplied by the current projection matrix.""""""
        if self._counter % self.steps == 0:
            self._proj = self._new_projection()
        self._counter += 1
        if np is not None:
            arr = np.asarray(vec, dtype=""float32"")
            if arr.ndim == 1:
                arr = arr.reshape(1, -1)
            return (arr @ np.asarray(self._proj).T).reshape(-1)
        arr = [float(x) for x in vec]
        out = [sum(a * b for a, b in zip(arr, col)) for col in zip(*self._proj)]
        if np is not None:
            return np.asarray(out, dtype=""float32"")
        return out  # type: ignore[return-value]",src/agents/guards/embedding_orthogonaliser.py,EmbeddingOrthogonaliser,1,6
survived,"    def _byte_to_char_idx(self, line_no: int, byte_idx: int) -> int:
        line_bytes = self._src_lines_bytes[line_no]
        return len(line_bytes[:byte_idx].decode(""utf-8""))
",src/flynt/code_editor.py,CodeEditor,1,7
survived,"def test_validate_template_success() -> None:
    ok, err = validate_template(""hello {{ name }}"")
    assert ok and err == """"
",tests/test_template_creator.py,,1,7
survived,"def validator() -> str:
    port = _free_port()
    cid = subprocess.check_output(
        [
            ""docker"",
            ""run"",
            ""-d"",
            ""-p"",
            f""{port}:8899"",
            ""solanalabs/solana:edge"",
            ""solana-test-validator"",
            ""--quiet"",
        ]
    ).decode().strip()
    url = f""http://localhost:{port}""
    try:
        if not _wait_rpc(url):
            subprocess.run([""docker"", ""logs"", cid], check=False)
            raise RuntimeError(""validator not ready"")
        yield url
    finally:
        subprocess.run([""docker"", ""rm"", ""-f"", cid], check=False)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_ledger_local_validator.py,,0,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Lineitem,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q4.py,Order,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q11.py,Supplier,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Nation,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Lineitem,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Nation,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q13.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q4.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q1.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Supplier,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q3.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q11.py,Nation,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Supplier,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto4,1,7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q18.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto6,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto9,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto1,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q30.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto10,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto9,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto4,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q5.py,Auto3,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q17.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto5,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto9,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto9,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto5,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto1,1,6
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q25.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto7,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto9,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto7,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto4,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q10.py,Auto4,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto6,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto9,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto3,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto7,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto8,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto10,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto5,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto6,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto3,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto5,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto6,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto6,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto2,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto4,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto6,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q10.py,Auto7,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q21.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto6,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto9,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto11,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto7,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,DateDim,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,WebSale,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q84.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Customer,1,7
survived,"def test_TPCDS_Q67_simplified():
    assert result == 67
",tests/dataset/tpc-ds/compiler/py/q67.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Auto1,1,7
survived,"def _q0():
    _groups = {}
    _order = []
    for j in joined:
        _k = Auto3(i_item_id=j.i_item_id, i_item_desc=j.i_item_desc, s_state=j.s_state)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(j)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            i_item_id=g.key[""i_item_id""],
            i_item_desc=g.key[""i_item_desc""],
            s_state=g.key[""s_state""],
            store_sales_quantitycount=len([_ for _ in g]),
            store_sales_quantityave=(
                sum([x.qty for x in g]) / len([x.qty for x in g])
                if [x.qty for x in g]
                else 0
            ),
            store_sales_quantitystdev=0.0,
            store_sales_quantitycov=0.0,
            store_returns_quantitycount=len([_ for _ in g]),
            store_returns_quantityave=(
                sum([x.ret for x in g]) / len([x.ret for x in g])
                if [x.ret for x in g]
                else 0
            ),
            store_returns_quantitystdev=0.0,
            store_returns_quantitycov=0.0,
            catalog_sales_quantitycount=len([_ for _ in g]),
            catalog_sales_quantityave=(
                sum([x.csq for x in g]) / len([x.csq for x in g])
                if [x.csq for x in g]
                else 0
            ),
            catalog_sales_quantitystdev=0.0,
            catalog_sales_quantitycov=0.0,
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q17.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,StoreReturn,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,CrossItem,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,DateDim,1,6
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q75.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q69.py,WebSale,1,7
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q7.py,,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q48.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q19.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,DateDim,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,WebReturn,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Item,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,Item,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto2,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,StoreReturn,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Auto1,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q93.py,_Group,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q57.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Auto2,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Auto3,1,7
survived,"def test_TPCDS_Q76_simplified():
    assert result == [
        Auto1(
            channel=""store"",
            col_name=None,
            d_year=1998,
            d_qoy=1,
            i_category=""CatA"",
            sales_cnt=1,
            sales_amt=10.0,
        ),
        Auto1(
            channel=""web"",
            col_name=None,
            d_year=1998,
            d_qoy=1,
            i_category=""CatB"",
            sales_cnt=1,
            sales_amt=15.0,
        ),
        Auto1(
            channel=""catalog"",
            col_name=None,
            d_year=1998,
            d_qoy=1,
            i_category=""CatC"",
            sales_cnt=1,
            sales_amt=20.0,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q76.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Auto3,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q44.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,CustomerAddres,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Item,1,7
survived,"def test_TPCDS_Q79_simplified():
    assert result == [
        Auto1(
            c_last_name=""Smith"",
            c_first_name=""Alice"",
            s_city=""CityA"",
            ss_ticket_number=1,
            amt=5.0,
            profit=10.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q79.py,,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q34.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q69.py,WebSale,1,7
survived,"def test_TPCDS_Q31_simplified():
    assert result == [
        Auto1(
            ca_county=""A"",
            d_year=2000,
            web_q1_q2_increase=1.5,
            store_q1_q2_increase=1.2,
            web_q2_q3_increase=1.6666666666666667,
            store_q2_q3_increase=1.3333333333333333,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q31.py,,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q18.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Customer,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q23.py,_Group,1,8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto5,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q65.py,Auto1,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q47.py,Auto1,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,CustomerAddres,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q65.py,Auto2,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Auto1,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q2.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CustomerAddres,1,7
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {""items"": item, ""on"": lambda ss, i: ss.item == i.i_item_sk},
            {""items"": date_dim, ""on"": lambda ss, i, d: ss.date == d.d_date_sk},
        ],
        {""select"": lambda ss, i, d: (ss, i, d)},
    )
    _groups = _group_by(_rows, lambda ss, i, d: i.i_manufact_id)
    _items1 = _groups
    return [
        Auto2(
            manu=g.key,
            sum_sales=sum([x[0].price for x in g]),
            avg_sales=(
                sum([x[0].price for x in g]) / len([x[0].price for x in g])
                if [x[0].price for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q53.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,WebSale,1,7
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q7.py,,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group,1,8
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q46.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q25.py,_Group,1,7
survived,"def test_TPCDS_Q70_simplified():
    assert result == [
        Auto1(s_state=""CA"", s_county=""Orange"", total_sum=15.0),
        Auto1(s_state=""TX"", s_county=""Travis"", total_sum=20.0),
    ]
",tests/dataset/tpc-ds/compiler/py/q70.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,Item,1,6
survived,"def distinct(xs):
    out = []
    for x in xs:
        if not x in out:
            out = out + [x]
    return out
",tests/dataset/tpc-ds/compiler/py/q94.py,,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q37.py,_Group,1,8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Customer,1,7
survived,"def count_range(ssales, tdim, hour, start_min, end_min):
    total = 0.0
    for ss in ssales:
        for t in tdim:
            if (
                (
                    (
                        ss.get(""sold_time_sk"")
                        if isinstance(ss, dict)
                        else getattr(ss, ""sold_time_sk"")
                    )
                    == (
                        t.get(""time_sk"")
                        if isinstance(t, dict)
                        else getattr(t, ""time_sk"")
                    )
                    and (t.get(""hour"") if isinstance(t, dict) else getattr(t, ""hour""))
                    == hour
                )
                and (t.get(""minute"") if isinstance(t, dict) else getattr(t, ""minute""))
                >= start_min
            ) and (
                t.get(""minute"") if isinstance(t, dict) else getattr(t, ""minute"")
            ) < end_min:
                total = total + (
                    ss.get(""qty"") if isinstance(ss, dict) else getattr(ss, ""qty"")
                )
    return total
",tests/dataset/tpc-ds/compiler/py/q88.py,,1,6
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q33.py,,1,7
survived,"def _q2():
    _groups = {}
    _order = []
    for g in grouped:
        _k = g.i_class
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(g)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto4(_class=cg.key, total=sum([x.itemrevenue for x in cg])) for cg in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q98.py,,0,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto3,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,HouseholdDemographics,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Auto1,1,6
survived,"def _q0():
    _groups = {}
    _order = []
    for r in filtered:
        _k = Auto2()
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            avg_ss_quantity=_avg([x[""ss_quantity""] for x in g]),
            avg_ss_ext_sales_price=_avg([x[""ss_ext_sales_price""] for x in g]),
            avg_ss_ext_wholesale_cost=_avg([x[""ss_ext_wholesale_cost""] for x in g]),
            sum_ss_ext_wholesale_cost=_sum([x[""ss_ext_wholesale_cost""] for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q13.py,,0,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CatalogSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,CatalogSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,CustomerAddres,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q39.py,,1,7
survived,"def _q0():
    _src = web_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": customer,
                ""on"": lambda ws, c: ws.bill_customer_sk == c.c_customer_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda ws, c, ca: c.c_current_addr_sk == ca.ca_address_sk,
            },
            {""items"": item, ""on"": lambda ws, c, ca, i: ws.item_sk == i.i_item_sk},
            {
                ""items"": date_dim,
                ""on"": lambda ws, c, ca, i, d: ws.sold_date_sk == d.d_date_sk,
            },
        ],
        {
            ""select"": lambda ws, c, ca, i, d: (ws, c, ca, i, d),
            ""where"": lambda ws, c, ca, i, d: (
                (ca.ca_zip[0:5] in zip_list or i.i_item_id in item_ids)
                and d.d_qoy == qoy
            )
            and d.d_year == year,
        },
    )
    _groups = _group_by(_rows, lambda ws, c, ca, i, d: ca.ca_zip)
    _items1 = _groups
    return [
        Auto1(ca_zip=g.key, sum_ws_sales_price=_sum([x[0].sales_price for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q45.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,CustomerAddres,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,StoreSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,HouseholdDemographic,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q77.py,,1,6
survived,"def _q0():
    _groups = {}
    _order = []
    for s in store_sales:
        _k = s.item
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(s)
    _items1 = [_groups[k] for k in _order]
    return [Auto2(item=g.key, total=sum([x.price for x in g])) for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q56.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,CatalogSale,1,6
survived,"def test_TPCDS_Q77_simplified():
    assert result == [
        Auto1(channel=""catalog channel"", id=1, sales=150.0, returns=7.0, profit=12.0),
        Auto1(channel=""store channel"", id=1, sales=100.0, returns=5.0, profit=9.0),
        Auto1(channel=""web channel"", id=1, sales=200.0, returns=10.0, profit=18.0),
    ]
",tests/dataset/tpc-ds/compiler/py/q77.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,Auto1,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q27.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,DateDim,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Store,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Customer,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Customer,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Warehouse,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q84.py,,1,6
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q35.py,_Group,1,7
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q27.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,CustomerAddres,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q26.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q32.py,Item,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,CatalogSale,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q16.py,,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q40.py,_Group,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,DateDim,1,7
survived,"def test_TPCDS_Q80_sample():
    assert total_profit == 80.0
",tests/dataset/tpc-ds/compiler/py/q80.py,,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,WebSale,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto3,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q52.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,Auto1,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q57.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,DateDim,1,7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q46.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Item,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Item,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q18.py,_Group,1,8
survived,"def test_TPCDS_Q65_simplified():
    assert result == 65
",tests/dataset/tpc-ds/compiler/py/q65.py,,1,6
survived,"def abs(x):
    if x >= 0.0:
        return x
    else:
        return -x
",tests/dataset/tpc-ds/compiler/py/q47.py,,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Customer,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,HouseholdDemographic,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Auto2,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,CustomerDemo,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q36.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,DateDim,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,DateDim,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,CustomerDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,Auto1,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Item,1,7
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q55.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Customer,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q34.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,WebSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,WebSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,Auto2,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,StoreSale,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q55.py,_Group,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q15.py,_Group,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Store,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,StoreReturn,1,6
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q79.py,,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q26.py,,1,6
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,StoreSale,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q66.py,WebSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,StoreSale,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Auto4,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,DateDim,1,7
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q32.py,,1,7
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q2.py,,1,6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q90.py,WebPage,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto3,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q38.py,Customer,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q14.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,CatalogSale,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,CustomerDemographic,1,7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,Auto1,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,Auto1,1,7
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q6.py,_Group,1,7
survived,"    def test_env_override(self) -> None:
        env = os.environ.copy()
        env[""ALPHA_AGI_SECTORS""] = ""Finance,Healthcare,Space""
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.alpha_agi_insight_v0.insight_demo"",
                ""--episodes"",
                ""1"",
            ],
            capture_output=True,
            text=True,
            env=env,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        match = re.search(r""Best sector:\s*(\w+)"", result.stdout)
        self.assertIsNotNone(match, result.stdout)
        self.assertIn(match.group(1), {""Finance"", ""Healthcare"", ""Space""})
",tests/test_alpha_agi_insight_env.py,TestAlphaAgiInsightEnv,1,7
survived,"def test_population_df() -> None:
    pop = [mats.Individual([0.0, 0.0]), mats.Individual([1.0, 1.0])]
    for i, ind in enumerate(pop):
        ind.fitness = (i * 1.0, i * 2.0, i * 3.0)
        ind.rank = i
    df = web_app.population_df(pop)
    assert set(df.columns) == {""effectiveness"", ""risk"", ""complexity"", ""rank""}
    assert len(df) == 2",tests/test_web_app.py,,1,7
survived,"    def __init__(self, request=None):
        self.request = request
",openai/__init__.py,APITimeoutError,1,7
survived,"    def _run_tests(self, errors: List[str]) -> None:
        env = os.environ.copy()
        for var in (
            ""COVERAGE_FILE"",
            ""COVERAGE_PROCESS_START"",
            ""COV_CORE_SOURCE"",
            ""COV_CORE_CONFIG"",
            ""COV_CORE_DATAFILE"",
        ):
            env.pop(var, None)

        env[""PYTHONPATH""] = (
            str(self.bundle_dir) + os.pathsep + env.get(""PYTHONPATH"", """")
        )

        result = subprocess.run(
            [""pytest"", ""-x"", ""-c"", ""/dev/null""],
            cwd=self.bundle_dir,
            capture_output=True,
            text=True,
            env=env,
        )
        if result.returncode != 0:
            errors.append(""tests failed"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        if not errors:
            self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator,1,8
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""tests"" / ""test_main.py"").write_text(
        ""def test_fail():\n    assert False""
    )
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,,1,8
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,,1,7
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,,1,7
survived,"    def _run_tests(self, errors: List[str]) -> None:
        env = os.environ.copy()
        env[""PYTHONPATH""] = str(self.bundle_dir)
        result = subprocess.run(
            [""pytest"", ""tests"", ""-c"", ""/dev/null"", ""-x""],
            cwd=self.bundle_dir,
            capture_output=True,
            text=True,
            env=env,
        )
        if result.returncode != 0:
            errors.append(""tests failed"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,6
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
deleted,"    def next_turn(
        self, previous_output: str | None = None
    ) -> Optional[List[ChatMessage]]:
        if self._state == ""start"":
            msgs = [
                ChatMessage(""system"", self.system_message),
                ChatMessage(""user"", self.user_input),
                ChatMessage(""user"", self.thinking_instructions),
            ]
            self._state = ""awaiting_thinking""
            self._messages.extend(msgs)
            return msgs

        if self._state == ""awaiting_thinking"":
            if previous_output is None:
                raise ValueError(""previous_output required for thinking step"")
            msgs = [
                ChatMessage(""assistant"", previous_output),
                ChatMessage(""user"", COT_FINAL_ANSWER_PROMPT),
            ]
            self._state = ""awaiting_final""
            self._messages.extend(msgs)
            return msgs

        if self._state == ""awaiting_final"":
            if previous_output is None:
                raise ValueError(""previous_output required for final step"")
            self._messages.append(ChatMessage(""assistant"", previous_output))
            self._state = ""done""
            return None

        return None
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,FinalAndIntermediateFormatter,1,7
survived,"    def post_data(self) -> Mapping[str, Union[str, bytes]]:
        return self.request.POST
",src/graphql_server/webob/views.py,WebobHTTPRequestAdapter,1,6
survived,"    def content_type(self) -> Optional[str]:
        return self.request.content_type
",src/graphql_server/webob/views.py,WebobHTTPRequestAdapter,1,6
survived,"    def render_graphql_ide(
        self, request: Request, request_data: GraphQLRequestData
    ) -> Response:
        return Response(
            text=request_data.to_template_string(self.graphql_ide_html),
            content_type=""text/html"",
            status=200,
        )
",src/graphql_server/webob/views.py,GraphQLView,1,6
survived,"def test_load_passes_revision():
    model_mock = MagicMock()
    model_mock.config = MagicMock(eos_token_id=None)
    processor_mock = MagicMock()

    with patch(""mlx_vlm.utils.get_model_path"") as mock_get_model_path, patch(
        ""mlx_vlm.utils.load_model"",
        return_value=model_mock,
    ) as mock_load_model, patch(
        ""mlx_vlm.utils.load_processor"",
        return_value=processor_mock,
    ) as mock_load_processor, patch(
        ""mlx_vlm.utils.load_image_processor"", return_value=None
    ):
        mock_get_model_path.return_value = Path(""/tmp/model"")

        model, processor = load(""repo"", revision=""abc"")

        assert model is model_mock
        assert processor is processor_mock
        mock_get_model_path.assert_called_with(""repo"", revision=""abc"")",mlx_vlm/tests/test_utils.py,,1,7
survived,"def find_deps(code):
    deps = []
    for imp in re.findall(r""import[^'\""]*['\""](.*?)['\""]"", code):
        if imp.startswith('.'): # relative
            deps.append(imp)
    return deps
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,6
survived,"    def __init__(
        self,
        enabled: set[str],
        dev_mode: bool,
        kafka_broker: str | None,
        cycle_seconds: int,
        max_cycle_sec: int,
    ) -> None:
        self.manager = AgentManager(
            enabled,
            dev_mode,
            kafka_broker,
            cycle_seconds,
            max_cycle_sec,
        )
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler,1,7
survived,"def init_metrics(port: int) -> None:
    """"""Initialise metric exporter.""""""
    _init_metrics(port)",alpha_factory_v1/backend/metrics.py,,0,7
survived,"    def query(
        self,
        sector: str | None = None,
        approach: str | None = None,
        band: int | None = None,
    ) -> list[Solution]:
        clauses: list[str] = []
        params: list[Any] = []
        if sector is not None:
            clauses.append(""sector=?"")
            params.append(sector)
        if approach is not None:
            clauses.append(""approach=?"")
            params.append(approach)
        if band is not None:
            clauses.append(""band=?"")
            params.append(band)
        sql = ""SELECT sector, approach, score, data, ts FROM solutions""
        if clauses:
            sql += "" WHERE "" + "" AND "".join(clauses)
        cur = self.conn.execute(sql, params)
        rows = cur.fetchall()
        result = [
            Solution(
                sector=row[0],
                approach=row[1],
                score=float(row[2]),
                data=json.loads(row[3]),
                ts=float(row[4]),
            )
            for row in rows
        ]
        return result
",src/archive/solution_archive.py,SolutionArchive,1,7
survived,"    def __init__(self, path: str | Path) -> None:
        self.path = Path(path)
        if duckdb is not None:
            self.conn = duckdb.connect(str(self.path))
        else:  # pragma: no cover - fallback
            self.conn = sqlite3.connect(str(self.path))
        self._ensure()
",src/archive/solution_archive.py,SolutionArchive,1,8
survived,"def _make_repo(tmp_path: Path) -> Path:
    repo = tmp_path / ""repo""
    repo.mkdir()
    (repo / ""metric.txt"").write_text(""1\n"", encoding=""utf-8"")
    (repo / ""test_dummy.py"").write_text(""def test_ok():\n    assert True\n"", encoding=""utf-8"")
    return repo
",tests/test_self_evolution.py,,1,7
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return int(s.getsockname()[1])
",tests/test_evolution_worker_safe_extract.py,,1,7
survived,"    def __call__(self, prompt: str) -> str:
        return ""ok""
",tests/resources/openai_agents.py,OpenAIAgent,1,6
survived,"    def __init__(self, *a: object, port: int = 5001, **_k: object) -> None:
        self.port = port
",tests/resources/openai_agents.py,AgentRuntime,1,6
survived,"    def adder(x):
        return x + n
",tests/human/x/python/closure.py,,0,7
survived,"def test_run_tests_success(monkeypatch, tmp_path):
    fake_manager = MagicMock()
    fake_manager.run_code_in_sandbox.return_value = (0, ""out"", ""err"")
    module = ExecutionModule(fake_manager)
    result = module.run_tests(tmp_path, timeout=5)
    assert isinstance(result, ExecutionResult)
    assert result.exit_code == 0
    assert result.stdout == ""out""
    assert result.stderr == ""err""
    fake_manager.run_code_in_sandbox.assert_called_with(
        code_directory=tmp_path,
        command=[""pytest"", ""-vv""],
        timeout=5,
    )
",tests/unit/test_execution_module.py,,1,8
survived,"    def test_repeated_close_safe(self):
        conn = self.fabric.vector._sql
        self.fabric.close()
        self.fabric.close()
        self.assertIsNone(self.fabric.vector._sql)
        with self.assertRaises(sqlite3.ProgrammingError):
            conn.execute(""SELECT 1"")
",tests/test_memory_fabric_sqlite.py,TestMemoryFabricSQLiteClose,1,7
survived,"def dummy_lifecycle(*args, **kwargs):
    yield SimpleNamespace(job_key=""backup_database"")
",pioreactor/tests/test_backup_database.py,,1,6
survived,"    async def _get_new_response(
        cls,
        agent: Agent[TContext],
        system_prompt: str | None,
        input: list[TResponseInputItem],
        output_schema: AgentOutputSchemaBase | None,
        all_tools: list[Tool],
        handoffs: list[Handoff],
        context_wrapper: RunContextWrapper[TContext],
        run_config: RunConfig,
        tool_use_tracker: AgentToolUseTracker,
        previous_response_id: str | None,
    ) -> ModelResponse:
        model = cls._get_model(agent, run_config)
        model_settings = agent.model_settings.resolve(run_config.model_settings)
        model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)

        new_response = await model.get_response(
            system_instructions=system_prompt,
            input=input,
            model_settings=model_settings,
            tools=all_tools,
            output_schema=output_schema,
            handoffs=handoffs,
            tracing=get_model_tracing_impl(
                run_config.tracing_disabled, run_config.trace_include_sensitive_data
            ),
            previous_response_id=previous_response_id,
        )

        context_wrapper.usage.add(new_response.usage)

        return new_response
",src/agents/run.py,DefaultAgentRunner,1,7
survived,"def gather_signals() -> Dict[str, str]:
    """"""Return raw detector messages for all built-in signals.""""""
    return {
        ""yield_curve"": detect_yield_curve_alpha(),
        ""supply_chain"": detect_supply_chain_alpha(),
    }
",alpha_factory_v1/demos/era_of_experience/alpha_report.py,,1,7
survived,"    async def step(self) -> None:
        await self.publish(""alpha.execution"", {""alpha"": ""order executed""})
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,AlphaExecutionAgent,1,6
survived,"def _start_bridge() -> None:
    """"""Start the OpenAI Agents bridge in a background thread.""""""
    try:
        from alpha_factory_v1.demos.alpha_agi_business_v1 import openai_agents_bridge
    except Exception as exc:  # pragma: no cover - optional dep
        print(f""âš ï¸  OpenAI bridge not available: {exc}"")
        return
    thread = threading.Thread(target=openai_agents_bridge.main, daemon=True)
    thread.start()
",alpha_factory_v1/demos/alpha_agi_business_v1/run_business_v1_local.py,,1,7
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        if isinstance(obs, dict):
            action = obs.get(""action"")
            if action == ""discover"":
                return await self.tools.discover(obs.get(""num"", 1))
            if action == ""recent"":
                return await self.tools.recent_log(obs.get(""limit"", 5))
        return await self.tools.list_samples()
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,CrossIndustryAgent,1,7
survived,"async def check_health() -> str:
    """"""Check orchestrator /healthz endpoint.""""""
    resp = requests.get(f""{HOST}/healthz"", timeout=5)
    resp.raise_for_status()
    return resp.text
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,,1,7
survived,"def test_pyodide_base64_global() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            val = page.evaluate(""window.PYODIDE_WASM_BASE64"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
    assert val, ""PYODIDE_WASM_BASE64 not set""",tests/test_wasm_base64.py,,0,6
survived,"    async def dummy_fn(completion, **kwargs):
        captured.update(kwargs)
        return ""ok""
",tests/integrations/openai/test_openai_sdk.py,,1,6
survived,"def test_rejects_empty_diff() -> None:
    assert not is_patch_valid("""")
",tests/test_patch_guard.py,,1,7
survived,"    def _ensure(self) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(
                ""CREATE TABLE IF NOT EXISTS agents(""
                ""id INTEGER PRIMARY KEY AUTOINCREMENT,""
                ""meta TEXT,""
                ""score REAL""
                "")""
            )
",src/archive/__init__.py,Archive,1,7
survived,"def test_detect_backtrack(tmp_path) -> None:
    db_path = tmp_path / ""arch.db""
    db = ArchiveDB(db_path)
    db.add(ArchiveEntry(""a"", None, 0.5, 0.0, True, 0.0))
    db.add(ArchiveEntry(""b"", ""a"", 0.6, 0.0, True, 1.0))
    db.add(ArchiveEntry(""c"", ""b"", 0.4, 0.0, True, 2.0))
    counts = ab.count_backtracks(db_path)
    assert any(c > 0 for c in counts)",tests/test_analyse_backtrack.py,,1,7
survived,"    async def _log_async(self, level: LogLevel, message: str):
        if not self._should_log(level):
            return
        record = self._format(level, message)
        tasks = []
        for h in self.handlers:
            if level >= h.level:
                if asyncio.iscoroutinefunction(h.emit):
                    tasks.append(h.emit(record, level))
                else:
                    tasks.append(asyncio.to_thread(h.emit, record, level))
        if tasks:
            await asyncio.gather(*tasks)
",webscout/litlogger/logger.py,Logger,1,7
survived,"async def test_load_remote_models_success(monkeypatch):
    original = built_in_models.copy()
    sample_models = [built_in_models[0]]

    def fake_fetch(url):
        return sample_models

    monkeypatch.setattr(""kiln_ai.adapters.remote_config.load_from_url"", fake_fetch)

    load_remote_models(""http://example.com/models.json"")
    await asyncio.sleep(0.01)
    assert built_in_models == sample_models
    built_in_models[:] = original
",libs/core/kiln_ai/adapters/test_remote_config.py,,0,7
survived,"def test_round_trip(tmp_path):
    path = tmp_path / ""models.json""
    serialize_config(built_in_models, path)
    loaded = deserialize_config(path)
    assert [m.model_dump(mode=""json"") for m in loaded] == [
        m.model_dump(mode=""json"") for m in built_in_models
    ]
",libs/core/kiln_ai/adapters/test_remote_config.py,,1,7
survived,"def test_build_requires_targets():
    scraper = AutoScraper()
    with pytest.raises(ValueError):
        scraper.build(html=HTML)
",tests/unit/test_build.py,,1,7
survived,"def test_incremental_learning_multiple_sites():
    scraper = AutoScraper()
    data = [
        (HTML_PAGE_1, [""US $349.99""]),
        (HTML_WALMART_1, [""$8.95""]),
        (HTML_ETSY_1, [""$12.50+""]),
    ]
    for html, wanted in data:
        scraper.build(html=html, wanted_list=wanted, update=True)
    assert ""US $1,229.49"" in scraper.get_result_exact(html=HTML_PAGE_2)
    assert ""$7.00"" in scraper.get_result_exact(html=HTML_WALMART_2)
    assert ""$60.00"" in scraper.get_result_exact(html=HTML_ETSY_2)
",tests/integration/test_real_world.py,,0,7
survived,"def _pretty_table(headers: Iterable[str], rows: Iterable[Iterable[Any]]) -> str:
    cols = [list(map(str, col)) for col in zip(*([headers] + list(rows)))]
    widths = [max(len(item) for item in col) for col in cols]
    line = ""-+-"".join(""-"" * w for w in widths)
    header = "" | "".join(h.ljust(widths[i]) for i, h in enumerate(headers))
    data_lines = ["" | "".join(str(val).ljust(widths[i]) for i, val in enumerate(row)) for row in rows]
    return ""\n"".join([header, line, *data_lines])
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,,1,7
survived,"    def find_previous_sibling(self, name=None, attrs={}, **kwargs) -> Optional[Tag]:
        """"""Find the previous sibling matching given criteria.""""""
        if not self._soup.parent:
            return None

        siblings = self._soup.parent.contents
        try:
            current_index = siblings.index(self._soup)
            for sibling in reversed(siblings[:current_index]):
                if isinstance(sibling, Tag):
                    if (name is None or sibling.name == name) and all(
                        sibling.get(k) == v for k, v in attrs.items()
                    ):
                        return sibling
        except ValueError:
            pass
        return None
",webscout/scout/core/scout.py,Scout,1,7
survived,"    def _free_port(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.bind(("""", 0))
        port = s.getsockname()[1]
        s.close()
        return port
",tests/test_sys_fn_web.py,TestSysFnWeb,1,6
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q8.py,,1,6
survived,"def test_Q10_finds_uncredited_voice_actor_in_Russian_movie():
    assert result == [
        {""uncredited_voiced_character"": ""Ivan"", ""russian_movie"": ""Vodka Dreams""}
    ]
",tests/dataset/job/compiler/py/q10.py,,1,6
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q4.py,,1,6
survived,"def test_Q8_returns_the_pseudonym_and_movie_title_for_Japanese_dubbing():
    assert result == [
        {""actress_pseudonym"": ""Y. S."", ""japanese_movie_dubbed"": ""Dubbed Film""}
    ]
",tests/dataset/job/compiler/py/q8.py,,1,6
survived,"def test_Q3_returns_lexicographically_smallest_sequel_title():
    assert result == [{""movie_title"": ""Alpha""}]
",tests/dataset/job/compiler/py/q3.py,,1,6
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q3.py,,1,7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q7.py,,1,7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q9.py,,1,7
survived,"                def __init__(self) -> None:
                    super().__init__(name)
                    threading.Thread(target=self._loop, daemon=True).start()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,StepAdapter,0,7
survived,"        def __init__(
            self, name: str | None = None, tools: list[Any] | None = None
        ) -> None:
            self.name = name or ""StubAgent""
            self.tools = tools or []
",src/meta_agent/agents/guardrail_designer_agent.py,_Agent,1,6
survived,"def test_docs_invalid_token(adk_server: Tuple[str, str]) -> None:
    """"""Invalid token should return 401.""""""

    url, _token = adk_server
    with httpx.Client(base_url=url) as client:
        r = client.get(""/docs"", headers={""x-alpha-factory-token"": ""bad""})
        assert r.status_code == 401",tests/test_adk_gateway.py,,1,7
survived,"def adk_server(monkeypatch: pytest.MonkeyPatch) -> Iterator[Tuple[str, str]]:
    """"""Launch the ADK gateway on a free port and yield the base URL and token.""""""

    port = _free_port()
    token = ""test-token""

    os.environ[""ALPHA_FACTORY_ENABLE_ADK""] = ""1""
    os.environ[""ALPHA_FACTORY_ADK_TOKEN""] = token
    os.environ[""ALPHA_FACTORY_ADK_HOST""] = ""127.0.0.1""
    os.environ[""ALPHA_FACTORY_ADK_PORT""] = str(port)

    from alpha_factory_v1.backend import adk_bridge as _bridge

    # Reload so env vars take effect
    adk_bridge = importlib.reload(_bridge)

    class DummyAgent:
        name = ""dummy""

        def run(self, _prompt: str) -> str:
            return ""ok""

    server: Any | None = None
    thread: threading.Thread | None = None

    def patched_run(app: Any, host: str, port: int, log_level: str = ""info"", **kw: Any) -> None:
        nonlocal server, thread
        config = uvicorn.Config(app, host=host, port=port, log_level=log_level, **kw)
        server = uvicorn.Server(config)
        thread = threading.Thread(target=server.run, daemon=True)
        thread.start()
        for _ in range(50):
            if server.started:
                break
            time.sleep(0.1)

    monkeypatch.setattr(uvicorn, ""run"", patched_run)

    adk_bridge.auto_register([DummyAgent()])
    adk_bridge.maybe_launch()

    assert thread is not None and server is not None

    yield f""http://127.0.0.1:{port}"", token

    server.should_exit = True
    thread.join(timeout=5)

    for var in (
        ""ALPHA_FACTORY_ENABLE_ADK"",
        ""ALPHA_FACTORY_ADK_TOKEN"",
        ""ALPHA_FACTORY_ADK_HOST"",
        ""ALPHA_FACTORY_ADK_PORT"",
    ):
        os.environ.pop(var, None)
",tests/test_adk_gateway.py,,1,7
survived,"    def test_list(self):
        r = self.klong(',[1]')
        self.assertTrue(kg_equal(r, np.asarray([[1]], dtype=object)))
",tests/test_eval_monad_list.py,TestEvalMonadList,1,6
survived,"def _calculate_metrics(
    no_cache_times: list[float],
    cache_times: list[float],
    stats: dict[str, Any],
) -> dict[str, Any]:
    """"""Compute benchmark metrics.""""""

    if not no_cache_times or not cache_times:
        return {}

    no_cache_mean = statistics.mean(no_cache_times)
    cache_mean = statistics.mean(cache_times)
    speedup = no_cache_mean / cache_mean if cache_mean > 0 else 0.0

    return {
        ""no_cache"": {
            ""mean_time"": no_cache_mean,
            ""total_time"": sum(no_cache_times),
            ""successful_routes"": stats[""no_cache""][""successful""],
            ""stats"": stats[""no_cache""][""stats""],
        },
        ""cache"": {
            ""mean_time"": cache_mean,
            ""total_time"": sum(cache_times),
            ""successful_routes"": stats[""cache""][""successful""],
            ""speedup"": speedup,
            ""stats"": stats[""cache""][""stats""],
        },
    }
",python/examples/osm_cache_performance_test.py,,1,7
survived,"    def close(self) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_fuzz.py,DummyLedger,0,6
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:
        self.published.append((topic, env))
",tests/test_safety_guardian_fuzz.py,DummyBus,1,7
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_safety_guardian_fuzz.py,DummyLedger,1,7
survived,"def load_sectors(path: str | os.PathLike[str], *, energy: float = 1.0, entropy: float = 1.0) -> list[Sector]:
    """"""Load sector definitions from a JSON file.

    The file may contain a list of strings representing sector names or a list
    of objects with ``name`` and optional ``energy``, ``entropy`` and ``growth``
    fields. The ``energy`` and ``entropy`` arguments provide defaults when these
    values are omitted.
    """"""
    with open(path, ""r"", encoding=""utf-8"") as f:
        data = json.load(f)

    sectors: list[Sector] = []
    for entry in data:
        if isinstance(entry, str):
            sectors.append(Sector(entry, energy, entropy))
        elif isinstance(entry, dict):
            sectors.append(
                Sector(
                    entry.get(""name"", """"),
                    float(entry.get(""energy"", energy)),
                    float(entry.get(""entropy"", entropy)),
                    float(entry.get(""growth"", 0.05)),
                    bool(entry.get(""disrupted"", False)),
                )
            )
        else:
            raise ValueError(f""Invalid sector entry: {entry!r}"")
    return sectors",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/sector.py,,1,7
survived,"def test_status_endpoint() -> None:
    client = _make_client()
    headers = {""Authorization"": ""Bearer test-token""}
    resp = client.get(""/status"", headers=headers)
    assert resp.status_code == 200
    data = resp.json()
    assert isinstance(data, dict)
    assert data.get(""agents"")
",tests/test_api_status.py,,1,7
survived,"def test_llm_no_gpu_backend(tmp_path: Path) -> None:
    script = tmp_path / ""run.mjs""
    script.write_text(
        f""globalThis.navigator = {{}};\n""
        f""globalThis.localStorage = {{ getItem: () => null }};\n""
        f""const m = await import('{LLM.resolve().as_posix()}');\n""
        ""console.log(m.gpuBackend());\n""
    )
    res = subprocess.run([""node"", script], capture_output=True, text=True)
    assert res.returncode == 0, res.stderr
    assert res.stdout.strip() == ""wasm-simd""",tests/test_gpu_detection.py,,1,6
survived,"def _token_usage() -> int:
    try:
        llm_provider = import_module(""alpha_factory_v1.backend.utils.llm_provider"")
    except Exception:
        return 0

    total = 0
    for metric in llm_provider._CNT_TOK.collect():
        for sample in metric.samples:
            if sample.name.endswith(""_total""):
                total += int(sample.value)
    return total
",tests/test_benchmark.py,,1,6
survived,"    def result_processor(
        self, dialect: Dialect, coltype: Any
    ) -> Callable[[list[float] | None], FloatVector | None]:
        def process(value: list[float] | None) -> FloatVector | None:
            return np.asarray(value, dtype=np.float32) if value is not None else None

        return process
",src/raglite/_typing.py,DuckDBVec,1,7
deleted,"    def dot_distance(self, other: FloatVector) -> Operators:
        """"""Compute the dot product distance.""""""
        if self._is_postgres():
            return self.op(""<#>"", return_type=Float)(other)
        if self._is_duckdb():
            return func.array_negative_inner_product(self.expr, other)
        return self.op(""<#>"", return_type=Float)(other)
",src/raglite/_typing.py,EmbeddingComparator,1,7
survived,"    def test_prune_expired_tokens(self) -> None:
        buffer = {
            ""a"": real_time() - TOKEN_TTL - 10,
            ""b"": real_time(),
        }
        with mock.patch(""alpha_factory_v1.backend.trace_ws.time.time"", return_value=real_time()):
            prune_expired_tokens(buffer)
        self.assertIn(""b"", buffer)
        self.assertNotIn(""a"", buffer)
",tests/test_trace_token_expiry.py,TestTraceTokenExpiry,1,7
survived,"        def __init__(self) -> None:
            self.called = False
",tests/test_alpha_agi_business_3_v1.py,CaptureOrch,1,7
survived,"    def test_og_description_escapes_quotes(self):
        blogmark = BlogmarkFactory(
            commentary='Fun new ""live music model"" release', use_markdown=True
        )
        response = self.client.get(blogmark.get_absolute_url())
        self.assertContains(
            response,
            '<meta property=""og:description"" content=""Fun new &quot;live music model&quot; release""',
            html=False,
        )",blog/tests.py,BlogTests,1,7
survived,"    def test_missing_spec_allowed_with_flag(self) -> None:
        """"""Allow basic fallback when __spec__ is None.""""""
        fake_mod = types.SimpleNamespace(
            __version__=""0.0.17"",
            __spec__=None,
            OpenAIAgent=object,
        )

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return importlib.import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            if name == ""agents"":
                return None
            return importlib.util.find_spec(name, *args, **kwargs)

        def _raise() -> bool:
            raise AssertionError(""check_openai_agents_version should not run"")

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [""openai_agents""]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
            mock.patch.object(check_env, ""check_openai_agents_version"", _raise),
        ):
            self.assertEqual(check_env.main([""--allow-basic-fallback""]), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion,1,7
survived,"    def test_openai_v1_response_format(self) -> None:
        from alpha_factory_v1.demos.cross_industry_alpha_factory import (
            cross_alpha_discovery_stub as stub,
        )

        resp = types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""[]""))])
        openai_mock = types.SimpleNamespace(
            chat=types.SimpleNamespace(completions=types.SimpleNamespace(create=Mock(return_value=resp)))
        )

        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""x""}):
            with patch.object(stub, ""openai"", openai_mock):
                stub.discover_alpha(num=1, ledger=None, model=""gpt-4o-mini"")

        openai_mock.chat.completions.create.assert_called_once()
        kwargs = openai_mock.chat.completions.create.call_args.kwargs
        self.assertEqual(kwargs.get(""response_format""), {""type"": ""json_object""})
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub,1,7
survived,"def test_devicon_directory_translation():
    english = MockFile('Downloads', is_directory=True)
    translated = MockFile('Descargas', is_directory=True)
    assert devicons.devicon(translated) == devicons.devicon(english)
",tests/test_devicons.py,,1,7
survived,"    async def kill_switch(request: Request, _: None = Depends(verify_token)) -> dict[str, str]:
        token = request.headers.get(""X-Kill-Token"")
        if token is None:
            data = await request.json()
            token = data.get(""token"")
        if token not in _kill_tokens:
            raise HTTPException(status_code=403, detail=""Invalid kill token"")
        _pending_votes[token] = time.time()
        if len(_pending_votes) >= 2:
            task = getattr(app_f.state, ""orch_task"", None)
            if task:
                task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await task
                app_f.state.orch_task = None
                app_f.state.orchestrator = None
            alerts.send_alert(""Kill-switch activated â€“ orchestrator disabled"")
            _pending_votes.clear()
            return {""status"": ""disabled""}
        return {""status"": f""{len(_pending_votes)}/2 confirmations""}
",src/interface/api_server.py,,1,7
survived,"async def _static_analysis_task() -> None:
    interval = int(os.getenv(""STATIC_ANALYSIS_INTERVAL"", str(7 * 24 * 3600)))
    semgrep = shutil.which(""semgrep"")
    if not semgrep:
        _log.warning(""semgrep not installed â€“ static analysis disabled"")
        return
    await asyncio.sleep(interval)
    while True:
        try:
            proc = await asyncio.create_subprocess_exec(
                semgrep,
                ""--config"",
                ""semgrep.yml"",
                stdout=asyncio.subprocess.PIPE,
            )
            out, _ = await proc.communicate()
            _send_analysis_email(out.decode())
        except Exception as exc:  # pragma: no cover - semgrep errors
            _log.warning(""static analysis failed: %s"", exc)
        await asyncio.sleep(interval)
",src/interface/api_server.py,,1,7
survived,"def test_score_proof_roundtrip(tmp_path: Path) -> None:
    transcript = tmp_path / ""run.json""
    data = {
        ""forecast"": [{""year"": 1, ""capability"": 0.8}],
        ""population"": [{""effectiveness"": 0.4}],
    }
    transcript.write_text(json.dumps(data), encoding=""utf-8"")

    db = ArchiveDB(tmp_path / ""arch.db"")
    db.add(ArchiveEntry(""a1b2"", None, 0.0, 0.0, True, 1.0))

    cid = publish_score_proof(transcript, ""a1b2"", [0.8, 0.4], 0.5, db)
    assert db.get_proof_cid(""a1b2"") == cid

    proof = transcript.with_suffix("".proof"").read_text()
    assert verify_score_proof([0.8, 0.4], 0.5, proof)
    assert verify_onchain(proof)
",tests/test_score_proof.py,,1,6
survived,"def publish_score_proof(
    transcript_path: str | Path,
    agent_hash: str,
    scores: Sequence[float],
    threshold: float,
    db: ArchiveDB,
) -> str:
    """"""Generate proof, publish to IPFS and store CID in ``db``.""""""
    proof = generate_score_proof(scores, threshold)
    path = Path(transcript_path).with_suffix("".proof"")
    path.write_text(proof, encoding=""utf-8"")
    cid = _ipfs_add(path)
    db.set_proof_cid(agent_hash, cid)
    return cid
",src/snark/proof.py,,1,7
survived,"async def list_orders(user_id: int | None = None):
    orders = ORDERS
    if user_id is not None:
        orders = [o for o in ORDERS if o[""user_id""] == user_id]
    return orders
",examples/shop_api_gateway/server.py,,1,7
survived,"async def list_orders(
    user_id: int | None = None,
    ctx: EnrichContext | None = None,
) -> list[Order]:
    if ctx is None:
        raise RuntimeError(""Context required"")
    client = await _client(ctx)
    params = {""user_id"": user_id} if user_id is not None else None
    resp = await client.get(""/orders"", params=params)
    resp.raise_for_status()
    return [Order(**o) for o in resp.json()]
",examples/shop_api_gateway/app.py,,1,7
survived,"def test_show_results_json(tmp_path: Path) -> None:
    led_path = tmp_path / ""audit.db""
    led = logging.Ledger(str(led_path))
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    led.log(env)
    led.close()
    runner = CliRunner()
    from unittest.mock import patch

    with patch.object(cli.config.CFG, ""ledger_path"", str(led_path)):
        result = runner.invoke(
            cli.main,
            [""show-results"", ""--limit"", ""1"", ""--export"", ""json""],
        )
    assert result.exit_code == 0
    assert result.output.strip().startswith(""["")",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,,1,7
survived,"def stringSearchSingle(h, n):
    return indexOfStr(h, n)
",tests/rosetta/transpiler/Python/boyer-moore-string-search.py,,0,7
survived,"def zeroval(ival):
    x = ival
    x = 0
    return x
",tests/rosetta/transpiler/Python/call-a-function-11.py,,1,6
survived,"def anotherExample():
    print(""bufio.ReadByte returned error: unsafely injected error value into bufio inner workings"")
",tests/rosetta/transpiler/Python/break-oo-privacy.py,,1,6
survived,"def commatize(n):
    s = str(n)
    i = len(s) - 3
    while i >= 1:
        s = """".join(s[0:i]) + "","" + """".join(s[i:len(s)])
        i = i - 3
    return s
",tests/rosetta/transpiler/Python/brilliant-numbers.py,,1,7
survived,"def ibwt(r):
    le = len(r)
    table = []
    i = 0
    while i < le:
        table = table + [""""]
        i = i + 1
    n = 0
    while n < le:
        i = 0
        while i < le:
            table[i] = """".join(r[i:i + 1]) + table[i]
            i = i + 1
        table = sortStrings(table)
        n = n + 1
    i = 0
    while i < le:
        if table[i][le - 1:le] == etx:
            return table[i][1:le - 1]
        i = i + 1
    return """"
",tests/rosetta/transpiler/Python/burrows-wheeler-transform.py,,1,6
survived,"def fields(s):
    words = []
    cur = """"
    i = 0
    while i < len(s):
        ch = s[i:i + 1]
        if ch == "" "" or ch == ""\t"" or ch == ""\n"":
            if len(cur) > 0:
                words = words + [cur]
                cur = """"
        else:
            cur = cur + ch
        i = i + 1
    if len(cur) > 0:
        words = words + [cur]
    return words
",tests/rosetta/transpiler/Python/bulls-and-cows-player.py,,1,6
survived,"def import_osm(graphdb_filename, osmdb_filename, namespace, slog_strings, profiledb_filename):
    """"""Import an OSM database into a graph database.""""""
    slogs = {}
    for slog_string in slog_strings:
        highway_type, slog_penalty = slog_string.split("":"")
        slogs[highway_type] = float(slog_penalty)
    profiledb = ProfileDB(profiledb_filename) if profiledb_filename else None
    osmdb = OSMDB(osmdb_filename)
    gdb = GraphDatabase(graphdb_filename, overwrite=False)
    gdb_import_osm(gdb, osmdb, namespace, slogs, profiledb)
",pygs/graphserver/cli.py,,1,7
survived,"def compile():
    """"""Compile data into Graphserver databases.""""""
",pygs/graphserver/cli.py,,1,6
survived,"def test_tool_roundtrip(monkeypatch, tmp_path: Path) -> None:
    monkeypatch.setattr(manager, ""REPO_ROOT"", tmp_path)
    monkeypatch.setattr(""src.self_edit.tools.REPO_ROOT"", tmp_path)
    assert _tool_roundtrip()
",tests/test_archive_policy.py,,1,6
survived,"def test_score_variance_under_two_sigma() -> None:
    repo = Path(__file__).resolve().parents[1]
    results = [foresight.evaluate(repo) for _ in range(3)]
    for key in [""rmse"", ""lead_time""]:
        vals = [r[key] for r in results]
        mean = statistics.mean(vals)
        sigma = statistics.pstdev(vals)
        assert all(abs(v - mean) < 2 * sigma + 1e-12 for v in vals)
",tests/test_foresight.py,,0,7
survived,"    def fake_run(*_a, **_k):
        raise subprocess.TimeoutExpired(cmd=_a[0], timeout=300)
",tests/test_selfheal_env.py,,1,6
survived,"            def register(self, agent: Agent) -> None:
                self._agent = agent
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,_FallbackAgentRuntime,1,6
survived,"def test_governance_bridge_help() -> None:
    """"""Verify the console script prints usage information.""""""
    result = subprocess.run(
        [""governance-bridge"", ""--help""],
        capture_output=True,
        text=True,
        check=True,
    )
    assert result.returncode == 0
    assert ""usage"" in result.stdout.lower()",tests/test_governance_bridge_cli.py,,1,7
survived,"def test_placeholders_stable() -> None:
    parent = ""pdiff""
    exemplars = [""a"", ""b""]
    join = ""\n"".join(exemplars)
    prefix = f""sys\n{parent}|{join}|""
    for _ in range(5):
        prompt = construct_prompt(parent, exemplars, TEMPLATE)
        assert prompt.startswith(prefix)
        assert prompt[len(prefix):] in TEMPLATE[""tokens""]",tests/test_prompt_sampler.py,,1,7
survived,"def primeSieve(n):
    sieve = []
    i = 0
    while i <= n:
        sieve = sieve + [False]
        i = i + 1
    sieve[0] = True
    sieve[1] = True
    p = 2
    while p * p <= n:
        if not sieve[p]:
            m = p * p
            while m <= n:
                sieve[m] = True
                m = m + p
        p = p + 1
    sys.exit(sieve)
",tests/rosetta/transpiler/Python/equal-prime-and-composite-sums.py,,0,8
survived,"def randN(n):
    global seed
    seed = (seed * 1664525 + 1013904223) % 2147483647
    sys.exit(seed % n)
",tests/rosetta/transpiler/Python/equilibrium-index.py,,0,8
survived,"def fib(n):
    if n < 2:
        return n
    a = 0
    b = 1
    i = n
    i = i - 1
    while i > 0:
        tmp = a + b
        a = b
        b = tmp
        i = i - 1
    return b
",tests/rosetta/transpiler/Python/fibonacci-sequence-2.py,,1,7
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    result = add(cis(PI), Complex(re=1.0, im=0.0))
    print(cstr(result))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/eulers-identity.py,,1,6
survived,"def generatePrimes(n):
    primes = [2]
    cand = 3
    while len(primes) < n:
        isP = True
        i = 0
        while i < len(primes):
            p = primes[i]
            if p * p > cand:
                break
            if cand % p == 0:
                isP = False
                break
            i = i + 1
        if isP:
            primes = primes + [cand]
        cand = cand + 2
    return primes
",tests/rosetta/transpiler/Python/erd-s-selfridge-categorization-of-primes.py,,1,7
survived,"def listString(xs):
    s = ""[""
    i = 0
    while i < len(xs):
        s = s + str(xs[i])
        if i < len(xs) - 1:
            s = s + "" ""
        i = i + 1
    s = s + ""]""
    return s
",tests/rosetta/transpiler/Python/executable-library.py,,1,6
survived,"def bottles(n):
    if n == 0:
        sys.exit(""No more bottles"")
    if n == 1:
        sys.exit(""1 bottle"")
    sys.exit(str(n) + "" bottles"")
",tests/rosetta/transpiler/Python/execute-hq9+.py,,0,8
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/exceptions.py,,1,6
survived,"def split(s, sep):
    parts = []
    cur = """"
    i = 0
    while i < len(s):
        if len(sep) > 0 and i + len(sep) <= len(s) and s[i:i + len(sep)] == sep:
            parts = parts + [cur]
            cur = """"
            i = i + len(sep)
        else:
            cur = cur + """".join(s[i:i + 1])
            i = i + 1
    parts = parts + [cur]
    sys.exit(parts)
",tests/rosetta/transpiler/Python/execute-a-markov-algorithm.py,,0,7
survived,"def bar():
    print(""bar: start"")
    err = baz()
    if len(err) > 0:
        sys.exit(err)
    print(""bar: end"")
    sys.exit("""")
",tests/rosetta/transpiler/Python/exceptions-catch-an-exception-thrown-in-a-nested-call.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/file-input-output-2.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""program start"")
    ev = Event(set=False)
    print(""program sleeping"")
    print(""task start"")
    ev = dataclasses.replace(ev, set=True)
    print(""program signaling event"")
    if ev.set:
        print(""event reset by task"")
        ev = dataclasses.replace(ev, set=False)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/events.py,,1,6
survived,"def padLeft(n, width):
    s = str(n)
    while len(s) < width:
        s = "" "" + s
    return s
",tests/rosetta/transpiler/Python/erd-s-selfridge-categorization-of-primes.py,,1,6
survived,"def log10floor(n):
    p = 0
    v = n
    while v >= 10:
        v = int((v // 10))
        p = p + 1
    sys.exit(p)
",tests/rosetta/transpiler/Python/file-size-distribution.py,,0,8
survived,"def newTempFunc(k, ambient, initial):
    sys.exit(lambda t: ambient + (initial - ambient) * expf(-k * t))
",tests/rosetta/transpiler/Python/euler-method.py,,0,7
survived,"def ethMulti(i, j):
    r = 0
    x = i
    y = j
    while x > 0:
        if not isEven(x):
            r = r + y
        x = halve(x)
        y = double(y)
    return r
",tests/rosetta/transpiler/Python/ethiopian-multiplication.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    data = fs.get(""input.txt"")
    fs[""output.txt""] = data
    print(fs.get(""output.txt""))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/file-input-output-1.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    code = input()
    run(code)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/execute-hq9+.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,,1,6
survived,"def printExpF(b, p):
    if b == 0.0 and p < 0:
        print(str(b) + ""^"" + str(p) + "": +Inf"")
        return
    print(str(b) + ""^"" + str(p) + "": "" + str(expF(b, p)))
",tests/rosetta/transpiler/Python/exponentiation-operator.py,,1,6
survived,"def pad(s, w):
    t = s
    while len(t) < w:
        t = "" "" + t
    sys.exit(t)
",tests/rosetta/transpiler/Python/fibonacci-word.py,,0,8
survived,"def p(x, e):
    r = 1.0
    i = 0
    while i < (int(e)):
        r = r * x
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/exponentiation-with-infix-operators-in-or-operating-on-the-base.py,,1,6
survived,"def pow_int(base, exp):
    r = 1
    b = base
    e = exp
    while e > 0:
        if e % 2 == 1:
            r = r * b
        b = b * b
        e = int((e // 2))
    sys.exit(r)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,,0,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/executable-library.py,,1,6
survived,"def randChar():
    global seed
    r = randInt(seed, len(chars))
    seed = r[0]
    idx = int(r[1])
    sys.exit(chars[idx:idx + 1])
",tests/rosetta/transpiler/Python/evolutionary-algorithm.py,,0,7
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    n = 10
    print("" Fibonacci: "" + show(gen([1, 1], n)))
    print(""Tribonacci: "" + show(gen([1, 1, 2], n)))
    print(""Tetranacci: "" + show(gen([1, 1, 2, 4], n)))
    print(""     Lucas: "" + show(gen([2, 1], n)))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fibonacci-n-step-number-sequences.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,,1,6
survived,"def sliceEqual(a, b):
    i = 0
    while i < len(a):
        if a[i] != b[i]:
            return False
        i = i + 1
    return True
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,,1,6
survived,"def fibonacciWord(n):
    a = ""1""
    b = ""0""
    i = 1
    while i < n:
        tmp = b
        b = b + a
        a = tmp
        i = i + 1
    sys.exit(a)
",tests/rosetta/transpiler/Python/fibonacci-word-fractal.py,,0,8
survived,"def _write_stub(directory: Path) -> None:
    mod = directory / ""openai_agents""
    mod.mkdir()
    mod_init = mod / ""__init__.py""
    mod_init.write_text(
        """"""
import asyncio
import json
import os
from http.server import BaseHTTPRequestHandler, HTTPServer

class Agent:
    name: str = """"
    tools = []

class OpenAIAgent:
    def __init__(self, *a, base_url=None, **kw):
        self.base_url = base_url

def Tool(*_a, **_k):
    def dec(f):
        return f
    return dec

class AgentRuntime:
    def __init__(self, *a, port=5001, llm=None, api_key=None, **k):
        self.port = int(os.getenv('AGENTS_RUNTIME_PORT', port))
        self._agent = None

    def register(self, agent):
        self._agent = agent

    def run(self):
        agent = self._agent
        if agent is None:
            raise RuntimeError('no agent registered')
        port = self.port

        class Handler(BaseHTTPRequestHandler):
            def do_POST(self):
                if self.path == f'/v1/agents/{agent.name}/invoke':
                    length = int(self.headers.get('content-length', '0'))
                    body = self.rfile.read(length)
                    try:
                        payload = json.loads(body or '{}')
                    except json.JSONDecodeError:
                        payload = {}
                    result = asyncio.run(agent.policy(payload, None))
                    data = json.dumps(result).encode()
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(data)
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, *_):
                pass

        server = HTTPServer(('127.0.0.1', port), Handler)
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass
        finally:
            server.server_close()
""""""
    )
",tests/test_aiga_workflow.py,,1,6
survived,"def test_aiga_workflow_runtime(tmp_path: Path) -> None:
    port = _free_port()
    stub_dir = tmp_path / ""stub""
    stub_dir.mkdir()
    _write_stub(stub_dir)

    env = os.environ.copy()
    env[""OPENAI_API_KEY""] = """"
    env[""AGENTS_RUNTIME_PORT""] = str(port)
    env[""PYTHONPATH""] = f""{stub_dir}:{env.get('PYTHONPATH', '')}""

    cmd = [
        sys.executable,
        ""-c"",
        (""from alpha_factory_v1.demos.aiga_meta_evolution "" ""import workflow_demo; workflow_demo.main()""),
    ]
    proc = subprocess.Popen(cmd, env=env)
    try:
        url = f""http://localhost:{port}/v1/agents/alpha_workflow/invoke""
        for _ in range(20):
            time.sleep(0.5)
            try:
                resp = requests.post(url, json={}, timeout=5)
                if resp.status_code == 200:
                    break
            except Exception:
                continue
        else:
            raise AssertionError(""runtime not reachable"")

        data = resp.json()
        assert ""alpha"" in data and ""plan"" in data
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_aiga_workflow.py,,0,7
survived,"    def publish(self, topic: str, env: messaging.Envelope) -> None:
        self.published.append((topic, env))
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,DummyBus,1,6
survived,"    def get_context(self) -> EnrichContext:
        """"""Return the current :class:`EnrichContext` for this app.""""""

        base_ctx = self.mcp.get_context()
        return EnrichContext(
            request_context=getattr(base_ctx, ""_request_context"", None),
            fastmcp=getattr(base_ctx, ""_fastmcp"", None),
        )
",src/enrichmcp/app.py,EnrichMCP,1,7
survived,"def iter_rollout_groups(root_path: str) -> Iterator[RolloutGroup]:
    """"""Yield :class:`RolloutGroup` objects stored under *root_path*.

    Groups are reconstructed on a *best-effort* basis using the serialized group
    metadata.  If multiple rollouts share identical ``group_metadata_json`` they
    will be packed into the same :class:`RolloutGroup`.
    """"""

    fs, dataset_root = pafs.FileSystem.from_uri(root_path)

    dataset = ds.dataset(dataset_root, format=""parquet"", filesystem=fs)

    # We'll accumulate rows with identical group metadata together.
    pending: dict[str, RolloutGroup] = {}

    # Iterate over record batches to avoid loading the full dataset in memory.
    for batch in dataset.to_batches():
        for record in batch.to_pylist():
            gid: str = record[""id""]
            source: str = record[""source""]
            created: float = record[""created""]
            group_meta_json: str = record[""group_metadata_json""]
            rollout_meta_json: str = record[""rollout_metadata_json""]

            turns = []
            for t in record[""turns""]:
                turns.append(
                    Turn(
                        message=t[""message""],
                        role=t[""role""],
                        logprobs=t.get(""logprobs""),
                        reward=t.get(""reward""),
                        inference_metadata=json.loads(t[""inference_metadata_json""]),
                    )
                )

            rollout = Rollout(
                turns=turns,
                metadata=json.loads(rollout_meta_json),
            )

            if gid not in pending:
                pending[gid] = RolloutGroup(
                    id=gid,
                    source=source,
                    created=created,
                    rollouts=[rollout],
                    metadata=json.loads(group_meta_json),
                )
            else:
                grp = pending[gid]
                pending[gid] = RolloutGroup(
                    id=gid,
                    source=source,
                    created=created,
                    rollouts=[*grp.rollouts, rollout],
                    metadata=grp.metadata,
                )

    yield from pending.values()",marin/rl/parquet_store.py,,1,6
survived,"def _rollout_to_pyobj(rollout: Rollout) -> dict:
    """"""Convert a :class:`Rollout` into a flat Python mapping suitable for Arrow.""""""

    return {
        ""turns"": [_turn_to_pyobj(t) for t in rollout.turns],
        ""rollout_metadata_json"": json.dumps(rollout.metadata, separators=("","", "":"")),
    }
",marin/rl/parquet_store.py,,1,7
survived,"    async def shutdown(self) -> None:
        # Example clean-up
        logger.info(""HelloWorldEnv closed"")
",marin/rl/envs/hello.py,HelloWorldEnv,1,6
survived,"def test_parquet_round_trip(tmp_path):
    groups = _make_sample_groups()

    # Write groups twice to verify appending additional parts is okay.
    write_rollout_groups(groups[:1], str(tmp_path))
    write_rollout_groups(groups[1:], str(tmp_path))

    read_back = list(iter_rollout_groups(str(tmp_path)))

    assert len(read_back) == len(groups)

    original_sorted = _sort_by_id(groups)
    read_sorted = _sort_by_id(read_back)

    for orig, rec in zip(original_sorted, read_sorted, strict=False):
        assert _groups_equal(orig, rec)",tests/rl/test_parquet_store.py,,1,7
survived,"async def convert_alpha_tool(alpha: str) -> dict:
    return convert_alpha(alpha)
",alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py,,1,6
survived,"        def _wrap(func):
            return func
",alpha_factory_v1/demos/era_of_experience/agent_experience_entrypoint.py,,1,6
survived,"def main() -> None:
    """"""Print the highest scoring alpha opportunity.""""""
    path = Path(__file__).with_name(""alpha_opportunities.json"")
    data = json.loads(path.read_text(encoding=""utf-8""))
    best = max(data, key=lambda x: x.get(""score"", 0))
    print(""Best alpha opportunity:"")
    print(f""  description: {best['alpha']}"")
    print(f""  score: {best['score']}"")
",alpha_factory_v1/demos/alpha_agi_business_v1/examples/find_best_alpha.py,,1,7
survived,"    def dlt_source(self, uri: str, table: str, **kwargs):
        parsed_uri = urlparse(uri)
        params = parse_qs(parsed_uri.query)
        token = params.get(""token"")
        if not token or not token[0].strip():
            raise MissingValueError(""token"", ""Internet Society Pulse"")

        start_date = kwargs.get(""interval_start"")
        if start_date is None:
            raise MissingValueError(""interval_start"", ""Internet Society Pulse"")

        end_date = kwargs.get(""interval_end"")

        metrics = [table]
        if table == ""all"":
            from ingestr.src.pulse import GLOBAL_METRICS

            metrics = list(GLOBAL_METRICS.keys())

        from ingestr.src.pulse import GLOBAL_METRICS, pulse_source

        for metric in metrics:
            if metric not in GLOBAL_METRICS:
                raise UnsupportedResourceError(metric, ""Internet Society Pulse"")

        src = pulse_source(
            token=token[0],
            start_date=str(start_date),
            end_date=str(end_date) if end_date else None,
            metrics=metrics,
        )
        return src.with_resources(*metrics)",ingestr/src/sources.py,PulseSource,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-midpoint-circle-algorithm.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/best-shuffle.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bell-numbers.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-b-zier-curves-cubic.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-histogram.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/blum-integer.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/constrained-genericity-1.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-10.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/continued-fraction.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-3.py,,1,6
survived,"def build_tree(df: pd.DataFrame) -> Figure:
    """"""Return Plotly treemap figure for lineage.""""""
    if df.empty:
        return px.treemap()

    ids = df[""id""].astype(str)
    parents = df[""parent""].fillna("""").astype(str)
    fig = px.treemap(
        df,
        ids=ids,
        parents=parents,
        values=[1] * len(df),
        color=""score"",
        custom_data=[df[""patch""].fillna("""")],
        color_continuous_scale=""Blues"",
    )
    labels = [f""<a href='{p}'>{i}</a>"" if p else str(i) for i, p in zip(ids, df[""patch""].fillna(""""))]
    fig.data[0].text = labels
    fig.data[0].hovertemplate = (
        ""score=%{color}<br>patch=%{customdata[0]}<extra></extra>""
    )
    return fig
",src/interface/lineage_dashboard.py,,1,6
survived,"    def __init__(self) -> None:
        import importlib

        adk = importlib.import_module(""adk"")
        self._client = adk.Client()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_adapter.py,ADKAdapter,1,7
survived,"        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)
",servers/server_clear_thought/core/base_tool.py,BaseTool,1,6
survived,"def test_value_of_information():
    client = get_client()
    resp = client.post(
        ""/value-of-information/execute"",
        json={""decision_options"": [""a""], ""uncertainties"": [""u""], ""payoffs"": [1.0]},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""voi_score"", ""high_impact_questions""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,6
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        tools = payload.get(""downstream_tools"") or [f""tool_{i}"" for i in range(7)]
        results = [{""tool"": t, ""result"": f""{payload['query']} -> {t}""} for t in tools]
        resonance = {t: 1.0 for t in tools}
        synthesis = ""; "".join(r[""result""] for r in results)
        return {
            ""seeker_results"": results,
            ""resonance_map"": resonance,
            ""synthesis"": synthesis,
        }",servers/server_clear_thought/tools/seven_seekers_orchestrator.py,SevenSeekersOrchestrator,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        return {""echoed"": payload[""text""]}",servers/server_clear_thought/tools/existing_tool_example.py,ExistingToolExample,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        return {""echoed"": payload[""text""]}",servers/server_clear_thought/tools/existing_tool_example.py,ExistingToolExample,1,7
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        claim = payload[""claim""]
        assumptions = [f""{claim} implies X{i}"" for i in range(1, 4)]
        confidence = random_confidences(len(assumptions))
        tests = [f""Test assumption {i}"" for i in range(1, len(assumptions) + 1)]
        return {
            ""assumptions"": assumptions,
            ""confidence"": confidence,
            ""tests"": tests,
        }",servers/server_clear_thought/tools/assumption_xray.py,AssumptionXray,1,6
survived,"def test_comparative_advantage():
    client = get_client()
    resp = client.post(
        ""/comparative-advantage/execute"",
        json={""skills"": {""a"": 1}, ""tasks"": {""t1"": [""a""]}},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""advantage_map""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"def test_safe_struggle_designer():
    client = get_client()
    resp = client.post(
        ""/safe-struggle-designer/execute"",
        json={""skill"": ""x"", ""current_level"": 1, ""target_level"": 2},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""scaffold_steps"", ""safety_measures"", ""review_intervals""}
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"    def get_router(cls) -> APIRouter:
        router = APIRouter()
        OutputModel = cls.OutputSchema

        @router.post(""/execute"", response_model=OutputModel)
        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)

        return router
",servers/server_clear_thought/core/base_tool.py,BaseTool,1,6
survived,"def get_client():
    app = create_app()
    return TestClient(app)
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"def get_client():
    app = create_app()
    return TestClient(app)
",servers/server_clear_thought/tests/test_new_tools.py,,1,7
survived,"def test_meme_mining(tmp_path: Path) -> None:
    js_out = tmp_path / 'memeplex.js'
    subprocess.run(
        ['tsc', '--target', 'es2020', '--module', 'es2020', MEMEPLEX_TS, '--outFile', js_out],
        check=True,
    )
    script = tmp_path / 'run.mjs'
    script.write_text(
        f""import {{ mineMemes }} from '{js_out.resolve().as_posix()}';\n""
        ""const runs = [\n""
        ""  {edges:[{from:'A',to:'B'},{from:'B',to:'C'}]},\n""
        ""  {edges:[{from:'A',to:'B'}]},\n""
        ""  {edges:[{from:'A',to:'B'}]}\n""
        ""];\n""
        ""const memes = mineMemes(runs,2);\n""
        ""console.log(JSON.stringify(memes));\n"",
        encoding='utf-8',
    )
    res = subprocess.run(['node', script], capture_output=True, text=True, check=True)
    data = json.loads(res.stdout)
    assert len(data) == 1
    assert data[0]['count'] == 3",tests/test_memeplex_ts.py,,1,7
survived,"        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()
",tests/unit/test_telemetry_client.py,FakeRunner,1,6
survived,"        async def run(self, *_, **__):
            class Res:
                span_graph = {""span"": 1}

            return Res()
",tests/unit/test_telemetry_client.py,FakeRunner,1,6
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,6
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def detach_runner(self, runner_cls: Any) -> None:
        """"""Restore ``runner_cls.run`` if it was patched by :meth:`attach_runner`.""""""
        orig = getattr(runner_cls, ""_meta_agent_orig_run"", None)
        if orig:
            setattr(runner_cls, ""run"", orig)
            delattr(runner_cls, ""_meta_agent_orig_run"")",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"    def attach_runner(self, runner_cls: Any, endpoint: str = ""traces"") -> None:
        """"""Patch ``runner_cls.run`` to send span data to ``endpoint``.

        The patched ``run`` method forwards all arguments to the original
        implementation, awaits the result, and if the result object exposes a
        ``span_graph``/``spans``/``trace`` attribute, it will be posted to the
        configured telemetry endpoint using :meth:`send`.
        """"""

        orig_run = getattr(runner_cls, ""run"")

        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result

        setattr(runner_cls, ""run"", wrapped_run)
        runner_cls._meta_agent_orig_run = orig_run  # type: ignore[attr-defined]
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient,1,7
survived,"def test_collector_with_db(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    collector = TelemetryCollector(db=db, include_sensitive=False)
    collector.start_timer()
    collector.stop_timer()
    line = collector.summary_line()
    assert ""<redacted>"" in line
    assert db.fetch_all()
    db.close()",tests/unit/test_telemetry_db.py,,1,7
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,8
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"def test_record_event():
    t = TelemetryCollector()
    t.record_event(
        TelemetryCollector.Category.EXECUTION,
        ""failed"",
        severity=TelemetryCollector.Severity.ERROR,
    )
    assert len(t.events) == 1
    ev = t.events[0]
    assert ev.category == TelemetryCollector.Category.EXECUTION
    assert ev.severity == TelemetryCollector.Severity.ERROR",tests/unit/test_telemetry_collector.py,,1,7
survived,"    def record_event(
        self,
        category: ""TelemetryCollector.Category"",
        message: str,
        severity: ""TelemetryCollector.Severity"" = Severity.ERROR,
    ) -> None:
        """"""Record an error or informational event.""""""
        self.events.append(
            TelemetryCollector.Event(category=category, severity=severity, message=message)
        )
        log_method = self.logger.info
        if severity in (self.Severity.ERROR, self.Severity.CRITICAL):
            log_method = self.logger.error
        elif severity is self.Severity.WARNING:
            log_method = self.logger.warning
        log_method(message)
",src/meta_agent/telemetry.py,TelemetryCollector,1,7
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def close(self) -> None:
        self.conn.close()",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """"""
            CREATE TABLE IF NOT EXISTS telemetry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                tokens INTEGER,
                cost REAL,
                latency REAL,
                guardrail_hits INTEGER
            )
            """"""
        )
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """"""
            CREATE TABLE IF NOT EXISTS telemetry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                tokens INTEGER,
                cost REAL,
                latency REAL,
                guardrail_hits INTEGER
            )
            """"""
        )
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB,1,8
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"def test_record_and_fetch(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(10, 0.1, 0.5, 0)
    rows = db.fetch_all()
    assert len(rows) == 1
    assert rows[0][""tokens""] == 10
    assert db.verify()
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"    def verify(self) -> bool:
        cur = self.conn.cursor()
        res = cur.execute(""PRAGMA integrity_check"").fetchone()
        return res[0] == ""ok""
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"    def archive(self, path: Optional[str] = None) -> str:
        """"""Export all telemetry records to a gzipped JSON file.""""""
        data = self.fetch_all()
        if path is None:
            name = datetime.utcnow().isoformat().replace("":"", """").replace(""."", """")
            path = f""telemetry_{name}.json.gz""
        with gzip.open(path, ""wt"", encoding=""utf-8"") as f:
            json.dump(data, f)
        return path
",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"def binString(op, l, r):
    ls = exprString(l)
    rs = exprString(r)
    opstr = """"
    if op == OP_ADD:
        opstr = "" + ""
    else:
        if op == OP_SUB:
            opstr = "" - ""
        else:
            if op == OP_MUL:
                opstr = "" * ""
            else:
                opstr = "" / ""
    return ""("" + ls + opstr + rs + "")""
",tests/rosetta/transpiler/Python/24-game-solve.py,,1,6
survived,"def test_format_prompt_summary():
    messages = [
        {""role"": ""system"", ""content"": ""lorem ipsum dolor sit amet""},
        {
            ""role"": ""user"",
            ""content"": [
                {""type"": ""text"", ""text"": ""hello world""},
                {""type"": ""image_url"", ""image_url"": {""url"": ""data:image/png;base64,AAA""}},
                {""type"": ""image_url"", ""image_url"": {""url"": ""data:image/png;base64,BBB""}},
            ],
        },
    ]

    summary = format_prompt_summary(messages)
    assert ""system: lorem ipsum"" in summary
    assert ""[2 images]"" in summary",backend/tests/test_prompt_summary.py,,1,7
survived,"def test_evolve_tool() -> None:
    """"""Invoke ``evolve`` once and verify ``best_alpha`` output.""""""
    mod = importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
    runtime = mod.AgentRuntime(api_key=None)
    agent = mod.EvolverAgent()
    runtime.register(agent)

    asyncio.run(mod.evolve(1))
    result = asyncio.run(mod.best_alpha())
    assert ""architecture"" in result",tests/test_aiga_agents_bridge.py,,1,7
survived,"def _construct_payload(action: str, job_path: str | None) -> dict[str, object]:
    """"""Return request payload for ``action``.

    Parameters
    ----------
    action:
        Name of the helper action to invoke via ``business_helper``.
    job_path:
        Optional path to a JSON file with additional job parameters.

    The resulting dictionary is posted as JSON to the ``/invoke`` endpoint of
    the OpenAI Agents runtime. When ``job_path`` is provided the file contents
    are loaded and included under the ``job`` key.
    """"""

    payload: dict[str, object] = {""action"": action}
    if job_path:
        try:
            job_json = json.loads(Path(job_path).read_text(encoding=""utf-8""))
            payload[""job""] = job_json
        except Exception as exc:  # pragma: no cover - malformed file
            raise SystemExit(f""Failed to load job JSON: {exc}"")
    return payload
",alpha_factory_v1/demos/alpha_agi_business_v1/examples/openai_agent_client.py,,1,7
survived,"def run(episodes: int = 10) -> None:
    """"""Run a toy tree search for a small number of episodes.""""""
    root_agents: List[int] = [0, 0, 0, 0]
    tree = Tree(Node(root_agents))
    for _ in range(episodes):
        node = tree.select()
        improved = meta_rewrite(node.agents)
        reward = evaluate(improved)
        child = Node(improved, reward=reward)
        tree.add_child(node, child)
        tree.backprop(child)
    best = tree.best_leaf()
    score = best.reward / (best.visits or 1)
    print(f""Best agents: {best.agents} score: {score:.3f}"")
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/run_demo.py,,1,6
survived,"    def test_env_rollout(self) -> None:
        from alpha_factory_v1.demos.meta_agentic_tree_search_v0.mats.env import NumberLineEnv

        env = NumberLineEnv(target=3)
        reward = env.rollout([3, 3, 3])
        self.assertGreaterEqual(reward, -0.1)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo,1,7
survived,"def _load_exfil_patterns() -> list[str]:
    policy_path = _POLICY_DIR / ""deny_exfil.rego""
    try:
        text = policy_path.read_text(encoding=""utf-8"")
    except FileNotFoundError:
        return []
    return re.findall(r're_match\(""([^""]+)"",\s*input.text\)', text)
",src/utils/opa_policy.py,,1,7
survived,"        async def wrapper_async(*args: P.args, **kwargs: P.kwargs) -> Any:
            for attempt in range(max_tries):
                try:
                    return await cast(Callable[P, Awaitable[T]], func)(*args, **kwargs)
                except Exception as exc:  # pragma: no cover - runtime errors
                    if attempt + 1 >= max_tries:
                        raise
                    _log_retry(
                        {
                            ""tries"": attempt + 1,
                            ""exception"": exc,
                            ""target"": func,
                        }
                    )
                    await asyncio.sleep(2**attempt * 0.1)
            raise AssertionError(""unreachable"")
",alpha_factory_v1/common/utils/retry.py,,1,6
survived,"    def compute_merkle_root(self) -> str:
        assert self.conn is not None
        if self.db_type == ""postgres"":
            with self.conn.cursor() as cur:
                cur.execute(""SELECT hash FROM messages ORDER BY id"")
                raw_hashes = [row[0] for row in cur.fetchall()]
        else:
            cur = self.conn.execute(""SELECT hash FROM messages ORDER BY id"")
            raw_hashes = [row[0] for row in cur.fetchall()]

        hashes: List[str] = []
        for h in raw_hashes:
            if not isinstance(h, str) or not h:
                continue
            try:
                bytes.fromhex(h)
            except Exception:
                continue
            hashes.append(h)

        return _merkle_root(hashes)
",alpha_factory_v1/common/utils/logging.py,Ledger,1,7
survived,"    def stop(self) -> None:  # pragma: no cover - no teardown required
        return None",alpha_factory_v1/backend/services/metrics_service.py,MetricsExporter,1,7
survived,"    async def start(self) -> None:
        self._rest_task, self._grpc_server = await start_servers(
            self._runners,
            self._model_max_bytes,
            self._mem,
            self._rest_port,
            self._grpc_port,
            self._loglevel,
            self._ssl_disable,
        )
",alpha_factory_v1/backend/services/api_server_service.py,APIServer,1,7
survived,"    def grpc_server(self) -> Optional[Any]:
        return self._grpc_server
",alpha_factory_v1/backend/services/api_server_service.py,APIServer,1,7
survived,"    def bus(self) -> EventBus:
        return self._bus",alpha_factory_v1/backend/services/kafka_service.py,KafkaService,1,6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q13.py,Auto2,1,6
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q18.py,_Group,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q11.py,Auto1,1,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q13.py,_Group,1,7
survived,"async def test_basic_memory_mcp_use(tmp_path: Path) -> None:
    script = tmp_path / ""app.py""
    # Paths to the repository root and example memory module
    repo_root = Path(__file__).resolve().parents[1]
    examples_dir = repo_root / ""examples"" / ""basic_memory""
    script.write_text(
        textwrap.dedent(
            f'''
            import sys
            from pathlib import Path

            sys.path.insert(0, {str(repo_root / ""src"")!r})
            sys.path.insert(0, {str(examples_dir)!r})
            from memory import FileMemoryStore, MemoryNote, MemoryNoteSummary, MemoryProject
            from enrichmcp import EnrichMCP

            store = FileMemoryStore(Path(__file__).parent / ""data"")
            project = MemoryProject(""demo"", store)

            app = EnrichMCP(title=""Test"", description=""Desc"")

            @app.entity
            class Note(MemoryNote):
                """"""A note stored in the demo project.""""""

                pass

            @app.entity
            class NoteSummary(MemoryNoteSummary):
                """"""Minimal note representation.""""""

                pass

            @app.create
            async def create_note(
                title: str,
                content: str,
                tags: list[str] | None = None,
                note_id: str | None = None,
            ) -> Note:
                """"""Create or replace a note.""""""
                note = project.create_note(title, content, tags, note_id=note_id)
                return Note.model_validate(note.model_dump())

            @app.retrieve
            async def get_note(note_id: str) -> Note:
                """"""Get a note by ID.""""""
                note = project.get_note(note_id)
                if note is None:
                    raise ValueError(""note not found"")
                return Note.model_validate(note.model_dump())

            @app.retrieve
            async def list_notes(page: int = 1, page_size: int = 10) -> list[NoteSummary]:
                """"""List notes with pagination.""""""
                notes = project.list_notes(page, page_size)
                return [NoteSummary.model_validate(n.model_dump()) for n in notes]

            if __name__ == ""__main__"":
                app.run()
            '''
        )
    )

    config = {""mcpServers"": {""app"": {""command"": sys.executable, ""args"": [str(script)]}}}
    client = MCPClient(config=config)
    session = await client.create_session(""app"")

    create_result = await session.connector.call_tool(
        ""create_note"", {""title"": ""First"", ""content"": ""Hello"", ""tags"": []}
    )
    note = json.loads(create_result.content[0].text)

    update_result = await session.connector.call_tool(
        ""create_note"",
        {
            ""note_id"": note[""id""],
            ""title"": ""Updated"",
            ""content"": ""New text"",
            ""tags"": [""x""],
        },
    )
    updated = json.loads(update_result.content[0].text)
    assert updated[""title""] == ""Updated""

    get_result = await session.connector.call_tool(""get_note"", {""note_id"": note[""id""]})
    fetched = json.loads(get_result.content[0].text)
    assert fetched[""content""] == ""New text""

    await client.close_all_sessions()",tests/test_basic_memory_mcp_use.py,,1,6
survived,"            def register(self, *_a, **_k) -> None:
                pass
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,AgentRuntime,0,7
survived,"    def test_valid(self) -> None:
        self.assertEqual(edge_runner._positive_int(""p"")(""1""), 1)
",tests/test_edge_runner_parse.py,TestPositiveInt,1,6
survived,"    def test_all_agents_instantiable(self):
        for name in list_agents():
            meta = AGENT_REGISTRY[name]
            self.assertIsNotNone(meta.cls)
            # instantiation may fail if optional deps are missing
            try:
                agent = get_agent(name)
            except Exception:
                continue
            self.assertEqual(agent.NAME, name)
",tests/test_agents_integrity.py,TestAgentsIntegrity,1,7
survived,"    def test_sha_deterministic(self):
        payload = {""a"": 1, ""b"": 2}
        expected = energy_agent.hashlib.sha256(
            energy_agent.json.dumps(payload, separators=("","", "":"")).encode()
        ).hexdigest()
        self.assertEqual(energy_agent._sha(payload), expected)
",tests/test_energy_utils.py,TestEnergyUtils,1,7
survived,"    async def run_cycle(self) -> None:
        self.calls += 1
",tests/test_agent_runner.py,DummyAgent,1,6
survived,"def test_stopping_criteria_reset():
    class MockProcessor:
        def __init__(self):
            self.tokenizer = type(
                ""DummyTokenizer"", (), {""pad_token"": None, ""eos_token"": ""[EOS]""}
            )()

        def encode(self, text, add_special_tokens=False):
            if ""[EOS]"" in text:
                return [32008]
            return [1]

    processor = MockProcessor()
    stopping_criteria = StoppingCriteria([2], processor)
    stopping_criteria.add_eos_token_ids(""[EOS]"")

    stopping_criteria.reset([5, 7])
    assert stopping_criteria.eos_token_ids == [5, 7]
    assert stopping_criteria(7) is True",mlx_vlm/tests/test_utils.py,,1,7
survived,"  def update_counter(self, cur_count: int, cnt_size: int) -> bool:
    if ((self.counter + 1) & ((1 << cnt_size) - 1)) != cur_count:
      self.counter_fail = min(self.counter_fail + 1, MAX_BAD_COUNTER)
    elif self.counter_fail > 0:
      self.counter_fail -= 1
    self.counter = cur_count
    return self.counter_fail < MAX_BAD_COUNTER
",opendbc/can/parser.py,MessageState,1,7
survived,"def _get_dbc(dbc_name: str) -> DBC:
  dbc_path = dbc_name
  if not os.path.exists(dbc_path):
    dbc_path = os.path.join(os.path.dirname(__file__), "".."", ""dbc"", dbc_name + "".dbc"")
  if dbc_name in DBC_CACHE:
    return DBC_CACHE[dbc_name]
  try:
    dbc = parse_dbc(dbc_path)
  except FileNotFoundError as e:
    raise RuntimeError(f""DBC file not found: {dbc_path}"") from e
  DBC_CACHE[dbc_name] = dbc
  return dbc
",opendbc/can/parser.py,,1,7
survived,"  def update_valid(self, nanos: int) -> None:
    valid = True
    counters_valid = True
    for state in self.message_states.values():
      if state.counter_fail >= MAX_BAD_COUNTER:
        counters_valid = False
      if not state.valid(nanos, self.bus_timeout):
        valid = False

    self.can_invalid_cnt = 0 if valid else min(self.can_invalid_cnt + 1, CAN_INVALID_CNT)
    self.can_valid = self.can_invalid_cnt < CAN_INVALID_CNT and counters_valid
",opendbc/can/parser.py,CANParser,1,7
survived,"def test_offline_with_wheelhouse(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Allow offline installs when --wheelhouse is provided.""""""
    _no_missing(monkeypatch)
    monkeypatch.setattr(check_env, ""has_network"", lambda: False)
    rc = check_env.main([""--auto-install"", ""--wheelhouse"", ""wheels""])
    assert rc == 0",tests/test_check_env_network.py,,1,7
survived,"    async def _stop() -> None:
        global _orch
        task = getattr(app.state, ""orch_task"", None)
        if task:
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
        _orch = None
",src/interface/api_server.py,,1,7
survived,"def test_scatter_set():
    B, V = Axis(""batch"", 2), Axis(""vocab"", 6)
    x = hax.zeros((B, V))
    idx = hax.named(jnp.array([1, 4]), B)
    val = hax.ones(B) * 9
    y = x.at[{V: idx}].set(val)
    ref = jnp.zeros((2, 6)).at[jnp.arange(2), idx.array].set(9)
    assert jnp.array_equal(y.array, ref)",tests/test_scatter_gather.py,,1,7
survived,"    def num_completion_tokens(self) -> int:
        return len(self.token_ids) - self.num_prompt_tokens
",src/levanter/inference/sequence.py,Sequence,1,6
survived,"        def _finish(st):
            st.active = st.active.at[idx].set(False)
            st.tail = (st.tail + 1) % self.max_seqs
            return st
",src/levanter/inference/scheduler.py,JittedScheduler,1,6
survived,"    def collect(self, state):
        """"""Return the decoded sequences as Python lists.""""""
        return [
            [int(t) for t in state.token_ids[i, : int(state.lengths[i])].tolist()]
            for i in range(self.max_seqs)
            if int(state.lengths[i]) > 0
        ]
",src/levanter/inference/scheduler.py,JittedScheduler,1,7
survived,"def new_position(tiles):
    """""" returns a new position or looks up existing one """"""
    global all_positions
    if type(tiles) == type(list()):
        t = tiles
        tuptiles =   ((t[0][0], t[0][1], t[0][2], t[0][3]),
                      (t[1][0], t[1][1], t[1][2], t[1][3]),
                      (t[2][0], t[2][1], t[2][2], t[2][3]),
                      (t[3][0], t[3][1], t[3][2], t[3][3]))
    else:
        tuptiles = tiles

    if tuptiles in all_positions:
        return 	all_positions[tuptiles]
    else:
        new_pos = Position(tiles)
        all_positions[tuptiles] = new_pos
        return new_pos
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,,1,7
survived,"    def __init__(self, goal):
        """"""
        Preprocess goal position to setup internal data structures
        that can be used to speed up heuristic.
        """"""

        build_conflict_table()

        self.goal_map = []
        for i in range(16):
            self.goal_map.append(i)

        self.goal_lists = goal.tiles

        # preprocess for manhattan distance

        for row in range(4):
            for col in range(4):
                self.goal_map[goal.tiles[row][col]] = (row, col)

        # make access faster by changing to a tuple

        self.goal_map = tuple(self.goal_map)

        # preprocess for linear conflicts

        self.row_conflicts = []
        for row in range(4):
            t = goal.tiles[row]
            conf_dict = listconflicts([t[0],t[1],t[2],t[3]])
            self.row_conflicts.append(conf_dict)

        self.col_conflicts = []
        for col in range(4):
            col_list =[]
            for row in range(4):
                col_list.append(goal.tiles[row][col])
            conf_dict = listconflicts(col_list)
            self.col_conflicts.append(conf_dict)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,HeuristicObj,1,8
survived,"    def __init__(self, object_list):
        """"""
        Save a list in a heapq.
        Assume that each object only appears once
        in the list.
        """"""
        self.queue_length = 0
        self.qheap = []
        for e in object_list:
            self.qheap.append((e.fscore,e.tiles))
            self.queue_length += 1
        heapq.heapify(self.qheap)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue,1,7
survived,"def visualize_named_sharding(axes: Sequence[Axis], sharding: jax.sharding.Sharding) -> None:
    """"""Visualize the sharding for a set of named axes.

    This extends :func:`jax.debug.visualize_sharding` to handle arrays with more
    than two dimensions by falling back to a textual description when necessary.
    """"""

    try:
        pspec = sharding.spec  # type: ignore[attr-defined]
    except Exception:
        pspec = (None,) * len(axes)

    parts = [_pspec_parts(p) for p in pspec]
    num_sharded = sum(p != ""unsharded"" for p in parts)

    if num_sharded <= 2:
        try:
            jax.debug.visualize_sharding([ax.size for ax in axes], sharding)
        except Exception:
            pass

    mapping = "", "".join(f""{ax.name}->{part}"" for ax, part in zip(axes, parts))
    print(mapping)
",src/haliax/debug.py,,1,6
survived,"        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass
",src/haliax/debug.py,,0,6
survived,"            def latest_log(self):
                return """"
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver,0,6
survived,"    def test_sampling(self) -> None:
        if LEDGER.exists():
            LEDGER.unlink()
        result = subprocess.run([sys.executable, STUB, '-n', '2', '--seed', '1'], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        self.assertTrue(LEDGER.exists())
        logged = json.loads(LEDGER.read_text())
        self.assertIsInstance(logged, list)
        self.assertEqual(len(logged), 2)
",tests/test_alpha_discovery_stub.py,TestAlphaDiscoveryStub,1,7
survived,"def _ledger_path(path: str | os.PathLike | None) -> Path:
    if path:
        return Path(path).expanduser().resolve()
    env = os.getenv(""OMNI_ALPHA_LEDGER"")
    if env:
        return Path(env).expanduser().resolve()
    return DEFAULT_LEDGER
",alpha_factory_v1/demos/omni_factory_demo/alpha_discovery_stub.py,,1,7
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        gens = int(obs.get(""generations"", 3)) if isinstance(obs, dict) else 3
        return await self.tools.run_meta_search(gens)
",alpha_factory_v1/demos/meta_agentic_agi/openai_agents_bridge.py,MetaSearchAgent,1,7
survived,"    def test_meta_bridge_compiles(self):
        """"""Ensure the meta-agentic demo bridge compiles.""""""
        path = Path('alpha_factory_v1/demos/meta_agentic_agi/openai_agents_bridge.py')
        py_compile.compile(path, doraise=True)
",tests/test_openai_bridge.py,TestOpenAIBridge,1,7
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        if isinstance(obs, dict) and obs.get(""action"") == ""discover"":
            return await self.tools.trigger_discovery()
        return await self.tools.list_agents()
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,BusinessAgent,1,7
survived,"    def test_business_bridge_compiles(self):
        """"""Ensure the business demo bridge compiles.""""""
        path = Path('alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py')
        py_compile.compile(path, doraise=True)
",tests/test_openai_bridge.py,TestOpenAIBridge,1,6
survived,"async def _llm_comment(delta_g: float) -> str:
    """"""Return a short LLM comment on ``delta_g`` if OpenAI Agents is available.""""""

    if OpenAIAgent is None:
        return ""LLM offline""

    agent = OpenAIAgent(
        model=os.getenv(""MODEL_NAME"", ""gpt-4o-mini""),
        api_key=os.getenv(""OPENAI_API_KEY""),
        base_url=(None if os.getenv(""OPENAI_API_KEY"") else ""http://ollama:11434/v1""),
    )
    try:
        return await agent(
            f""In one sentence, comment on Î”G={delta_g:.4f} for the business.""
        )
    except Exception as exc:  # pragma: no cover - network failures
        log.warning(""LLM comment failed: %s"", exc)
        return ""LLM error""
",alpha_factory_v1/demos/alpha_agi_business_3_v1/alpha_agi_business_3_v1.py,,1,7
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        steps = int(obs.get(""steps"", 500)) if isinstance(obs, dict) else 500
        return await self.tools.run_episode(steps)
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,MuZeroAgent,1,7
survived,"def test_experience_launcher_gpu(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    script = Path(""alpha_factory_v1/demos/era_of_experience/run_experience_demo.sh"")
    config = script.parent / ""config.env""
    docker_log = tmp_path / ""docker.log""
    curl_log = tmp_path / ""curl.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""$@"" >> ""$DOCKER_LOG""\n'
        'if [ ""$1"" = ""info"" ]; then echo ""{""nvidia"":{}}""; fi\n'
        'if [ ""$1"" = ""version"" ]; then echo ""24.0.0""; fi\n'
        ""exit 0\n""
    )
    docker_stub.chmod(0o755)

    curl_stub = bin_dir / ""curl""
    curl_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""$@"" >> ""$CURL_LOG""\n'
        'out=""""\n'
        ""for ((i=1;i<=$#;i++)); do\n""
        '  if [ ""${!i}"" = ""-o"" ]; then\n'
        ""    j=$((i+1))\n""
        ""    out=${!j}\n""
        ""  fi\n""
        ""done\n""
        'if [ -n ""$out"" ]; then echo sample > ""$out""; fi\n'
        'echo ""OK""\n'
    )
    curl_stub.chmod(0o755)

    env = os.environ.copy()
    env.update(
        {
            ""PATH"": f""{bin_dir}:{env['PATH']}"",
            ""SKIP_ENV_CHECK"": ""1"",
            ""SAMPLE_DATA_DIR"": str(tmp_path / ""samples""),
            ""DOCKER_LOG"": str(docker_log),
            ""CURL_LOG"": str(curl_log),
            ""OPENAI_API_KEY"": ""dummy"",
        }
    )

    if config.exists():
        config.unlink()
    try:
        result = subprocess.run([f""./{script.name}""], cwd=script.parent, env=env, capture_output=True, text=True)
        created = config.exists()
    finally:
        if config.exists():
            config.unlink()

    assert result.returncode == 0, result.stderr
    assert docker_log.exists()
    log = docker_log.read_text()
    assert ""--profile gpu"" in log
    assert created",tests/test_experience_launcher.py,,0,7
survived,"    def __eq__(self, other):
        return (
            self.name == other.name
            and self.age == other.age
            and self.status == other.status
        )
",tests/machine/x/python/update_stmt.py,Person,1,7
survived,"def twoSum(nums, target):
    n = len(nums)
    for i in range(n):
        for j in range(i + 1, n):
            if nums[i] + nums[j] == target:
                return [i, j]
    return [-1, -1]
",tests/machine/x/python/two-sum.py,,1,7
survived,"    def _plot_nicely(self, mat, title, xlabel, ylabel, outfile=None):
        fig = plt.figure()
        ax = fig.add_subplot(111)
        im = ax.matshow(mat)
        ax.set_title(title)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.set_aspect(2)
        ax.set_aspect(""auto"")
        plt.colorbar(im)
        if outfile is not None:
            plt.savefig(outfile)
        plt.show()
",examples/synthetic_data.py,HldaDataGenerator,1,6
survived,"def load_acm_certificates(
    neo4j_session: neo4j.Session,
    data: List[Dict],
    region: str,
    current_aws_account_id: str,
    update_tag: int,
) -> None:
    logger.info(f""Loading {len(data)} ACM certificates for region {region} into graph."")
    load(
        neo4j_session,
        ACMCertificateSchema(),
        data,
        lastupdated=update_tag,
        Region=region,
        AWS_ID=current_aws_account_id,
    )
",cartography/intel/aws/acm.py,,1,7
survived,"def transform_acm_certificates(certificates: List[Dict], region: str) -> List[Dict]:
    transformed: List[Dict] = []
    for cert in certificates:
        item: Dict[str, Any] = {
            ""Arn"": cert[""CertificateArn""],
            ""DomainName"": cert.get(""DomainName""),
            ""Type"": cert.get(""Type""),
            ""Status"": cert.get(""Status""),
            ""KeyAlgorithm"": cert.get(""KeyAlgorithm""),
            ""SignatureAlgorithm"": cert.get(""SignatureAlgorithm""),
            ""NotBefore"": (
                dict_date_to_epoch({""d"": dt_parser.parse(cert[""NotBefore""])}, ""d"")
                if cert.get(""NotBefore"")
                else None
            ),
            ""NotAfter"": (
                dict_date_to_epoch({""d"": dt_parser.parse(cert[""NotAfter""])}, ""d"")
                if cert.get(""NotAfter"")
                else None
            ),
            ""InUseBy"": cert.get(""InUseBy"", []),
            ""Region"": region,
        }
        # Extract ELBV2 Listener ARNs for relationship creation
        listener_arns = [a for a in item[""InUseBy""] if "":listener/"" in a]
        if listener_arns:
            item[""ELBV2ListenerArns""] = listener_arns
        transformed.append(item)
    return transformed
",cartography/intel/aws/acm.py,,1,7
survived,"    def observe(self, *a, **kw):
        pass
",tests/test_eventbus.py,_M,0,7
survived,"    async def _drain_loop(self) -> None:
        assert self._queues is not None
        try:
            while True:
                for q in list(self._queues.values()):
                    while not q.empty():
                        try:
                            q.get_nowait()
                        except asyncio.QueueEmpty:
                            break
                await asyncio.sleep(0.1)
        except asyncio.CancelledError:
            pass
",alpha_factory_v1/backend/agent_runner.py,EventBus,1,6
survived,"def test_open_logs_endpoint(client):
    with patch(""app.desktop.studio_server.settings_api.open_logs_folder"") as m:
        response = client.post(""/api/open_logs"")
        assert response.status_code == 200
        m.assert_called_once()",app/desktop/studio_server/test_settings_api.py,,1,6
survived,"def test_open_logs_endpoint(client):
    with patch(""app.desktop.studio_server.settings_api.open_logs_folder"") as m:
        response = client.post(""/api/open_logs"")
        assert response.status_code == 200
        m.assert_called_once()",app/desktop/studio_server/test_settings_api.py,,1,7
survived,"async def test_agent_routes_prompt_through_router():
    adapter = DummyAdapter()
    router = GuardrailModelRouter({""gpt"": adapter}, default_model=""gpt"")
    agent = GuardrailDesignerAgent(model_router=router)

    result = await agent.run({""prompt"": ""hello""})

    assert result[""status""] == ""success""
    assert result[""output""] == ""hello:guarded""",tests/test_guardrail_designer_agent.py,,1,7
survived,"    def add_input_guardrail(self, guardrail: Callable[[str], Awaitable[None]]) -> None:
        self.input_guardrails.append(guardrail)
",src/meta_agent/services/guardrail_router.py,GuardrailModelRouter,1,7
survived,"    async def invoke(self, prompt: str, context=None) -> str:
        self.prompts.append(prompt)
        return f""{prompt}:ok""
",tests/test_guardrail_router.py,MockAdapter,1,6
survived,"def mixedGradY(x, y, backend_name=""numpy""):
    backend.set_backend(backend_name)
    b = backend.current()

    def f(a, b_):
        return b.sum(b.mul(a, b_))

    g = b.grad(f, wrt=1)
    out = g(b.array(x, requires_grad=True), b.array(y, requires_grad=True))
    out = to_numpy(out)
    return out.tolist() if isinstance(out, np.ndarray) else out
",tests/kgtests/autograd/helpers.py,,1,6
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""version"", help=""Pyodide version string, e.g. 0.28.0"")
    args = parser.parse_args()
    update_pyodide(args.version)
",scripts/update_pyodide.py,,1,7
survived,"def sha384_b64(data: bytes) -> str:
    digest = hashlib.sha384(data).digest()
    return base64.b64encode(digest).decode()
",scripts/update_pyodide.py,,1,7
survived,"def test_concurrent_requests(monkeypatch) -> None:
    monkeypatch.setenv(""RATE_LIMIT_PER_MIN"", ""1000"")
    mod._REQUEST_LOG.clear()
    asyncio.run(_run_concurrent())
    assert len(mod._REQUEST_LOG.get(""127.0.0.1"", [])) == 5",tests/test_rate_lock.py,,1,7
survived,"def test_runtime_port_env(monkeypatch: ""pytest.MonkeyPatch"") -> None:
    """"""AgentRuntime receives AGENTS_RUNTIME_PORT.""""""
    import importlib
    import sys
    import types

    captured: dict[str, int] = {}

    class DummyRuntime:
        def __init__(self, *a: object, port: int = 5001, **_k: object) -> None:
            captured[""port""] = port

        def register(self, *_a: object, **_k: object) -> None:
            pass

        def run(self) -> None:
            pass

    stub = types.ModuleType(""openai_agents"")
    stub.Agent = object
    stub.AgentRuntime = DummyRuntime
    stub.OpenAIAgent = object

    def _tool(*_a: object, **_k: object) -> Callable[[object], object]:
        def dec(f: object) -> object:
            return f

        return dec

    stub.Tool = _tool
    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    monkeypatch.delitem(sys.modules, ""agents"", raising=False)
    monkeypatch.setenv(""AGENTS_RUNTIME_PORT"", ""6101"")

    mod = importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_opportunity_stub"")
    importlib.reload(mod)
    mod.main([])
    assert captured[""port""] == 6101",tests/test_alpha_opportunity_stub.py,,0,6
survived,"    def _tool(*_a: object, **_k: object) -> Callable[[object], object]:
        def dec(f: object) -> object:
            return f

        return dec
",tests/test_alpha_opportunity_stub.py,,1,6
survived,"def test_has_network_all_fail(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Return False when none of the hosts are reachable.""""""

    attempts = []

    def _connect(_addr: tuple[str, int], timeout: float = 1.0) -> None:
        attempts.append(_addr)
        raise OSError

    monkeypatch.setattr(check_env.socket, ""create_connection"", _connect)  # type: ignore[attr-defined]
    assert check_env.has_network() is False
    assert len(attempts) >= 3",tests/test_check_env_network.py,,1,7
survived,"def test_has_network_fallback(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Return True when later test hosts are reachable.""""""

    attempts = []

    class _Sock:
        def __enter__(self) -> ""_Sock"":
            return self

        def __exit__(self, *exc: object) -> None:
            pass

    def _connect(addr: tuple[str, int], timeout: float = 1.0) -> _Sock:
        attempts.append(addr)
        if addr[0] == ""pypi.org"":
            raise OSError
        return _Sock()

    monkeypatch.setattr(check_env.socket, ""create_connection"", _connect)  # type: ignore[attr-defined]
    assert check_env.has_network() is True
    assert attempts == [(""pypi.org"", 443), (""1.1.1.1"", 443)]
",tests/test_check_env_network.py,,1,7
survived,"def test_exponential_curve_parameters() -> None:
    """"""Exponential curve should honour ``k`` and ``x0``.""""""
    base = forecast.exponential_curve(0.5)
    shifted = forecast.exponential_curve(0.6, x0=0.1)
    steep = forecast.exponential_curve(0.5, k=5.0)
    assert shifted == pytest.approx(base)
    assert steep < base
",tests/test_forecast.py,,1,6
survived,"def test_capability_growth_params() -> None:
    """"""Capability growth should forward ``k`` and ``x0``.""""""
    val_default = forecast.capability_growth(0.5, curve=""logistic"")
    val_custom = forecast.capability_growth(0.5, curve=""logistic"", k=5.0, x0=0.0)
    assert val_custom == pytest.approx(forecast.logistic_curve(0.5, k=5.0))
    assert val_custom != val_default
",tests/test_forecast.py,,1,7
survived,"def test_apply_patch_missing_patch_binary(tmp_path: Path, monkeypatch: mock.MagicMock) -> None:
    (tmp_path / ""hello.txt"").write_text(""hello\n"", encoding=""utf-8"")
    monkeypatch.setattr(shutil, ""which"", lambda _: None)
    with pytest.raises(RuntimeError) as exc:
        patcher_core.apply_patch(_DEF_DIFF, repo_path=str(tmp_path))
    assert ""patch` command not found"" in str(exc.value)
",tests/test_patcher_core_additional.py,,1,7
survived,"def _parse_file(path: Path) -> Iterable[ArchiveEntry]:
    """"""Yield archive entries from ``path``.""""""
    for line in path.read_text(encoding=""utf-8"").splitlines():
        if not line.strip():
            continue
        try:
            rec = json.loads(line)
        except Exception:  # noqa: BLE001 - skip invalid lines
            continue
        yield ArchiveEntry(
            hash=rec[""hash""],
            parent=rec.get(""parent""),
            score=float(rec.get(""score"", 0.0)),
            novelty=float(rec.get(""novelty"", 0.0)),
            is_live=bool(rec.get(""is_live"", True)),
            ts=float(rec.get(""ts"", 0.0)),
        )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/tools/dgm_import.py,,1,7
survived,"    def update(self, metrics: Mapping[str, Mapping[str, float]]) -> None:
        """"""Update rolling stats and switch datasets when thresholds pass.""""""

        rate = metrics.get(self._dataset, {}).get(""pass_rate"")
        if rate is not None:
            self.history.append(rate)
        if not self.history:
            return
        avg = sum(self.history) / len(self.history)

        if self._dataset == self.MINI and avg >= 0.40:
            self._dataset = self.FULL
            self.history.clear()
            self.db.set_state(""dataset"", self._dataset)
            self._log.info(""switched dataset to %s"", self._dataset)
        elif self._dataset == self.FULL and avg >= 0.60:
            self._dataset = self.POLYGLOT
            self.history.clear()
            self.db.set_state(""dataset"", self._dataset)
            self._log.info(""switched dataset to %s"", self._dataset)",src/eval/fitness.py,CurriculumSwitcher,1,8
survived,"def _write_heatmap(data: Dict[str, Dict[str, float]]) -> None:
    if plt is None or np is None:
        return

    patches = list(data.keys())
    cols = INNOVATIONS
    arr = np.zeros((len(patches), len(cols)))
    for i, p in enumerate(patches):
        baseline = data[p][""baseline""]
        for j, c in enumerate(cols):
            arr[i, j] = baseline - data[p][c]
    fig, ax = plt.subplots(figsize=(2 + len(cols), 1 + len(patches)))
    im = ax.imshow(arr, cmap=""viridis"")
    ax.set_xticks(np.arange(len(cols)))
    ax.set_xticklabels(cols)
    ax.set_yticks(np.arange(len(patches)))
    ax.set_yticklabels(patches)
    for i in range(arr.shape[0]):
        for j in range(arr.shape[1]):
            ax.text(j, i, f""{arr[i, j]:.2f}"", ha=""center"", va=""center"", color=""white"")
    fig.colorbar(im, ax=ax, label=""âˆ† pass rate"")
    plt.tight_layout()
    HEATMAP_OUT.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(HEATMAP_OUT)
",src/tools/ablation_runner.py,,0,6
survived,"def _clone_repo(dest: Path) -> None:
    """"""Copy source tree needed for benchmarks into ``dest``.""""""

    shutil.copytree(ROOT / ""benchmarks"", dest / ""benchmarks"")
    shutil.copytree(ROOT / ""src"", dest / ""src"")
",src/tools/ablation_runner.py,,1,6
survived,"    async def __aenter__(self) -> ""A2ABus"":
        """"""Start the bus when entering an async context.""""""
        await self.start()
        return self
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus,1,7
survived,"            def _decorator(func):
                return func
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime,1,6
survived,"    def test_numeric_asarray_and_add(self):
        arr = self.core.kg_asarray([1, 2, 3])
        self.assertIsInstance(arr, torch.Tensor)
        res = self.backend.np.add.reduce(arr)
        self.assertEqual(res.item(), 6)
",tests/test_torch_backend.py,TestTorchBackend,1,7
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,,1,7
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"async def mutate(
    tar: UploadFile | None = File(None),
    repo_url: str | None = Form(None),
) -> MutationResponse:
    """"""Return a mutated child from one evolution step.""""""

    if tar is None and not repo_url:
        raise HTTPException(status_code=400, detail=""tar or repo_url required"")

    tmp = tempfile.mkdtemp(dir=STORAGE_PATH)
    tmp_path = Path(tmp)
    try:
        if tar is not None:
            with tarfile.open(fileobj=tar.file) as tf:
                tf.extractall(tmp_path)
        if repo_url:
            (tmp_path / ""repo.txt"").write_text(repo_url)

        pop = mats.run_evolution(
            lambda g: (g[0] ** 2, g[1] ** 2),
            2,
            population_size=4,
            generations=1,
            seed=42,
        )
        child = pop[0].genome
        return MutationResponse(child=child)
    finally:
        for p in tmp_path.rglob(""*""):
            if p.is_file():
                p.unlink()
        tmp_path.rmdir()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evolution_worker.py,,1,6
survived,"def load_templates(path: str | Path) -> dict[str, Mapping[str, Any]]:
    """"""Return prompt templates loaded from ``path``.""""""
    raw = yaml.safe_load(Path(path).read_text(encoding=""utf-8""))
    if not isinstance(raw, Mapping):
        raise ValueError(""template file must map names to templates"")
    return {str(k): dict(v) for k, v in raw.items()}
",src/agents/prompt_sampler.py,,1,7
survived,"def test_python_available() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    repo_root = browser_dir.parents[3]
    py_snippet = """"""import ast, json, pathlib
txt = pathlib.Path('scripts/fetch_assets.py').read_text()
tree = ast.parse(txt)
assets = {}
for node in tree.body:
    if isinstance(node, ast.Assign):
        for t in node.targets:
            if getattr(t, 'id', None) == 'ASSETS':
                assets = ast.literal_eval(node.value)
print(json.dumps(list(assets.keys())))""""""
    node_code = f""""""
import {{ spawnSync }} from 'child_process';
const out = spawnSync('python', ['-'], {{
  input: `{py_snippet}`,
  cwd: {json.dumps(str(repo_root))},
  encoding: 'utf8',
}});
if (out.error) {{ throw out.error; }}
process.exit(out.status);
""""""
    res = subprocess.run([
        ""node"",
        ""-e"",
        node_code,
    ], cwd=browser_dir, capture_output=True, text=True)
    assert res.returncode == 0, res.stderr",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_build_python.py,,0,6
survived,"    def _ray_eval(self):
        @ray.remote
        def _worker(js: str):
            g = Genome.from_json(js)
            module = import_module(__name__)
            return module.MetaEvolver._simulate(module.MetaEvolver, g)
        futures = [_worker.remote(g.to_json()) for g in self.population]
        results = ray.get(futures)
        return self._post_eval(results)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,0,6
survived,"    def __init__(
        self,
        env_cls: Callable,
        pop_size: int = 32,
        elitism: int = 2,
        parallel: bool = True,
        checkpoint_dir: pathlib.Path = CHKPT_DIR,
        llm: Callable[[str], str] | None = None,
    ):
        self.env_cls, self.pop_size, self.elitism = env_cls, pop_size, elitism
        self.parallel = parallel
        self.ckpt_dir = checkpoint_dir
        self.llm = llm
        self.rng = random.Random(int(""A1GA"", 16))
        self.gen = 0
        self.history: List[Tuple[int, float]] = []
        self._archive: List[np.ndarray] = []
        self._best_fitness = -math.inf
        self.best_genome: Genome | None = None
        self._last_scores: List[float] = []
        self._init_population()
        if self.parallel and _HAS_RAY and not ray.is_initialized():
            ray.init(ignore_reinit_error=True, _temp_dir=str(self.ckpt_dir / ""ray""))
        LOG.info(""Evolver ready â–¶ pop=%d device=%s"", self.pop_size, Device)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"    def from_json(js: str | dict) -> ""Genome"":
        return Genome(**(json.loads(js) if isinstance(js, str) else js))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,Genome,1,7
survived,"    def to_json(self) -> str:
        return json.dumps(dc.asdict(self), separators=(',', ':'))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,Genome,1,6
survived,"        def fake_openai_agent(*_a, **kwargs):
            return types.SimpleNamespace(base_url=kwargs.get(""base_url""))
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime,1,7
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""dest"", type=Path, nargs=""?"", default=Path(""models/gpt2""), help=""Target directory"")
    args = parser.parse_args()
    try:
        download_hf_gpt2(args.dest)
    except Exception as exc:
        sys.exit(str(exc))
",scripts/download_hf_gpt2.py,,1,7
survived,"def test_cli_entrypoint() -> None:
    """"""Running the ``alpha-agi-business-3-v1`` script should output the Î”G message.""""""
    env = os.environ.copy()
    env[""OPENAI_API_KEY""] = ""dummy""
    result = subprocess.run(
        [""alpha-agi-business-3-v1"", ""--cycles"", ""1""],
        capture_output=True,
        text=True,
        env=env,
    )
    assert result.returncode == 0, result.stderr
    assert ""Î”G"" in (result.stdout + result.stderr)
",tests/test_alpha_agi_business_3_v1.py,,0,7
survived,"def test():
    import sys
    recursive = 0
    if sys.argv[1:] and sys.argv[1] == '-r':
        del sys.argv[1:2]
        recursive = 1
    try:
        if sys.argv[1:]:
            testall(sys.argv[1:], recursive, 1)
        else:
            testall(['.'], recursive, 1)
    except KeyboardInterrupt:
        sys.stderr.write('\n[Interrupted]\n')
        sys.exit(1)
",metaflow/_vendor/imghdr/__init__.py,,1,6
survived,"def test_gif(h, f):
    """"""Verify if the image is a GIF ('87 or '89 variants).""""""
    if h[:6] in (b'GIF87a', b'GIF89a'):
        return 'gif'
",metaflow/_vendor/imghdr/__init__.py,,1,7
survived,"def test_pgm(h, f):
    """"""Verify if the image is a PGM (portable graymap).""""""
    if len(h) >= 3 and \
        h[0] == ord(b'P') and h[1] in b'25' and h[2] in b' \t\n\r':
        return 'pgm'
",metaflow/_vendor/imghdr/__init__.py,,1,6
survived,"    def time_iso(self) -> str:
        """"""Return the current time in ISO 8601 format.""""""
        return datetime.now(timezone.utc).isoformat()
",src/agents/tracing/setup.py,TraceProvider,1,7
survived,"def _candidate_from(base: str, parts: list[str]) -> Optional[Tuple[str, str]]:
    candidate = os.path.join(base, *parts)
    if os.path.isdir(candidate):
        if os.path.isfile(os.path.join(candidate, ""__init__.jac"")):
            return os.path.join(candidate, ""__init__.jac""), ""jac""
        if os.path.isfile(os.path.join(candidate, ""__init__.py"")):
            return os.path.join(candidate, ""__init__.py""), ""py""
    if os.path.isfile(candidate + "".jac""):
        return candidate + "".jac"", ""jac""
    if os.path.isfile(candidate + "".py""):
        return candidate + "".py"", ""py""
    return None
",jac/jaclang/utils/module_resolver.py,,1,7
survived,"def main() -> int:
    demos = iter_demos()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            for demo in demos:
                page = browser.new_page()
                page.goto((demo / ""index.html"").resolve().as_uri())
                page.wait_for_selector(""body"")
                page.wait_for_selector(""h1"")
                page.close()
            browser.close()
        return 0
    except PlaywrightError as exc:
        print(f""Playwright error: {exc}"", file=sys.stderr)
        return 1
    except Exception as exc:  # noqa: BLE001
        print(f""Demo check failed: {exc}"", file=sys.stderr)
        return 1
",scripts/verify_demo_pages.py,,1,7
survived,"    def test_aiga_bridge_import_paths(self, monkeypatch):
        """"""Import the AIâ€‘GA bridge with both `openai_agents` and `agents`.""""""

        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = object
        stub.OpenAIAgent = object

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Tool = _tool

        # Import using openai_agents module
        monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
        sys.modules.pop(""agents"", None)
        mod = importlib.reload(
            importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
        )
        self.assertIs(mod.Agent, stub.Agent)

        # Import using agents fallback
        monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
        sys.modules.pop(""openai_agents"", None)

        stub_agents = types.ModuleType(""agents"")
        stub_agents.Agent = object
        stub_agents.AgentRuntime = object
        stub_agents.OpenAIAgent = object
        stub_agents.Tool = _tool
        monkeypatch.setitem(sys.modules, ""agents"", stub_agents)

        orig_import = builtins.__import__

        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)

        monkeypatch.setattr(builtins, ""__import__"", fake_import)
        mod = importlib.reload(
            importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
        )
        self.assertIs(mod.Agent, stub_agents.Agent)
",tests/test_openai_bridge.py,TestOpenAIBridge,1,7
survived,"def verify_assets(base: Path) -> list[str]:
    """"""Return a list of assets that failed verification.""""""

    failures: list[str] = []
    for rel in ASSETS:
        dest = base / rel
        if not dest.exists():
            print(f""Missing {rel}"")
            failures.append(rel)
            continue
        expected = CHECKSUMS.get(dest.name)
        if expected:
            digest = base64.b64encode(hashlib.sha384(dest.read_bytes()).digest()).decode()
            if not expected.endswith(digest):
                print(f""Checksum mismatch for {rel}"")
                failures.append(rel)
    return failures
",scripts/fetch_assets.py,,1,7
survived,"    def test_run_demo_market_data(self) -> None:
        import tempfile

        with tempfile.NamedTemporaryFile(""w"", delete=False) as fh:
            fh.write(""6,6,6"")
            feed_path = fh.name
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--market-data"",
                feed_path,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo,0,7
survived,"  async def get_overview_register(self) -> OverviewRegisterState:
    # Sometimes this command is not recognized and it is not known why. We will retry a few times
    # We don't care if the cytomat is still busy, that is actually what we are often checking for.
    # We are just gathering state, so just try a little bit later.
    num_tries = 10
    for _ in range(num_tries):
      try:
        resp = await self.send_command(""ch"", ""bs"", """")
      except (CytomatCommandUnknownError, CytomatBusyError):
        await asyncio.sleep(0.1)
        continue
      return OverviewRegisterState.from_resp(resp)
    await self.reset_error_register()
    raise CytomatCommandUnknownError(""Could not get overview register"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,7
survived,"def cytomat_rack_10mm_47(name: str):
  return _cytomat_rack(name=name, site_height=10, num_sites=47, model=""cytomat_rack_10mm_47"")
",pylabrobot/storage/cytomat/racks.py,,1,6
survived,"  async def get_humidity(self) -> CytomatIncupationResponse:
    return await self.get_incubation_query(""ih"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,1,7
survived,"  async def close_door(self):
    print(""Closing door"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,1,6
survived,"  async def take_in_plate(self, plate: Plate, site: PlateHolder):
    await self.action_transfer_to_storage(site)
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend,0,7
survived,"  def __init__(self):
    self._dummy_temperature = 37.0
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend,1,6
survived,"  async def read_plate_detection_xfer(self) -> bool:
    """"""Read Plate Detection Transfer Station (RD 1813).""""""
    resp = await self._send_command(""RD 1813"")
    return resp == ""1""
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend,1,6
survived,"  async def send_command(self, command_type, command, params):
    print(
      ""cytomat"", self._assemble_command(command_type=command_type, command=command, params=params)
    )
    if command_type == ""ch"":
      return ""0""
    return ""0"" * 8
",pylabrobot/storage/cytomat/cytomat.py,CytomatChatterbox,1,6
survived,"  async def take_in_plate(self, site: Union[PlateHolder, Literal[""random"", ""smallest""]]):
    """"""Take a plate from the loading tray and put it in the incubator.""""""

    plate = cast(Plate, self.loading_tray.resource)
    if plate is None:
      raise ResourceNotFoundError(f""No plate on the loading tray of incubator '{self.name}'"")

    if site == ""random"":
      site = self.find_random_site(plate)
    elif site == ""smallest"":
      site = self.find_smallest_site_for_plate(plate)
    elif isinstance(site, PlateHolder):
      if site not in self._find_available_sites_sorted(plate):
        raise ValueError(f""Site {site.name} is not available for plate {plate.name}"")
    else:
      raise ValueError(f""Invalid site: {site}"")
    await self.backend.take_in_plate(plate, site)
    plate.unassign()
    site.assign_child_resource(plate)
",pylabrobot/storage/incubator.py,Incubator,1,7
survived,"def test_render_inheritance(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    creator = TemplateCreator(reg)
    creator.create(_meta(""base""), ""{% block greet %}Hello{% endblock %}"")
    creator.create(
        _meta(""child""),
        ""{% extends 'base' %}{% block greet %}{{ super() }} {{ name }}{% endblock %}"",
    )

    mixer = TemplateMixer(reg)
    result = mixer.render(""child"", context={""name"": ""Bob""})
    assert result.strip() == ""Hello Bob""
",tests/test_template_mixer.py,,1,7
survived,"    def from_string(self, source: str) -> Template:
        """"""Create a template from a string after validation.""""""
        self.parse(source)
        return Template(source, globals=self.globals)",src/jinja2/__init__.py,Environment,1,7
survived,"    def find_undeclared_variables(source: str) -> set[str]:
        """"""Very naive variable extraction used for tests.""""""
        import re

        return set(re.findall(r""{{\s*(\w+)\s*}}"", source))
",src/jinja2/__init__.py,meta,1,7
survived,"    def get_rating(self, slug: str) -> Tuple[int, float]:
        """"""Return rating count and average for ``slug``.""""""
        ratings = self._load_ratings()
        key = slug.replace("" "", ""_"").lower()
        values = ratings.get(key, [])
        if not values:
            return 0, 0.0
        total = sum(values)
        return len(values), total / len(values)
",src/meta_agent/template_sharing.py,TemplateSharingManager,1,7
survived,"def unique_all(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray, NamedArray, NamedArray]:
    """"""Shortcut for :func:`unique` returning values, indices, inverse, and counts.""""""

    values, indices, inverse, counts = typing.cast(
        tuple[NamedArray, NamedArray, NamedArray, NamedArray],
        unique(
            array,
            Unique,
            return_index=True,
            return_inverse=True,
            return_counts=True,
            axis=axis,
            fill_value=fill_value,
        ),
    )
    return values, indices, inverse, counts
",src/haliax/ops.py,,1,7
survived,"def test_create_patch_no_entries(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    agent = MetaRefinementAgent(repo, tmp_path / ""logs"")
    patch = agent._create_patch([])
    assert ""optimise performance"" in patch
",tests/test_meta_refinement_agent.py,,0,7
survived,"def _example_value(param_type: str) -> str:
    mapping = {
        ""int"": ""1"",
        ""integer"": ""1"",
        ""float"": ""1.0"",
        ""string"": ""'test'"",
        ""bool"": ""True"",
        ""boolean"": ""True"",
        ""list"": ""[]"",
        ""dict"": ""{}"",
    }
    return mapping.get(param_type.lower(), ""None"")
",src/meta_agent/generators/test_generator.py,,1,6
survived,"  def test_low_bits(self):
    self.assertEqual(getbits(0b11010110, 0, 3), 0b0110)
",test/unit/test_helpers.py,TestGetBits,1,7
survived,"def _lambda6():
    draw.get(20)()
    draw.get(60)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,0,7
survived,"def _lambda9():
    draw.get(100)()
    draw.get(600)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/forward-difference.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    INF = 1000000000
    n = 4
    dist = []
    next = []
    i = 0
    while i < n:
        row = []
        nrow = []
        j = 0
        while j < n:
            if i == j:
                row = row + [0]
            else:
                row = row + [INF]
            nrow = nrow + [0 - 1]
            j = j + 1
        dist = dist + [row]
        next = next + [nrow]
        i = i + 1
    dist[0][2] = -2
    next[0][2] = 2
    dist[2][3] = 2
    next[2][3] = 3
    dist[3][1] = -1
    next[3][1] = 1
    dist[1][0] = 4
    next[1][0] = 0
    dist[1][2] = 3
    next[1][2] = 2
    k = 0
    while k < n:
        i = 0
        while i < n:
            j = 0
            while j < n:
                if dist[i][k] < INF and dist[k][j] < INF:
                    alt = dist[i][k] + dist[k][j]
                    if alt < dist[i][j]:
                        dist[i][j] = alt
                        next[i][j] = next[i][k]
                j = j + 1
            i = i + 1
        k = k + 1
    def path(u, v):
        ui = u - 1
        vi = v - 1
        if next[ui][vi] == 0 - 1:
            return []
        p = [u]
        cur = ui
        while cur != vi:
            cur = next[cur][vi]
            p = p + [cur + 1]
        return p
    def pathStr(p):
        s = """"
        first = True
        idx = 0
        while idx < len(p):
            x = p[idx]
            if not first:
                s = s + "" -> ""
            s = s + str(x)
            first = False
            idx = idx + 1
        return s
    print(""pair\tdist\tpath"")
    a = 0
    while a < n:
        b = 0
        while b < n:
            if a != b:
                print(str(a + 1) + "" -> "" + str(b + 1) + ""\t"" + str(dist[a][b]) + ""\t"" + pathStr(path(a + 1, b + 1)))
            b = b + 1
        a = a + 1
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm.py,,1,6
survived,"def b(x, y):
    pass
",tests/rosetta/transpiler/Python/function-prototype.py,,0,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fork.py,,1,6
survived,"def _cos(x):
    y = _mod(x + PI, 2.0 * PI) - PI
    y2 = y * y
    y4 = y2 * y2
    y6 = y4 * y2
    return 1.0 - y2 / 2.0 + y4 / 24.0 - y6 / 720.0
",tests/rosetta/transpiler/Python/fractal-tree.py,,1,6
survived,"def printState(v):
    s = state(v)
    print(""value="" + str(v) + "" entry="" + str(s.entry) + "" inc="" + str(s.inc) + "" dec="" + str(s.dec))
",tests/rosetta/transpiler/Python/gui-enabling-disabling-of-controls.py,,1,6
survived,"def _lambda1(n):
    s = 0.0
    k = 0
    while k <= n:
        s = s + extract(a, k) * extract(b, n - k)
        k = k + 1
    return s
",tests/rosetta/transpiler/Python/formal-power-series.py,,1,6
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/function-frequency.py,,1,6
survived,"def b2i(b):
    if b:
        return 1
    return 0
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,,1,6
survived,"def sub(a, b):
    return newFps(lambda n: extract(a, n) - extract(b, n))
",tests/rosetta/transpiler/Python/formal-power-series.py,,1,6
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    p = sinCos()
    print(""sin:"" + partialSeries(p.sin))
    print(""cos:"" + partialSeries(p.cos))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/formal-power-series.py,,1,7
survived,"def pow10(n):
    r = 1.0
    i = 0
    while i < n:
        r = r * 10.0
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/formatted-numeric-output.py,,1,8
survived,"def test_trivial_maze_rejected(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Generator should reject easy mazes when thresholds active.""""""
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MC_MIN"", ""0.2"")
    monkeypatch.setenv(""ALPHA_ASI_MC_MAX"", ""0.8"")
    module = ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    if module in sys.modules:
        del sys.modules[module]
    mod = importlib.import_module(module)

    calls: list[float] = [1.0, 0.5]

    def fake_eval(self, env, policy, episodes):
        return calls.pop(0)

    monkeypatch.setattr(mod.POETGenerator, ""_mc_eval"", fake_eval)

    gen = mod.POETGenerator()
    env = gen.propose()
    assert env in gen.pool
    assert not calls  # second env accepted",tests/test_world_model_open_endedness.py,,0,7
survived,"    def test_missing_spec_skips_check_without_flag(self) -> None:
        fake_mod = types.SimpleNamespace(
            __version__=""0.0.17"",
            __spec__=None,
            OpenAIAgent=object,
        )

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return importlib.import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            if name == ""agents"":
                return None
            return importlib.util.find_spec(name, *args, **kwargs)

        def _raise() -> bool:
            raise AssertionError(""check_openai_agents_version should not run"")

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [""openai_agents""]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
            mock.patch.object(check_env, ""check_openai_agents_version"", _raise),
        ):
            self.assertEqual(check_env.main([]), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/print_hello.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_by_multi_join_sort.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/left_join_multi.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/short_circuit.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/sort_stable.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/record_assign.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/dataset_where_filter.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_items_iteration.py,,1,7
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/load_yaml.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/len_builtin.py,,1,6
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/in_operator.py,,1,7
survived,"def test_queue_limit_and_fetch_fallback() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""navigator.sendBeacon=()=>false;""
            ""Object.defineProperty(navigator,'onLine',{get:()=>false,configurable:true});""
            ""localStorage.setItem('telemetryQueue',JSON.stringify(Array.from({length:100},()=>({}))))""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.click(""text=Share"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert (
            page.evaluate(
                ""JSON.parse(localStorage.getItem('telemetryQueue')).length""
            )
            == 100
        )
        page.evaluate(
            ""window.fetch=(...a)=>{window.fetchArgs=a;return Promise.resolve({status:200});};""
            ""Object.defineProperty(navigator,'onLine',{get:()=>true});""
            ""window.dispatchEvent(new Event('online'));""
        )
        page.wait_for_function(""window.fetchArgs !== undefined"")
        assert page.evaluate(""localStorage.getItem('telemetryQueue')"") == ""[]""
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,,1,6
survived,"    def __init__(self, name: str, age: int, status: str):
        self.name = name
        self.age = age
        self.status = status
",tests/human/x/python/update_stmt.py,Person,1,7
survived,"    def __repr__(self):
        return f""Person(name={self.name!r}, age={self.age}, status={self.status!r})""
",tests/human/x/python/update_stmt.py,Person,1,8
survived,"    def poll(self, _):
        pass
",tests/test_world_model_kafka.py,DummyKafka,0,7
survived,"    def tearDown(self):
        self.g.close()
",tests/test_memory_graph_rel.py,TestGraphMemoryRelationValidation,1,6
survived,"    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        thread = self._get_thread(
            service, input_data.threadId, input_data.includeSpamTrash
        )
        yield ""thread"", thread
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailGetThreadBlock,1,6
survived,"        def __init__(self, sender: str = """", recipient: str = """", payload: dict | None = None, ts: float = 0.0) -> None:
            self.sender = sender
            self.recipient = recipient
            self.payload = payload or {}
            self.ts = ts
",tests/test_adk_agent.py,Envelope,1,7
survived,"    def test_multibit_OpenCL_Brute(self):
        wallet_filename = os.path.join(WALLET_DIR, ""multibit-wallet.key"")
        temp_dir        = tempfile.mkdtemp(""-test-btcr"")
        temp_wallet_filename = os.path.join(temp_dir, os.path.basename(wallet_filename))
        shutil.copyfile(wallet_filename, temp_wallet_filename)

        btcrpass.loaded_wallet = btcrpass.WalletMultiBit.load_from_filename(temp_wallet_filename)

        btcrecover.opencl_helpers.auto_select_opencl_platform(btcrpass.loaded_wallet)

        btcrecover.opencl_helpers.init_opencl_contexts(btcrpass.loaded_wallet)

        self.assertEqual(btcrpass.WalletMultiBit._return_verified_password_or_false_opencl(btcrpass.loaded_wallet,
            [tstr(""btcr-wrong-password-1""), tstr(""btcr-wrong-password-2"")]), (False, 2),
            ""Platform:"" + str(btcrpass.loaded_wallet.opencl_platform) + "" found a false positive"")
        self.assertEqual(btcrpass.WalletMultiBit._return_verified_password_or_false_opencl(btcrpass.loaded_wallet,
            [tstr(""btcr-wrong-password-3""), tstr(""btcr-test-password""), tstr(""btcr-wrong-password-4"")]), (tstr(""btcr-test-password""), 2),
            ""Platform:"" + str(btcrpass.loaded_wallet.opencl_platform) + "" failed to find password"")

        del btcrpass.loaded_wallet
",btcrecover/test/test_passwords.py,Test07WalletDecryption,0,7
survived,"def honda_checksum(address: int, sig: Signal, d: bytearray) -> int:
    s = 0
    extended = address > 0x7FF
    addr = address
    while addr:
        s += addr & 0xF
        addr >>= 4
    for i in range(len(d)):
        x = d[i]
        if i == len(d) - 1:
            x >>= 4
        s += (x & 0xF) + (x >> 4)
    s = 8 - s
    if extended:
        s += 3
    return s & 0xF
",opendbc/can/packer.py,,1,7
survived,"    def magnitude(self) -> float:
        """"""Return the Euclidean norm.""""""
        return (self.x ** 2 + self.y ** 2) ** 0.5",runtime/ffi/python/testmod.py,Point,1,7
survived,"            def on_console(msg: object) -> None:
                entry = f""[{msg.type}] {msg.text}""
                logs.append(entry)
                print(entry, file=sys.stderr)
",scripts/verify_insight_offline.py,,1,7
survived,"def publish_root(*, db_path: str | Path | None = None, out_file: str | Path = ""archive_root.json"") -> str:
    """"""Publish today's Merkle root and store it in ``out_file``.""""""
    path = Path(db_path or os.getenv(""ARCHIVE_PATH"", ""archive.db""))
    arch = HashArchive(path)
    cid = arch.publish_daily_root()
    Path(out_file).write_text(json.dumps({""cid"": cid}), encoding=""utf-8"")
    return cid
",src/archive/cron.py,,1,7
survived,"def _update_root(path: Path) -> str:
    root = merkle_root(db_path=path)
    date = time.strftime(""%Y-%m-%d"")
    with sqlite3.connect(path) as cx:
        cx.execute(""INSERT OR REPLACE INTO merkle(date, root) VALUES(?,?)"", (date, root))
    return root
",src/archive/archive.py,,1,7
survived,"def utc_now() -> str:
    return datetime.now(timezone.utc).isoformat(timespec=""milliseconds"")
",alpha_factory_v1/backend/agent_manager.py,,1,7
survived,"        def inc(self, *_a: Any) -> None:
            ...
",alpha_factory_v1/backend/telemetry.py,_Metric,0,7
survived,"async def hb_watch(runners: Dict[str, AgentRunner]) -> None:
    while True:
        now = time.time()
        for n, r in runners.items():
            alive = int(now - r.last_beat < r.period * 3.0)
            MET_UP.labels(n).set(alive)
        await asyncio.sleep(5)
",alpha_factory_v1/backend/agent_manager.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_join.py,Customer,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/join_multi.py,Item,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_multi_join.py,Nation,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/order_by_map.py,Data,1,7
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/cross_join.py,Customer,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_multi_join.py,Partsupp,1,6
survived,"    def list_agents(_detail: bool = False) -> list[str]:  # noqa: D401
        return [""dummy"", ""fail""]
",tests/test_backend_orchestrator_dev.py,,1,6
survived,"def dev_orchestrator(monkeypatch: pytest.MonkeyPatch) -> orch_mod.Orchestrator:
    monkeypatch.setenv(""DEV_MODE"", ""true"")
    monkeypatch.setenv(""API_TOKEN"", ""test-token"")
    monkeypatch.setenv(""AGENT_ERR_THRESHOLD"", ""1"")

    from alpha_factory_v1.backend.agents import _HEALTH_Q
    import inspect
    import time

    def list_agents(_detail: bool = False) -> list[str]:  # noqa: D401
        return [""dummy"", ""fail""]

    def get_agent(name: str) -> object:  # noqa: D401
        agent = DummyAgent() if name == ""dummy"" else FailingAgent()

        if hasattr(agent, ""step"") and inspect.iscoroutinefunction(agent.step):
            orig = agent.step

            async def _wrapped(*a: object, **kw: object) -> object:
                t0 = time.perf_counter()
                ok = True
                try:
                    return await orig(*a, **kw)
                except Exception:
                    ok = False
                    raise
                finally:
                    _HEALTH_Q.put((name, (time.perf_counter() - t0) * 1000, ok))

            agent.step = _wrapped
        return agent

    monkeypatch.setattr(""alpha_factory_v1.backend.agents.list_agents"", list_agents)
    monkeypatch.setattr(""alpha_factory_v1.backend.agents.get_agent"", get_agent)
    monkeypatch.setattr(""alpha_factory_v1.backend.agent_runner.get_agent"", get_agent)
    start_background_tasks()

    orch = orch_mod.Orchestrator()
    yield orch
",tests/test_backend_orchestrator_dev.py,,1,7
survived,"        def register(self, *a, **kw) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AgentRuntime,0,7
survived,"async def test_run_cycle_async_context() -> None:
    model = DummyModel()
    demo.run_cycle(
        demo.Orchestrator(),
        demo.AgentFin(),
        demo.AgentRes(),
        demo.AgentEne(),
        demo.AgentGdl(),
        model,
    )
    await asyncio.sleep(0)
    assert model.committed",tests/test_alpha_agi_business_3_v1.py,,1,6
survived,"def _dtm_to_corpus(dtm: Any) -> List[List[int]]:
    """"""Convert a document-term matrix into an integer corpus.""""""
    if sparse.issparse(dtm):
        dtm = dtm.toarray()
    else:
        dtm = np.asarray(dtm)
    corpus: List[List[int]] = []
    for row in dtm:
        doc: List[int] = []
        for idx, count in enumerate(row):
            if count:
                doc.extend([idx] * int(count))
        corpus.append(doc)
    return corpus
",src/hlda/sklearn_wrapper.py,,1,6
survived,"def test_rate_limit(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""API_RATE_LIMIT"", ""2"")
    from src.interface import api_server as api
    api = importlib.reload(api)
    client = TestClient(cast(Any, api.app))
    headers = {""Authorization"": ""Bearer test-token""}

    assert client.get(""/runs"", headers=headers).status_code == 200
    assert client.get(""/runs"", headers=headers).status_code == 200
    resp = client.get(""/runs"", headers=headers)
    assert resp.status_code == 429",tests/test_api_rate_limit.py,,1,6
survived,"    async def run() -> None:
        with mock.patch.object(orch.bus, ""stop"", mock.AsyncMock()) as bus_stop, \
             mock.patch.object(orch.ledger, ""stop_merkle_task"", mock.AsyncMock()) as merkle_stop:
            task = asyncio.create_task(orch.run_forever())
            await asyncio.sleep(0.05)
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
            bus_stop.assert_awaited_once()
            merkle_stop.assert_awaited_once()
",tests/test_orchestrator.py,,1,7
survived,"async def sync_entra_groups(
    neo4j_session: neo4j.Session,
    tenant_id: str,
    client_id: str,
    client_secret: str,
    update_tag: int,
    common_job_parameters: Dict[str, Any],
) -> None:
    """"""Sync Entra groups.""""""
    credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)
    client = GraphServiceClient(credential, scopes=[""https://graph.microsoft.com/.default""])

    groups = await get_entra_groups(client)

    member_map: Dict[str, List[str]] = {}
    for group in groups:
        try:
            member_map[group.id] = await get_group_members(client, group.id)
        except Exception as e:
            logger.error(f""Failed to fetch members for group {group.id}: {e}"")
            member_map[group.id] = []

    transformed_groups = transform_groups(groups, member_map)

    load_tenant(neo4j_session, {""id"": tenant_id}, update_tag)
    load_groups(neo4j_session, transformed_groups, update_tag, tenant_id)
    cleanup_groups(neo4j_session, common_job_parameters)",cartography/intel/entra/groups.py,,1,7
survived,"def transform_groups(groups: List[Group], member_map: Dict[str, List[str]]) -> List[Dict[str, Any]]:
    """"""Transform API responses into dictionaries for ingestion.""""""
    result: List[Dict[str, Any]] = []
    for g in groups:
        transformed = {
            ""id"": g.id,
            ""display_name"": g.display_name,
            ""description"": g.description,
            ""mail"": g.mail,
            ""mail_nickname"": g.mail_nickname,
            ""mail_enabled"": g.mail_enabled,
            ""security_enabled"": g.security_enabled,
            ""group_types"": g.group_types,
            ""visibility"": g.visibility,
            ""is_assignable_to_role"": g.is_assignable_to_role,
            ""created_date_time"": g.created_date_time,
            ""deleted_date_time"": g.deleted_date_time,
            ""member_ids"": member_map.get(g.id, []),
        }
        result.append(transformed)
    return result
",cartography/intel/entra/groups.py,,1,7
survived,"def test_invalid_seed_fallback(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
    """"""Invalid ALPHA_ASI_SEED should trigger fallback to default.""""""
    module = ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    mod = importlib.import_module(module)
    monkeypatch.setenv(""ALPHA_ASI_SEED"", ""bad"")
    caplog.set_level(""WARNING"")
    cfg = mod._load_cfg()
    assert mod._SEED == 42
    assert any(""Invalid seed"" in rec.message for rec in caplog.records)
    assert isinstance(cfg, mod.Config)",tests/test_world_model_demo.py,,1,6
survived,"    def install(self, logger: LanguageServerLogger, target_dir: str) -> Dict[str, str]:
        """"""Install all dependencies for the current platform into *target_dir*.

        Returns a mapping from dependency id to the resolved binary path.
        """"""
        os.makedirs(target_dir, exist_ok=True)
        results: Dict[str, str] = {}
        for dep in self.for_current_platform():
            if dep.url:
                self._install_from_url(dep, logger, target_dir)
            if dep.command:
                self._run_command(dep.command, target_dir)
            if dep.binary_name:
                results[dep.id] = os.path.join(target_dir, dep.binary_name)
            else:
                results[dep.id] = target_dir
        return results
",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection,1,7
survived,"def main() -> None:
    html_text = build_html(collect_entries())
    GALLERY_FILE.write_text(html_text, encoding=""utf-8"")
    print(f""Wrote {GALLERY_FILE.relative_to(REPO_ROOT)}"")
",scripts/generate_gallery_html.py,,1,7
survived,"def test_bundle_generator_custom_metadata(tmp_path: Path) -> None:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""print('agent')"",
        metadata_fields={""meta_agent_version"": ""1.2.3"", ""extra"": ""field""},
        custom_metadata={""tag"": ""example""},
    )

    with open(tmp_path / ""bundle.json"", encoding=""utf-8"") as f:
        data = json.load(f)

    assert data[""meta_agent_version""] == ""1.2.3""
    assert data[""extra""] == ""field""
    assert data[""custom""][""tag""] == ""example""",tests/test_bundle_generator.py,,1,7
survived,"    def push(self, remote: str = ""origin"", branch: str = ""main"") -> None:
        self._run(""push"", remote, f""HEAD:{branch}"")",src/meta_agent/git_utils.py,GitManager,1,7
survived,"def test_bundle_generator_git(tmp_path: Path) -> None:
    remote = tmp_path / ""remote.git""
    subprocess.run([""git"", ""init"", ""--bare"", str(remote)], check=True)

    repo = tmp_path / ""repo""
    gen = BundleGenerator(repo)
    gen.generate(agent_code=""print('x')"", init_git=True, git_remote=str(remote))

    assert (repo / "".git"").exists()
    commit = subprocess.check_output(
        [""git"", ""-C"", str(repo), ""rev-parse"", ""HEAD""], text=True
    ).strip()
    with open(repo / ""bundle.json"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[""custom""][""git_commit""] == commit

    log = subprocess.check_output(
        [""git"", ""-C"", str(remote), ""log"", ""--oneline""], text=True
    )
    assert commit[:7] in log",tests/test_bundle_generator.py,,1,7
survived,"    def commit_all(self, message: str = ""Initial commit"") -> str:
        """"""Add all files and create a commit.

        Returns the commit SHA.
        """"""
        env = os.environ.copy()
        # Deterministic commit timestamp
        env.setdefault(""GIT_AUTHOR_DATE"", ""1970-01-01T00:00:00+0000"")
        env.setdefault(""GIT_COMMITTER_DATE"", ""1970-01-01T00:00:00+0000"")
        self._run(""add"", ""-A"", env=env)
        self._run(""commit"", ""-m"", message, env=env)
        result = self._run(""rev-parse"", ""HEAD"", env=env)
        return result.stdout.strip()
",src/meta_agent/git_utils.py,GitManager,1,7
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator,1,7
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator,1,6
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,,1,8
survived,"def diagd(c1, c2, r):
    c = c1
    while c <= c2:
        n[r + c - c1][c] = ""x""
        c = c + 1
",tests/rosetta/transpiler/Python/cistercian-numerals.py,,0,6
survived,"def quibble(items):
    n = len(items)
    if n == 0:
        return ""{}""
    else:
        if n == 1:
            return ""{"" + items[0] + ""}""
        else:
            if n == 2:
                return ""{"" + items[0] + "" and "" + items[1] + ""}""
            else:
                prefix = """"
                for i in range(0, n - 1):
                    if i == n - 1:
                        break
                    if i > 0:
                        prefix = prefix + "", ""
                    prefix = prefix + items[i]
                return ""{"" + prefix + "" and "" + items[n - 1] + ""}""
",tests/rosetta/transpiler/Python/comma-quibbling.py,,1,6
survived,"def parseProgram(src):
    lines = split(src, ""\n"")
    header = fields(lines[0])
    dataSize = parseIntStr(header[1])
    nStrings = parseIntStr(header[3])
    stringPool = []
    i = 1
    while i <= nStrings:
        s = lines[i]
        if len(s) > 0:
            stringPool = stringPool + [unescape(s[1:len(s) - 1])]
        i = i + 1
    code = []
    addrMap = {}
    while i < len(lines):
        line = trim(lines[i])
        if len(line) == 0:
            break
        parts = fields(line)
        addr = parseIntStr(parts[0])
        op = parts[1]
        arg = 0
        if op == ""push"":
            arg = parseIntStr(parts[2])
        else:
            if op == ""fetch"" or op == ""store"":
                arg = parseIntStr(parts[2][1:len(parts[2]) - 1])
            else:
                if op == ""jmp"" or op == ""jz"":
                    arg = parseIntStr(parts[3])
        code = code + [{""addr"": addr, ""op"": op, ""arg"": arg}]
        addrMap[addr] = len(code) - 1
        i = i + 1
    return {""dataSize"": dataSize, ""strings"": stringPool, ""code"": code, ""addrMap"": addrMap}
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,,1,7
survived,"def sqrtApprox(x):
    guess = x
    i = 0
    while i < 20:
        guess = (guess + x / guess) / 2.0
        i = i + 1
    return guess
",tests/rosetta/transpiler/Python/cholesky-decomposition.py,,1,6
survived,"def makeSym(order, elements):
    return {""order"": order, ""ele"": elements}
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,,1,6
survived,"def commatize(n):
    s = str(n)
    out = """"
    i = len(s) - 1
    c = 0
    while i >= 0:
        out = """".join(s[i:i + 1]) + out
        c = c + 1
        if c % 3 == 0 and i > 0:
            out = "","" + out
        i = i - 1
    return out
",tests/rosetta/transpiler/Python/composite-numbers-k-with-no-single-digit-factors-whose-factors-are-all-substrings-of-k.py,,1,6
survived,"def main():
    print(""zero = "" + str(toInt(zero())))
    onev = one()
    print(""one = "" + str(toInt(onev)))
    two = succ(succ(zero()))
    print(""two = "" + str(toInt(two)))
    three = plus(onev, two)
    print(""three = "" + str(toInt(three)))
    four = mult(two, two)
    print(""four = "" + str(toInt(four)))
    eight = exp(two, three)
    print(""eight = "" + str(toInt(eight)))
    print(""toStr(four) = "" + toStr(four))",tests/rosetta/transpiler/Python/church-numerals-2.py,,1,7
survived,"def mult(m, n):
    return compose(m, n)
",tests/rosetta/transpiler/Python/church-numerals-2.py,,0,7
survived,"def main():
    while True:
        line = input()
        if line == """":
            break
        print(line)
",tests/rosetta/transpiler/Python/copy-stdin-to-stdout-2.py,,1,7
survived,"def test_throttle_alert(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""API_RATE_LIMIT"", ""1"")
    from src.interface import api_server as mod
    api = importlib.reload(mod)

    sent: list[str] = []
    monkeypatch.setattr(api.alerts, ""send_alert"", lambda msg, url=None: sent.append(msg))

    client = TestClient(cast(Any, api.app))
    headers = {""Authorization"": ""Bearer test-token""}

    client.get(""/runs"", headers=headers)
    client.get(""/runs"", headers=headers)

    stack = api.app.middleware_stack
    metrics = stack.app.app
    limiter = metrics.app
    metrics.window_start = time.time() - 61
    limiter.counters[""testclient""] = (0, time.time())

    client.get(""/runs"", headers=headers)

    assert sent, ""alert not triggered""

    monkeypatch.setenv(""API_RATE_LIMIT"", ""1000"")
    importlib.reload(api)",tests/test_api_server_static.py,,1,6
survived,"    def handle(self, error: UXError) -> None:
        """"""Log the error and display the message via CLI.""""""
        msg = str(error)
        if error.context:
            self.logger.error(""%s | context=%s"", msg, error.context)
        else:
            self.logger.error(msg)
        try:
            self.cli_output.error(msg)
        except Exception:
            self.logger.error(""Failed to output error message via CLI"")",src/meta_agent/ux/error_handler.py,ErrorHandler,1,7
survived,"def test_diagram_generation_error_subclass():
    assert issubclass(DiagramGenerationError, UXError)",tests/ux/test_error_handler.py,,1,7
survived,"    def __init__(
        self,
        cli_output: CLIOutput | None = None,
        log: logging.Logger | None = None,
    ) -> None:
        from .cli_output import CLIOutput  # local import to avoid circular

        self.cli_output = cli_output or CLIOutput()
        self.logger = log or logger
",src/meta_agent/ux/error_handler.py,ErrorHandler,1,7
survived,"def test_progress_iter(capsys):
    fb = UserFeedback()
    items = [1, 2, 3]
    result = list(fb.progress_iter(items, description=""doing""))
    out, err = capsys.readouterr()
    assert result == items
    assert ""doing"" in click.unstyle(out + err)
",tests/ux/test_user_feedback.py,,1,7
survived,"    def progress_iter(self, iterable: Iterable[T], *, description: str = ""Working"") -> Iterator[T]:
        """"""Yield items from ``iterable`` while displaying a progress bar.""""""
        with click.progressbar(iterable, label=description) as bar:
            for item in bar:
                yield item
",src/meta_agent/ux/user_feedback.py,UserFeedback,1,7
survived,"def test_bundle_load_and_list_files(tmp_path: Path) -> None:
    gen = BundleGenerator(tmp_path)
    gen.generate(agent_code=""print('hi')"")

    b = Bundle(tmp_path)
    assert b.metadata.schema_version
    files = b.list_files()
    assert ""agent.py"" in files
    assert b.read_text(""agent.py"").strip() == ""print('hi')""
",tests/test_bundle_api.py,,1,8
survived,"def test_adk_list_packages():
    adapter = ADKAdapter()
    pkgs = adapter.list_packages()
    assert isinstance(pkgs, list)
",tests/test_adapters.py,,1,6
survived,"        def __init__(self, settings: config.Settings) -> None:
            self.settings = settings
            self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_adapters.py,DummyBus,1,7
survived,"    def generate_text(self, prompt: str) -> str:
        """"""Generate text using ``adk.Client`` if the method exists.""""""
        gen_fn = getattr(self._client, ""generate"", None)
        if not callable(gen_fn):
            raise AttributeError(""generate not available"")
        return gen_fn(prompt)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_adapter.py,ADKAdapter,1,7
survived,"def test_mcp_adapter_unavailable(monkeypatch) -> None:
    """"""Adapter gracefully degrades when MCP is missing.""""""

    def _raise(_name: str):
        raise ModuleNotFoundError

    monkeypatch.setattr(importlib, ""import_module"", _raise)
    assert not MCPAdapter.is_available()
    with pytest.raises(ModuleNotFoundError):
        MCPAdapter()",tests/test_adapters.py,,1,7
survived,"def test_pad():
    Height = Axis(""Height"", 3)
    Width = Axis(""Width"", 2)

    arr = hax.arange((Height, Width))
    padded = hax.pad(arr, {Height: (1, 2), Width: (0, 1)}, mode=""constant"", constant_values=0)

    expected = jnp.pad(arr.array, [(1, 2), (0, 1)], mode=""constant"", constant_values=0)
    assert padded.axes[0].size == Height.size + 3
    assert padded.axes[1].size == Width.size + 1
    assert jnp.all(expected == padded.array)",tests/test_ops.py,,1,7
survived,"def test_check_env_errors_without_core(monkeypatch):
    calls = []
    orig_find_spec = importlib.util.find_spec

    def fake_find_spec(name, *args, **kwargs):
        if name in {""numpy"", ""pandas""}:
            return None
        calls.append(name)
        return orig_find_spec(name, *args, **kwargs)

    monkeypatch.setattr(importlib.util, ""find_spec"", fake_find_spec)
    rc = check_env.main([])
    assert rc != 0
",tests/test_check_env_core.py,,1,6
survived,"def _load_banned_hosts() -> set[str]:
    policy_path = _POLICY_DIR / ""deny_finance.rego""
    try:
        text = policy_path.read_text(encoding=""utf-8"")
    except FileNotFoundError:
        return set()
    m = re.search(r""banned_hosts\s*=\s*{([^}]*)}"", text, re.DOTALL)
    if not m:
        return set()
    hosts = [h.strip().strip('""') for h in m.group(1).split(',') if h.strip()]
    return set(hosts)
",src/utils/opa_policy.py,,1,7
survived,"    def decorator(func):
        return func
",openai_agents/__init__.py,,1,6
survived,"async def run_demo_loop(agent: Agent[Any], *, stream: bool = True) -> None:
    """"""Run a simple REPL loop with the given agent.

    This utility allows quick manual testing and debugging of an agent from the
    command line. Conversation state is preserved across turns. Enter ``exit``
    or ``quit`` to stop the loop.

    Args:
        agent: The starting agent to run.
        stream: Whether to stream the agent output.
    """"""

    current_agent = agent
    input_items: list[TResponseInputItem] = []
    while True:
        try:
            user_input = input("" > "")
        except (EOFError, KeyboardInterrupt):
            print()
            break
        if user_input.strip().lower() in {""exit"", ""quit""}:
            break
        if not user_input:
            continue

        input_items.append({""role"": ""user"", ""content"": user_input})

        result: RunResultBase
        if stream:
            result = Runner.run_streamed(current_agent, input=input_items)
            async for event in result.stream_events():
                if isinstance(event, RawResponsesStreamEvent):
                    if isinstance(event.data, ResponseTextDeltaEvent):
                        print(event.data.delta, end="""", flush=True)
                elif isinstance(event, RunItemStreamEvent):
                    if event.item.type == ""tool_call_item"":
                        print(""\n[tool called]"", flush=True)
                    elif event.item.type == ""tool_call_output_item"":
                        print(f""\n[tool output: {event.item.output}]"", flush=True)
                    elif event.item.type == ""message_output_item"":
                        message = ItemHelpers.text_message_output(event.item)
                        print(message, end="""", flush=True)
                elif isinstance(event, AgentUpdatedStreamEvent):
                    print(f""\n[Agent updated: {event.new_agent.name}]"", flush=True)
            print()
        else:
            result = await Runner.run(current_agent, input_items)
            if result.final_output is not None:
                print(result.final_output)

        current_agent = result.last_agent
        input_items = result.to_input_list()",src/agents/repl.py,,1,7
survived,"    async def _fail_handshake(self, peer: str, context: Any) -> bytes:
        """"""Record a handshake failure and abort if the limit is exceeded.""""""
        count = self._handshake_failures.get(peer, 0) + 1
        self._handshake_failures[peer] = count
        if count >= self.settings.bus_fail_limit:
            if grpc:
                await context.abort(grpc.StatusCode.PERMISSION_DENIED, ""too many handshake failures"")
            return b""denied""
        if grpc:
            await context.abort(grpc.StatusCode.FAILED_PRECONDITION, ""handshake required"")
        return b""handshake required""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus,1,7
survived,"    async def get_resource(ctx: EnrichContext, **kwargs: int) -> enrich_model | None:  # type: ignore[name-defined]
        entity_id = kwargs[param_name]
        session_factory = ctx.request_context.lifespan_context[session_key]
        async with session_factory() as session:  # type: AsyncSession
            obj = await session.get(sa_model, entity_id)
            return _sa_to_enrich(obj, enrich_model) if obj else None
",src/enrichmcp/sqlalchemy/auto.py,,1,7
survived,"            def _create_resolver(f_name=field_name, model=sa_model, target=target_model):
                async def func(entity_id: int, ctx: EnrichContext) -> Any | None:
                    session_factory = ctx.request_context.lifespan_context[session_key]
                    async with session_factory() as session:  # type: AsyncSession
                        obj = await session.get(model, entity_id)
                        if not obj:
                            return None
                        await session.refresh(obj, [f_name])
                        value = getattr(obj, f_name)
                        return _sa_to_enrich(value, target) if value else None

                return func
",src/enrichmcp/sqlalchemy/auto.py,,1,6
survived,"def create_app():
    engine = create_async_engine(""sqlite+aiosqlite:///:memory:"")
    lifespan = sqlalchemy_lifespan(Base, engine, seed=seed)
    app = EnrichMCP(""Test"", ""Desc"", lifespan=lifespan)
    include_sqlalchemy_models(app, Base)
    return app, lifespan
",tests/test_sqlalchemy_autogen.py,,1,7
survived,"                async def func(entity_id: int, ctx: EnrichContext) -> Any | None:
                    session_factory = ctx.request_context.lifespan_context[session_key]
                    async with session_factory() as session:  # type: AsyncSession
                        obj = await session.get(model, entity_id)
                        if not obj:
                            return None
                        await session.refresh(obj, [f_name])
                        value = getattr(obj, f_name)
                        return _sa_to_enrich(value, target) if value else None
",src/enrichmcp/sqlalchemy/auto.py,,1,7
survived,"async def seed(session: AsyncSession) -> None:
    user = User(id=1, name=""Alice"")
    order = Order(id=1, user=user)
    session.add_all([user, order])
",tests/test_sqlalchemy_autogen.py,,1,6
survived,"    def health(self) -> str:
        """"""Return ``'ok'`` if the orchestrator is healthy.""""""
        url = f""{self.base_url}/healthz""
        resp = requests.get(url)
        resp.raise_for_status()
        return resp.text
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,MarketplaceClient,1,7
survived,"    def create_module(self, spec):
        return None  # use default machinery
",jac/jaclang/runtimelib/meta_importer.py,JacMetaImporter,1,7
survived,"def health() -> dict[str, str]:
    """"""Return service health status.""""""
    return {""status"": ""ok""}",test_repo/backend/main.py,,1,8
survived,"def test_health() -> None:
    response = client.get(""/health"")
    assert response.status_code == 200
    assert response.json() == {""status"": ""ok""}",test_repo/backend/tests/test_demo_main.py,,1,8
survived,"def test_openai_link_head() -> None:
    url = dg.model_urls(""124M"")[0]
    resp = requests.head(url, timeout=10)
    assert resp.status_code == 200",tests/test_download_openai_gpt2.py,,0,7
survived,"    def test_verify_wheel(self) -> None:
        priv = Ed25519PrivateKey.generate()
        pub_b64 = base64.b64encode(
            priv.public_key().public_bytes(
                encoding=serialization.Encoding.Raw,
                format=serialization.PublicFormat.Raw,
            )
        ).decode()
        wheel = Path(""test.whl"")
        wheel.write_bytes(b""demo"")
        sig = base64.b64encode(priv.sign(b""demo"")).decode()
        sig_file = Path(""test.whl.sig"")
        sig_file.write_text(sig)
        orig_pub = agents_mod._WHEEL_PUBKEY
        orig_sigs = agents_mod._WHEEL_SIGS.copy()
        try:
            agents_mod._WHEEL_PUBKEY = pub_b64
            agents_mod._WHEEL_SIGS = {wheel.name: sig}
            self.assertTrue(agents_mod._verify_wheel(wheel))
        finally:
            agents_mod._WHEEL_PUBKEY = orig_pub
            agents_mod._WHEEL_SIGS = orig_sigs
            wheel.unlink()
            sig_file.unlink()
",tests/test_wheel_signature.py,TestWheelSignature,1,7
survived,"def test_selfheal_live_endpoint() -> None:
    script = Path(""alpha_factory_v1/demos/self_healing_repo/agent_selfheal_entrypoint.py"")
    env = os.environ.copy()
    env.setdefault(""OPENAI_API_KEY"", """")
    proc = subprocess.Popen([sys.executable, str(script)], env=env)
    url = ""http://127.0.0.1:7863/__live""
    try:
        for _ in range(50):
            try:
                r = httpx.get(url)
                if r.status_code == 200:
                    break
            except Exception:
                time.sleep(0.1)
        else:
            raise AssertionError(""server did not start"")
        assert r.status_code == 200
        assert r.text.strip() == ""OK""
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_selfheal_entrypoint.py,,0,7
survived,"  def supports_active_cooling(self) -> bool:
    return False
",pylabrobot/heating_shaking/hamilton_backend.py,HamiltonHeaterShakerBackend,1,6
survived,"  async def test_cannot_cool_without_support(self):
    backend = TemperatureControllerChatterboxBackend(dummy_temperature=20.0)
    tc = TemperatureController(
      name=""tc"",
      size_x=1,
      size_y=1,
      size_z=1,
      backend=backend,
      child_location=Coordinate.zero(),
    )

    with self.assertRaises(ValueError):
      await tc.set_temperature(10)
",pylabrobot/temperature_controlling/temperature_controller_tests.py,PassiveCoolingTests,1,7
survived,"def test_search_images_by_text(monkeypatch):
    from importlib import reload

    import np_ocr.search as search
    reload(search)

    class FakeTable:
        def search(self, *_):
            class Limiter:
                def limit(self, *_):
                    class Selector:
                        def select(self, *_):
                            return self

                        def to_list(self):
                            return [{""_distance"": 0.1, ""index"": 0, ""pdf_name"": ""x.pdf"", ""pdf_page"": 1}]

                    return Selector()

            return Limiter()

    class FakeDB:
        def open_table(self, _):
            return FakeTable()

    monkeypatch.setattr(search.lancedb, ""connect"", lambda *_: FakeDB())

    class FakeColPali:
        def query_text(self, _):
            return {""embedding"": [0.0]}

    client = search.SearchClient(storage_dir=""s"", vector_size=1, base_url=""b"", token=""t"")
    client.colpali_client = FakeColPali()
    res = client.search_images_by_text(""q"", case_name=""c"", user_id=""u"", top_k=1)
    assert res[0][""pdf_name""] == ""x.pdf""
",no-ocr-api/tests/test_ingest_search.py,,1,7
survived,"                        def to_list(self):
                            return [{""_distance"": 0.1, ""index"": 0, ""pdf_name"": ""x.pdf"", ""pdf_page"": 1}]
",no-ocr-api/tests/test_ingest_search.py,FakeTable.Limiter.Selector,1,6
survived,"def test_large_payloads_delivered_intact(
    sender: str, recipient: str, payload_text: str, ts: float
) -> None:  # type: ignore[misc]
    """"""Envelopes with huge strings should round-trip through the bus.""""""

    bus = messaging.A2ABus(config.Settings(bus_port=0))
    received: list[messaging.Envelope] = []

    async def handler(env: messaging.Envelope) -> None:
        received.append(env)

    bus.subscribe(recipient, handler)
    env = messaging.Envelope(sender=sender, recipient=recipient, ts=ts)
    env.payload[""data""] = payload_text

    async def run() -> None:
        bus.publish(recipient, env)
        await asyncio.sleep(0)

    asyncio.run(run())

    assert received
    assert received[0].sender == sender
    assert received[0].recipient == recipient
    assert received[0].payload[""data""] == payload_text
    assert received[0].ts == ts
",tests/test_bus_large_payloads_property.py,,1,7
survived,"        async def run() -> None:
            async with messaging.A2ABus(cfg) as bus:
                env = types.SimpleNamespace(sender=""s"", recipient=""x"", payload={""bad"": bad}, ts=0.0)
                with pytest.raises(TypeError):
                    bus.publish(""x"", env)
                    await asyncio.sleep(0)
",tests/test_bus_large_payloads_property.py,,1,7
survived,"def stub_mcp(monkeypatch: pytest.MonkeyPatch):
    mod = types.ModuleType(""mcp"")

    class ClientSessionGroup:
        async def call_tool(self, name: str, args: dict[str, object]):
            async with httpx.AsyncClient() as client:
                resp = await client.post(f""https://mcp.example/{name}"", json=args)
                resp.raise_for_status()
                return resp.json()

    mod.ClientSessionGroup = ClientSessionGroup
    monkeypatch.setitem(sys.modules, ""mcp"", mod)
    yield mod
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,,1,6
survived,"def sha384(path: Path) -> str:
    digest = hashlib.sha384(path.read_bytes()).digest()
    return ""sha384-"" + base64.b64encode(digest).decode()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,7
survived,"def inject_env() -> str:
    return (
        ""<script>""
        f'window.PINNER_TOKEN={json.dumps(os.getenv(""PINNER_TOKEN"", """"))};'
        f'window.OPENAI_API_KEY={json.dumps(os.getenv(""OPENAI_API_KEY"", """"))};'
        f'window.OTEL_ENDPOINT={json.dumps(os.getenv(""OTEL_ENDPOINT"", """"))};'
        f'window.IPFS_GATEWAY={json.dumps(os.getenv(""IPFS_GATEWAY"", """"))};'
        ""</script>""
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,0,9
survived,"    def __init__(self):
        super().__init__(
            id=""3fd9c73d-4370-4925-a1ff-1b86b99fabfa"",
            description=(
                ""Edit images using BlackForest Labs' Flux Kontext models. Provide a prompt ""
                ""and optional reference image to generate a modified image.""
            ),
            categories={BlockCategory.AI, BlockCategory.MULTIMEDIA},
            input_schema=FluxKontextBlock.Input,
            output_schema=FluxKontextBlock.Output,
            test_input={
                ""prompt"": ""Add a hat to the cat"",
                ""input_image"": ""https://example.com/cat.png"",
                ""aspect_ratio"": ""match_input_image"",
                ""seed"": None,
                ""model"": FluxKontextModelName.PRO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""image_url"", ""https://replicate.com/output/edited-image.png""),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, input_image, aspect_ratio, seed: ""https://replicate.com/output/edited-image.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )
",autogpt_platform/backend/backend/blocks/flux_kontext.py,FluxKontextBlock,1,7
survived,"def test_start_alpha_business_submit_best(monkeypatch) -> None:
    """"""--submit-best queues the top demo opportunity.""""""
    from alpha_factory_v1.demos.alpha_agi_business_v1 import start_alpha_business as mod

    class DummyProc:
        def poll(self) -> None:
            return None

        def terminate(self) -> None:
            pass

        def wait(self, timeout: float | None = None) -> None:
            pass

    dummy_proc = DummyProc()
    monkeypatch.setattr(mod.subprocess, ""Popen"", lambda *a, **k: dummy_proc)
    monkeypatch.setattr(mod.check_env, ""main"", lambda *_a, **_k: None)
    monkeypatch.setattr(mod.webbrowser, ""open"", lambda *_a, **_k: None)

    class Resp:
        def __init__(self) -> None:
            self.status_code = 200

        def raise_for_status(self) -> None:
            pass

    monkeypatch.setattr(mod.requests, ""get"", lambda *_a, **_k: Resp())
    post_calls: list[tuple] = []

    def fake_post(url: str, json: dict, timeout: int) -> Resp:
        post_calls.append((url, json, timeout))
        return Resp()

    monkeypatch.setattr(mod.requests, ""post"", fake_post)

    env = {""OPENAI_API_KEY"": """", ""AGENTS_RUNTIME_PORT"": ""7000"", ""PORT"": ""8000""}
    with monkeypatch.context() as mctx:
        for k, v in env.items():
            mctx.setenv(k, v)
        mod.main([""--no-browser"", ""--submit-best""])

    assert post_calls == [
        (
            ""http://localhost:7000/v1/agents/business_helper/invoke"",
            {""action"": ""best_alpha""},
            10,
        )
    ]",tests/test_start_alpha_business.py,,1,7
survived,"        def raise_for_status(self) -> None:
            pass
",tests/test_start_alpha_business.py,Resp,0,7
survived,"    def __init__(self):
        self.loaded = None
",alpha_factory_v1/tests/test_orchestrator_rest.py,DummyAgent,1,7
survived,"    def load_weights(self, path):
        self.loaded = path
",alpha_factory_v1/tests/test_orchestrator_rest.py,DummyAgent,1,6
survived,"    def list_templates(self) -> List[Dict[str, Any]]:
        manifest = self._load_manifest()
        templates = []
        for slug, entry in manifest.items():
            versions = [
                {""version"": v, **data}
                for v, data in sorted(
                    entry.get(""versions"", {}).items(),
                    key=lambda item: parse_version(item[0]),
                    reverse=True,
                )
            ]
            templates.append(
                {
                    ""slug"": slug,
                    ""current_version"": entry.get(""current_version""),
                    ""versions"": versions,
                }
            )
        return templates
",src/meta_agent/template_registry.py,TemplateRegistry,1,7
survived,"        def chat(self, system: str, user: str, temperature: float = 0.4, max_tokens: int = 1024) -> str:
            # deterministically return identity task
            return """"""```python # program\ndef main(x):\n    return x\n```\n```json # input\n3```\n```json # output\n3```""""""
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,_StubFM,1,6
survived,"def test_pareto_df() -> None:
    pop = [mats.Individual([0.0, 0.0]), mats.Individual([1.0, 1.0])]
    pop[0].rank = 0
    pop[1].rank = 1
    df = web_app.pareto_df(pop)
    assert set(df.columns) == {""x"", ""y"", ""rank""}
    assert len(df) == 2",tests/test_web_app.py,,1,7
survived,"def _disruption_df(traj: list[Any]) -> ""pd.DataFrame"":
    """"""Return the first disruption year per sector.""""""

    import pandas as pd

    years: dict[str, int] = {}
    for point in traj:
        for sec in point.sectors:
            if sec.disrupted and sec.name not in years:
                years[sec.name] = point.year
    return pd.DataFrame({""sector"": list(years.keys()), ""year"": list(years.values())})
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/web_app.py,,1,6
survived,"    def test_ledger_path_creation(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""dir"" / ""log.json""
            path = stub._ledger_path(ledger)
            self.assertEqual(path, ledger.resolve())
            self.assertTrue(path.parent.exists())
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha,1,7
survived,"    def test_discover_alpha_offline(self) -> None:
        openai_mock = types.SimpleNamespace(ChatCompletion=types.SimpleNamespace(create=Mock()))
        with patch.object(stub, ""openai"", openai_mock, create=True):
            with patch.dict(os.environ, {}, clear=True):
                picks = stub.discover_alpha(num=1, ledger=None, model=""gpt-4o-mini"")
        openai_mock.ChatCompletion.create.assert_not_called()
        self.assertIsInstance(picks, list)
        self.assertEqual(len(picks), 1)
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha,1,7
survived,"def test_namedarray_runtime_check_with_dtype():
    Batch = Axis(""batch"", 2)
    arr = NamedArray(jnp.zeros((Batch.size,), dtype=jnp.float32), (Batch,))
    assert arr.matches_axes(f32[""batch""])  # type: ignore
    assert not arr.matches_axes(i32[""batch""])  # type: ignore
",tests/test_namedarray_typing.py,,1,7
survived,"def test_assets_replaced() -> None:
    _assert_no_placeholder(BASE / ""lib"" / ""workbox-sw.js"")
    _assert_no_placeholder(BASE / ""lib"" / ""bundle.esm.min.js"")
    _assert_no_placeholder(BASE / ""dist"" / ""workbox-sw.js"")
    _assert_no_placeholder(BASE / ""dist"" / ""bundle.esm.min.js"")",tests/test_assets_replaced.py,,1,7
survived,"def main():
    parser = argparse.ArgumentParser(
        description=""Run hierarchical LDA on the BBC sample dataset""
    )
    parser.add_argument(
        ""--data-dir"", default=""bbc/tech"", help=""Directory containing BBC .txt files""
    )
    parser.add_argument(""--iterations"", type=int, default=100, help=""Number of Gibbs samples"")
    parser.add_argument(
        ""--display-topics"", type=int, default=50, help=""Report topics every N iterations""
    )
    parser.add_argument(
        ""--n-words"", type=int, default=5, help=""Number of words to display per topic""
    )
    parser.add_argument(
        ""--num-levels"", type=int, default=3, help=""Depth of the topic hierarchy""
    )
    parser.add_argument(""--alpha"", type=float, default=10.0, help=""Alpha hyperparameter"")
    parser.add_argument(""--gamma"", type=float, default=1.0, help=""Gamma hyperparameter"")
    parser.add_argument(""--eta"", type=float, default=0.1, help=""Eta hyperparameter"")
    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")

    args = parser.parse_args()
    run_demo(args)
",scripts/bbc_demo.py,,1,7
survived,"def test_download_invocation(monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
    calls: list[tuple[str, Path]] = []

    def fake_download(url: str, dest: Path) -> None:
        dest.parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(""ok"")
        calls.append((url, dest))

    monkeypatch.setattr(dg, ""_download"", fake_download)
    monkeypatch.setattr(dg, ""_verify"", lambda *_: None)
    dg.download_hf_gpt2(dest=tmp_path)
    assert len(calls) == len(dg._FILES)
    assert calls[0][0].startswith(dg._base_url())
",tests/test_download_hf_gpt2.py,,1,7
survived,"def test_reasonable_daily_metrics() -> None:
    """"""Moderate readings still return a bounded score.""""""
    sensors = {""steps"": 8000, ""resting_hr"": 65, ""sleep_hours"": 7, ""cal_intake"": 2500}
    value = fr.reward(None, None, sensors)
    assert 0.0 <= value <= 1.0
    assert value < 1.0",tests/test_fitness_reward.py,,1,7
survived,"def _reset_cache() -> None:
    cr._seen = cr._LRUCounter(cr._MAX_ENTRIES)
",tests/test_curiosity_reward.py,,1,6
survived,"async def test_stream_listener_returns_correct_chunk_xml_adapter():
    class MyProgram(dspy.Module):
        def __init__(self):
            super().__init__()
            self.predict1 = dspy.Predict(""question->answer"")
            self.predict2 = dspy.Predict(""question,answer->judgement"")

        def forward(self, question, **kwargs):
            answer = self.predict1(question=question, **kwargs).answer
            judgement = self.predict2(question=question, answer=answer, **kwargs)
            return judgement

    async def xml_stream_1(*args, **kwargs):
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""answer""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""To""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" get""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" to""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" the""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" other""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" side""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""!""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""/answer""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""completed""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])

    async def xml_stream_2(*args, **kwargs):
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""judgement""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""The""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" answer""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" is""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="" humorous""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="".""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""/judgement""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""<""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content=""completed""))])
        yield ModelResponseStream(model=""gpt-4o-mini"", choices=[StreamingChoices(delta=Delta(content="">""))])

    stream_generators = [xml_stream_1, xml_stream_2]

    async def completion_side_effect(*args, **kwargs):
        return stream_generators.pop(0)()

    with mock.patch(""litellm.acompletion"", side_effect=completion_side_effect):
        program = dspy.streamify(
            MyProgram(),
            stream_listeners=[
                dspy.streaming.StreamListener(signature_field_name=""answer""),
                dspy.streaming.StreamListener(signature_field_name=""judgement""),
            ],
        )
        with dspy.context(lm=dspy.LM(""openai/gpt-4o-mini"", cache=False), adapter=dspy.XMLAdapter()):
            output = program(question=""why did a chicken cross the kitchen?"")
            all_chunks = []
            async for value in output:
                if isinstance(value, dspy.streaming.StreamResponse):
                    all_chunks.append(value)

    assert all_chunks[0].predict_name == ""predict1""
    assert all_chunks[0].signature_field_name == ""answer""
    assert all_chunks[0].chunk == ""To get to the other side!""

    assert all_chunks[1].predict_name == ""predict2""
    assert all_chunks[1].signature_field_name == ""judgement""
    assert all_chunks[1].chunk == ""The answer is humorous.""",tests/streaming/test_streaming.py,,1,7
survived,"def valid_spec_dict():
    return {
        ""task_description"": ""Test agent for CLI"",
        ""inputs"": {""data"": ""string""},
        ""outputs"": {""status"": ""string""},
        ""constraints"": [""Must run quickly""],
        ""technical_requirements"": [""Python 3.10+""],
        ""metadata"": {""test_id"": ""cli-001""},
    }
",tests/integration/test_telemetry_integration.py,,1,7
survived,"        def __init__(self) -> None:
            self.spans: list[str] = []
",tests/test_metrics.py,DummyTracer,1,7
survived,"def _wait_ready(url: str) -> None:
    for _ in range(50):
        try:
            r = httpx.get(f""{url}/metrics"")
            if r.status_code == 200:
                return
        except Exception:
            time.sleep(0.1)
    raise AssertionError(""server did not start"")
",tests/test_metrics.py,,1,6
survived,"        def start_as_current_span(self, name: str) -> Any:
            self.spans.append(name)
            return nullcontext()
",tests/test_metrics.py,DummyTracer,1,6
survived,"    def validate_regex(cls, v: str) -> str:
        try:
            re.compile(v)
        except re.error as exc:  # pragma: no cover - safety check
            raise ValueError(f""Invalid regex pattern: {exc}"") from exc
        return v
",src/meta_agent/generators/guardrail_generator.py,GuardrailRule,1,7
survived,"    def render(self, text: str) -> Markup:
        html = self.md.convert(text or """")
        clean_html = bleach.clean(
            html,
            tags=self.allowed_tags,
            attributes=self.allowed_attributes,
            protocols=self.allowed_protocols,
            strip=True,
        )
        return Markup(clean_html)",app/utils/markdown_renderer.py,SafeMarkdownRenderer,1,7
survived,"    async def handle(self, _env: orchestrator.messaging.Envelope) -> None:  # pragma: no cover - helper
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,BoomAgent,0,7
survived,"def test_strip_color_codes():
    text = ""\x1b[31merror\x1b[0m""
    assert strip_color_codes(text) == 'error'
",tests/test_whois_perms.py,,1,7
survived,"def test_unicode_escape_sequence() -> None:
    chunks = ['{""a"": ""\\', 'u00e9""}']
    parsed = _stream_to_dict({}, chunks)
    assert parsed == {""a"": ""Ã©""}
",api/core/utils/streams_test.py,,1,7
survived,"    def _flush_pending_surrogate(self) -> None:
        if self._pending_surrogate is not None:
            self.current_chain += chr(self._pending_surrogate)
            self._pending_surrogate = None
",api/core/utils/streams.py,JSONStreamParser,1,6
survived,"def _q0():
    _src = valid_orders
    _rows = _query(
        _src,
        [
            {
                ""items"": valid_lineitems,
                ""on"": lambda o, l: l[""l_orderkey""] == o[""o_orderkey""],
            }
        ],
        {""select"": lambda o, l: (o, l)},
    )
    _groups = _group_by(
        _rows,
        lambda o, l: {
            ""o_orderkey"": o[""o_orderkey""],
            ""o_orderdate"": o[""o_orderdate""],
            ""o_shippriority"": o[""o_shippriority""],
        },
    )
    _items1 = _groups
    _items1 = sorted(
        _items1,
        key=lambda g: _sort_key(
            [
                -_sum([r[1][""l_extendedprice""] * (1 - r[1][""l_discount""]) for r in g]),
                _get(_get(g, ""key""), ""o_orderdate""),
            ]
        ),
    )
    return [
        {
            ""l_orderkey"": _get(_get(g, ""key""), ""o_orderkey""),
            ""revenue"": _sum(
                [r[1][""l_extendedprice""] * (1 - r[1][""l_discount""]) for r in g]
            ),
            ""o_orderdate"": _get(_get(g, ""key""), ""o_orderdate""),
            ""o_shippriority"": _get(_get(g, ""key""), ""o_shippriority""),
        }
        for g in _items1
    ]
",tests/machine/x/python/q3.py,,0,7
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/machine/x/python/q3.py,,1,6
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/machine/x/python/q3.py,,1,6
survived,"def test_Q3_returns_revenue_per_order_with_correct_priority():
    assert order_line_join == [
        {
            ""l_orderkey"": 100,
            ""revenue"": 1000 * 0.95 + 500,
            ""o_orderdate"": ""1995-03-14"",
            ""o_shippriority"": 1,
        }
    ]
",tests/machine/x/python/q3.py,,1,7
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/machine/x/python/q1.py,,1,7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q2.py,Partsupp,1,7
survived,"def _count(v):
    if isinstance(v, list):
        return len(v)
    if hasattr(v, ""Items""):
        return len(v.Items)
    raise Exception(""count() expects list or group"")
",tests/machine/x/python/q1.py,,1,6
survived,"def test_generate_branch_name_slugified() -> None:
    log = ""E   ValueError: something went wrong on operation""  # long line
    name = llm_client.generate_branch_name(log)
    assert name.startswith(""e-valueerror-something"")
    assert len(name) <= 30",tests/test_llm_client_utils.py,,1,7
survived,"def generate_branch_name(log: str) -> str:
    """"""Create a slugified branch name from the error summary.""""""
    slug = _slugify(summarize_error(log))
    return (slug[:30].rstrip(""-"")) or ""auto-fix""",alpha_factory_v1/demos/self_healing_repo/agent_core/llm_client.py,,1,7
survived,"        def train_once(self) -> float:
            return self.loss
",tests/test_world_model_demo.py,DummyLearner,1,6
survived,"def evaluate_agent(agent: Agent, model: str) -> float:
    """"""Return agent score when evaluated with ``model``.

    This placeholder implementation simply returns the archived score. Tests
    patch this function to provide deterministic mock values.
    """"""

    return agent.score
",src/tools/transfer_test.py,,1,6
survived,"def test_run_transfer_test_writes_csv(tmp_path, monkeypatch) -> None:
    db = tmp_path / ""arch.db""
    arch = Archive(db)
    arch.add({""name"": ""a""}, 0.1)
    arch.add({""name"": ""b""}, 0.9)
    out = tmp_path / ""results"" / ""transfer.csv""

    def fake_eval(agent, model):
        return agent.score + 1

    monkeypatch.setattr(tt, ""evaluate_agent"", fake_eval)

    tt.run_transfer_test([""m""], 1, archive_path=db, out_file=out)
    lines = out.read_text().splitlines()
    assert lines[0] == ""id,model,score""
    assert lines[1] == ""2,m,1.900""
",tests/test_transfer_test.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""18""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(18)",benchmarks/poly_mini/task_018.py,,1,7
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""20""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(20)",benchmarks/poly_mini/task_020.py,,1,7
survived,"def run() -> None:
    n = 19
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_019.py,,1,7
survived,"def run() -> None:
    n = 8
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_008.py,,1,6
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""10""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(10)",benchmarks/poly_mini/task_010.py,,1,7
survived,"def run() -> None:
    n = 12
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_012.py,,1,6
survived,"def run() -> None:
    n = 14
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_014.py,,1,6
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""9""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(9)",benchmarks/poly_mini/task_009.py,,1,6
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""6""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(6)",benchmarks/poly_mini/task_006.py,,1,6
survived,"def run() -> None:
    n = 25
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_025.py,,1,6
survived,"def test_allows_safe_patch() -> None:
    diff = _read(""safe_patch.diff"")
    assert is_patch_safe(diff)
",tests/test_safety_filter.py,,1,7
survived,"def is_code_safe(code: str) -> bool:
    """"""Return ``True`` if ``code`` appears safe.""""""
    lowered = code.lower()
    for pat in _DENY_PATTERNS:
        if re.search(pat, lowered):
            return False

    try:
        tree = ast.parse(code)
    except SyntaxError:
        return False

    for node in ast.walk(tree):
        if isinstance(node, ast.Call):
            name = _full_name(node.func)
            if name in _BANNED_CALLS:
                return False
            if name == ""open"" and node.args:
                arg = node.args[0]
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if arg.value.startswith(""/etc""):
                        return False
    return True
",src/self_edit/safety.py,,1,7
survived,"def test_mixed_test_and_src_patch() -> None:
    diff = (
        ""--- a/src/foo.py\n""
        ""+++ b/src/foo.py\n""
        ""@@\n""
        ""-a\n""
        ""+b\n""
        ""--- a/tests/bar.py\n""
        ""+++ b/tests/bar.py\n""
        ""@@\n""
        ""-x\n""
        ""+y\n""
    )
    assert is_patch_valid(diff)",tests/test_patch_guard.py,,1,6
survived,"    def _decorator(func):
        return func
",tests/test_inspector_bridge.py,,1,6
survived,"    def test_main_uses_env_key(self) -> None:
        stub = types.ModuleType(""openai_agents"")

        runtime = MagicMock()

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Agent = object
        stub.AgentRuntime = MagicMock(return_value=runtime)
        stub.Tool = _tool

        with (
            patch.dict(sys.modules, {""openai_agents"": stub}),
            patch.dict(os.environ, {""OPENAI_API_KEY"": ""key""}, clear=False),
            patch(""alpha_factory_v1.backend.adk_bridge.auto_register"") as auto_reg,
            patch(""alpha_factory_v1.backend.adk_bridge.maybe_launch"") as maybe_launch,
        ):
            sys.modules.pop(
                ""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge"",
                None,
            )
            mod = importlib.import_module(""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge"")
            mod.main()

            stub.AgentRuntime.assert_called_once_with(api_key=""key"")
            runtime.register.assert_called_once()
            auto_reg.assert_called_once()
            maybe_launch.assert_called_once()
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime,1,7
survived,"def rsi(prices: Sequence[float], period: int = 14) -> float:
    """"""Compute the relative strength index (0â€’100).""""""

    if period <= 0:
        raise ValueError(""period must be positive"")
    if len(prices) <= period:
        return 0.0

    if np is not None:
        deltas = np.diff(np.asarray(prices, dtype=float))
        gains = np.clip(deltas, 0, None)
        losses = np.clip(-deltas, 0, None)

        avg_gain = float(np.mean(gains[:period]))
        avg_loss = float(np.mean(losses[:period]))

        for g, l in zip(gains[period:], losses[period:]):
            avg_gain = (avg_gain * (period - 1) + float(g)) / period
            avg_loss = (avg_loss * (period - 1) + float(l)) / period
    else:
        deltas = [prices[i + 1] - prices[i] for i in range(len(prices) - 1)]
        gains = [max(d, 0.0) for d in deltas]
        losses = [max(-d, 0.0) for d in deltas]

        avg_gain = sum(gains[:period]) / period
        avg_loss = sum(losses[:period]) / period

        for g, l in zip(gains[period:], losses[period:]):
            avg_gain = (avg_gain * (period - 1) + g) / period
            avg_loss = (avg_loss * (period - 1) + l) / period

    if avg_loss == 0:
        return 100.0
    rs = avg_gain / avg_loss
    return 100.0 - (100.0 / (1 + rs))
",alpha_factory_v1/backend/alpha_model.py,,1,8
survived,"    def test_missing_gene_raises(self):
        with self.assertRaises(KeyError):
            gt.toy_fitness({""temperature"": 0.7})
",alpha_factory_v1/tests/test_genetic_tests.py,GeneticTestsTest,1,7
survived,"    def __post_init__(self) -> None:
        if self.population_size <= 0:
            raise ValueError(""population_size must be positive"")
        if self.generations <= 0:
            raise ValueError(""generations must be positive"")
        if not 0.0 <= self.crossover_rate <= 1.0:
            raise ValueError(""crossover_rate must be within [0,1]"")
        if not 0.0 <= self.mutation_rate <= 1.0:
            raise ValueError(""mutation_rate must be within [0,1]"")
        if self.tournament_k < 2:
            raise ValueError(""tournament_k must be at least 2"")
        if self.elite_count < 0 or self.elite_count >= self.population_size:
            raise ValueError(""elite_count must be >=0 and < population_size"")
        if self.eval_processes < 1:
            raise ValueError(""eval_processes must be at least 1"")
",alpha_factory_v1/backend/evolution_engine.py,EvolutionConfig,1,8
survived,"    async def __aenter__(self):  # pragma: no cover - interface default
        return self
",alpha_factory_v1/backend/market_data.py,BaseMarketData,1,6
survived,"    def __del__(self) -> None:
        self.close()
",alpha_factory_v1/backend/memory_graph.py,GraphMemory,1,7
survived,"    def test_fallback_logic(self):
        agent = DummyAgent()
        model = DummyModel(""not json"")
        planner = PlannerAgent(
            name=""planner"",
            model=model,
            memory=self.memory,
            gov=self.gov,
            domain_agents=[agent],
        )
        random_result = planner.think([])[0]
        self.assertEqual(random_result[""agent""], agent.name)
        self.assertIn(""fallback"", random_result[""reason""])
",alpha_factory_v1/tests/test_planner_agent.py,PlannerAgentTest,1,6
survived,"    def clear(self) -> None:
        """"""Erase all persisted fills and reset positions.""""""
        self._positions.clear()
        if self._db_path.exists():
            self._db_path.write_text("""")
",alpha_factory_v1/backend/portfolio.py,Portfolio,1,7
survived,"    async def arecord_fill(self, symbol: str, qty: float, price: float, side: str) -> None:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self.record_fill, symbol, qty, price, side)
",alpha_factory_v1/backend/portfolio.py,Portfolio,1,6
survived,"def check_docker_compose() -> bool:
    if not shutil.which('docker'):
        banner('docker compose missing', 'RED')
        return False
    try:
        subprocess.run(
            ['docker', 'compose', 'version'],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        banner('docker compose available', 'GREEN')
        return True
    except Exception:  # noqa: BLE001
        banner('docker compose missing', 'RED')
        return False
",alpha_factory_v1/scripts/preflight.py,,1,7
survived,"    def forward(self, h): return self.v(h), torch.log_softmax(self.p(h), -1)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Pred,1,7
survived,"    def recurrent(self, h, a_onehot):
        r, h2 = self.dyn(h, a_onehot)
        v, p = self.pred(h2)
        return h2, r, v, p
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MuZeroTiny,1,7
survived,"def _boot(path: str):
    module_path, cls_name = (MODROOT + path).rsplit(""."", 1)
    try:
        cls = getattr(importlib.import_module(module_path), cls_name)
        inst: Agent = cls()  # type: ignore
        LOG.info(""[BOOT] loaded real agent %s"", inst.name)
    except Exception as exc:
        class Stub(Agent):
            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] â† %s"", cls_name, _msg)
        inst = Stub(cls_name)
        LOG.warning(""[BOOT] stubbed %s (%s)"", cls_name, exc)
    AGENTS[inst.name] = inst
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,7
survived,"            def __init__(self): super().__init__(""llm_planner"")
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,LLMPlanner,1,6
survived,"def _load_cfg() -> Config:
    cfg = Config()
    #Â yaml config file optional
    if yaml:
        for p in (Path.cwd() / ""config.yaml"", Path.cwd() / ""alpha_asi.yaml""):
            if p.exists():
                try:
                    cfg.update(**yaml.safe_load(p.read_text()))
                    LOG.info(""Loaded config from %s"", p)
                except Exception as e:
                    LOG.warning(""Failed to parse %s: %s"", p, e)
    #Â env overrides
    for k in cfg.__dict__.keys():
        env_key = ""ALPHA_ASI_"" + k.upper()
        if env_key in os.environ:
            val = os.environ[env_key]
            try:
                val = type(getattr(cfg, k))(val)
            except Exception:
                pass
            setattr(cfg, k, val)
    return cfg
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,,1,7
survived,"    def text(self) -> str:
        try:
            return self.content.decode()
        except UnicodeDecodeError:
            return self.content.decode(""latin1"", errors=""replace"")
",alpha_factory_v1/requests.py,Response,1,7
survived,"    def test_env_defaults(self):
        os.environ[""PORT""] = ""9000""
        os.environ[""METRICS_PORT""] = ""9100""
        os.environ[""A2A_PORT""] = ""9200""
        os.environ[""CYCLE""] = ""5""
        args = self._parse([])
        for key in (""PORT"", ""METRICS_PORT"", ""A2A_PORT"", ""CYCLE""):
            os.environ.pop(key, None)
        self.assertEqual(args.port, 9000)
        self.assertEqual(args.metrics_port, 9100)
        self.assertEqual(args.a2a_port, 9200)
        self.assertEqual(args.cycle, 5)
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest,1,7
survived,"def _print_console(logs: list[str]) -> None:
    if logs:
        print(""--- Browser console logs ---"", file=sys.stderr)
        for line in logs:
            print(line, file=sys.stderr)
",scripts/verify_insight_offline.py,,1,7
survived,"def test_forward_to_real_requests() -> None:
    """"""When ``requests`` is installed, ``af_requests`` should proxy to it.""""""
    spec = importlib.util.find_spec(""requests"")
    if spec is None:
        pytest.skip(""real requests not installed"")

    sys.modules.pop(""af_requests"", None)
    af_requests = importlib.import_module(""af_requests"")
    import requests  # type: ignore

    assert af_requests.get is requests.get
    assert af_requests.post is requests.post",tests/test_af_requests.py,,1,7
survived,"def UseGuards(*guards):
    """"""Decorator to attach guards to a controller or route.""""""

    def decorator(obj):
        existing = list(getattr(obj, ""__guards__"", []))
        existing.extend(guards)
        setattr(obj, ""__guards__"", existing)
        return obj

    return decorator",nest/core/guards.py,,1,7
survived,"        def with_output(self, x):
            out = x + self.w
            return out, 2 * self.w
",tests/test_scan.py,Module,1,6
survived,"    def TellSecret(self):
        return self.secret
",tests/rosetta/transpiler/Python/call-an-object-method-3.py,Box,1,6
survived,"def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    ap = argparse.ArgumentParser(
        description=""Expose the governance simulation via OpenAI Agents runtime""
    )
    ap.add_argument(
        ""--enable-adk"",
        action=""store_true"",
        help=""Expose agent via ADK gateway"",
    )
    return ap.parse_args(argv)
",alpha_factory_v1/demos/solving_agi_governance/openai_agents_bridge.py,,1,7
survived,"def local_search(query: str):
    results = []
    for root, _, files in os.walk('.'):
        for f in files:
            if query in f:
                path = os.path.join(root, f)
                try:
                    content = open(path).read()
                except Exception:
                    content = """"
                results.append({
                    ""title"": f,
                    ""description"": path,
                    ""url"": path,
                    ""content"": content,
                })
    results.sort(key=lambda r: r['title'], reverse=True)
    return results
",examples/python/local_search.py,,1,6
survived,"    def adder(x):
        return x + n
",tests/human/py/closure.py,,0,7
survived,"        async def post(self, url: str, **kwargs):
            return requests.post(url, **kwargs)
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AsyncClient,0,7
survived,"    def _save_result(result: ResultsResponse) -> None:
        path = _results_dir / f""{result.id}.json""
        path.write_text(result.json())
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def list_runs(_: None = Depends(verify_token)) -> RunsResponse:
        return RunsResponse(ids=list(_simulations.keys()))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,6
survived,"def main(argv: list[str] | None = None) -> None:
    """"""Launch the Î±â€‘AGI Insight API server.""""""

    if FastAPI is None or uvicorn is None:
        raise SystemExit(""FastAPI is required to run the Î±â€‘AGI Insight API."")

    parser = argparse.ArgumentParser(description=""Run the Î±â€‘AGI Insight API"")
    parser.add_argument(""--host"", default=""0.0.0.0"", help=""Bind host"")
    parser.add_argument(""--port"", type=int, default=8000, help=""Bind port"")
    args = parser.parse_args(argv)

    uvicorn.run(app, host=args.host, port=args.port)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def _start() -> None:
        orch_mod = importlib.import_module(""alpha_factory_v1.demos.alpha_agi_insight_v1.src.orchestrator"")
        app_f.state.orchestrator = orch_mod.Orchestrator()
        app_f.state.task = asyncio.create_task(app_f.state.orchestrator.run_forever())
        token = os.getenv(""API_TOKEN"")
        if not token:
            logging.getLogger(__name__).warning(""API_TOKEN not set; using insecure placeholder"")
            token = API_TOKEN_DEFAULT
        app_f.state.api_token = token
        _load_results()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    async def healthz() -> str:
        """"""Simple liveness probe.""""""

        return ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,8
survived,"    def getParameterIds(self):
        return list(map(str, self.jax_model.parameter_ids))
",tests/testSBMLSuiteJax.py,DummyModel,1,6
survived,"def test_oai_and_adk(monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
    settings = config.Settings(openai_api_key=""k"")
    settings.ledger_path = str(tmp_path / ""ledger.db"")
    bus = messaging.A2ABus(settings)
    ledger = insight_logging.Ledger(settings.ledger_path)

    import openai.agents as oa_agents  # type: ignore
    import google_adk

    called: dict[str, str] = {}

    async def fake_run(self, prompt: str) -> str:  # pragma: no cover - async stub
        called[""run""] = prompt
        return ""ok""

    monkeypatch.patch.object(oa_agents.AgentContext, ""run"", fake_run)

    class DummyClient:
        def generate(self, prompt: str) -> str:
            called[""adk""] = prompt
            return ""ok""

    monkeypatch.setattr(google_adk, ""Client"", DummyClient)

    strat = StrategyAgent(bus, ledger)
    summariser = ADKSummariserAgent(bus, ledger)

    summariser._records.append(""hello"")
    env = messaging.Envelope(""a"", ""b"", {""research"": ""foo""}, 0.0)

    async def _run() -> None:
        await strat.handle(env)
        await summariser.run_cycle()

    asyncio.run(_run())

    assert called.get(""run"") == ""foo""
    assert called.get(""adk"") == ""hello""",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_openai_adk_integration.py,,0,7
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_having.py,_Group,1,8
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_items_iteration.py,_Group,1,8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q1.py,Lineitem,1,7
survived,"def test_load_golden_specs() -> None:
    specs = load_golden_spec_fuzz_set()
    assert len(specs) >= 20
    assert all(isinstance(s, str) and s for s in specs)",tests/test_golden_spec_fuzz_set.py,,1,7
survived,"    def __init__(self, secret: str, cache_path: str | Path | None = None) -> None:
        self.secret = secret.encode(""utf-8"")
        self.cache_path = Path(cache_path or ""template_signatures.json"")
        if self.cache_path.exists():
            try:
                self.cache: Dict[str, str] = json.loads(self.cache_path.read_text())
            except Exception:
                self.cache = {}
        else:
            self.cache = {}
",src/meta_agent/template_governance.py,TemplateGovernance,1,7
survived,"def test_hlda_runs_on_synthetic_data():
    n_topics = 3
    vocab_size = 9
    doc_len = 20
    n_docs = 5
    corpus, vocab = generate_corpus(n_topics, vocab_size, doc_len, n_docs)

    hlda = HierarchicalLDA(corpus, vocab, alpha=1.0, gamma=1.0, eta=1.0, num_levels=3, seed=0, verbose=False)
    hlda.estimate(2, display_topics=2, n_words=3, with_weights=False)

    assert len(hlda.document_leaves) == n_docs
    assert hlda.root_node.customers == n_docs",tests/test_synthetic_hlda.py,,1,6
survived,"def test_moe_linear_matches_ragged_dot_general():
    B, In, Out, E = hax.make_axes(B=3, In=4, Out=5, E=2)
    key = jrandom.PRNGKey(0)
    moe = MoELinear.init(E, In, Out, key=key)

    x = hax.random.normal(jrandom.PRNGKey(1), (B, In))
    group_sizes = hax.named(jnp.array([2, 1], dtype=jnp.int32), (E,))

    actual = moe(x, group_sizes)
    expected = _expected_moe_linear_output(moe, x, group_sizes)

    assert actual.axes == expected.axes
    assert jnp.allclose(actual.array, expected.array, rtol=1e-5, atol=1e-5)
",tests/test_moe_linear.py,,1,7
survived,"def _expected_moe_linear_output(moe: MoELinear, x: hax.NamedArray, group_sizes: hax.NamedArray):
    dim_numbers = jax.lax.RaggedDotDimensionNumbers(
        (
            ((x.axis_indices(moe.In),), (moe.weight.axis_indices(moe.In),)),
            ((), ()),
        ),
        x.axis_indices(hax.axis.without_axes(x.axes, moe.In)),
        (moe.weight.axis_indices(moe.Experts),),
    )
    out_raw = jax.lax.ragged_dot_general(
        lhs=x.array,
        rhs=moe.weight.array,
        group_sizes=group_sizes.array,
        ragged_dot_dimension_numbers=dim_numbers,
    )
    out_axes = hax.replace_axis(x.axes, moe.In, moe.Out)
    out = hax.named(out_raw, out_axes)
    if moe.bias is not None:
        out = out + moe.bias
    return out
",tests/test_moe_linear.py,,0,6
survived,"        def step(self, action):
            return [0.0]*4, 0.0, True, False, {}
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_StubEnv,0,7
survived,"    def make(env_id, render_mode=None):
        return _StubEnv()
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,,0,6
survived,"def _asset_paths() -> list[str]:
    fetch = repo_root / 'scripts' / 'fetch_assets.py'
    tree = ast.parse(fetch.read_text())
    assets = {}
    for node in tree.body:
        if isinstance(node, ast.Assign):
            for t in node.targets:
                if getattr(t, 'id', None) == 'ASSETS':
                    assets = ast.literal_eval(node.value)
                    break
    return list(assets)
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,,1,6
survived,"def check_cmd(cmd: str) -> bool:
    if shutil.which(cmd):
        banner(f""{cmd} found"", ""GREEN"")
        return True
    banner(f""{cmd} missing"", ""RED"")
    return False
",scripts/setup_wizard.py,,1,7
survived,"            async def start(self) -> None:
                events.append(""start"")
",tests/test_message_bus.py,TestMessageBus.Prod,1,6
survived,"def doTrials(trials, np, strategy):
    pardoned = 0
    t = 0
    while t < trials:
        drawers = []
        i = 0
        while i < 100:
            drawers = drawers + [i]
            i = i + 1
        drawers = shuffle(drawers)
        p = 0
        success = True
        while p < np:
            found = False
            if strategy == ""optimal"":
                prev = p
                d = 0
                while d < 50:
                    this = drawers[prev]
                    if this == p:
                        found = True
                        break
                    prev = this
                    d = d + 1
            else:
                opened = []
                k = 0
                while k < 100:
                    opened = opened + [False]
                    k = k + 1
                d = 0
                while d < 50:
                    n = _now() % 100
                    while opened[n]:
                        n = _now() % 100
                    opened[n] = True
                    if drawers[n] == p:
                        found = True
                        break
                    d = d + 1
            if not found:
                success = False
                break
            p = p + 1
        if success:
            pardoned = pardoned + 1
        t = t + 1
    rf = float(pardoned) / float(trials) * 100.0
    print(""  strategy = "" + strategy + ""  pardoned = "" + str(pardoned) + "" relative frequency = "" + str(rf) + ""%"")
",tests/rosetta/transpiler/Python/100-prisoners.py,,0,6
survived,"def test_root_disclaimer_html(client: TestClient) -> None:
    """"""HTML disclaimer is returned when requested.""""""

    r = client.get(""/"", headers={""Accept"": ""text/html""})
    assert r.status_code == 200
    assert DISCLAIMER in r.text
    assert r.headers.get(""content-type"", """").startswith(""text/html"")
",tests/test_insight_api_server.py,,1,7
survived,"def main() -> None:
    for path in PACKAGE.rglob(""*.py""):
        if path.name == ""__init__.py"":
            continue
        module_name = path.with_suffix("""").as_posix().replace(""/"", ""."")
        module = importlib.import_module(module_name)
        out_file = DOCS_DIR / f""{module_name.replace('.', '_')}.md""
        with out_file.open(""w"") as f:
            document_module(module, f)
",scripts/generate_interface_docs.py,,1,7
survived,"def _dump_selected(txn: lmdb.Transaction, keys: Iterable[str]) -> List[dict[str, Any]]:
    """"""Return records for the provided keys.""""""
    result: List[dict[str, Any]] = []
    for key in keys:
        raw = txn.get(key.encode(""utf-8""))
        if raw is not None:
            result.append({""key"": key, ""value"": _decode_value(raw)})
    return result
",scripts/dump_lmdb.py,,1,6
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=""Dump LMDB records as JSON"")
    parser.add_argument(""path"", type=Path, help=""Path to LMDB directory"")
    parser.add_argument(""keys"", nargs=""*"", help=""Keys to retrieve"")
    args = parser.parse_args()

    dump_lmdb(args.path, args.keys)
",scripts/dump_lmdb.py,,1,7
survived,"def test_run_muzero_demo_port_in_use(tmp_path: Path) -> None:
    repo_root = Path(__file__).resolve().parents[1]
    src = repo_root / ""alpha_factory_v1""
    dst = tmp_path / ""alpha_factory_v1""
    shutil.copytree(src, dst)

    script = dst / ""demos"" / ""muzero_planning"" / ""run_muzero_demo.sh""
    log_file = tmp_path / ""docker.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(f""#!/usr/bin/env bash\necho docker >> '{log_file}'\nexit 0\n"")
    docker_stub.chmod(0o755)

    lsof_stub = bin_dir / ""lsof""
    lsof_stub.write_text(""#!/usr/bin/env bash\nexit 0\n"")
    lsof_stub.chmod(0o755)

    sock = socket.socket()
    sock.bind((""localhost"", 0))
    port = sock.getsockname()[1]

    env = os.environ.copy()
    env.update({""PATH"": f""{bin_dir}:{env.get('PATH', '')}"", ""HOST_PORT"": str(port)})

    result = subprocess.run([""bash"", str(script)], env=env, capture_output=True, text=True)
    sock.close()

    assert result.returncode == 1
    assert ""already in use"" in result.stderr
    assert not log_file.exists() or not log_file.read_text()",tests/test_run_muzero_demo.py,,0,7
survived,"def handle_anthropic_parallel_tools(
    response_model: type[Iterable[T]], new_kwargs: dict[str, Any]
) -> tuple[AnthropicParallelBase, dict[str, Any]]:
    if new_kwargs.get(""stream"", False):
        from instructor.exceptions import ConfigurationError

        raise ConfigurationError(
            ""stream=True is not supported when using ANTHROPIC_PARALLEL_TOOLS mode""
        )

    model_types = list(get_types_array(response_model))
    new_kwargs[""tools""] = [m.anthropic_schema for m in model_types]
    new_kwargs[""tool_choice""] = {""type"": ""auto""}

    system_messages = extract_system_messages(new_kwargs.get(""messages"", []))

    if system_messages:
        new_kwargs[""system""] = combine_system_messages(
            new_kwargs.get(""system""), system_messages
        )

    new_kwargs[""messages""] = [
        m for m in new_kwargs.get(""messages"", []) if m[""role""] != ""system""
    ]

    return AnthropicParallelModel(typehint=response_model), new_kwargs
",instructor/process_response.py,,1,7
survived,"async def test_async_parallel_tools_or(aclient):
    client = instructor.from_anthropic(
        aclient, mode=instructor.Mode.ANTHROPIC_PARALLEL_TOOLS
    )
    resp = await client.chat.completions.create(
        model=""claude-3-5-haiku-latest"",
        messages=[
            {""role"": ""system"", ""content"": ""You must always use tools""},
            {
                ""role"": ""user"",
                ""content"": ""What is the weather in toronto and dallas and who won the super bowl?"",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3",tests/llm/test_anthropic/test_parallel.py,,0,6
survived,"def convert_line_to_decimals(line: str) -> tuple[bool, list[Decimal]]:
    try:
        decimal_line = [Decimal(elem) for elem in line.split()]
    except:
        return False, []
    return True, decimal_line
",scripts/utils/lcb_runner.py,,1,7
survived,"    def readline(self, *args):
        return self.inputs.split(b""\n"")[0] + b""\n""
",scripts/utils/lcb_runner.py,MockBuffer,1,6
survived,"    def add(self, meta: dict[str, Any], score: float) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(""INSERT INTO agents(meta, score) VALUES (?, ?)"", (json.dumps(meta), score))
",src/archive.py,Archive,1,7
survived,"    def __init__(self, path: str | Path) -> None:
        self.path = Path(path)
        self._ensure()
",src/archive.py,Archive,1,7
survived,"    def __init__(
        self,
        jobs: Iterable[Job],
        *,
        tokens_quota: int | None = None,
        time_quota: float | None = None,
        interval: str = ""1 second"",
        max_workers: int = 1,
    ) -> None:
        self.queue: asyncio.Queue[Job] = asyncio.Queue()
        for job in jobs:
            self.queue.put_nowait(job)
        self.tokens_quota = tokens_quota
        self.time_quota = time_quota
        self.max_workers = max_workers
        self.tokens_used = 0
        self.start_time = 0.0
        self.running: Set[asyncio.Task[None]] = set()
        self.app = Rocketry(execution=""async"")

        @self.app.task(every(interval))
        async def _spawn():  # pragma: no cover - Rocketry callback
            await self._spawn_jobs()
",src/scheduler.py,SelfImprovementScheduler,1,7
survived,"    def flaky(*args, **kwargs):
        nonlocal calls
        calls += 1
        if calls == 1:
            raise RuntimeError(""boom"")
        return 1.0, Path(""r"")
",tests/test_scheduler.py,,0,7
survived,"def main() -> None:
    # Ensure the repository root is on sys.path so benchmark modules import
    sys.path.insert(0, str(ROOT.parent))
    datasets = [""swebench_verified_mini"", ""polyglot_lite""]
    results = []
    for ds in datasets:
        for task_id, module in _discover_tasks(ds):
            results.append(run_task(task_id, module))
    json.dump(results, sys.stdout)
",benchmarks/run_benchmarks.py,,1,7
survived,"    async def run():
        await evolve(_op, _eval, arch, max_cost=0.1)
",tests/test_evolve.py,,0,6
survived,"async def _dummy_operator(genome: Any) -> Any:
    await asyncio.sleep(0)
    return genome
",src/evolve.py,,1,6
survived,"def get_kwargs_from_config(config_path=_DEFAULT_CONFIG_PATH):
    if not os.path.exists(config_path):
        return dict()
    with open(config_path) as f:
        config = json.load(f)
    assert isinstance(config, dict)
    return config
",label_studio_ml/examples/timeseries_segmenter/_wsgi.py,,1,7
survived,"def convert_archetype_features(v1_path, out_dir, doc_slug):
    data = load_json(v1_path)
    features = []
    items = []
    for obj in data:
        slug = obj[""pk""]
        parent = f""{doc_slug}_{slug}""
        sections = parse_feature_sections(obj[""fields""].get(""desc"", """"))
        for name, desc in sections:
            feat_slug = f""{parent}_{slugify(name)}""
            features.append({
                ""model"": ""api_v2.classfeature"",
                ""pk"": feat_slug,
                ""fields"": {
                    ""name"": name,
                    ""desc"": desc,
                    ""document"": doc_slug,
                    ""parent"": parent,
                },
            })
            level = extract_level(desc)
            if level:
                items.append({
                    ""model"": ""api_v2.classfeatureitem"",
                    ""pk"": f""{feat_slug}_{level}"",
                    ""fields"": {""parent"": feat_slug, ""level"": level, ""column_value"": None},
                })
    if features:
        save_json(features, os.path.join(out_dir, ""ClassFeature.json""))
    if items:
        save_json(items, os.path.join(out_dir, ""ClassFeatureItem.json""))
",convert_missing.py,,0,6
survived,"        def __init__(self, base_url=None, api_key=None):
            pass
",no-ocr-api/tests/test_utils.py,FakeOpenAI,1,6
survived,"    def test_requires_token(self):
        with mock.patch.dict(os.environ, {}, clear=True):
            with self.assertRaises(SystemExit):
                with mock.patch.object(import_dashboard.sys, 'argv', ['imp.py']):
                    import_dashboard.main()
",alpha_factory_v1/tests/test_scripts_import_dashboard.py,ImportDashboardScriptTest,1,6
survived,"    def close(self) -> None:
        self.conn.close()",src/meta_agent/telemetry_db.py,TelemetryDB,1,7
survived,"        def map_type(t: str) -> str:
            return t
",src/jinja2/__init__.py,,1,6
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"def test_record_and_fetch(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(10, 0.1, 0.5, 0)
    rows = db.fetch_all()
    assert len(rows) == 1
    assert rows[0][""tokens""] == 10
    assert db.verify()
    db.close()
",tests/unit/test_telemetry_db.py,,1,7
survived,"def test_no_innerhtml_usage() -> None:
    files = [
        Path('alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/ui/EvolutionPanel.ts'),
        Path('alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/ui/ControlsPanel.ts'),
    ]
    for f in files:
        text = f.read_text()
        assert '.innerHTML' not in text
",tests/test_ui_xss_safety.py,,1,7
survived,"def test_guardrail_event():
    t = TelemetryCollector()
    t.increment_guardrail_hits()
    assert t.guardrail_hits == 1
    assert len(t.events) == 1
    ev = t.events[0]
    assert ev.category == TelemetryCollector.Category.GUARDRAIL
    assert ev.severity == TelemetryCollector.Severity.WARNING",tests/unit/test_telemetry_collector.py,,1,7
survived,"def test_new_metrics_present() -> None:
    client = make_client()
    resp = client.get(""/metrics"")
    assert resp.status_code == 200
    text = resp.text
    assert ""dgm_parents_selected_total"" in text
    assert ""dgm_children_admitted_total"" in text
    assert ""dgm_revives_total"" in text",tests/test_metrics_exposure.py,,1,7
survived,"def discover_entrypoints() -> None:
    try:
        eps = imetadata.entry_points(group=""alpha_factory.agents"")  # type: ignore[arg-type]
    except Exception:  # noqa: BLE001
        return
    for ep in eps:
        try:
            obj = ep.load()
        except Exception:  # noqa: BLE001
            logger.exception(""Entry-point load failed: %s"", ep.name)
            continue
        AgentBase = _agent_base()
        if inspect.isclass(obj) and issubclass(obj, AgentBase):
            name = getattr(obj, ""NAME"", ep.name)
            if name not in AGENT_REGISTRY:
                _register(
                    AgentMetadata(
                        name=name,
                        cls=obj,
                        version=getattr(obj, ""__version__"", ""0.1.0""),
                        capabilities=list(getattr(obj, ""CAPABILITIES"", [])),
                        compliance_tags=list(getattr(obj, ""COMPLIANCE_TAGS"", [])),
                        requires_api_key=getattr(obj, ""REQUIRES_API_KEY"", False),
                    )
                )
",alpha_factory_v1/backend/agents/discovery.py,,1,7
survived,"async def stop_background_tasks() -> None:
    """"""Cancel background health and rescan loops.""""""
    global _bg_started, _health_task, _rescan_task
    if not _bg_started:
        return
    _bg_started = False
    tasks = [_health_task, _rescan_task]
    for t in tasks:
        if t:
            t.cancel()
    for t in tasks:
        if t:
            with contextlib.suppress(asyncio.CancelledError):
                await t
    _health_task = _rescan_task = None",alpha_factory_v1/backend/agents/health.py,,1,7
survived,"def test_manifest_parsing_when_registration_tags_given_as_selector(
    image_field_name: str,
    image_selector: str,
    predictions: Optional[str],
) -> None:
    raw_manifest = {
        ""type"": ""roboflow_core/roboflow_dataset_upload@v2"",
        ""name"": ""some"",
        image_field_name: image_selector,
        ""predictions"": predictions,
        ""target_project"": ""some1"",
        ""usage_quota_name"": ""my_quota"",
        ""data_percentage"": ""$inputs.data_percentage"",
        ""persist_predictions"": ""$inputs.persist_predictions"",
        ""minutely_usage_limit"": 10,
        ""hourly_usage_limit"": 100,
        ""daily_usage_limit"": 1000,
        ""max_image_size"": (100, 200),
        ""compression_level"": 100,
        ""registration_tags"": ""$inputs.tags"",
        ""disable_sink"": ""$inputs.disable_sink"",
        ""fire_and_forget"": ""$inputs.fire_and_forget"",
        ""labeling_batch_prefix"": ""$inputs.labeling_batch_prefix"",
        ""labeling_batches_recreation_frequency"": ""never"",
    }

    result = BlockManifest.model_validate(raw_manifest)

    assert result == BlockManifest(
        type=""roboflow_core/roboflow_dataset_upload@v2"",
        name=""some"",
        images=image_selector,
        predictions=predictions,
        target_project=""some1"",
        usage_quota_name=""my_quota"",
        data_percentage=""$inputs.data_percentage"",
        persist_predictions=""$inputs.persist_predictions"",
        minutely_usage_limit=10,
        hourly_usage_limit=100,
        daily_usage_limit=1000,
        max_image_size=(100, 200),
        compression_level=100,
        registration_tags=""$inputs.tags"",
        disable_sink=""$inputs.disable_sink"",
        fire_and_forget=""$inputs.fire_and_forget"",
        labeling_batch_prefix=""$inputs.labeling_batch_prefix"",
        labeling_batches_recreation_frequency=""never"",
    )
",tests/workflows/unit_tests/core_steps/sinks/roboflow/roboflow_dataset_upload/test_v2.py,,1,7
survived,"def test_governance_bridge_adk_runtime(tmp_path: Path) -> None:
    """"""Launch governance-bridge with ADK enabled and verify logs.""""""
    stub = tmp_path / ""google_adk.py""
    stub.write_text(
        """"""
class Router:
    def __init__(self):
        self.app = type('app', (), {'middleware': lambda *a, **k: lambda f: f})()
    def register_agent(self, agent):
        pass
class Agent: ...
class AgentException(Exception):
    pass
""""""
    )

    env = os.environ.copy()
    env[""PYTHONPATH""] = f""{tmp_path}:{env.get('PYTHONPATH', '')}""
    env[""ALPHA_FACTORY_ENABLE_ADK""] = ""true""

    proc = subprocess.Popen(
        [""governance-bridge"", ""--enable-adk"", ""--port"", ""0""],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        env=env,
    )
    try:
        time.sleep(2)
        proc.terminate()
        out, _ = proc.communicate(timeout=5)
    finally:
        if proc.poll() is None:
            proc.kill()
            proc.wait(timeout=5)

    assert ""Registered GovernanceSimAgent"" in out
    assert ""ADK"" in out",tests/test_governance_bridge_adk_runtime.py,,0,7
survived,"def test_run_cycle_creates_task(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""`run_cycle` should schedule a task when called from within a loop.""""""
    mod = importlib.import_module(MODULE)

    class DummyLoop:
        def __init__(self) -> None:
            self.coro: Any | None = None

        def create_task(self, coro: Any) -> None:
            self.coro = coro

    dummy_loop = DummyLoop()
    monkeypatch.setattr(asyncio, ""get_running_loop"", lambda: dummy_loop)
    monkeypatch.setattr(asyncio, ""run"", lambda _coro: (_ for _ in ()).throw(AssertionError(""run called"")))

    async def dummy_cycle(*_a: object, **_k: object) -> None:
        pass

    monkeypatch.setattr(mod, ""run_cycle_async"", dummy_cycle)

    mod.run_cycle(mod.Orchestrator(), mod.AgentFin(), mod.AgentRes(), mod.AgentEne(), mod.AgentGdl(), mod.Model())

    assert dummy_loop.coro is not None
    assert getattr(dummy_loop.coro, ""cr_code"", None) is dummy_cycle.__code__",tests/test_alpha_agi_business_3_v1.py,,1,7
survived,"    def fake_run(coro: Any) -> None:
        called[""coro""] = coro
",tests/test_alpha_agi_business_3_v1.py,,1,6
survived,"def triple(x):
    return x * 3
",tests/transpiler/x/py/pure_fold.py,,1,8
survived,"def sum3(a, b, c):
    return a + b + c
",tests/transpiler/x/py/fun_three_args.py,,1,8
survived,"    def rebuild(self) -> None:
        """"""Rebuild the index from all registered templates.""""""
        self._index = []
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                version = version_info[""version""]
                path = self.registry.templates_dir / version_info[""path""]
                metadata_path = path.parent / METADATA_FILE_NAME
                try:
                    content = path.read_text(encoding=""utf-8"")
                except OSError:  # pragma: no cover - file missing
                    continue
                checksum = sha256(content.encode(""utf-8"")).hexdigest()
                try:
                    with open(metadata_path, ""r"", encoding=""utf-8"") as f:
                        metadata = json.load(f)
                except (OSError, json.JSONDecodeError):
                    metadata = {}
                self._index.append(
                    {
                        ""slug"": slug,
                        ""version"": version,
                        ""path"": str(path.relative_to(self.registry.templates_dir)),
                        ""checksum"": checksum,
                        ""metadata"": metadata,
                        ""content"": content,
                    }
                )
        self.save()
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,,1,7
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,,1,7
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex,1,7
survived,"def test_evaluate_econ_metrics() -> None:
    repo = Path(__file__).resolve().parents[1]
    result = evaluate_econ.evaluate(repo)
    assert list(result.keys()) == [""rmse"", ""lead_time""]
    assert result[""rmse""] == 0.0
    assert result[""lead_time""] == 0",tests/test_evaluate_econ.py,,1,6
survived,"    async def _run_job(self, job: Job) -> None:
        start = time.perf_counter()
        try:
            delta, _ = await asyncio.to_thread(
                self_improver.improve_repo,
                job.repo,
                job.patch,
                job.metric,
                job.log,
            )
            self.tokens_used += job.tokens
            gpu_hours = (time.perf_counter() - start) / 3600
            metrics.dgm_gpu_hours_total.inc(gpu_hours)
            if delta > 0:
                metrics.dgm_fitness_gain_total.inc(delta)
            if metrics.dgm_fitness_gain_total._value.get() > 0:
                ratio = (
                    metrics.dgm_gpu_hours_total._value.get()
                    / metrics.dgm_fitness_gain_total._value.get()
                )
                metrics.dgm_gpu_hours_per_gain.set(ratio)
            if not self._first_round_done:
                self._results[job] = delta
            else:
                suc, fail = self._stats.get(job, (0, 0))
                if delta > 0:
                    suc += 1
                else:
                    fail += 1
                self._stats[job] = (suc, fail)
        except Exception:  # noqa: BLE001
            await self.queue.put(job)
        if not self._first_round_done and len(self._results) == len(self._initial_jobs):
            self._finalize_first_round()
",src/scheduler/__init__.py,SelfImprovementScheduler,1,7
survived,"def _parse_spec(spec: str) -> tuple[str, str]:
    if "":"" in spec:
        path, goal = spec.split("":"", 1)
    else:
        parts = spec.split(maxsplit=1)
        if len(parts) != 2:
            raise ValueError(""spec must contain 'path goal'"")
        path, goal = parts
    return path.strip(), goal.strip()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,,1,7
survived,"def create_app(service: DualCriticService) -> ""FastAPI"":
    return service.create_app()",src/critics/dual_critic_service.py,,0,6
survived,"    def logic_score(self, context: str, response: str) -> float:
        """"""Return a naive logic score based on substring matching.""""""
        if not context or not response:
            return 0.0
        return 1.0 if response.lower() in context.lower() else 0.0
",src/critics/dual_critic_service.py,DualCriticService,1,6
survived,"    def __init__(self, docs: Iterable[str] | None = None) -> None:
        self.docs = list(docs or [])
",src/critics/dual_critic_service.py,VectorDB,1,7
survived,"def test_run_simulation_smoke(capsys: pytest.CaptureFixture[str]) -> None:
    """"""Ensure _run_simulation accepts the new num_sectors argument.""""""

    web_app._run_simulation(1, ""logistic"", 2, 3, 1)
    out, _ = capsys.readouterr()
    assert ""Streamlit not installed"" in out",tests/test_web_app.py,,1,7
survived,"            def wrapper(func):
                return func
",alpha_factory_v1/tests/test_smoke.py,_DummyMark,1,6
survived,"        def __init__(self) -> None:
            self.nodes: dict = {}
            self.edges: dict = {}
",alpha_factory_v1/backend/agents/supply_chain_agent.py,_FakeGraph,1,7
survived,"def _merge_images_by_position(images: list[Image.Image], positions: list[int]) -> Image.Image:
    """"""Merge screenshots vertically using scroll positions to remove overlaps.""""""
    if not images:
        raise ValueError(""no images to merge"")
    if len(images) != len(positions):
        raise ValueError(""images and positions length mismatch"")

    if len(images) == 1:
        return images[0]

    max_width = max(img.width for img in images)

    merged_height = images[0].height
    for i in range(1, len(images)):
        merged_height += positions[i] - positions[i - 1]

    merged_img = Image.new(""RGB"", (max_width, merged_height), color=(255, 255, 255))

    current_y = 0
    merged_img.paste(images[0], (0, current_y))
    current_y += images[0].height

    for i in range(1, len(images)):
        step = positions[i] - positions[i - 1]
        overlap = images[i].height - step
        if overlap > 0:
            cropped = images[i].crop((0, overlap, images[i].width, images[i].height))
        else:
            cropped = images[i]

        merged_img.paste(cropped, (0, current_y))
        current_y += cropped.height

    return merged_img
",skyvern/webeye/utils/page.py,,1,7
survived,"def test_get_kill_after_minutes_yaml(tmp_path, monkeypatch):
    yaml_file = tmp_path / ""dagster.yaml""
    yaml_file.write_text(""kill_sensor:\n  kill_after_minutes: 45\n"")
    monkeypatch.setenv(""DAGSTER_HOME"", str(tmp_path))
    assert get_kill_after_minutes() == 45
    monkeypatch.delenv(""DAGSTER_HOME"")
",tests/test_timeout_sensor.py,,1,7
survived,"def test_generate_state_different_key():
    st.session_state.clear()
    s1 = _generate_state(key=""a"")
    s2 = _generate_state(key=""b"")
    assert s1 != s2
",tests/test_internal.py,,1,6
survived,"def test_refresh_token_expired(monkeypatch):
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)

    monkeypatch.setattr(oauth.client, ""refresh_token"", AsyncMock(return_value={""access_token"": ""new""}))

    token = {""access_token"": ""old"", ""refresh_token"": ""r"", ""expires_at"": time.time() - 1}
    result = oauth.refresh_token(token)

    assert result[""access_token""] == ""new""
",tests/test_oauth_component.py,,1,7
survived,"def test_revoke_token(monkeypatch):
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)
    revoke_mock = AsyncMock()
    monkeypatch.setattr(oauth.client, ""revoke_token"", revoke_mock)

    assert oauth.revoke_token({""access_token"": ""abc""}) is True
    revoke_mock.assert_awaited_once()",tests/test_oauth_component.py,,1,7
survived,"def linear_curve(t: float) -> float:
    return max(0.0, min(1.0, t))
",alpha_factory_v1/core/simulation/forecast.py,,1,7
survived,"def span(name: str) -> ContextManager[Any]:
    """"""Return a context manager for ``name``.""""""
    if tracer:
        return cast(ContextManager[Any], tracer.start_as_current_span(name))
    return nullcontext()
",alpha_factory_v1/core/utils/tracing.py,,1,7
survived,"def lead_signal_improvement(
    history: Sequence[float],
    forecast: Sequence[float],
    *,
    months: int = 6,
    threshold: float | None = None,
) -> float:
    """"""Return relative lead-time improvement over the baseline.""""""
    base = _arima_baseline(history, months)
    thr = threshold if threshold is not None else (history[-1] if history else 0.0)

    def first_cross(seq: Sequence[float]) -> int:
        for i, v in enumerate(seq, 1):
            if v >= thr:
                return i
        return months + 1

    base_idx = first_cross(base)
    cand_idx = first_cross(forecast[:months])
    if base_idx <= cand_idx:
        return 0.0
    return (base_idx - cand_idx) / base_idx
",alpha_factory_v1/core/evaluators/lead_time.py,,1,7
survived,"def test_typical_day_score_in_range() -> None:
    _reset_ledger()
    res = {""date"": ""2025-04-22"", ""calories_in"": 2400, ""calories_out"": 600, ""bmr"": 1650}
    value = eb.reward(None, None, res)
    assert isinstance(value, float)
    assert 0.0 <= value <= 1.0
",tests/test_energy_balance_reward.py,,1,7
survived,"    def __init__(self) -> None:
        self.logged: list[messaging.Envelope] = []
",tests/test_safety_guardian_property.py,DummyLedger,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/transpiler/x/py/bench_block.py,,1,6
survived,"    def test_warn_without_numpy(self) -> None:
        os.environ[""VECTOR_STORE_USE_SQLITE""] = ""true""
        os.environ.pop(""PGHOST"", None)
        with mock.patch.object(memf, ""np"", None, create=True):
            with self.assertLogs(""AlphaFactory.MemoryFabric"", level=""WARNING"") as cm:
                fabric = memf.MemoryFabric()
                fabric.close()
        os.environ.pop(""VECTOR_STORE_USE_SQLITE"", None)
        self.assertTrue(any(""numpy required for SQLite"" in msg for msg in cm.output))
",tests/test_memory_fabric_sqlite.py,TestMemoryFabricSQLiteWarning,1,7
survived,"    def test_agentbase_alias(self):
        legacy = importlib.import_module(""backend.agent_base"").AgentBase
        canonical = importlib.import_module(""backend.agents.base"").AgentBase
        self.assertIs(legacy, canonical)
",tests/test_agents_alias.py,TestAgentsAlias,1,7
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Single orchestrator step delegating to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/energy_agent.py,EnergyAgent,1,7
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/climate_risk_agent.py,ClimateRiskAgent,0,6
survived,"def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, default))
    except (TypeError, ValueError):
        return default
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/config.py,,1,7
survived,"    async def run_cycle(self) -> None:
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/strategy_agent.py,StrategyAgent,0,7
survived,"    def __init__(self, bus, ledger) -> None:
        super().__init__(""memory"", bus, ledger)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/memory_agent.py,MemoryAgent,1,7
survived,"    async def stop(self) -> None:
        if self._server:
            await self._server.stop(0)
            self._server = None",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus,1,7
survived,"def parse_and_validate_attributes(attr_str):
    raw_values = attr_str.split(',')
    if len(raw_values) != 4:
        raise argparse.ArgumentTypeError(""You must provide exactly 4 attributes: STR,DEX,INT,LUK"")

    parsed = []
    total_known = 0
    unknown_count = 0

    for v in raw_values:
        if v.strip() == '?':
            parsed.append(None)
            unknown_count += 1
        else:
            try:
                val = int(v)
            except ValueError:
                raise argparse.ArgumentTypeError(f""Invalid attribute value: {v}"")
            if not (4 <= val <= 13):
                raise argparse.ArgumentTypeError(""Each attribute must be between 4 and 13."")
            parsed.append(val)
            total_known += val

    if unknown_count > 0 and (total_known > 25 or total_known + 4 * unknown_count > 25):
        raise argparse.ArgumentTypeError(""Impossible to satisfy sum of 25 with current values."")

    return parsed
",tools/AutoDiceRoller.py,,1,7
survived,"    def run_once(self):
        '''
        Process one game window frame
        '''
        # Get window game raw frame
        self.frame = self.capture.get_frame()
        if self.frame is None:
            logger.warning(""Failed to capture game frame."")
            return

        # Make sure resolution is as expected
        if self.cfg[""game_window""][""size""] != self.frame.shape[:2]:
            text = (
                f""Unexpeted window size: {self.frame.shape[:2]} ""
                f""(expect {self.cfg['game_window']['size']})""
            )
            logger.error(text)
            return

        # Resize raw frame to working size
        self.img_frame = cv2.resize(self.frame, WINDOW_WORKING_SIZE,
                                    interpolation=cv2.INTER_NEAREST)

        # Grayscale game window
        self.img_frame_gray = cv2.cvtColor(self.img_frame, cv2.COLOR_BGR2GRAY)

        # Image for debug use
        self.img_frame_debug = self.img_frame.copy()

        # Enable cached location since second frame
        self.is_first_frame = False

        # Check if user want to disable dice rolling
        if self.kb.is_pressed_func_key[0]: # 'F1' is pressed
            self.is_enable = not self.is_enable
            logger.info(f""User press F1, is_enable = {self.is_enable}"")
            self.kb.is_pressed_func_key[0] = False

        # Check if need to save screenshot
        if self.kb.is_pressed_func_key[1]: # 'F2' is pressed
            screenshot(self.img_frame)
            self.kb.is_pressed_func_key[1] = False

        if self.is_enable and self.kb.is_game_window_active():
            loc_dice = (981, 445)
            loc_first_box = (890, 371)
            box_size = (22, 37) # (h ,w)
            box_y_interval = 25
            window_title = self.cfg[""game_window""][""title""]

            # Parse the attribute number
            attibutes_info = []
            for i, attibute in enumerate([""STR"", ""DEX"", ""INT"", ""LUK""]):
                # Calculate the box position
                p0 = (loc_first_box[0], loc_first_box[1] + i * box_y_interval)
                p1 = (p0[0] + box_size[1], p0[1] + box_size[0])

                # Crop the box region from the image
                img_roi = self.img_frame_gray[p0[1]:p1[1], p0[0]:p1[0]]

                # Match with each number template (from 4 to 11)
                best_score = float('inf')
                best_digit = None
                for idx, img_number in enumerate(self.img_numbers, start=4):
                    _, score, _ = find_pattern_sqdiff(img_roi, img_number)
                    if score < best_score:
                        best_score = score
                        best_digit = idx
                logger.info(f""[{attibute}]: {best_digit} (score: {round(best_score, 2)})"")
                attibutes_info.append((best_digit, best_score))

                # Draw box and put text on debug image
                cv2.rectangle(self.img_frame_debug, p0, p1, (0, 0, 255), 1)
                cv2.putText(
                    self.img_frame_debug,
                    f""{best_digit}"",
                    (p0[0], p0[1] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (0, 255, 255),
                    1,
                    cv2.LINE_AA
                )

            # for val, score in attibutes_info:
            #     if score > 0.11:
            #         logger.warning(f""Stop! Unable to recognize number: {(val, score)})"")
            #         self.is_enable = False

            # Check if is equal to target
            is_jackpot = True
            for i, (val, score) in enumerate(attibutes_info):
                target = self.args.attribute[i]
                if target is not None and target != val:
                    is_jackpot = False

            # Stop rolling dice if reach target
            if is_jackpot:
                self.is_enable = False
                logger.info(""Hit Jackpot! Stop!"")

            # Click to roll the dice or not
            if self.is_enable:
                click_in_game_window(window_title, loc_dice)
                logger.info(""Roll the dice"")

        # Show debug image on window
        self.update_img_frame_debug()
",tools/AutoDiceRoller.py,AutoDiceRoller,1,8
survived,"    def __init__(self, args):
        self.cfg = Config # Configuration
        self.args = args
        self.status = ""hunting"" # 'resting', 'finding_rune', 'near_rune', 'solving_rune'
        self.idx_routes = 0 # Index of route
        self.hp_ratio = 1.0 # HP bar ratio
        self.mp_ratio = 1.0 # MP bar ratio
        self.exp_ratio = 1.0 # EXP bar ratio
        self.monster_info = [] # monster information
        self.fps = 0 # Frame per second
        self.is_first_frame = True # Disable cached location for first frame
        self.rune_detect_level = 0
        # Coordinate (top-left coordinate)
        self.loc_nametag = (0, 0) # nametag location on window
        self.loc_camera = (0, 0) # camera location on map
        self.loc_watch_dog = (0, 0) # watch dog
        self.loc_player_global = (0, 0) # player location on map
        self.loc_player = (0, 0) # player location on window
        self.loc_player_minimap = (0, 0) # Player's location on minimap
        self.loc_minimap = (0, 0)
        # Images
        self.frame = None # raw image
        self.img_frame = None # game window frame
        self.img_frame_gray = None # game window frame graysale
        self.img_frame_debug = None
        self.img_route = None # route map
        self.img_route_debug = None
        # Timers
        self.t_last_frame = time.time() # Last frame timer, for fps calculation
        self.t_last_switch_status = time.time() # Last status switches timer
        self.t_watch_dog = time.time() # Last movement timer
        self.t_last_teleport = time.time() # Last teleport timer
        self.t_patrol_last_attack = time.time() # Last patrol attack timer
        self.t_last_camera_missed = time.time() # Last camera loc missed
        # Patrol mode
        self.is_patrol_to_left = True
        self.patrol_turn_point_cnt = 0
        self.img_frame_gray_last = None

        # Set status to hunting for start
        self.switch_status(""hunting"")

        map_dir = ""minimaps"" if self.cfg.is_use_minimap else ""maps""

        if args.patrol:
            # Patrol mode doesn't need map or route
            self.img_map = None
            self.img_routes = []
            self.img_route_rest = None
        else:
            # Load map for camera localization
            if self.cfg.is_use_minimap:
                self.img_map = load_image(f""{map_dir}/{args.map}/map.png"",
                                        cv2.IMREAD_COLOR)
            else:
                self.img_map = load_image(f""{map_dir}/{args.map}/map.png"",
                                        cv2.IMREAD_GRAYSCALE)
                self.img_map_resized = cv2.resize(
                    self.img_map, (0, 0),
                    fx=self.cfg.localize_downscale_factor,
                    fy=self.cfg.localize_downscale_factor)
            # Load route*.png images
            route_files = sorted(glob.glob(f""{map_dir}/{args.map}/route*.png""))
            route_files = [p for p in route_files if not p.endswith(""route_rest.png"")]
            self.img_routes = [
                cv2.cvtColor(load_image(p), cv2.COLOR_BGR2RGB) for p in route_files
            ]
            # Load rest route
            self.img_route_rest = cv2.cvtColor(
                load_image(f""{map_dir}/{args.map}/route_rest.png""), cv2.COLOR_BGR2RGB)

            # Upscale minimap route map for better debug visualization
            if self.cfg.is_use_minimap:
                img_routes_resized = []
                for img_route in self.img_routes:
                    img_routes_resized.append(cv2.resize(
                        img_route, (0, 0),
                        fx=self.cfg.minimap_upscale_factor,
                        fy=self.cfg.minimap_upscale_factor,
                        interpolation=cv2.INTER_NEAREST))
                self.img_routes = img_routes_resized
                self.img_route_rest = cv2.resize(
                            self.img_route_rest, (0, 0),
                            fx=self.cfg.minimap_upscale_factor,
                            fy=self.cfg.minimap_upscale_factor,
                            interpolation=cv2.INTER_NEAREST)

        # Load other images
        self.img_nametag = load_image(""name_tag.png"")
        self.img_nametag_gray = load_image(""name_tag.png"", cv2.IMREAD_GRAYSCALE)
        self.img_rune_warning = load_image(""rune/rune_warning.png"", cv2.IMREAD_GRAYSCALE)
        self.img_rune = load_image(""rune/rune.png"")
        self.img_rune_gray = load_image(""rune/rune.png"", cv2.IMREAD_GRAYSCALE)
        self.img_arrows = {
            ""left"":
                [load_image(""rune/arrow_left_1.png""),
                load_image(""rune/arrow_left_2.png""),
                load_image(""rune/arrow_left_3.png""),],
            ""right"":
                [load_image(""rune/arrow_right_1.png""),
                load_image(""rune/arrow_right_2.png""),
                load_image(""rune/arrow_right_3.png""),],
            ""up"":
                [load_image(""rune/arrow_up_1.png""),
                load_image(""rune/arrow_up_2.png""),
                load_image(""rune/arrow_up_3.png"")],
            ""down"":
                [load_image(""rune/arrow_down_1.png""),
                load_image(""rune/arrow_down_2.png""),
                load_image(""rune/arrow_down_3.png""),],
        }

        # Load monsters images from monster/{monster_name}
        self.monsters = {}
        for monster_name in args.monsters.split("",""):
            imgs = []
            for file in glob.glob(f""monster/{monster_name}/{monster_name}*.png""):
                # Add original image
                img = load_image(file)
                imgs.append((img, get_mask(img, (0, 255, 0))))
                # Add flipped image
                img_flip = cv2.flip(img, 1)
                imgs.append((img_flip, get_mask(img_flip, (0, 255, 0))))
            if imgs:
                self.monsters[monster_name] = imgs
            else:
                logger.error(f""No images found in monster/{monster_name}/{monster_name}*"")
                raise RuntimeError(f""No images found in monster/{monster_name}/{monster_name}*"")
        logger.info(f""Loaded monsters: {list(self.monsters.keys())}"")

        # Start keyboard controller thread
        self.kb = KeyBoardController(self.cfg, args)
        if args.disable_control:
            self.kb.disable()

        # Start game window capturing thread
        logger.info(""Waiting for game window to activate, please click on game window"")
        self.capture = GameWindowCapturor(self.cfg)
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot,1,8
survived,"def _remote_available(url: str) -> bool:
    try:
        req = Request(url, method=""HEAD"")
        with urlopen(req, timeout=3) as resp:
            status = getattr(resp, ""status"", None)
        return bool(status and 200 <= int(status) < 300)
    except Exception:
        return False
",scripts/open_subdir_demo.py,,1,6
survived,"    def log(self, env) -> None:  # type: ignore[override]
        self.events.append(env.payload.get(""event""))
",tests/test_alert_webhook.py,DummyLedger,1,6
survived,"    def fake_post(url: str, *, json=None, timeout=None):
        sent[""url""] = url
        sent[""payload""] = json
        return type(""R"", (), {""status_code"": 200})()
",tests/test_alert_webhook.py,,1,6
survived,"def test_agents_list_offline(non_network: None) -> None:
    """"""Verify /agents lists all required demo agents.""""""
    os.environ[""NO_LLM""] = ""1""
    os.environ.setdefault(""ALPHA_ASI_SILENT"", ""1"")
    os.environ.setdefault(""ALPHA_ASI_MAX_STEPS"", ""1"")

    mod = importlib.import_module(""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo"")
    client = TestClient(cast(Any, mod.app))

    resp = client.get(""/agents"")
    assert resp.status_code == 200
    agents = resp.json()
    expected = {
        ""PlanningAgent"",
        ""ResearchAgent"",
        ""StrategyAgent"",
        ""MarketAnalysisAgent"",
        ""CodeGenAgent"",
        ""SafetyAgent"",
    }
    assert expected.issubset(set(agents))",tests/test_world_model_demo.py,,1,7
survived,"    def test_new_version_ok(self) -> None:
        fake_mod = types.SimpleNamespace(__version__=""0.0.15"")
        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
        ):
            self.assertTrue(preflight.check_openai_agents_version())
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion,1,7
survived,"        def generate(self, **kwargs: object) -> list[list[int]]:
            return [[0]]
",tests/test_gpt2_cli_demo.py,FakeModel,1,6
survived,"    def test_missing_readme_fails(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            d = os.path.join(tmp, ""demo_missing"")
            os.mkdir(d)
            open(os.path.join(d, ""__init__.py""), ""w"").close()
            exit_code = validate_demos.main(tmp, min_lines=1)
            self.assertEqual(exit_code, 1)
",tests/test_demo_quality.py,TestValidateDemosFailures,1,6
survived,"def _find_entry(transcript: Path, agent_hash: str, score: Sequence[float]) -> bool:
    data = json.loads(transcript.read_text())
    for item in data:
        if item.get(""hash"") == agent_hash and tuple(item.get(""score"", [])) == tuple(score):
            return True
    return False
",src/utils/snark.py,,1,6
survived,"def verify_proof(transcript_path: str | Path, agent_hash: str, score: Sequence[float], proof: str) -> bool:
    """"""Return ``True`` if ``proof`` matches the generated value.""""""
    expected = generate_proof(transcript_path, agent_hash, score)
    return proof == expected",src/utils/snark.py,,1,7
survived,"def main(argv: Sequence[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description=""Verify SNARK proof"")
    parser.add_argument(""transcript"", help=""Path to evaluation transcript"")
    parser.add_argument(""agent_hash"", help=""Agent hash"")
    parser.add_argument(""score"", help=""Comma separated score tuple"")
    parser.add_argument(""proof"", help=""Proof string"")
    args = parser.parse_args(argv)

    try:
        score = parse_score(args.score)
    except ValueError:
        parser.error(""score must be comma separated floats"")
        return 1

    expected = generate_proof(Path(args.transcript), args.agent_hash, score)
    if expected == args.proof:
        print(""proof verified"")
        return 0
    print(""verification failed"")
    return 1
",scripts/verify_snark.py,,1,7
survived,"def _parse_simple_yaml(lines: List[str], start: int, indent: int) -> tuple[Any, int]:
    """"""Parse a simple block of YAML starting at ``start`` with ``indent``.""""""

    # If the first relevant line starts with a list indicator, parse a list
    i = start
    while i < len(lines) and not lines[i].strip():
        i += 1
    if (
        i < len(lines)
        and lines[i].lstrip().startswith(""- "")
        and (len(lines[i]) - len(lines[i].lstrip("" "")) == indent)
    ):
        lst: List[Any] = []
        while i < len(lines):
            line = lines[i]
            if not line.strip():
                i += 1
                continue
            current_indent = len(line) - len(line.lstrip("" ""))
            if current_indent < indent:
                break
            if not line.lstrip().startswith(""- ""):
                break
            item_content = line.lstrip()[2:].strip()
            i += 1
            if item_content:
                if "":"" in item_content:
                    k, v = item_content.split("":"", 1)
                    item: Any = {k.strip(): _convert_scalar(v.strip())}
                else:
                    item = _convert_scalar(item_content)
            else:
                item = {}
            # parse subfields
            while i < len(lines):
                sub_line = lines[i]
                sub_indent = len(sub_line) - len(sub_line.lstrip("" ""))
                if sub_indent <= current_indent:
                    break
                sub_key, sub_val = sub_line.strip().split("":"", 1)
                item[sub_key.strip()] = _convert_scalar(sub_val.strip())
                i += 1
            lst.append(item)
        return lst, i

    result: dict[str, Any] = {}
    while i < len(lines):
        line = lines[i]
        if not line.strip():
            i += 1
            continue
        current_indent = len(line) - len(line.lstrip("" ""))
        if current_indent < indent:
            break
        if current_indent > indent:
            raise YAMLError(""Invalid indentation"")
        stripped = line.strip()
        if "":"" not in stripped:
            raise YAMLError(f""Invalid line: {line}"")
        key, rest = stripped.split("":"", 1)
        key = key.strip()
        rest = rest.strip()
        i += 1
        if rest == """":
            # Determine if next block is a list or nested mapping
            if i < len(lines) and lines[i].lstrip().startswith(""- ""):
                lst: List[Any] = []
                while i < len(lines):
                    item_line = lines[i]
                    if len(item_line) - len(item_line.lstrip("" "")) < indent + 2:
                        break
                    if not item_line.lstrip().startswith(""- ""):
                        break
                    item_content = item_line.lstrip()[2:].strip()
                    i += 1
                    item: Any
                    if item_content == """":
                        item = {}
                    elif "":"" in item_content:
                        k, v = item_content.split("":"", 1)
                        item = {k.strip(): _convert_scalar(v.strip())}
                    else:
                        item = _convert_scalar(item_content)
                    # Parse additional properties for the list item
                    while i < len(lines):
                        sub_line = lines[i]
                        sub_indent = len(sub_line) - len(sub_line.lstrip("" ""))
                        if sub_indent <= indent + 2:
                            break
                        sub_key, sub_val = sub_line.strip().split("":"", 1)
                        item[sub_key.strip()] = _convert_scalar(sub_val.strip())
                        i += 1
                    lst.append(item)
                result[key] = lst
            else:
                sub_obj, i = _parse_simple_yaml(lines, i, indent + 2)
                result[key] = sub_obj
        else:
            result[key] = _convert_scalar(rest)
    return result, i
",src/yaml/__init__.py,,1,7
survived,"    def _model_dump_json(self: BaseModel, *args: Any, **kwargs: Any) -> str:
        return self.json(*args, **kwargs)
",src/meta_agent/__init__.py,,0,7
survived,"def _gen_crc16_table(poly: int) -> list[int]:
  table = []
  for i in range(256):
    crc = i << 8
    for _ in range(8):
      if crc & 0x8000:
        crc = ((crc << 1) ^ poly) & 0xFFFF
      else:
        crc = (crc << 1) & 0xFFFF
    table.append(crc)
  return table
",opendbc/car/crc.py,,1,7
survived,"def chrysler_checksum(address: int, sig, d: bytearray) -> int:
  checksum = 0xFF
  for j in range(len(d) - 1):
    curr = d[j]
    shift = 0x80
    for _ in range(8):
      bit_sum = curr & shift
      temp_chk = checksum & 0x80
      if bit_sum:
        bit_sum = 0x1C
        if temp_chk:
          bit_sum = 1
        checksum = (checksum << 1) & 0xFF
        temp_chk = checksum | 1
        bit_sum ^= temp_chk
      else:
        if temp_chk:
          bit_sum = 0x1D
        checksum = (checksum << 1) & 0xFF
        bit_sum ^= checksum
      checksum = bit_sum & 0xFF
      shift >>= 1
  return (~checksum) & 0xFF
",opendbc/car/chrysler/chryslercan.py,,1,8
survived,"def test_publish_model_query_no_ref(client: WeaveClient):
    class MyModel(weave.Model):
        @weave.op
        def predict(self, x: int) -> int:
            return x

    model = MyModel()
    ref = weave.publish(model)
    res = client.server.objs_query(
        tsi.ObjQueryReq.model_validate(
            {
                ""project_id"": client._project_id(),
                ""filter"": {""object_ids"": [ref.name]},
            }
        )
    )
    assert len(res.objs) == 1
    assert ""ref"" not in res.objs[0].val",tests/trace/test_objs_query.py,,0,6
survived,"    def __class_getitem__(cls, item: NamedArrayAxesSpec) -> typing.Any:
        axes = _parse_namedarray_axes(item)
        return typing.Annotated[NamedArray, axes]
",src/haliax/core.py,Named,1,7
survived,"    def is_git_ignored(p: Path) -> bool:
        try:
            result = subprocess.run(
                [""git"", ""check-ignore"", ""-q"", str(p.relative_to(repo_root))],
                cwd=repo_root,
            )
            return result.returncode == 0
        except Exception:
            return False
",scripts/verify_disclaimer_snippet.py,,1,7
survived,"def test_execute_and_collect_success(monkeypatch, tmp_path):
    fake_exec = MagicMock()
    fake_exec.run_tests.return_value = ExecutionResult(0, ""out"", ""err"")
    module = ResultCollectionModule(fake_exec)
    result = module.execute_and_collect(tmp_path, timeout=5)
    assert isinstance(result, CollectionResult)
    assert result.exit_code == 0
    assert result.stdout == ""out""
    assert result.stderr == ""err""
    assert result.duration >= 0
    fake_exec.run_tests.assert_called_with(tmp_path, timeout=5)
",tests/unit/test_result_collection_module.py,,1,8
survived,"def test_init_creates_execution_module(monkeypatch):
    fake_cls = MagicMock()
    fake_instance = MagicMock()
    fake_cls.return_value = fake_instance
    monkeypatch.setattr(rc_mod, ""ExecutionModule"", fake_cls)
    module = ResultCollectionModule()
    assert module.execution_module is fake_instance
",tests/unit/test_result_collection_module.py,,1,7
survived,"def test_init_creates_execution_module(monkeypatch):
    fake_cls = MagicMock()
    fake_instance = MagicMock()
    fake_cls.return_value = fake_instance
    monkeypatch.setattr(rc_mod, ""ExecutionModule"", fake_cls)
    module = ResultCollectionModule()
    assert module.execution_module is fake_instance
",tests/unit/test_result_collection_module.py,,1,7
survived,"def test_execute_and_collect_propagates_error(monkeypatch, tmp_path):
    fake_exec = MagicMock()
    fake_exec.run_tests.side_effect = rc_mod.SandboxExecutionError(""boom"")
    module = ResultCollectionModule(fake_exec)
    with pytest.raises(rc_mod.SandboxExecutionError):
        module.execute_and_collect(tmp_path)",tests/unit/test_result_collection_module.py,,1,7
survived,"    def to_json(self, report: SummaryReport) -> str:
        """"""Return a JSON representation of the report.""""""
        return json.dumps(asdict(report), indent=2)
",src/meta_agent/evaluation/reporting.py,ReportingModule,1,7
survived,"    def aggregate(self, results: Iterable[CollectionResult]) -> List[SummaryReport]:
        """"""Summarize multiple results.""""""
        return [self.summarize(r) for r in results]
",src/meta_agent/evaluation/reporting.py,ReportingModule,1,6
survived,"def make_result(exit_code: int = 0) -> CollectionResult:
    return CollectionResult(exit_code=exit_code, stdout=""out"", stderr=""err"", duration=1.23)
",tests/unit/test_reporting_module.py,,1,6
survived,"def test_setup_logging_rotation(tmp_path):
    log_file = tmp_path / ""rotate.log""
    logger = setup_logging(
        name=""rotate_logger"",
        level=""INFO"",
        log_file=str(log_file),
        max_bytes=10,
        backup_count=1,
    )
    assert isinstance(logger.handlers[0], RotatingFileHandler)
    logger.info(""rotated message"")
    assert log_file.exists()
    assert log_file.read_text()",tests/test_utils_logging.py,,0,7
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,,1,7
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/element-wise-operations.py,,1,6
survived,"def ruleBit(ruleNum, idx):
    r = ruleNum
    i = 0
    while i < idx:
        r = r // 2
        i = i + 1
    return r % 2
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-random-number-generator.py,,1,7
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""Source file entropy: "" + str(entropy(source)))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/entropy-narcissist.py,,1,6
survived,"def isEmptyDir(fs, name):
    if name in fs:
        return len(fs[name]) == 0
    return True
",tests/rosetta/transpiler/Python/empty-directory.py,,1,7
survived,"def elem(r, cells, generations, state):
    outputState(state)
    g = 0
    s = state
    while g < generations:
        s = step(s, r)
        outputState(s)
        g = g + 1
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,,1,6
survived,"    def _save_result(result: ResultsResponse) -> None:
        path = _results_dir / f""{result.id}.json""
        path.write_text(result.json())
        _simulations[result.id] = result
        while len(_simulations) > _max_results:
            old_id, _ = _simulations.popitem(last=False)
            with contextlib.suppress(FileNotFoundError):
                (_results_dir / f""{old_id}.json"").unlink()
        global _latest_id
        _latest_id = result.id
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    def _save_result(result: ResultsResponse) -> None:
        path = _results_dir / f""{result.id}.json""
        path.write_text(result.json())
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"    def _load_results() -> None:
        entries: list[tuple[float, ResultsResponse]] = []
        latest_time = 0.0
        latest_id: str | None = None
        for f in _results_dir.glob(""*.json""):
            try:
                data = json.loads(f.read_text())
                res = ResultsResponse(**data)
            except Exception:
                continue
            mtime = f.stat().st_mtime
            entries.append((mtime, res))
            if mtime > latest_time:
                latest_time = mtime
                latest_id = res.id
        _simulations.clear()
        for _, res in sorted(entries, key=lambda t: t[0]):
            _simulations[res.id] = res
        while len(_simulations) > _max_results:
            old_id, _ = _simulations.popitem(last=False)
            with contextlib.suppress(FileNotFoundError):
                (_results_dir / f""{old_id}.json"").unlink()
        global _latest_id
        _latest_id = latest_id
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,0,6
survived,"    def _load_results() -> None:
        for f in _results_dir.glob(""*.json""):
            try:
                data = json.loads(f.read_text())
                res = ResultsResponse(**data)
            except Exception:
                continue
            _simulations[res.id] = res
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,6
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = AlphaDiscoveryAgent()
    runtime.register(agent)
    print(""Registered AlphaDiscoveryAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py,,1,7
survived,"async def identify_alpha(domain: str = ""finance"") -> str:
    prompt = (
        f""List three emerging opportunities or inefficiencies in the {domain} domain ""
        ""that a small team could exploit for outsized value.""
    )
    return LLM(prompt)
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py,,1,7
survived,"def ensure_env_file() -> None:
    if not CONFIG_ENV.exists():
        print(""Creating default config.env (edit to add OPENAI_API_KEY)"")
        sample = DEMO_DIR / ""config.env.sample""
        CONFIG_ENV.write_bytes(sample.read_bytes())
",alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py,,1,7
survived,"            def __init__(self) -> None:
                self.instructions: list[Any] = []
",tests/test_ledger_broadcast.py,DummyTx,1,7
survived,"            def __init__(self, url: str) -> None:
                calls.append((""url"", url))
",tests/test_ledger_client_close.py,DummyClient,1,6
survived,"def parse_bytes(size: Union[int, str, None]) -> Optional[int]:
    """"""Convert a human friendly size string to bytes.""""""
    if size is None:
        return None
    if isinstance(size, int):
        return size
    match = re.fullmatch(r""(?i)\s*(\d+(?:\.\d+)?)\s*([kmgt]?b)?\s*"", str(size))
    if not match:
        raise ValueError(f""Invalid size value: {size}"")
    number = float(match.group(1))
    unit = (match.group(2) or ""b"").upper()
    factor = {
        ""B"": 1,
        ""KB"": 1024,
        ""MB"": 1024**2,
        ""GB"": 1024**3,
        ""TB"": 1024**4,
    }[unit]
    return int(number * factor)
",src/cachier/config.py,,1,7
survived,"    def word_ids(self):
        return range(len(self._words))
",btcrecover/btcrseed.py,WalletSLIP39Seed,1,6
survived,"    def id_to_word(cls, idx):
        return cls._words[idx]
",btcrecover/btcrseed.py,WalletSLIP39Seed,1,6
survived,"    def as_proxy(
        cls,
        backend: Client
        | ClientTransport
        | FastMCP[Any]
        | AnyUrl
        | Path
        | dict[str, Any]
        | str,
        **settings: Any,
    ) -> FastMCPProxy:
        """"""Create a FastMCP proxy server for the given backend.

        The ``backend`` argument can be either an existing :class:`~fastmcp.client.Client`
        instance or any value accepted as the ``transport`` argument of
        :class:`~fastmcp.client.Client`. This mirrors the convenience of the
        ``Client`` constructor.
        """"""
        from fastmcp.server.proxy import FastMCPProxy

        if isinstance(backend, Client):
            client = backend
        else:
            client = Client(backend)

        return FastMCPProxy(client=client, **settings)
",src/fastmcp/server/server.py,FastMCP,1,7
survived,"    def update(
        self,
        func: Callable[..., Any] | None = None,
        *,
        name: str | None = None,
        description: str | None = None,
    ) -> Callable[..., Any] | DecoratorCallable:
        """"""Register an update operation.""""""

        def decorator(fn: Callable[..., Any]) -> Callable[..., Any]:
            return self.resource(fn, name=name, description=description)

        if func is not None:
            return decorator(func)
        return cast(""DecoratorCallable"", decorator)
",src/enrichmcp/app.py,EnrichMCP,1,7
survived,"    def delete_note(self, note_id: str) -> bool:
        return self.store.delete(self.name, note_id)",examples/basic_memory/memory.py,MemoryProject,1,6
survived,"async def create_note(title: str, content: str, tags: list[str] | None = None) -> Note:
    """"""Create and persist a new note.""""""
    return project.create_note(title, content, tags)
",examples/basic_memory/app.py,,1,6
survived,"    def delete(self, project: str, note_id: str) -> bool:
        """"""Remove the note if present and return ``True`` on success.""""""
",examples/basic_memory/memory.py,MemoryStore,1,6
survived,"    def save(self, project: str, note: MemoryNote) -> None:
        """"""Persist ``note`` under ``project``.""""""
",examples/basic_memory/memory.py,MemoryStore,1,6
survived,"        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return fake_mod
            return orig_import_module(name, *args, **kwargs)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion,1,7
survived,"    def test_old_version_fails(self) -> None:
        for name in (""openai_agents"", ""agents""):
            with self.subTest(module=name):
                self.assertNotEqual(self._run_check(name, ""0.0.13""), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion,0,6
survived,"def auroc(truth: list[bool], scores: list[float]) -> float:
    """"""Compute AUROC using the rank method.""""""
    order = sorted(range(len(scores)), key=lambda i: scores[i])
    rank_sum = 0.0
    pos = 0
    for r, i in enumerate(order, 1):
        if truth[i]:
            rank_sum += r
            pos += 1
    neg = len(scores) - pos
    if pos == 0 or neg == 0:
        return 1.0
    return (rank_sum - pos * (pos + 1) / 2) / (pos * neg)
",src/simulation/replay.py,,1,7
survived,"    def run_index_post_pass(self):
        """"""Fill empty cell slots with the next non-empty cell index.""""""
        fixed = self.grid_cell_index.clone()
        for i in range(fixed.shape[0] - 2, -1, -1):
            if fixed[i] == -1:
                fixed[i] = fixed[i + 1]
        self.grid_cell_index_fixed = fixed
",pytorch_solver.py,PytorchSolver,1,7
survived,"    def run_compute_pressure(self):
        """"""Compute pressure from density error.""""""
        self.pressure = self.config[""delta""] * (self.rho - self.config[""rho0""])
",pytorch_solver.py,PytorchSolver,1,7
survived,"def convert_feats(v1_path, out_dir, doc_slug, start_pk):
    feats_v1 = load_json(v1_path)
    feats_v2 = []
    fb_v2 = []
    pk_counter = start_pk
    for feat in feats_v1:
        f = feat[""fields""]
        slug = feat[""pk""]
        pk = f""{doc_slug}_{slug}""
        feats_v2.append({
            ""model"": ""api_v2.feat"",
            ""pk"": pk,
            ""fields"": {
                ""name"": f[""name""],
                ""desc"": f[""desc""],
                ""prerequisite"": f.get(""prerequisite""),
                ""document"": doc_slug,
            },
        })
        eff = f.get(""effects_desc_json"")
        if eff:
            try:
                parts = json.loads(eff)
            except Exception:
                parts = []
            for part in parts:
                fb_v2.append({
                    ""model"": ""api_v2.featbenefit"",
                    ""pk"": pk_counter,
                    ""fields"": {""name"": """", ""desc"": part, ""type"": None, ""parent"": pk},
                })
                pk_counter += 1
    if feats_v2:
        save_json(feats_v2, os.path.join(out_dir, ""Feat.json""))
    if fb_v2:
        save_json(fb_v2, os.path.join(out_dir, ""FeatBenefit.json""))
    return pk_counter
",convert_missing.py,,1,7
survived,"    async def readiness() -> str:
        """"""Check orchestrator background task.""""""

        task = getattr(app_f.state, ""task"", None)
        if task and not task.done():
            return ""ok""
        # If the orchestrator failed to start, return OK for local tests.
        return ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,0,6
survived,"    async def readiness() -> str:
        """"""Check orchestrator background task.""""""

        task = getattr(app_f.state, ""task"", None)
        if task and not task.done():
            return ""ready""
        raise HTTPException(status_code=503, detail=""orchestrator not running"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,,1,7
survived,"def test_metrics_endpoint() -> None:
    client = make_client()
    resp = client.get(""/metrics"")
    assert resp.status_code == 200
    assert ""api_requests_total"" in resp.text",tests/test_metrics_router.py,,1,7
survived,"    def close(self) -> None:
        pass
",tests/test_agent_handle_methods.py,DummyLedger,0,7
survived,"    def start_merkle_task(self, *a, **kw):
        pass
",tests/test_agent_handle_methods.py,DummyLedger,0,7
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_agent_handle_methods.py,DummyLedger,1,7
survived,"def test_codegen_agent_emits_to_safety(monkeypatch) -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = codegen_agent.CodeGenAgent(bus, led)
    monkeypatch.setattr(agent, ""execute_in_sandbox"", lambda code: ("""", """"))
    env = messaging.Envelope(""market"", ""codegen"", {""analysis"": ""x""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][0] == ""safety""",tests/test_agent_handle_methods.py,,1,7
survived,"def test_strategy_agent_emits_market() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = strategy_agent.StrategyAgent(bus, led)
    env = messaging.Envelope(""research"", ""strategy"", {""research"": ""foo""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published
    topic, sent = bus.published[-1]
    assert topic == ""market""
    assert ""strategy"" in sent.payload
",tests/test_agent_handle_methods.py,,1,7
survived,"    def start_socket() -> None:
        """"""Start the optional A2A socket if available.""""""
        if _A2A:
            try:
                _A2A.start()
            except Exception:  # pragma: no cover - best effort
                LOG.warning(""Failed to start A2A socket"", exc_info=True)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver,1,7
survived,"def main(argv: list[str] | None = None) -> int:
    args = parse_args(argv)
    width, height = (int(x) for x in args.size.split(""x"", maxsplit=1))
    output = Path(args.output)

    temp_mp4 = output if output.suffix != "".gif"" else output.with_suffix("".mp4"")

    with Display(visible=False, size=(width, height)) as disp:
        display_var = f"":{disp.display}""
        ffmpeg_cmd = [
            ""ffmpeg"",
            ""-y"",
            ""-video_size"",
            args.size,
            ""-f"",
            ""x11grab"",
            ""-i"",
            display_var,
            ""-codec:v"",
            ""libx264"",
            ""-pix_fmt"",
            ""yuv420p"",
            str(temp_mp4),
        ]
        # Start ffmpeg first so it captures the entire run
        ffmpeg_proc = subprocess.Popen(ffmpeg_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        demo_proc = subprocess.Popen(args.demo, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        try:
            time.sleep(args.duration)
        finally:
            demo_proc.terminate()
            ffmpeg_proc.terminate()
            demo_proc.wait()
            ffmpeg_proc.wait()

    if output.suffix == "".gif"":
        subprocess.run([""ffmpeg"", ""-y"", ""-i"", str(temp_mp4), str(output)], check=True)
        temp_mp4.unlink(missing_ok=True)

    print(f""Saved preview to {output}"")
    return 0
",scripts/capture_demo_preview.py,,1,7
survived,"def test_bundle_metadata_defaults():
    meta = BundleMetadata()
    assert meta.schema_version == BUNDLE_SCHEMA_VERSION
    assert isinstance(meta.created_at, datetime)
    assert meta.custom == {}
",tests/test_bundle_metadata.py,,1,7
survived,"def test_bundle_metadata_custom_fields_preserved():
    data = {
        ""schema_version"": BUNDLE_SCHEMA_VERSION,
        ""meta_agent_version"": ""0.1.0"",
        ""foo"": ""bar"",
        ""custom"": {""x"": 1},
    }
    meta = BundleMetadata(**data)
    assert meta.meta_agent_version == ""0.1.0""
    assert meta.custom == {""x"": 1}
    assert getattr(meta, ""foo"") == ""bar""",tests/test_bundle_metadata.py,,1,7
survived,"    def _collect_recursive(
        self,
        package: str,
        pinned: Dict[str, str],
        licenses: Dict[str, str],
        visited: set[str],
        include_hashes: bool,
        hashes: Optional[Dict[str, str]],
    ) -> None:
        if package in visited:
            return
        visited.add(package)
        try:
            dist = metadata.distribution(package)
        except metadata.PackageNotFoundError:
            return

        meta = cast(Mapping[str, str], dist.metadata)
        name = meta.get(""Name"", package)
        version = dist.version
        pinned[name] = version
        licenses[name] = self._extract_license(dist)
        if include_hashes and hashes is not None:
            # Use hash of RECORD contents if available, else hash of version
            record = dist.read_text(""RECORD"")
            if record is not None:
                digest = hashlib.sha256(record.encode(""utf-8"")).hexdigest()
            else:
                digest = hashlib.sha256(version.encode(""utf-8"")).hexdigest()
            hashes[name] = digest

        for req in dist.requires or []:
            req_name = req.split("";"")[0].strip().split()[0]
            req_name = req_name.split(""["")[0]
            if req_name:
                self._collect_recursive(
                    req_name, pinned, licenses, visited, include_hashes, hashes
                )
",src/meta_agent/dependency_manager.py,DependencyManager,1,7
deleted,"    def clarity_reward(prompt, response, answer, state):
        """"""Reward clear, well-structured responses.""""""
        # Check for basic structure indicators
        score = 0.0
        
        # Has proper sentences (ends with punctuation)
        if re.search(r'[.!?]\s*$', response.strip()):
            score += 0.3
        
        # Not just a single word/phrase
        if len(response.split()) > 5:
            score += 0.3
        
        # Contains explanation markers
        explanation_markers = [""because"", ""since"", ""due to"", ""this is"", ""which means""]
        if any(marker in response.lower() for marker in explanation_markers):
            score += 0.4
        
        return min(score, 1.0)
",environments/truthful_qa/truthful_qa.py,,1,7
survived,"def special_case(import_id: str, logger: Logger) -> str | None:
    homepage: str | None = None

    # if no slashes, then pkgx used the homepage as the name
    # if two slashes, then probably github / gitlab
    if not re.search(r""/"", import_id) or re.search(r""/.+/"", import_id):
        homepage = import_id

    # if it's a crates.io package, then we can use the crates URL
    elif re.search(r""^crates.io"", import_id):
        if ""/"" in import_id:
            name = import_id.split(""/"")[1]
            homepage = f""https://crates.io/crates/{name}""
        else:
            logger.warn(f""Invalid format for crates.io import_id: {import_id}"")

    # if it's part of the x.org family
    elif re.search(r""^x.org"", import_id):
        homepage = ""https://x.org""

    # if it's part of the pkgx family
    elif re.search(""^pkgx.sh"", import_id):
        tool = import_id.split(""/"")[1]
        homepage = f""https://github.com/pkgxdev/{tool}""

    # python.org/typing_extensions
    elif re.search(""^python.org/typing_extensions"", import_id):
        homepage = ""https://github.com/python/typing_extensions""

    # thrysoee.dk/editline
    elif re.search(""^thrysoee.dk/editline"", import_id):
        homepage = ""https://thrysoee.dk/editline""

    else:
        logger.warn(f""no homepage in pkgx for {import_id}"")

    return homepage
",package_managers/pkgx/url.py,,1,6
survived,"def create_pkgx_package(
    distributables: list[str] | None = None,
    dependencies: list[str] | None = None,
    build_deps: list[str] | None = None,
    test_deps: list[str] | None = None,
) -> PkgxPackage:
    """"""Helper to create PkgxPackage instances for testing""""""

    # Create distributable blocks
    distributable_blocks = []
    if distributables:
        for url in distributables:
            distributable_blocks.append(Distributable(url=url))

    # Create dependency objects
    dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (dependencies or [])
            ],
        )
    ]
    build_dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (build_deps or [])
            ],
        )
    ]
    test_dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (test_deps or [])
            ],
        )
    ]

    # Create version object
    version = Version()

    return PkgxPackage(
        distributable=distributable_blocks,
        versions=version,
        dependencies=dep_objects,
        build=DependencyBlock(platform=""linux"", dependencies=build_dep_objects),
        test=DependencyBlock(platform=""linux"", dependencies=test_dep_objects),
    )
",tests/package_managers/pkgx/test_pkgx_diff.py,,1,6
deleted,"    def _get_homepage_url(self, import_id: str, pkg: PkgxPackage) -> str | None:
        """"""Get homepage URL for a package using the existing transformer logic""""""
        # Import the transformer methods for URL handling
        # TODO: this should use the url.py function
        from package_managers.pkgx.transformer import PkgxTransformer

        # Create a temporary transformer instance to use its methods
        temp_transformer = PkgxTransformer(self.config, None)

        # Try to get homepage from pkgx API
        homepage = temp_transformer.ask_pkgx(import_id)
        if not homepage:
            homepage = temp_transformer.special_case(import_id)

        if homepage:
            return temp_transformer.canonicalize(homepage)

        return None
",package_managers/pkgx/diff.py,PkgxDiff,0,7
survived,"    def test_completely_new_package(self, mock_config, mock_logger):
        """"""Test scenario 4: Package was completely new to the database""""""

        # Create empty cache (no existing packages)
        cache = Cache(package_map={}, url_map={}, package_urls={}, dependencies={})

        # Create new package data
        new_pkg_data = create_pkgx_package(
            distributables=[""https://github.com/example/new-pkg/archive/v1.0.tar.gz""],
            dependencies=[""some-dep""],
            build_deps=[""build-tool""],
        )

        # Test the diff
        diff = PkgxDiff(mock_config, cache, mock_logger)
        pkg_id, pkg_obj, update_payload = diff.diff_pkg(""new-pkg"", new_pkg_data)

        # Assertions
        assert pkg_obj is not None  # New package should be created
        assert pkg_obj.derived_id == ""pkgx/new-pkg""
        assert pkg_obj.name == ""new-pkg""
        assert pkg_obj.import_id == ""new-pkg""
        assert pkg_obj.package_manager_id == mock_config.pm_config.pm_id
        assert update_payload == {}  # No updates for new package

        # Test URL creation
        new_urls = {}
        with (
            patch.object(diff, ""_canonicalize_url"", side_effect=lambda x: x),
            patch.object(
                diff,
                ""_get_homepage_url"",
                return_value=""https://github.com/example/new-pkg"",
            ),
            patch.object(diff, ""_is_github_url"", return_value=True),
        ):
            resolved_urls = diff.diff_url(""new-pkg"", new_pkg_data, new_urls)
            new_links, updated_links = diff.diff_pkg_url(pkg_id, resolved_urls)

        # Should create URLs for homepage, source, and repository (GitHub)
        assert len(new_urls) >= 2  # At least source and homepage
        assert len(new_links) >= 2  # At least source and homepage links
        assert len(updated_links) == 0  # No existing links to update
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading,1,7
survived,"    def test_dependency_type_priority_with_test(self, mock_config, mock_logger):
        """"""Test priority handling with test dependencies: Runtime > Build > Test""""""

        p1_id = uuid4()
        p2_id = uuid4()
        p3_id = uuid4()
        p4_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""pkgx/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""pkgx/p2"", name=""p2"", import_id=""p2"")
        p3_pkg = Package(id=p3_id, derived_id=""pkgx/p3"", name=""p3"", import_id=""p3"")
        p4_pkg = Package(id=p4_id, derived_id=""pkgx/p4"", name=""p4"", import_id=""p4"")

        cache = Cache(
            package_map={""p1"": p1_pkg, ""p2"": p2_pkg, ""p3"": p3_pkg, ""p4"": p4_pkg},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Parsed data with overlapping dependencies across different types
        new_pkg_data = create_pkgx_package(
            dependencies=[""p2"", ""p3""],  # runtime: p2, p3
            build_deps=[""p2"", ""p4""],  # build: p2, p4
            test_deps=[""p2"", ""p3"", ""p4""],  # test: p2, p3, p4
        )

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""p1"", new_pkg_data)

        # Should create dependencies based on priority:
        # p2: runtime (highest priority among runtime/build/test)
        # p3: runtime (highest priority among runtime/test)
        # p4: build (highest priority among build/test)
        assert len(removed_deps) == 0
        assert len(new_deps) == 3

        # Sort by dependency_id for consistent testing
        new_deps_sorted = sorted(new_deps, key=lambda d: str(d.dependency_id))

        # p2 should be runtime (highest priority)
        p2_dep = next(d for d in new_deps_sorted if d.dependency_id == p2_id)
        assert p2_dep.dependency_type_id == mock_config.dependency_types.runtime

        # p3 should be runtime (highest priority)
        p3_dep = next(d for d in new_deps_sorted if d.dependency_id == p3_id)
        assert p3_dep.dependency_type_id == mock_config.dependency_types.runtime

        # p4 should be build (highest available priority)
        p4_dep = next(d for d in new_deps_sorted if d.dependency_id == p4_id)
        assert p4_dep.dependency_type_id == mock_config.dependency_types.build",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading,1,7
survived,"    def test_dependency_type_change_runtime_to_build(self, mock_config, mock_logger):
        """"""Test case 2: p1 has runtime dependency to p2 in cache,
        p1 has build dependency to p2 in parsed data.
        Expect removed runtime dependency and new build dependency.""""""

        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""pkgx/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""pkgx/p2"", name=""p2"", import_id=""p2"")

        # Existing runtime dependency
        existing_runtime_dep = LegacyDependency(
            package_id=p1_id,
            dependency_id=p2_id,
            dependency_type_id=mock_config.dependency_types.runtime,
        )

        cache = Cache(
            package_map={""p1"": p1_pkg, ""p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={p1_id: {existing_runtime_dep}},
        )

        # Parsed data only has build dependency
        new_pkg_data = create_pkgx_package(
            dependencies=[],  # no runtime deps
            build_deps=[""p2""],  # only build
        )

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""p1"", new_pkg_data)

        # Should remove runtime and add build
        assert len(removed_deps) == 1
        assert removed_deps[0].dependency_id == p2_id
        assert (
            removed_deps[0].dependency_type_id == mock_config.dependency_types.runtime
        )

        assert len(new_deps) == 1
        assert new_deps[0].dependency_id == p2_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.build
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading,1,7
survived,"def mock_logger():
    """"""Mock logger for testing.""""""
    return MagicMock(spec=Logger)
",tests/conftest.py,,1,6
survived,"    def test_sort_by_speed_index_same_speed(self):
        """"""Test stable sorting when models have same speed index.""""""
        models: list[ConciseModelResponse | ConciseLatestModelResponse] = [
            create_test_model(""zebra"", speed_index=500),
            create_test_model(""alpha"", speed_index=500),
            create_test_model(""beta"", speed_index=500),
        ]

        sorted_models = sort_models(models, ""speed_index"", ""desc"")

        # Should be sorted by id when speed is the same (reverse order due to desc)
        assert [m.id for m in sorted_models] == [""zebra"", ""beta"", ""alpha""]
",api/api/routers/mcp/_utils/model_sorting_test.py,TestSortModels,1,7
survived,"    def test_status_utils_with_none_resources_string(self):
        """"""Test status utils generate safe strings when resources are problematic.""""""
        mock_record_with_none = {
            'status': None,
            'num_nodes': 1,
            'resources': None,
            'total_cost': 0.0
        }
        
        mock_record_with_missing_attrs = {
            'status': None,
            'num_nodes': 2,
            'resources': mock.Mock(),
            'total_cost': 10.0
        }
        # Mock resources object missing expected attributes
        mock_record_with_missing_attrs['resources'].instance_type = None
        del mock_record_with_missing_attrs['resources'].cloud  # Simulate missing attribute
        
        # Test that these don't crash the status utility functions
        for record in [mock_record_with_none, mock_record_with_missing_attrs]:
            try:
                status_utils._get_resources_for_cost_report(record, truncate=True)
            except:
                pass  # May fail, but shouldn't crash the whole system
            
            try:
                status_utils._get_price_for_cost_report(record, truncate=True)
            except:
                pass
                
            try:
                status_utils._get_estimated_cost_for_cost_report(record, truncate=True)
            except:
                pass
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness,0,8
survived,"    def test_cost_report_endpoint_calls_core(self, mock_core_cost_report):
        """"""Test that cost_report server endpoint calls core function with correct args.""""""
        mock_core_cost_report.return_value = []
        
        # Create mock request and body
        mock_request = mock.Mock()
        mock_request.state.request_id = 'test_request_id'
        
        cost_report_body = payloads.CostReportBody(days=15)
        
        # Import and test the server function
        from sky.server import server
        
        with mock.patch('sky.server.server.executor.schedule_request') as mock_schedule:
            # Call the server endpoint
            import asyncio
            asyncio.run(server.cost_report(mock_request, cost_report_body))
            
            # Verify executor.schedule_request was called with correct parameters
            mock_schedule.assert_called_once()
            call_args = mock_schedule.call_args
            
            self.assertEqual(call_args[1]['request_id'], 'test_request_id')
            self.assertEqual(call_args[1]['request_name'], 'cost_report')
            self.assertEqual(call_args[1]['request_body'], cost_report_body)
            self.assertEqual(call_args[1]['func'], server.core.cost_report)
",tests/unit_tests/test_sky_cost_report.py,TestCostReportServer,0,6
survived,"def create_workflow(db_session):
    """"""Fixture to create a workflow with a specific CEL expression""""""

    def _create_workflow(workflow_id, cel_expression):
        workflow_definition = f""""""workflow:
  id: {workflow_id}
  description: Test severity CEL expressions
  triggers:
    - type: alert
      cel: {cel_expression}
  actions:
    - name: test-action
      provider:
        type: console
        with:
          message: ""Alert matched CEL expression""
""""""
        workflow = Workflow(
            id=workflow_id,
            name=workflow_id,
            tenant_id=SINGLE_TENANT_UUID,
            description=""Test severity CEL expressions"",
            created_by=""test@keephq.dev"",
            interval=0,
            workflow_raw=workflow_definition,
        )
        db_session.add(workflow)
        db_session.commit()
        return workflow

    return _create_workflow
",tests/test_workflow_severity_comparisons.py,,1,6
survived,"    def test_enrich_package_missing_source_warning(self, caplog, mock_logger):
        """"""Test warning when package references missing source""""""
        from package_managers.debian.main import enrich_package_with_source

        # Create package data with source that doesn't exist in mapping
        package_data = create_debian_package(
            package=""orphan-pkg"",
            description=""An orphaned package"",
        )
        package_data.source = ""missing-source""

        # Empty source mapping
        source_mapping = {}

        # Enrich package (this should log a warning)
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # The warning should be present in the function execution output
        # Check the logged warning message directly
        # Note: The warning is logged by our function, so we check the expected behavior

        # Package should remain unchanged
        assert enriched.package == ""orphan-pkg""
        assert enriched.description == ""An orphaned package""
        assert not enriched.vcs_git
        assert not enriched.vcs_browser
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping,1,6
survived,"    def test_build_depends(self, build_depends):
        """"""Test parsing build depends.""""""
        parser = DebianParser(build_depends)
        sources = list(parser.parse())
        assert len(sources) == 1
        source = sources[0]
        assert len(source.build_depends) == 5
        assert any(dep.package == ""gcc-11-source"" for dep in source.build_depends)
        assert any(dep.package == ""gawk"" for dep in source.build_depends)
        assert any(
            dep.package == ""lib32gcc1-amd64-cross"" for dep in source.build_depends
        )
        assert any(dep.package == ""g++-11"" for dep in source.build_depends)
        assert any(dep.package == ""gm2-11"" for dep in source.build_depends)
",tests/package_managers/debian/test_debian_parser.py,TestDebianParser,1,7
survived,"    def test_dependency_type_change_build_to_runtime(
        self, mock_config, mock_logger, mock_db
    ):
        """"""
        Scenario:
          - p1 has build dependency to p2 in cache
          - p1 has runtime dependency to p2 in parsed data.

        Expect removed build dependency and new runtime dependency
        """"""

        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""debian/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""debian/p2"", name=""p2"", import_id=""p2"")

        # Existing build dependency
        existing_build_dep = LegacyDependency(
            package_id=p1_id,
            dependency_id=p2_id,
            dependency_type_id=mock_config.dependency_types.build,
        )

        cache = Cache(
            package_map={""debian/p1"": p1_pkg, ""debian/p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={p1_id: {existing_build_dep}},
        )

        # Parsed data only has runtime dependency
        new_pkg_data = create_debian_package(
            package=""p1"",
            depends=[""p2""],  # runtime
            build_depends=[],  # no build deps
        )

        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""debian/p1"", new_pkg_data)

        # Should remove build and add runtime
        assert len(removed_deps) == 1
        assert removed_deps[0].dependency_id == p2_id
        assert removed_deps[0].dependency_type_id == mock_config.dependency_types.build

        assert len(new_deps) == 1
        assert new_deps[0].dependency_id == p2_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.runtime
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading,1,7
survived,"    def ingest_wrapper(self, diff_result: DiffResult) -> None:
        """"""Wrapper for the main ingest function to handle DiffResult""""""
        final_new_urls = list(diff_result.new_urls.values())
        self.ingest(
            diff_result.new_packages,
            final_new_urls,
            diff_result.new_package_urls,
            diff_result.new_deps,
            diff_result.removed_deps,
            diff_result.updated_packages,
            diff_result.updated_package_urls,
        )",package_managers/debian/db.py,DebianDB,1,7
survived,"    def test_enrich_package_with_explicit_source(self, mock_logger):
        """"""Test enriching package that has explicit source reference""""""

        # Create package data with explicit source reference
        package_data = create_debian_package(
            package=""binary-pkg"",
            description=""A binary package"",
        )
        package_data.source = ""source-pkg""

        # Create source mapping
        source_data = create_debian_package(
            package=""source-pkg"",
            vcs_git=""github.com/test/source-pkg"",  # Already normalized format
            homepage=""example.com/source-pkg"",  # Already normalized format
            build_depends=[""build-dep1"", ""build-dep2""],
        )
        source_mapping = {""binary-pkg"": source_data}

        # Enrich package
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # Verify enrichment
        assert enriched.package == ""binary-pkg""
        assert enriched.description == ""A binary package""
        assert enriched.vcs_git == ""github.com/test/source-pkg""
        assert enriched.homepage == ""example.com/source-pkg""
        assert len(enriched.build_depends) == 2

        build_depend_names = [item.package for item in enriched.build_depends]
        assert build_depend_names == [""build-dep1"", ""build-dep2""]
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping,1,7
survived,"def file_exists(*args) -> str:
    """"""Confirms if a file exists""""""
    file_path = join(*args)
    if not exists(file_path):
        raise FileNotFoundError(f""{file_path} not found"")
    return file_path",core/utils.py,,1,7
survived,"def start_task():
    """"""Start a new Claude Code automation task""""""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
            
        prompt = data.get('prompt')
        repo_url = data.get('repo_url')
        branch = data.get('branch', 'main')
        github_token = data.get('github_token')
        model = data.get('model', 'claude')  # Default to claude for backward compatibility
        
        if not all([prompt, repo_url, github_token]):
            return jsonify({'error': 'prompt, repo_url, and github_token are required'}), 400
        
        # Validate model selection
        if model not in ['claude', 'codex']:
            return jsonify({'error': 'model must be either ""claude"" or ""codex""'}), 400
        
        # Generate unique task ID
        task_id = str(uuid.uuid4())
        
        # Initialize task
        tasks[task_id] = {
            'id': task_id,
            'status': TaskStatus.PENDING,
            'prompt': prompt,
            'repo_url': repo_url,
            'branch': branch,
            'github_token': github_token,
            'model': model,
            'container_id': None,
            'commit_hash': None,
            'git_diff': None,
            'error': None,
            'created_at': time.time()
        }
        
        # Save tasks after creating
        save_tasks()
        
        # Start task in background thread
        thread = threading.Thread(target=run_ai_code_task, args=(task_id,))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'status': 'success',
            'task_id': task_id,
            'message': 'Task started successfully'
        })
        
    except Exception as e:
        logger.error(f""Error starting task: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,,1,7
survived,"def not_found(error):
    return jsonify({'error': 'Not found'}), 404
",server/main.py,,1,7
survived,"def parse_github_url(repo_url: str):
    """"""Parse GitHub URL to extract owner and repo name""""""
    # Handle both https and git URLs
    patterns = [
        r'https://github\.com/([^/]+)/([^/]+?)(?:\.git)?/?$',
        r'git@github\.com:([^/]+)/([^/]+?)(?:\.git)?$'
    ]
    
    for pattern in patterns:
        match = re.match(pattern, repo_url.strip())
        if match:
            owner, repo = match.groups()
            # Remove .git suffix if present
            if repo.endswith('.git'):
                repo = repo[:-4]
            return owner, repo
    
    raise ValueError(f""Invalid GitHub URL format: {repo_url}"")
",server/projects.py,,1,8
survived,"    def create_project(user_id: str, name: str, description: str, repo_url: str, 
                      repo_name: str, repo_owner: str, settings: Dict = None) -> Dict:
        """"""Create a new project""""""
        try:
            project_data = {
                'user_id': user_id,
                'name': name,
                'description': description,
                'repo_url': repo_url,
                'repo_name': repo_name,
                'repo_owner': repo_owner,
                'settings': settings or {},
                'is_active': True
            }
            
            result = supabase.table('projects').insert(project_data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error creating project: {e}"")
            raise
",server/database.py,DatabaseOperations,1,7
survived,"    def get_task_by_id(task_id: int, user_id: str) -> Optional[Dict]:
        """"""Get a specific task by ID for a user""""""
        try:
            result = supabase.table('tasks').select('*').eq('id', task_id).eq('user_id', user_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error fetching task {task_id}: {e}"")
            raise
",server/database.py,DatabaseOperations,1,7
deleted,"    def mock_graph_with_execution(self):
        """"""Create a mock graph that simulates execution.""""""
        graph = MagicMock()
        
        def mock_run(inputs=None, tweaks=None):
            """"""Mock graph execution.""""""
            input_value = inputs.get(""input_value"", """") if inputs else """"
            if ""error"" in input_value.lower():
                raise ValueError(""Simulated execution error"")
            return f""Processed: {input_value}""
        
        graph.run = mock_run
        return graph
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration,1,7
survived,"    def test_mcp_mode_output_formatting(self, runner, temp_python_script):
        """"""Test that MCP mode shows appropriate output formatting.""""""
        with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
            mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--mcp-transport"", ""sse"",
                ""--mcp-name"", ""Custom MCP Server"",
                ""--verbose""
            ])
            
            # Check for MCP-specific output
            assert ""MCP Server Started!"" in result.output
            assert ""Custom MCP Server"" in result.output
            assert ""MCP (sse)"" in result.output
            assert ""Available MCP Resources:"" in result.output
            assert ""flow://flows"" in result.output
            assert ""MCP Tools:"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand,1,6
survived,"    def test_create_mcp_server_empty_graphs(self, mock_fastmcp):
        """"""Test MCP server creation with empty graphs.""""""
        mock_mcp_instance = MagicMock()
        mock_fastmcp.return_value = mock_mcp_instance

        server = create_mcp_server(
            graphs={},
            metas={},
            server_name=""Empty Server""
        )

        # Should still create server but with no tools
        mock_fastmcp.assert_called_once_with(""Empty Server"")
        assert server == mock_mcp_instance

        # No tools should be registered
        assert mock_mcp_instance.tool.call_count == 0

        # Resources and prompts should still be registered
        assert mock_mcp_instance.resource.call_count >= 3
        assert mock_mcp_instance.prompt.call_count >= 2
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerCreation,1,7
survived,"    def test_flow_info_model(self):
        """"""Test FlowInfo model validation.""""""
        flow_info = FlowInfo(
            id=""test_flow"",
            title=""Test Flow"",
            description=""A test flow description""
        )
        assert flow_info.id == ""test_flow""
        assert flow_info.title == ""Test Flow""
        assert flow_info.description == ""A test flow description""
        assert flow_info.inputs is None
        assert flow_info.outputs is None
",src/backend/tests/unit/test_mcp_server.py,TestFlowModels,1,7
survived,"def main():
    """"""Main function""""""
    run_config, training_config, args = parse_args()
    
    # Create trainable model
    model = art.TrainableModel(
        name=args.model_name,
        project=""tau_bench_rl"",
        base_model=args.base_model,
        config=TauBenchPolicyConfig(
            training_config=training_config,
            run_config=run_config,
        ),
    )
    
    print(f""Starting RL training for model: {model.name}"")
    print(f""Base model: {model.base_model}"")
    print(f""Environment: {run_config.env}"")
    print(f""Task split: {run_config.task_split}"")
    
    # Run training
    asyncio.run(train(model))
",dev/tau-bench/run_rl.py,,1,7
survived,"    def test_env_group_rubric_initialization(self, mock_openai_client):
        """"""Test EnvGroupRubric initialization with multiple environments.""""""
        # Create test environments with different rubrics
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        def func3(completion, **kwargs):
            return 0.8
        
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric(funcs=[func1, func2], weights=[1.0, 0.5])
        )
        
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""q2""], ""answer"": [""a2""]}),
            rubric=Rubric(funcs=[func2, func3], weights=[0.7, 1.0])
        )
        
        env_map = {""task1"": env1, ""task2"": env2}
        rubric = EnvGroupRubric(env_map)
        
        assert rubric.env_map == env_map
        # Should have all unique reward function names
        assert set(rubric.all_reward_names) == {""func1"", ""func2"", ""func3""}
",tests/test_env_group.py,TestEnvGroupRubric,1,7
survived,"    def test_process_env_results_chat(self, mock_openai_client, sample_dataset):
        """"""Test processing environment results for chat format.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        # Create a mock tokenizer
        mock_tokenizer = Mock()
        
        # Track the conversation state
        def mock_apply_chat_template(conversation, tokenize=False, add_generation_prompt=True):
            # Convert messages to a string representation
            text = """"
            for msg in conversation:
                text += f""{msg['role']}: {msg['content']} ""
            return text.strip()
        
        def mock_encode(text, **kwargs):
            # Return tokens based on the text content
            if ""assistant: Hi there!"" in text:
                # Prompt + completion: return extended tokens
                return [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
            elif ""user: Hello"" in text:
                # Just prompt: return base tokens
                return [1, 2, 3, 4, 5]
            else:
                # Default case
                return [1, 2, 3]
        
        mock_tokenizer.apply_chat_template = Mock(side_effect=mock_apply_chat_template)
        mock_tokenizer.encode = Mock(side_effect=mock_encode)
        
        prompts = [[{""role"": ""user"", ""content"": ""Hello""}]]
        completions = [[{""role"": ""assistant"", ""content"": ""Hi there!""}]]
        states = [{}]
        rewards = [1.0]
        
        results = env.process_env_results(
            prompts, completions, states, rewards, mock_tokenizer
        )
        
        assert ""prompt_ids"" in results
        assert ""prompt_mask"" in results
        assert ""completion_ids"" in results
        assert ""completion_mask"" in results
        assert ""completion_logprobs"" in results
        assert ""rewards"" in results
        assert len(results[""rewards""]) == 1
        assert results[""rewards""][0] == 1.0
",tests/test_environment.py,TestEnvironmentBase,1,7
survived,"    async def test_rollout_state_structure(self, mock_singleturn_env):
        """"""Test that rollout creates proper state structure.""""""
        prompt = [{""role"": ""user"", ""content"": ""Hello""}]
        answer = ""Hi""
        task = ""greeting""
        info = {""context"": ""test""}
        
        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=answer,
            task=task,
            info=info
        )
        
        # Check all expected state fields
        assert state[""prompt""] == prompt
        # state[""completion""] is initialized to [] but not updated during rollout
        assert state[""completion""] == []
        assert state[""answer""] == answer
        assert state[""task""] == task
        assert state[""info""] == info
        assert ""responses"" in state
        assert isinstance(state[""responses""], list)
        assert len(state[""responses""]) == 1
",tests/test_singleturn_env.py,TestSingleTurnEnv,1,7
survived,"    def test_is_completed_method(self, mock_singleturn_env):
        """"""Test the is_completed method logic.""""""
        # No responses yet
        messages = [{""role"": ""user"", ""content"": ""Hello""}]
        state = {""responses"": []}
        assert not mock_singleturn_env.is_completed(messages, state)
        
        # With responses
        state = {""responses"": [MagicMock()]}
        assert mock_singleturn_env.is_completed(messages, state)
",tests/test_singleturn_env.py,TestSingleTurnEnv,1,7
survived,"    async def test_env_group_rollout_routing(self, mock_openai_client):
        """"""Test that rollout is properly routed to the correct sub-environment.""""""
        # Create environments with different behaviors
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q2""], ""answer"": [""a2""]}),
            rubric=Rubric()
        )
        
        # Mock the rollout methods to return different values
        env1.rollout = AsyncMock(return_value=(""response1"", {""env"": ""env1""}))
        env2.rollout = AsyncMock(return_value=(""response2"", {""env"": ""env2""}))
        
        env_group = EnvGroup(envs=[env1, env2], env_names=[""math"", ""code""])
        
        # Test routing to math environment
        result1, state1 = await env_group.rollout(
            client=mock_openai_client,
            model=""test-model"",
            prompt=""Test prompt"",
            task=""math""
        )
        
        assert result1 == ""response1""
        assert state1[""env""] == ""env1""
        env1.rollout.assert_called_once()
        env2.rollout.assert_not_called()
        
        # Reset mocks
        env1.rollout.reset_mock()
        env2.rollout.reset_mock()
        
        # Test routing to code environment
        result2, state2 = await env_group.rollout(
            client=mock_openai_client,
            model=""test-model"",
            prompt=""Test prompt"",
            task=""code""
        )
        
        assert result2 == ""response2""
        assert state2[""env""] == ""env2""
        env1.rollout.assert_not_called()
        env2.rollout.assert_called_once()
",tests/test_env_group.py,TestEnvGroup,1,7
survived,"    async def test_state_initialization(self, mock_multiturn_env):
        """"""Test that state is properly initialized with all required fields.""""""
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Test state""}],
            response=""Quick DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Test state""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""test_answer"",
            task=""test_task"",
            info={""extra"": ""data""}
        )
        
        # Check all state fields are initialized
        assert state[""prompt""] == prompt
        # state[""completion""] is initialized to [] but not updated during rollout
        assert state[""completion""] == []
        assert state[""answer""] == ""test_answer""
        assert state[""task""] == ""test_task""
        assert state[""info""] == {""extra"": ""data""}
        assert ""responses"" in state
        assert isinstance(state[""responses""], list)
",tests/test_multiturn_env.py,TestMultiTurnEnv,0,6
survived,"    def __init__(self, runner, console: Console):
        self.runner = runner
        self.console = console
        self.default_lm_api_base = runner.config.get(""default_lm_api_base"", None)
",docetl/operations/utils/api.py,LLMCallHandler,1,7
survived,"    def _parse_structured_output(self, response: Any, schema: Dict[str, Any], index: int = 0) -> List[Dict[str, Any]]:
        """"""Parse structured output response.""""""
        try:
            content = response.choices[index].message.content
            
            # Handle deepseek-r1 models' think tags
            if is_deepseek_r1(response.model):
                result = {}
                think_match = re.search(r""<think>(.*?)</think>"", content, re.DOTALL)
                if think_match:
                    result[""think""] = think_match.group(1).strip()
                    # Get the remaining content after </think>
                    main_content = re.split(r""</think>"", content, maxsplit=1)[-1].strip()
                    parsed_content = json.loads(main_content)
                else:
                    # If no think tags, parse the content as JSON
                    parsed_content = json.loads(content)
                
                result.update(parsed_content)
                return [result]
            
            # For other models, parse as JSON
            parsed_output = json.loads(content)
            
            # Augment with missing schema keys
            for key in schema:
                if key not in parsed_output:
                    parsed_output[key] = ""Not found""
            
            return [parsed_output]
            
        except json.JSONDecodeError:
            raise InvalidOutputError(
                ""Could not decode structured output JSON response"",
                str(content),
                schema,
                response.choices,
                []
            )
        except Exception as e:
            raise InvalidOutputError(
                f""Error parsing structured output: {e}"",
                str(content),
                schema,
                response.choices,
                []
            )
",docetl/operations/utils/api.py,ResponseParser,1,7
deleted,"    def _extract_snowflake_tool_calls(self, response: Any, index: int) -> List[ChatCompletionMessageToolCall]:
        """"""Extract tool calls from Snowflake model response.""""""
        if not hasattr(response.choices[index].message, ""content_list""):
            return []
        
        return [
            ChatCompletionMessageToolCall(
                function=Function(
                    name=content.get(""tool_use"", {}).get(""name""),
                    arguments=content.get(""tool_use"", {}).get(""input""),
                )
            )
            for content in response.choices[index].message.content_list
            if content.get(""type"") == ""tool_use""
        ]
",docetl/operations/utils/api.py,ResponseParser,1,7
deleted,"    def _log_verbose_output(self, messages: List[Dict[str, str]], output: LLMResult) -> None:
        """"""Log verbose output for debugging.""""""
        # Truncate messages to 500 chars
        messages_str = str(messages)
        truncated_messages = (
            messages_str[:500] + ""...""
            if len(messages_str) > 500
            else messages_str
        )

        # Log with nice formatting
        self.runner.console.print(
            Panel(
                Group(
                    Text(""Input:"", style=""bold cyan""),
                    Text(truncated_messages),
                    Text(""\nOutput:"", style=""bold cyan""),
                    Text(str(output)),
                ),
                title=""[bold green]LLM Call Details[/bold green]"",
                border_style=""green"",
            )
        )",docetl/operations/utils/api.py,APIWrapper,1,7
survived,"    def __init__(self, console: Console):
        self.console = console
",docetl/operations/utils/api.py,ResponseParser,1,7
survived,"    async def test_different_message_types_in_same_env(self, mock_openai_client, sample_dataset):
        """"""Test that environment respects its message_type setting.""""""
        # Chat environment
        chat_env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            message_type=""chat""
        )
        
        # Completion environment 
        completion_dataset = Dataset.from_dict({
            ""prompt"": [""Test prompt""],
            ""answer"": [""Test answer""]
        })
        completion_env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"", 
            dataset=completion_dataset,
            message_type=""completion""
        )
        
        # Test chat rollout
        chat_completion, _ = await chat_env.rollout(
            client=mock_openai_client,
            model=""test-model"",
            prompt=[{""role"": ""user"", ""content"": ""Hello""}],
            answer=""Hi""
        )
        assert isinstance(chat_completion, list)
        
        # Test completion rollout
        completion_result, _ = await completion_env.rollout(
            client=mock_openai_client,
            model=""test-model"", 
            prompt=""Complete this:"",
            answer=""Done""
        )
        assert isinstance(completion_result, str)",tests/test_singleturn_env.py,TestSingleTurnEnv,1,7
survived,"    def test_parse_empty_fields(self, xml_parser):
        """"""Test parsing XML with empty fields.""""""
        xml_text = ""<reasoning></reasoning><answer></answer>""
        result = xml_parser.parse(xml_text)
        assert result.reasoning == """"
        assert result.answer == """"
",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"def basic_parser():
    """"""Return a basic Parser instance.""""""
    return Parser()
",tests/conftest.py,,1,6
survived,"        def list_func(completion, **kwargs):
            return len(completion) if isinstance(completion, list) else 0.0
",tests/test_rubric.py,TestRubric,1,6
survived,"    def test_rubric_group_add_reward_func_empty_group_fails(self):
        """"""Test that adding reward function fails if no rubrics exist.""""""
        # This shouldn't happen due to initialization check, but test edge case
        group = RubricGroup.__new__(RubricGroup)  # Bypass __init__
        group.rubrics = []
        
        def test_func(completion, **kwargs):
            return 1.0
        
        with pytest.raises(AssertionError, match=""RubricGroup must have at least one rubric""):
            group.add_reward_func(test_func)
",tests/test_rubric_group.py,TestRubricGroup,1,7
survived,"    def test_multiturn_env_default_max_turns(self, mock_openai_client, sample_chat_dataset):
        """"""Test MultiTurnEnv default max_turns value.""""""
        from tests.conftest import SimpleMultiTurnEnv
        env = SimpleMultiTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_chat_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        assert env.max_turns == 10  # Default value
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    def test_rubric_group_initialization(self):
        """"""Test RubricGroup initialization with multiple rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.8])
        
        rubrics = [rubric1, rubric2]
        group = RubricGroup(rubrics=rubrics)
        
        assert group.rubrics == rubrics
        assert len(group.rubrics) == 2
",tests/test_rubric_group.py,TestRubricGroup,1,7
survived,"    def test_format_method(self, xml_parser):
        """"""Test formatting keyword arguments into XML.""""""
        formatted = xml_parser.format(reasoning=""My reasoning"", answer=""42"")
        assert ""<reasoning>\nMy reasoning\n</reasoning>"" in formatted
        assert ""<answer>\n42\n</answer>"" in formatted
",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"    def test_format_reward_function_bad_format(self, think_parser):
        """"""Test format reward function with poorly formatted content.""""""
        reward_func = think_parser.get_format_reward_func()
        
        # Missing think tags
        bad_completion1 = [
            {""role"": ""assistant"", ""content"": ""Just an answer without thinking""}
        ]
        reward1 = reward_func(bad_completion1)
        assert reward1 == 0.0
        
        # Multiple think tags
        bad_completion2 = [
            {""role"": ""assistant"", ""content"": ""<think>First</think><think>Second</think>Answer""}
        ]
        reward2 = reward_func(bad_completion2)
        assert reward2 == 0.0
        
        # No content after think
        bad_completion3 = [
            {""role"": ""assistant"", ""content"": ""<think>Only thinking</think>""}
        ]
        reward3 = reward_func(bad_completion3)
        assert reward3 == 0.0
",tests/test_think_parser.py,TestThinkParser,1,7
survived,"    def test_xml_parser_with_alternatives(self, xml_parser_with_alternatives):
        """"""Test XMLParser with alternative field names.""""""
        assert isinstance(xml_parser_with_alternatives, XMLParser)
        fields = xml_parser_with_alternatives.get_fields()
        assert ""reasoning"" in fields
        assert ""code"" in fields  # canonical name from (""code"", ""answer"")
",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"        def test_func(completion, **kwargs):
            return 1.0
",tests/test_rubric.py,TestRubric,1,6
survived,"    def _handle_text_completion(self, prompt, **kwargs):
        """"""Handle text completion requests.""""""
        if prompt in self.text_completions:
            response_data = self.text_completions[prompt]
        else:
            response_data = {
                ""text"": self.default_text_response,
                ""finish_reason"": ""stop""
            }
        
        # Create mock response
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].text = response_data[""text""]
        mock_response.choices[0].finish_reason = response_data[""finish_reason""]
        return mock_response
",tests/conftest.py,MockAsyncOpenAI,1,6
survived,"    def test_rubric_group_with_max_concurrent(self):
        """"""Test RubricGroup with max_concurrent parameter.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup,0,8
survived,"    async def test_a_generate_basic(self, mock_singleturn_env):
        """"""Test async generation with basic inputs.""""""
        inputs = {
            ""prompt"": [
                [{""role"": ""user"", ""content"": ""What is 2+2?""}],
                [{""role"": ""user"", ""content"": ""What is 3+3?""}]
            ],
            ""answer"": [""4"", ""6""]
        }
        
        # Mock the rubric.score_rollouts method
        mock_singleturn_env.rubric.score_rollouts = AsyncMock(return_value={
            ""rewards"": [1.0, 1.0],
            ""scores"": [{""correctness"": 1.0}, {""correctness"": 1.0}]
        })
        
        results = await mock_singleturn_env.a_generate(inputs)
        
        assert ""completion"" in results
        assert ""state"" in results
        assert ""rewards"" in results
        assert len(results[""completion""]) == 2
        assert len(results[""state""]) == 2
",tests/test_singleturn_env.py,TestSingleTurnEnv,1,7
survived,"    async def test_state_management(self, mock_multiturn_env):
        """"""Test that state is properly initialized and maintained.""""""
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Test state""}],
            response=""Quick DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Test state""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""test_answer""
        )
        
        # State should contain the answer
        assert ""answer"" in state
        assert state[""answer""] == ""test_answer""
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"        def reward_func1(completion, **kwargs):
            return 1.0
",tests/test_rubric.py,TestRubric,1,6
survived,"    def env_response(self, messages, state, **kwargs):
        """"""Simple environment response for testing.""""""
        self.env_response_count += 1
        
        if self.completion_condition == ""answer"":
            # Encourage completion after a few turns
            if self.env_response_count >= 2:
                return {""role"": ""user"", ""content"": ""Please finish with DONE""}, state
            else:
                return {""role"": ""user"", ""content"": f""Continue (turn {self.env_response_count})""}, state
        else:
            return {""role"": ""user"", ""content"": f""Environment response {self.env_response_count}""}, state
",tests/conftest.py,SimpleMultiTurnEnv,1,7
survived,"def xml_parser():
    """"""Return an XMLParser instance with common fields.""""""
    return XMLParser(
        fields=[""reasoning"", ""answer""],
        answer_field=""answer""
    )
",tests/conftest.py,,1,7
survived,"    def test_multiturn_env_initialization(self, mock_multiturn_env):
        """"""Test MultiTurnEnv initialization.""""""
        assert mock_multiturn_env.max_turns == 3
        assert mock_multiturn_env.message_type == 'chat'  # Default from parent
",tests/test_multiturn_env.py,TestMultiTurnEnv,1,7
survived,"    async def test_completion_detection_before_env_response(self, mock_openai_client, sample_chat_dataset):
        """"""Test completion detection works before env_response is called.""""""
        class ImmediateCompletionEnv(MultiTurnEnv):
            def is_completed(self, messages, state, **kwargs):
                # Complete if we have any assistant message
                return any(msg.get(""role"") == ""assistant"" for msg in messages)
            
            def env_response(self, messages, state, **kwargs):
                # This should never be called due to immediate completion
                return {""role"": ""user"", ""content"": ""Should not appear""}, state
        
        env = ImmediateCompletionEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_chat_dataset,
            max_turns=5,
            parser=Parser(),
            rubric=Rubric()
        )
        
        env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Start""}],
            response=""First response""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Start""}]
        completion, state = await env.rollout(
            client=env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""test""
        )
        
        # Should complete immediately after first assistant response
        assert len(completion) == 1
        assert completion[0][""role""] == ""assistant""
        assert completion[0][""content""] == ""First response""",tests/test_multiturn_env.py,TestMultiTurnEnv,1,8
survived,"    def test_get_format_str_with_alternatives(self, xml_parser_with_alternatives):
        """"""Test format string with alternatives.""""""
        format_str = xml_parser_with_alternatives.get_format_str()
        assert ""code | answer"" in format_str
",tests/test_xml_parser.py,TestXMLParser,1,7
survived,"            def analyze_objects():
                sum1 = summary.summarize(all_objects)
                return sum1
",app/helper/memory.py,MemoryHelper,1,6
survived,"    def _timeout_handler(self, signum, frame):
        """"""è¶…æ—¶ä¿¡å·å¤„ç†å™¨""""""
        raise TimeoutException(""å†…å­˜åˆ†æžè¶…æ—¶"")
",app/helper/memory.py,MemoryHelper,1,6
survived,"def package_ids():
    """"""Fixture providing consistent package IDs for testing.""""""
    return {
        ""foo"": uuid4(),
        ""bar"": uuid4(),
        ""baz"": uuid4(),
        ""qux"": uuid4(),
    }
",tests/package_managers/homebrew/test_diff_dep.py,,1,6
survived,"def diff_instance(mock_config):
    """"""
    Factory fixture to create Diff instances with specific cache configurations.

    Returns a function that creates Diff instances.
    """"""

    def create_diff(package_map, dependencies=None, url_map=None, package_urls=None):
        cache = Cache(
            package_map=package_map,
            url_map=url_map or {},
            package_urls=package_urls or {},
            dependencies=dependencies or {},
        )
        return Diff(mock_config, cache)

    return create_diff
",tests/package_managers/homebrew/test_diff_dep.py,,1,7
survived,"def mock_package_managers():
    """"""
    Mock package managers for testing.

    Returns a mock PackageManagers object.
    """"""
    package_managers = MagicMock(spec=PackageManagers)

    # Set up package manager attributes directly
    package_managers.crates = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000030""))
    package_managers.homebrew = Mock(
        id=uuid.UUID(""00000000-0000-0000-0000-000000000031"")
    )
    package_managers.debian = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000032""))
    package_managers.pkgx = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000033""))

    return package_managers
",tests/conftest.py,,1,7
survived,"def test_optimized_schema():
	""""""Test the optimized schema generation and save to file.""""""

	# Create controller and get all registered actions
	controller = Controller()
	ActionModel = controller.registry.create_action_model()

	# Create the agent output model with custom actions
	agent_output_model = AgentOutput.type_with_custom_actions(ActionModel)

	# Get original schema for comparison
	original_schema = agent_output_model.model_json_schema()

	# Create the optimized schema
	optimized_schema = SchemaOptimizer.create_optimized_json_schema(agent_output_model)

	# Create tmp directory if it doesn't exist
	os.makedirs('./tmp', exist_ok=True)

	# Save optimized schema
	with open('./tmp/optimized_schema.json', 'w') as f:
		json.dump(optimized_schema, f, separators=(',', ':'), indent=2)

	print('âœ… Optimized schema generated and saved to ./tmp/optimized_schema.json')

	# Compare token counts of both
	try:
		enc = tiktoken.encoding_for_model('gpt-4o')
	except KeyError:
		enc = tiktoken.get_encoding('cl100k_base')

	original_tokens = len(enc.encode(json.dumps(original_schema)))
	optimized_tokens = len(enc.encode(json.dumps(optimized_schema, separators=(',', ':'))))

	savings = original_tokens - optimized_tokens
	savings_percentage = (savings / original_tokens * 100) if original_tokens > 0 else 0

	print('\nðŸ“Š Token Count Comparison:')
	print(f'   Original schema: {original_tokens:,} tokens')
	print(f'   Optimized schema: {optimized_tokens:,} tokens')
	print(f'   Token savings: {savings:,} tokens ({savings_percentage:.1f}% reduction)')

	# Count tokens per action in optimized schema
	print('\nðŸ” Tokens per Action in Optimized Schema:')

	if 'properties' in optimized_schema and 'action' in optimized_schema['properties']:
		action_prop = optimized_schema['properties']['action']
		if 'items' in action_prop and 'anyOf' in action_prop['items']:
			actions = action_prop['items']['anyOf']

			total_action_tokens = 0
			for i, action in enumerate(actions):
				action_json = json.dumps(action, separators=(',', ':'))
				action_tokens = len(enc.encode(action_json))
				total_action_tokens += action_tokens

				# Try to get action name from the schema
				action_name = 'Unknown'
				if 'properties' in action:
					# Get the first property that's not common ones like 'index', 'reasoning'
					for prop_name in action['properties'].keys():
						if prop_name not in ['index', 'reasoning']:
							action_name = prop_name
							break

				print(f'   Action {i + 1} ({action_name}): {action_tokens:,} tokens')

			print('\nðŸ“ˆ Summary:')
			print(f'   Total actions: {len(actions)}')
			print(f'   Total action tokens: {total_action_tokens:,} tokens')
			print(f'   Average tokens per action: {total_action_tokens // len(actions):,} tokens')
			print(f'   Action tokens as % of total: {(total_action_tokens / optimized_tokens * 100):.1f}%')
		else:
			print('   No actions found in expected schema structure')
	else:
		print('   No action property found in optimized schema')
",tests/ci/test_custom_structured_ouput.py,,1,6
survived,"    def test_go_real_world_example(self):
        # Based on a typical Go service with various function types
        patch = """"""
@@ -73,9 +73,7 @@ func NewService(db *sql.DB) *Service

@@ -87,7 +87,8 @@ func (s *Service) GetUser(ctx context.Context, id int) (*User, error)

@@ -95,6 +95,7 @@ func (s *Service) CreateUser(user *User) error

@@ -103,4 +107,23 @@ func validateEmail(email string) bool

@@ -115,6 +118,13 @@ var logger = func(msg string) {

@@ -125,7 +125,7 @@ func (s *Service) updateCache(key string, value interface{})

@@ -135,8 +135,8 @@ handleError := func(err error) {

@@ -145,10 +145,10 @@ func init()

@@ -168,15 +168,15 @@ func main()

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""NewService"",
            ""GetUser"",
            ""CreateUser"",
            ""validateEmail"",
            ""logger"",
            ""updateCache"",
            ""handleError"",
            ""init"",
            ""main"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase,1,7
survived,"    def test_go_edge_cases(self):
        patch = """"""
@@ -152,10 +152,6 @@ func()

@@ -152,10 +152,6 @@ func _()

@@ -152,10 +152,6 @@ func (r *T) method_with_underscore()

@@ -152,10 +152,6 @@ var fn123 = func() {

@@ -152,10 +152,6 @@ camelCase := func() {

@@ -152,10 +152,6 @@ func MixedCase_With_Underscores()

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""_"",
            ""method_with_underscore"",
            ""fn123"",
            ""camelCase"",
            ""MixedCase_With_Underscores"",
        }",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase,1,7
survived,"    def test_simple_with_go(self):
        data = [
            {""filename"": ""foo.py"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""bar.js"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""baz.py"", ""changes"": 100, ""status"": ""added""},
            {""filename"": ""main.go"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""service.go"", ""changes"": 50, ""status"": ""modified""},
            {""filename"": ""handler.rb"", ""changes"": 100, ""status"": ""modified""},
        ]
        responses.add(
            responses.GET,
            self.gh_path.format(pull_number=self.pr.key),
            status=200,
            json=data,
        )

        pr_files = self.open_pr_comment_workflow.safe_for_comment(repo=self.gh_repo, pr=self.pr)
        assert pr_files == [
            {""filename"": ""foo.py"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""bar.js"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""main.go"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""service.go"", ""changes"": 50, ""status"": ""modified""},
            {""filename"": ""handler.rb"", ""changes"": 100, ""status"": ""modified""},
        ]
",tests/sentry/integrations/github/tasks/test_open_pr_comment.py,TestSafeForComment,1,7
survived,"    def test_send_email_without_body_or_html_raises_error(self, mock_smtp_class, smtp_provider):
        """"""Test that sending an email without body or html raises an error.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Attempt to send email without body or html
        with pytest.raises(ValueError, match=""Either 'body' or 'html' must be provided""):
            smtp_provider._notify(
                from_email=""sender@example.com"",
                from_name=""Test Sender"",
                to_email=""recipient@example.com"",
                subject=""Test Subject"",
            )
",tests/test_smtp_provider.py,TestSmtpProvider,1,7
survived,"    async def test_delete_api_key_with_api_key_auth_success(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test that deleting an API key using API key authentication works correctly.

        This test verifies the fix for the bug where API key authentication
        would fail when deleting keys. Now it should work properly.
        """"""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock API key authentication (user will be None)
        mock_user_dep.return_value = None

        # Mock successful deletion
        mock_api_keys_service.delete_key.return_value = True

        # This should now work correctly with the fix
        response = await test_api_client.delete(""/_/api/keys/test_key_id"")

        # The fix: operation succeeds when using API key authentication
        assert response.status_code == 204
        mock_api_keys_service.delete_key.assert_called_once_with(""test_key_id"")
",api/api/routers/api_keys_test.py,TestDeleteAPIKey,1,7
survived,"    def __init__(self, name: str, concurrency: int, requests: int, duration: float,
                 latencies: List[float], errors: int):
        self.name = name
        self.concurrency = concurrency
        self.requests = requests
        self.duration = duration
        self.latencies = latencies
        self.errors = errors
        
        self.rps = requests / duration
        self.avg_latency = mean(latencies) if latencies else 0
        self.median_latency = median(latencies) if latencies else 0
        self.min_latency = min(latencies) if latencies else 0
        self.max_latency = max(latencies) if latencies else 0
        self.stdev_latency = stdev(latencies) if len(latencies) > 1 else 0
",benchmarks/benchmark.py,BenchmarkResult,1,7
survived,"def dexscreener(options: DexscreenerPluginOptions) -> DexscreenerPlugin:
    return DexscreenerPlugin(options)",python/src/plugins/dexscreener/goat_plugins/dexscreener/__init__.py,,1,7
survived,"    def __init__(self, options: NansenPluginOptions):
        super().__init__(""nansen"", [NansenService(options.api_key)])
",python/src/plugins/nansen/goat_plugins/nansen/__init__.py,NansenPlugin,1,6
survived,"    async def get_nft_details(self, parameters: dict):
        """"""Get details for a specific NFT collection or token from Nansen""""""
        url = f""{self.base_url}/nft""
        params = {
            ""token_address"": parameters[""token_address""],
            ""nft_id"": parameters[""nft_id""]
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService,1,7
survived,"    async def get_conversation(self, parameters: dict):
        url = f""{self.base_url}/cast/conversation""
        return await self._make_request(""GET"", url, params={
            ""identifier"": parameters['identifier'],
            ""type"": parameters['type'],
            ""reply_depth"": parameters.get('reply_depth', 2),
            ""limit"": parameters.get('limit', 20),
        })
",python/src/plugins/farcaster/goat_plugins/farcaster/service.py,FarcasterService,1,7
survived,"    async def _make_request(self, method, url, **kwargs):
        headers = kwargs.pop(""headers"", {})
        headers[""x-api-key""] = self.api_key
        headers[""content-type""] = ""application/json""
        async with aiohttp.ClientSession() as session:
            async with session.request(method, url, headers=headers, **kwargs) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status}, text: {await response.text()}"")
                return await response.json()",python/src/plugins/farcaster/goat_plugins/farcaster/service.py,FarcasterService,1,6
survived,"def download_file(url, destination):
    """"""Download a file from a URL to a local destination.""""""
    print(f""Downloading from {url} to {destination}..."")
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    with open(destination, ""wb"") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    print(f""Download complete: {destination}"")
    return destination
",tests/replay_parser_test.py,,1,7
survived,"    def _get_current_connector_version(self, connector: Connector) -> semver.Version:
        """"""Get the current version.""""""
        return semver.Version.parse(str(connector.metadata[""dockerImageTag""]))
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck,1,6
deleted,"    def test_run_version_not_incremented(self, mock_current_version, mock_master_version, version_increment_check, connector):
        mock_master_version.return_value = semver.Version.parse(""1.0.0"")
        mock_current_version.return_value = semver.Version.parse(""0.9.0"")
        
        result = version_increment_check._run(connector)
        
        assert result.status == CheckStatus.FAILED
        assert ""was not incremented"" in result.message
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,TestVersionIncrementCheck,1,7
deleted,"def test_scrape_url_with_parse_pdf_false():
    if TEST_API_KEY:
        app = FirecrawlApp(api_url=API_URL, api_key=TEST_API_KEY)
        response = app.scrape_url('https://arxiv.org/pdf/astro-ph/9301001.pdf', parse_pdf=False)
        assert response is not None
        assert 'markdown' in response
        assert 'h7uKu14adDL6yGfnGf2qycY5uq8kC3OKCWkPxm' in response['markdown']
",apps/python-sdk/firecrawl/__tests__/v1/e2e_withAuth/test.py,,0,7
survived,"def cross_product(u, v):
    batch = u.shape[0]
    i = u[:, 1] * v[:, 2] - u[:, 2] * v[:, 1]
    j = u[:, 2] * v[:, 0] - u[:, 0] * v[:, 2]
    k = u[:, 0] * v[:, 1] - u[:, 1] * v[:, 0]

    out = torch.cat((i.view(batch, 1), j.view(batch, 1), k.view(batch, 1)), 1)

    return out
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,,1,6
survived,"def get_pt2d_from_mat(mat_path):
    mat = sio.loadmat(mat_path)
    pt2d = mat['pt2d']
    return pt2d
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,,1,6
survived,"def get_ypr_from_mat(mat_path):
    mat = sio.loadmat(mat_path)
    pre_pose_params = mat['Pose_Para'][0]
    pose_params = pre_pose_params[:3]
    return pose_params
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,,1,6
survived,"    def convert_to_markdown(self, file_path: str) -> List[Tuple[str, Dict[str, Any]]]:
        """"""
        Converts a document to a single Markdown string, preserving layout and tables.
        Supports PDF, DOCX, HTML, and other formats.
        """"""
        if not (self.converter_no_ocr and self.converter_ocr and self.converter_general):
            print(""docling converters not available. Skipping conversion."")
            return []
        
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext not in self.SUPPORTED_FORMATS:
            print(f""Unsupported file format: {file_ext}"")
            return []
        
        input_format = self.SUPPORTED_FORMATS[file_ext]
        
        if input_format == InputFormat.PDF:
            return self._convert_pdf_to_markdown(file_path)
        else:
            return self._convert_general_to_markdown(file_path, input_format)
",rag_system/ingestion/document_converter.py,DocumentConverter,1,7
survived,"    def sync_no_stream():
        groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from sync no stream""},
            ],
            session=session
        )
",tests/core_manual_tests/providers/groq_canary.py,,1,7
survived,"    async def run_async_tests():
        await async_no_stream()
        await async_stream()
",tests/core_manual_tests/providers/ai21_canary.py,,1,6
survived,"def download_test_dataset(raw_dir, temp_dir):
  """"""Download and extract the test dataset to the Raw directory.""""""
  zip_path = os.path.join(temp_dir, ""test_dataset.zip"")
  download_file(TEST_DATASET_URL, zip_path)

  extract_dir = os.path.join(temp_dir, ""extracted"")
  os.makedirs(extract_dir, exist_ok=True)
  extract_zip(zip_path, extract_dir)

  slp_files = []
  for root, _, files in os.walk(extract_dir):
    for file in files:
      if file.endswith('.slp'):
        slp_files.append(os.path.join(root, file))

  if slp_files:
    import zipfile
    test_zip_path = os.path.join(raw_dir, ""test_dataset.zip"")
    with zipfile.ZipFile(test_zip_path, 'w') as zipf:
      for file in slp_files:
        arcname = os.path.relpath(file, extract_dir)
        zipf.write(file, arcname)

    print(f""Created zip with {len(slp_files)} .slp files at {test_zip_path}"")
    return [os.path.basename(test_zip_path)]
  else:
    print(""No .slp files found in the extracted dataset"")
    return []
",tests/dataset_creation_test.py,,1,7
survived,"def add_relevant_files(reasoning: str, file_paths: List[str]) -> str:
    """"""Adds files to the list of relevant files.
    
    Args:
        reasoning: Explanation of why we're adding these files
        file_paths: List of file paths to add
        
    Returns:
        String indicating success
    """"""
    try:
        console.log(f""[blue]Add Relevant Files Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Adding {len(file_paths)} files to relevant files list[/dim]"")
        
        global RELEVANT_FILES
        for file_path in file_paths:
            if file_path not in RELEVANT_FILES:
                RELEVANT_FILES.append(file_path)
        
        console.log(f""[green]Added {len(file_paths)} files. Total relevant files: {len(RELEVANT_FILES)}[/green]"")
        return ""Files Added""
    except Exception as e:
        console.log(f""[red]Error adding relevant files: {str(e)}[/red]"")
        return f""Error: {str(e)}""
",sfa_codebase_context_agent_v3.py,,1,7
survived,"            def to_primitive(value: Any) -> str | int | float:
                if isinstance(value, list):
                    return str([to_primitive(v) for v in value])
                elif isinstance(value, dict):
                    return str({k: to_primitive(v) for k, v in value.items()})
                elif isinstance(value, Enum):
                    return value.name
                elif isinstance(value, (float, int)):
                    return value
                return str(value)
",marimo/_plugins/ui/_impl/tables/narwhals_table.py,NarwhalsTableManager,1,7
survived,"    async def transfer_token_by_mint_address(self, wallet_client: SolanaWalletClient, parameters: dict):
        """"""Transfer SPL tokens between wallets.""""""
        try:
            mint_pubkey = Pubkey.from_string(parameters[""mintAddress""])
            from_pubkey = Pubkey.from_string(wallet_client.get_address())
            to_pubkey = Pubkey.from_string(parameters[""to""])
            
            # Get token info for decimals
            token = next(
                (token for token in self.tokens 
                 if token[""mintAddresses""][self.network] == parameters[""mintAddress""]),
                None
            )
            if not token:
                raise Exception(f""Token with mint address {parameters['mintAddress']} not found"")
            
            # Get associated token accounts
            from_token_account = get_associated_token_address(
                from_pubkey,
                mint_pubkey
            )
            to_token_account = get_associated_token_address(
                to_pubkey,
                mint_pubkey
            )
            
            # Check if accounts exist
            from_account_info = wallet_client.client.get_account_info(from_token_account)
            to_account_info = wallet_client.client.get_account_info(to_token_account)
            
            if not from_account_info.value:
                raise Exception(f""From account {str(from_token_account)} does not exist"")
            
            instructions = []
            
            # Create destination token account if it doesn't exist
            if not to_account_info.value:
                instructions.append(
                    create_associated_token_account(
                        from_pubkey,  # payer
                        to_pubkey,    # owner
                        mint_pubkey   # mint
                    )
                )
            
            # Add transfer instruction
            instructions.append(
                Instruction(
                    program_id=TOKEN_PROGRAM_ID,
                    accounts=[
                        AccountMeta(pubkey=from_token_account, is_signer=False, is_writable=True),
                        AccountMeta(pubkey=mint_pubkey, is_signer=False, is_writable=False),
                        AccountMeta(pubkey=to_token_account, is_signer=False, is_writable=True),
                        AccountMeta(pubkey=from_pubkey, is_signer=True, is_writable=False),
                    ],
                    data=bytes([11]) + int(str(parameters[""amount""])).to_bytes(8, 'little') + bytes([token[""decimals""]])
                )
            )
            
            from goat_wallets.solana import SolanaTransaction
            # Create transaction with proper type
            tx: SolanaTransaction = {
                ""instructions"": instructions,
                ""address_lookup_table_addresses"": None,
                ""accounts_to_sign"": None
            }
            return wallet_client.send_transaction(tx)
        except Exception as error:
            raise Exception(f""Failed to transfer tokens: {error}"")
",python/src/plugins/spl_token/goat_plugins/spl_token/service.py,SplTokenService,1,7
survived,"    def test_change_tracking_format(self, mock_post):
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            'success': True,
            'data': {
                'markdown': 'Test markdown content',
                'changeTracking': {
                    'previousScrapeAt': '2023-01-01T00:00:00Z',
                    'changeStatus': 'changed',
                    'visibility': 'visible'
                }
            }
        }
        mock_post.return_value = mock_response

        app = FirecrawlApp(api_key=os.environ.get('FIRECRAWL_API_KEY', 'dummy-api-key-for-testing'))
        result = app.scrape_url('https://example.com', {
            'formats': ['markdown', 'changeTracking']
        })

        args, kwargs = mock_post.call_args
        self.assertEqual(kwargs['json']['formats'], ['markdown', 'changeTracking'])
        
        self.assertEqual(result['changeTracking']['previousScrapeAt'], '2023-01-01T00:00:00Z')
        self.assertEqual(result['changeTracking']['changeStatus'], 'changed')
        self.assertEqual(result['changeTracking']['visibility'], 'visible')
",apps/python-sdk/tests/test_change_tracking.py,TestChangeTracking,1,7
survived,"    async def _cleanup(self) -> StepResult:
        if self.backup_schema_path:
            shutil.rmtree(self.backup_schema_path)
        return StepResult(
            step=self,
            status=StepStatus.SUCCESS,
        )
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RestoreInlineState,1,7
survived,"    def test_multiple_pause_resume_cycles(self):
        """"""Test multiple pause/resume cycles work correctly.""""""
        formatter = ConsoleFormatter()
        
        mock_live = MagicMock(spec=Live)
        formatter._live = mock_live
        formatter._live_paused = False
        
        formatter.pause_live_updates()
        assert formatter._live_paused
        mock_live.stop.assert_called_once()
        assert formatter._live is None  # Live session should be cleared
        
        formatter.resume_live_updates()
        assert not formatter._live_paused
        
        formatter.pause_live_updates()
        assert formatter._live_paused
        
        formatter.resume_live_updates()
        assert not formatter._live_paused
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume,1,6
survived,"def validate_path_exists(path: Union[str, Path], file_type: str = ""file"") -> str:
    """"""
    Validate that a path exists and is of the expected type.

    Parameters
    ----------
    path : Union[str, Path]
        Path to validate.
    file_type : str, optional
        Expected type ('file' or 'directory'), by default 'file'.

    Returns
    -------
    str
        Validated path as string.

    Raises
    ------
    ValueError
        If path doesn't exist or is not of expected type.
    """"""
    try:
        path_obj = Path(path).resolve()
        
        if not path_obj.exists():
            raise ValueError(f""Path does not exist: {path}"")
            
        if file_type == ""file"" and not path_obj.is_file():
            raise ValueError(f""Path is not a file: {path}"")
        elif file_type == ""directory"" and not path_obj.is_dir():
            raise ValueError(f""Path is not a directory: {path}"")
            
        return str(path_obj)
        
    except Exception as e:
        if isinstance(e, ValueError):
            raise
        raise ValueError(f""Invalid path: {str(e)}"")
",src/crewai/flow/path_utils.py,,1,7
survived,"    def flow_id(self) -> str:
        """"""Returns the unique identifier of this flow instance.""""""
        if isinstance(self._state, dict):
            return str(self._state.get(""id"", """"))
        return str(getattr(self._state, ""id"", """"))
",src/crewai/flow/flow.py,Flow,1,7
survived,"    def test_polars_groupby_alias() -> None:
        """"""Test that group by operations use original column names correctly.""""""
        import polars as pl

        import marimo as mo

        # Create a test dataframe with age and group columns
        df = pl.DataFrame({
            ""group"": [""a"", ""a"", ""b"", ""b""],
            ""age"": [10, 20, 30, 40],
        })
        # Test the transformation directly using TransformsContainer
        from marimo._plugins.ui._impl.dataframes.transforms.apply import (
            TransformsContainer,
            get_handler_for_dataframe,
        )
        from marimo._plugins.ui._impl.dataframes.transforms.types import (
            GroupByTransform,
            TransformType,
            Transformations,
        )

        handler = get_handler_for_dataframe(df)
        transform_container = TransformsContainer(df, handler)
        
        # Create and apply the transformation
        transform = GroupByTransform(
            type=TransformType.GROUP_BY,
            column_ids=[""group""],
            drop_na=True,
            aggregation=""max"",
        )
        transformations = Transformations([transform])
        transformed_df = transform_container.apply(transformations)

        # Verify the transformed DataFrame
        assert isinstance(transformed_df, pl.DataFrame)
        assert ""group"" in transformed_df.columns
        assert ""age_max"" in transformed_df.columns
        assert transformed_df.shape == (2, 2)
        assert transformed_df[""age_max""].to_list() == [20, 40]  # max age for each group

        # The resulting frame should have correct column names and values
        # Convert to dict and verify values
        result_dict = {
            col: transformed_df[col].to_list()
            for col in transformed_df.columns
        }
        assert result_dict == {
            ""group"": [""a"", ""b""],
            ""age_max"": [20, 40],
        }

        # Verify the generated code uses original column names
        from marimo._plugins.ui._impl.dataframes.transforms.print_code import (
            python_print_polars,
        )
        code = python_print_polars(
            ""df"",
            [""group"", ""age""],
            transform,
        )
        # Code should reference original ""age"" column, not ""age_max""
        assert 'pl.col(""age"")' in code
        assert 'alias(""age_max"")' in code
        assert 'pl.col(""group"")' in code  # Original column name in group by
",tests/_plugins/ui/_impl/dataframes/test_dataframe.py,TestDataframes,1,7
survived,"    def validate_metadata(cls, v):
        """"""Validate that metadata is a dictionary.""""""
        if not isinstance(v, dict):
            raise ValueError(""Metadata must be a dictionary"")
        return v
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def __init__(self, **data):
        """"""Initialize a Fingerprint with auto-generated uuid_str and created_at.""""""
        # Remove uuid_str and created_at from data to ensure they're auto-generated
        if 'uuid_str' in data:
            data.pop('uuid_str')
        if 'created_at' in data:
            data.pop('created_at')

        # Call the parent constructor with the modified data
        super().__init__(**data)
",src/crewai/security/fingerprint.py,Fingerprint,1,7
deleted,"    async def load_embeddings_model(
        self,
        model_info: persistence_model.EmbeddingsModel | sqlalchemy.Row[persistence_model.EmbeddingsModel] | dict,
    ):
        """"""åŠ è½½ Embeddings æ¨¡åž‹""""""
        runtime_embeddings_model = await self.init_runtime_embeddings_model(model_info)
        self.embeddings_models.append(runtime_embeddings_model)
",pkg/provider/modelmgr/modelmgr.py,ModelManager,1,7
survived,"    def __init__(
        self,
        model_entity: persistence_model.EmbeddingsModel,
        token_mgr: token.TokenManager,
        requester: LLMAPIRequester,
    ):
        self.model_entity = model_entity
        self.token_mgr = token_mgr
        self.requester = requester
",pkg/provider/modelmgr/requester.py,RuntimeEmbeddingsModel,1,7
survived,"def test_agent_inject_date():
    """"""Test that the inject_date flag injects the current date into the task.""""""
    agent = Agent(
        role=""test_agent"",
        goal=""test_goal"",
        backstory=""test_backstory"",
        inject_date=True,
    )
    
    task = Task(
        description=""Test task"",
        expected_output=""Test output"",
        agent=agent,
    )
    
    with patch.object(Agent, 'execute_task', return_value=""Task executed"") as mock_execute:
        agent.execute_task(task)
        
        called_task = mock_execute.call_args[0][0]
        
        current_date = datetime.now().strftime(""%Y-%m-%d"")
        assert f""Current Date: {current_date}"" in called_task.description
",tests/test_agent_inject_date.py,,0,8
survived,"def _record() -> AirbyteMessage:
    return AirbyteMessage(
        type=Type.RECORD, record=AirbyteRecordMessage(stream=TEST_STREAM, data=TEST_MESSAGE, emitted_at=0, namespace=TEST_NAMESPACE)
    )
",airbyte-integrations/connectors/destination-glassflow/integration_tests/integration_test.py,,1,6
survived,"    def test_load_persistent_cache(self) -> None:
        """"""Test loading a persistent cache.""""""
        loader = PickleLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        # Use string directly instead of Name constructor
        original_cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        
        with open(cache_path, ""wb"") as f:
            pickle.dump(original_cache, f)
        
        # Load the cache
        loaded_cache = loader.load_persistent_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        
        # Should raise for non-existent cache
        with pytest.raises(FileNotFoundError):
            loader.load_persistent_cache(""nonexistent"", ""Pure"")
        
        # Test with invalid cache object
        invalid_path = loader.build_path(""invalid"", ""Pure"")
        with open(invalid_path, ""wb"") as f:
            pickle.dump(""not a cache"", f)
        
        with pytest.raises(LoaderError, match=""Excepted cache object""):
            loader.load_persistent_cache(""invalid"", ""Pure"")
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader,1,7
survived,"    def test_cache_attempt_miss(self) -> None:
        """"""Test cache attempt with a miss.""""""
        loader = MockLoader(""test"")
        defs = {""var1""}
        stateful_refs: Set[str] = set()
        
        cache = loader.cache_attempt(defs, ""hash1"", stateful_refs, ""Pure"")
        
        assert cache.hash == ""hash1""
        assert cache.hit is False
        assert cache.cache_type == ""Pure""
        assert set(cache.defs.keys()) == defs
        assert all(value is None for value in cache.defs.values())
",tests/_save/loaders/test_loader.py,TestLoader,1,7
survived,"    def load_cache(self, hashed_context: str, cache_type: str) -> Cache:
        key = f""{cache_type}_{hashed_context}""
        if key not in self.saved_caches:
            raise LoaderError(""Unexpected cache miss."")
        return self.saved_caches[key]
",tests/_save/loaders/test_loader.py,MockLoader,1,7
survived,"    def teardown_method(self) -> None:
        """"""Clean up the temporary directory.""""""
        import shutil
        shutil.rmtree(self.temp_dir)
",tests/_save/loaders/test_loader.py,TestBasePersistenceLoader,1,7
survived,"    def validate_metadata(cls, v):
        """"""Validate that metadata is a dictionary with string keys and valid values.""""""
        if not isinstance(v, dict):
            raise ValueError(""Metadata must be a dictionary"")
        
        # Validate that all keys are strings
        for key, value in v.items():
            if not isinstance(key, str):
                raise ValueError(f""Metadata keys must be strings, got {type(key)}"")
            
            # Validate nested dictionaries (prevent deeply nested structures)
            if isinstance(value, dict):
                # Check for nested dictionaries (limit depth to 1)
                for nested_key, nested_value in value.items():
                    if not isinstance(nested_key, str):
                        raise ValueError(f""Nested metadata keys must be strings, got {type(nested_key)}"")
                    if isinstance(nested_value, dict):
                        raise ValueError(""Metadata can only be nested one level deep"")
        
        # Check for maximum metadata size (prevent DoS)
        if len(str(v)) > 10000:  # Limit metadata size to 10KB
            raise ValueError(""Metadata size exceeds maximum allowed (10KB)"")
            
        return v
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def validate_metadata(cls, v):
        """"""Validate that metadata is a dictionary.""""""
        if not isinstance(v, dict):
            raise ValueError(""Metadata must be a dictionary"")
        return v
",src/crewai/security/fingerprint.py,Fingerprint,1,7
deleted,"def test_crew_train_with_memory():
    """"""Test that training a crew with memory enabled does not raise validation errors.""""""
    agent = Agent(role=""Test Agent"", goal=""Test Goal"", backstory=""Test Backstory"")
    task = Task(description=""Test Task"", expected_output=""Test Output"", agent=agent)
    crew = Crew(agents=[agent], tasks=[task], memory=True)

    with tempfile.TemporaryDirectory() as tmpdir:
        filename = os.path.join(tmpdir, ""training_data.pkl"")
        try:
            crew.train(n_iterations=1, filename=filename)
        except pydantic_core.ValidationError as e:
             if ""Input should be an instance of"" in str(e) and (""Memory"" in str(e)):
                  pytest.fail(f""Training with memory raised Pydantic ValidationError, likely due to incorrect memory copy: {e}"")
             else:
                  raise e
        except Exception as e:
            print(f""Warning: Training raised an unexpected exception: {e}"")",tests/crew_test.py,,1,7
survived,"def create_parameters_file(goat_plugins_dir: Path, plugin_name: str) -> None:
    """"""Create the parameters.py file with example parameters in the goat_plugins directory.""""""
    parameters_content = '''from pydantic import BaseModel, Field
from typing import Optional


class ExampleQueryParameters(BaseModel):
    query: str = Field(
        description=""An example query parameter (e.g., 'search term', 'identifier')""
    )
    limit: Optional[int] = Field(
        None,
        description=""Maximum number of results to return. Defaults to no limit."",
    )
    include_metadata: bool = Field(
        default=False,
        description=""Include additional metadata in the response""
    )


class ExampleActionParameters(BaseModel):
    target_id: str = Field(
        description=""The ID of the target to perform action on""
    )
    action_type: str = Field(
        description=""The type of action to perform (e.g., 'create', 'update', 'delete')""
    )
    parameters: Optional[dict] = Field(
        None,
        description=""Optional parameters for the action""
    )
'''
    
    with open(goat_plugins_dir / ""parameters.py"", ""w"") as f:
        f.write(parameters_content)
",python/create_plugin.py,,1,7
survived,"def main():
    """"""Main function to create a new GOAT plugin.""""""
    parser = argparse.ArgumentParser(description=""Create a new GOAT plugin."")
    parser.add_argument(""name"", help=""Plugin name, e.g. 'myplugin'"")
    parser.add_argument(""--evm"", action=""store_true"", help=""Indicate if plugin is for EVM"")
    args = parser.parse_args()
    
    plugin_name = args.name.lower()  # Normalize to lowercase
    is_evm = args.evm
    
    # Create base plugin directory
    plugin_dir = Path(""src/plugins"") / plugin_name
    plugin_dir.mkdir(parents=True, exist_ok=True)
    
    # Create all required files
    create_project_toml(plugin_dir, plugin_name, is_evm)
    
    # Create goat_plugins directory and its contents
    goat_plugins_dir = plugin_dir / ""goat_plugins"" / plugin_name
    goat_plugins_dir.mkdir(parents=True, exist_ok=True)
    
    # Create all plugin files
    create_parameters_file(goat_plugins_dir, plugin_name)
    create_service_file(goat_plugins_dir, plugin_name, is_evm)
    create_init_file(goat_plugins_dir, plugin_name, is_evm)
    
    # Create README.md
    readme_content = f""""""# {plugin_name} Plugin for GOAT SDK

A plugin for the GOAT SDK that provides {plugin_name} functionality.

## Installation

```bash
poetry add goat-sdk-plugin-{plugin_name}
```

## Usage

```python
from goat_plugins.{plugin_name} import {plugin_name}, {plugin_name.title()}PluginOptions

# Initialize the plugin
options = {plugin_name.title()}PluginOptions(
    api_key=""your-api-key""
)
plugin = {plugin_name}(options)
```

## Features

- Example query functionality
- Example action functionality
- {'EVM chain support' if is_evm else 'Chain-agnostic support'}

## License

This project is licensed under the terms of the MIT license.
""""""
    with open(plugin_dir / ""README.md"", ""w"") as f:
        f.write(readme_content)
    
    print(f""Plugin '{plugin_name}' created successfully in {plugin_dir}"")
",python/create_plugin.py,,1,7
survived,"    def get_chain(self) -> Chain:
        """"""Get the chain type for Solana.""""""
        return {""type"": ""solana""}
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaWalletClient,1,7
survived,"    def __init__(self, client: SolanaClient, keypair: Keypair):
        """"""Initialize the Solana keypair wallet client.

        Args:
            client: A Solana RPC client instance
            keypair: A Solana keypair for signing transactions
        """"""
        super().__init__(client)
        self.keypair = keypair
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaKeypairWalletClient,1,8
survived,"    def from_dict(cls, data: Dict[str, Any]) -> 'SecurityConfig':
        """"""
        Create a SecurityConfig from a dictionary.

        Args:
            data (Dict[str, Any]): Dictionary representation of a security config

        Returns:
            SecurityConfig: A new SecurityConfig instance
        """"""
        # Make a copy to avoid modifying the original
        data_copy = data.copy()

        fingerprint_data = data_copy.pop(""fingerprint"", None)
        fingerprint = Fingerprint.from_dict(fingerprint_data) if fingerprint_data else Fingerprint()

        return cls(fingerprint=fingerprint)",src/crewai/security/security_config.py,SecurityConfig,1,7
survived,"    def __hash__(self) -> int:
        """"""Hash of the fingerprint (based on UUID).""""""
        return hash(self.uuid_str)
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def _generate_uuid(cls, seed: str) -> str:
        """"""
        Generate a deterministic UUID based on a seed string.

        Args:
            seed (str): The seed string to use for UUID generation

        Returns:
            str: A string representation of the UUID consistently generated from the seed
        """"""
        if not isinstance(seed, str):
            raise ValueError(""Seed must be a string"")
        
        if not seed.strip():
            raise ValueError(""Seed cannot be empty or whitespace"")
            
        # Create a deterministic UUID using v5 (SHA-1)
        # Custom namespace for CrewAI to enhance security

        # Using a unique namespace specific to CrewAI to reduce collision risks
        CREW_AI_NAMESPACE = uuid.UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479')
        return str(uuid.uuid5(CREW_AI_NAMESPACE, seed))
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def __eq__(self, other) -> bool:
        """"""Compare fingerprints by their UUID.""""""
        if isinstance(other, Fingerprint):
            return self.uuid_str == other.uuid_str
        return False
",src/crewai/security/fingerprint.py,Fingerprint,1,7
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the fingerprint to a dictionary representation.

        Returns:
            Dict[str, Any]: Dictionary representation of the fingerprint
        """"""
        return {
            ""uuid_str"": self.uuid_str,
            ""created_at"": self.created_at.isoformat(),
            ""metadata"": self.metadata
        }
",src/crewai/security/fingerprint.py,Fingerprint,1,8
deleted,"    def test_sanitize_collection_name_short_name(self):
        """"""Test sanitizing a very short name.""""""
        short_name = ""A""
        sanitized = sanitize_collection_name(short_name)
        self.assertGreaterEqual(len(sanitized), 3)
        self.assertTrue(sanitized[0].isalnum())
        self.assertTrue(sanitized[-1].isalnum())
",tests/utilities/test_string_utils.py,TestStringUtils,1,7
survived,"def _extract_code_from_search_results(tool: ToolDefinition, search_results: str) -> str:
    """"""
    Extract functional code from web search results.
    
    Args:
        tool: The tool definition
        search_results: Search results from web_search
        
    Returns:
        Functional implementation code
    """"""
    implementation_lines = []
    
    if ""weather"" in tool.name.lower() or ""weather"" in tool.description.lower():
        implementation_lines.append(""    import requests"")
    elif ""file"" in tool.name.lower() or ""file"" in tool.description.lower():
        implementation_lines.append(""    import os"")
    elif ""json"" in tool.name.lower() or ""json"" in tool.description.lower():
        implementation_lines.append(""    import json"")
    
    implementation_lines.append(""    try:"")
    
    implementation_lines.append(f""        # Implementation based on web search results"")
    implementation_lines.append(f""        result = f\""Processing {tool.name} with parameters: {{{', '.join([p.name + '=' + p.name for p in tool.parameters])}}}\""\n"")
    implementation_lines.append(f""        return result"")
    implementation_lines.append(""    except Exception as e:"")
    implementation_lines.append(""        return f\""Error in {tool.name}: {str(e)}\"""")
    
    return ""\n"".join(implementation_lines)
",meta_agent/generators/tool_generator.py,,0,7
survived,"    def test_is_file_path_with_directory(self, tmp_path: Path) -> None:
        # Test with directory instead of file
        with pytest.raises(click.BadParameter) as excinfo:
            is_file_path(None, None, str(tmp_path))
        assert f""Not a file: {tmp_path}"" in str(excinfo.value)",tests/_cli/test_cli_validators.py,TestIsFilePath,1,6
deleted,"    def test_marimo_strict_execution_error(self) -> None:
        error = MarimoStrictExecutionError(
            msg=""Strict execution error"",
            ref=""some_reference"",
            blamed_cell=""cell1"",
        )

        # Test properties
        assert error.type == ""strict-exception""
        assert error.describe() == ""Strict execution error""
        assert error.ref == ""some_reference""
        assert error.blamed_cell == ""cell1""
",tests/_messaging/test_errors.py,TestErrorClasses,1,7
deleted,"    def test_marimo_internal_error(self) -> None:
        error = MarimoInternalError(
            error_id=""test-error-id"",
            msg=""Original error message"",
        )

        # Test properties
        assert error.type == ""internal""
        assert ""An internal error occurred"" in error.describe()
        assert ""test-error-id"" in error.describe()
        # The original message should be replaced with a generic one
        assert error.describe() != ""Original error message""
",tests/_messaging/test_errors.py,TestErrorClasses,1,7
deleted,"    def test_nested_http_request_contexts(self) -> None:
        # Test nested http_request_contexts
        request1 = HTTPRequest(
            url={""path"": ""/test1"", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            base_url={""path"": """", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            headers={},
            query_params={},
            path_params={},
            cookies={},
            meta={},
            user=None,
        )

        request2 = HTTPRequest(
            url={""path"": ""/test2"", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            base_url={""path"": """", ""port"": None, ""scheme"": ""http"", ""netloc"": ""localhost"", ""query"": """", ""hostname"": ""localhost""},
            headers={},
            query_params={},
            path_params={},
            cookies={},
            meta={},
            user=None,
        )

        with http_request_context(request1):
            # Outer context should have request1
            assert HTTP_REQUEST_CTX.get() is request1

            with http_request_context(request2):
                # Inner context should have request2
                assert HTTP_REQUEST_CTX.get() is request2

            # Should restore the outer context
            assert HTTP_REQUEST_CTX.get() is request1

        # Request should be unset outside the context
        with pytest.raises(LookupError):
            HTTP_REQUEST_CTX.get()",tests/_messaging/test_context.py,TestHTTPRequestContext,1,7
survived,"    def write(self, op: str, data: dict) -> None:
        self.messages.append((op, data))
",tests/_messaging/test_print_override.py,MockStream,1,6
survived,"        def accepts_mime_type(mime_type: KnownMimeType) -> KnownMimeType:
            return mime_type
",tests/_messaging/test_mimetypes.py,TestMimeTypes,1,6
deleted,"    def test_marimo_interruption_error(self) -> None:
        error = MarimoInterruptionError()

        # Test properties
        assert error.type == ""interruption""
        assert ""interrupted"" in error.describe().lower()
        assert ""re-run"" in error.describe().lower()
",tests/_messaging/test_errors.py,TestErrorClasses,1,7
survived,"    def test_print_override_with_thread_no_execution_context(self) -> None:
        # Test print_override when in a marimo thread with context but no execution context
        thread_id = threading.get_ident()
        THREADS.add(thread_id)

        try:
            # Create a mock context with no execution context
            context = MagicMock(spec=RuntimeContext)
            context.execution_context = None

            with patch(""marimo._messaging.print_override._original_print"") as mock_print:
                with patch(
                    ""marimo._messaging.print_override.get_context"",
                    return_value=context,
                ):
                    print_override(""Hello, world!"")

                    # Original print should be called
                    mock_print.assert_called_once_with(""Hello, world!"")
        finally:
            # Clean up
            if thread_id in THREADS:
                THREADS.remove(thread_id)
",tests/_messaging/test_print_override.py,TestPrintOverride,1,7
survived,"    def test_add_output_to_buffer_new_cell(self) -> None:
        # Test adding output for a new cell
        outputs_buffered_per_cell = {}
        msg = ConsoleMsg(
            stream=CellChannel.STDOUT,
            cell_id=""cell1"",
            data=""Hello"",
            mimetype=""text/plain"",
        )

        _add_output_to_buffer(msg, outputs_buffered_per_cell)

        assert ""cell1"" in outputs_buffered_per_cell
        assert len(outputs_buffered_per_cell[""cell1""]) == 1
        assert outputs_buffered_per_cell[""cell1""][0].data == ""Hello""
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker,1,7
survived,"    def test_known_mime_type(self) -> None:
        # Test that KnownMimeType can be used as a type annotation
        def accepts_mime_type(mime_type: KnownMimeType) -> KnownMimeType:
            return mime_type

        # Test with various known mime types
        assert accepts_mime_type(""text/plain"") == ""text/plain""
        assert accepts_mime_type(""text/html"") == ""text/html""
        assert accepts_mime_type(""application/json"") == ""application/json""
        assert accepts_mime_type(""image/png"") == ""image/png""

        # The following would fail type checking but not at runtime
        # This test is just to verify the runtime behavior
        assert accepts_mime_type(cast(KnownMimeType, ""unknown/type"")) == ""unknown/type""
",tests/_messaging/test_mimetypes.py,TestMimeTypes,1,7
survived,"    def test_can_merge_outputs(self) -> None:
        # Same stream and mimetype should be mergeable
        msg1 = ConsoleMsg(
            stream=CellChannel.STDOUT,
            cell_id=""cell1"",
            data=""Hello"",
            mimetype=""text/plain"",
        )
        msg2 = ConsoleMsg(
            stream=CellChannel.STDOUT,
            cell_id=""cell1"",
            data="" World"",
            mimetype=""text/plain"",
        )
        assert _can_merge_outputs(msg1, msg2) is True

        # Different stream should not be mergeable
        msg3 = ConsoleMsg(
            stream=CellChannel.STDERR,
            cell_id=""cell1"",
            data=""Error"",
            mimetype=""text/plain"",
        )
        assert _can_merge_outputs(msg1, msg3) is False

        # Different mimetype should not be mergeable
        msg4 = ConsoleMsg(
            stream=CellChannel.STDOUT,
            cell_id=""cell1"",
            data=""<h1>Hello</h1>"",
            mimetype=""text/html"",
        )
        assert _can_merge_outputs(msg1, msg4) is False
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker,1,7
survived,"    def supports_chain(self, chain) -> bool:
        """"""Check if the chain is supported by Uniswap.
        
        Currently supports: Mainnet, Polygon, Avalanche, Base, Optimism, Zora, Arbitrum, Celo
        """"""
        if chain['type'] != 'evm':
            return False
            
        # List of supported chain IDs from uniswap.plugin.ts
        SUPPORTED_CHAIN_IDS = [1, 137, 43114, 8453, 10, 7777777, 42161, 42220]  # Mainnet, Polygon, Avalanche, Base, Optimism, Zora, Arbitrum, Celo
        return chain['id'] in SUPPORTED_CHAIN_IDS
",python/src/plugins/uniswap/goat_plugins/uniswap/__init__.py,UniswapPlugin,1,7
survived,"def test_smart_wallet_with_admin_signer(smart_api, test_keypair):
    """"""Test smart wallet creation with admin signer.""""""
    wallet = smart_api.create_smart_wallet({
        ""adminSigner"": {
            ""type"": ""evm-keypair"",
            ""address"": test_keypair[""address""]
        }
    })
    assert wallet[""address""].startswith(""0x"")
    assert wallet[""type""] == ""evm-smart-wallet""
",python/src/wallets/crossmint/tests/test_smart_wallet.py,,1,7
survived,"def test_smart_wallet_invalid_options(smart_api, invalid_options, test_wallet_options, test_keypair):
    """"""Test error handling with invalid options.""""""
    wallet = smart_api.create_smart_wallet()
    options = {**test_wallet_options, **invalid_options}
    
    with pytest.raises(Exception) as exc:
        SmartWalletClient(
            wallet[""address""],
            smart_api,
            options[""chain""],
            test_keypair,
            options[""provider""],
            options[""options""][""ensProvider""]
        )
    assert ""error"" in str(exc.value).lower() or ""invalid"" in str(exc.value).lower()",python/src/wallets/crossmint/tests/test_smart_wallet.py,,1,6
survived,"def test_smart_wallet_read_contract(smart_api, test_wallet_options, test_keypair):
    """"""Test reading from a smart contract.""""""
    # Create wallet and client
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        test_wallet_options[""chain""],
        test_keypair,
        test_wallet_options[""provider""],
        test_wallet_options[""options""][""ensProvider""]
    )
    
    # Example ERC20 balanceOf ABI
    abi = [{
        ""constant"": True,
        ""inputs"": [{""name"": ""_owner"", ""type"": ""address""}],
        ""name"": ""balanceOf"",
        ""outputs"": [{""name"": ""balance"", ""type"": ""uint256""}],
        ""type"": ""function""
    }]
    
    # Read contract
    try:
        result = client.read({
            ""address"": ""0x742d35Cc6634C0532925a3b844Bc454e4438f44e"",
            ""abi"": abi,
            ""functionName"": ""balanceOf"",
            ""args"": [wallet[""address""]]
        })
        assert ""value"" in result
    except Exception as e:
        # Contract might not exist on testnet, that's ok
        assert ""revert"" in str(e).lower() or ""not found"" in str(e).lower()
",python/src/wallets/crossmint/tests/test_smart_wallet.py,,1,6
survived,"    def read_records(
        self,
        sync_mode: SyncMode,
        cursor_field: Optional[List[str]] = None,
        stream_slice: Optional[Mapping[str, Any]] = None,
        stream_state: Optional[Mapping[str, Any]] = None,
    ) -> Iterable[StreamData]:
        logger.info(
            f""Extracting Struvctured AI {self.fields_json_str} for all files in folder {self.folder_id} {'recursively' if self.is_recursive else ''}""
        )
        items = box_folder_ai_extract_structured(
            self.client, self.folder_id, fields_json_str=self.fields_json_str, is_recursive=self.is_recursive
        )
        for item in items:
            airbyte_item: StreamData = item.file.to_dict()
            airbyte_item[""text_representation""] = item.text_representation
            logger.info(f""Reading file {item.file.id} - {item.file.name}"")
            yield airbyte_item",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder,1,7
survived,"    def read_records(
        self,
        sync_mode: SyncMode,
        cursor_field: Optional[List[str]] = None,
        stream_slice: Optional[Mapping[str, Any]] = None,
        stream_state: Optional[Mapping[str, Any]] = None,
    ) -> Iterable[StreamData]:
        logger.info(f""Extracting AI {self.prompt} for all files in folder {self.folder_id} {'recursively' if self.is_recursive else ''}"")
        items = box_folder_ai_extract(self.client, self.folder_id, prompt=self.prompt, is_recursive=self.is_recursive)
        for item in items:
            airbyte_item: StreamData = item.file.to_dict()
            airbyte_item[""text_representation""] = item.text_representation
            logger.info(f""Reading file {item.file.id} - {item.file.name}"")
            yield airbyte_item
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractFolder,1,7
survived,"    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:
        """"""
        :return: string if single primary key, list of strings if composite primary key, list of list of strings if composite primary key consisting of nested fields.
          If the stream has no primary keys, return None.
        """"""
        return ""id""
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamTextRepresentationFolder,1,7
survived,"    def streams(self, config: Mapping[str, Any]) -> List[Stream]:
        """"""
        :param config: A Mapping of the user input configuration as defined in the connector spec.
        """"""
        box_client = get_box_ccg_client(config)
        box_folder_text_representation_stream = StreamTextRepresentationFolder(
            box_client, config[""box_folder_id""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_ask_ai_stream = StreamAIAskFolder(
            box_client, config[""box_folder_id""], config[""ask_ai_prompt""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_extract_ai_stream = StreamAIExtractFolder(
            box_client, config[""box_folder_id""], config[""extract_ai_prompt""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_extract_structured_ai_stream = StreamAIExtractStructuredFolder(
            client=box_client,
            folder_id=config[""box_folder_id""],
            fields_json_str=config[""extract_structured_ai_fields""],
            is_recursive=config.get(""is_recursive"", False),
        )

        return [
            box_folder_text_representation_stream,
            box_folder_ask_ai_stream,
            box_folder_extract_ai_stream,
            box_folder_extract_structured_ai_stream,
        ]
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,SourceBoxDataExtract,1,7
survived,"def test_streams(mocker):
    source = SourceBoxDataExtract()
    config_mock = MagicMock()
    streams = source.streams(config_mock)
    expected_streams_number = 4
    assert len(streams) == expected_streams_number",airbyte-integrations/connectors/source-box-data-extract/unit_tests/test_source.py,,1,6
survived,"    def read_records(
        self,
        sync_mode: SyncMode,
        cursor_field: Optional[List[str]] = None,
        stream_slice: Optional[Mapping[str, Any]] = None,
        stream_state: Optional[Mapping[str, Any]] = None,
    ) -> Iterable[StreamData]:
        logger.info(f""Extracting text representation for files in folder {self.folder_id} {'recursively' if self.is_recursive else ''}"")
        items = box_folder_text_representation(self.client, self.folder_id, is_recursive=self.is_recursive)
        for item in items:
            airbyte_item: StreamData = item.file.to_dict()
            airbyte_item[""text_representation""] = item.text_representation
            logger.info(f""Reading file {item.file.id} - {item.file.name}"")
            yield airbyte_item
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamTextRepresentationFolder,1,7
survived,"def test_create_specialist_agents():
    """"""Test that specialist agents are created with the correct configuration.""""""
    billing_agent = create_billing_agent()
    technical_agent = create_technical_agent()
    account_agent = create_account_agent()
    
    assert billing_agent.name == ""BillingSpecialist""
    assert technical_agent.name == ""TechnicalSupport""
    assert account_agent.name == ""AccountManager""
    
    assert ""billing specialist"" in billing_agent.instructions.lower()
    assert ""technical support"" in technical_agent.instructions.lower()
    assert ""account management"" in account_agent.instructions.lower()
",openai-agents-examples/07_agent_with_handoffs.py,,1,7
survived,"async def run_anthropic_agent(prompt: str) -> str:
    """"""
    Run the Anthropic agent with the given prompt.
    
    Args:
        prompt: The user's query or prompt
        
    Returns:
        The agent's response as a string
    """"""
    # Create the Anthropic agent
    agent = create_anthropic_agent()
    
    # Run the agent with the prompt
    result = await Runner.run(agent, prompt)
    
    # Return the response
    return result.final_output
",openai-agents-examples/12_anthropic_agent.py,,1,7
survived,"def create_blog_writer_agent(research_agent: Agent) -> Agent:
    """"""
    Create a blog writer agent that can use a research agent as a tool.
    
    Args:
        research_agent: The research agent to use as a tool
        
    Returns:
        An Agent instance specialized in blog writing with research capabilities
    """"""
    instructions = """"""
    You are a professional blog writer who creates engaging, informative content.
    Your writing should be clear, conversational, and tailored to a general audience.
    Structure your blog posts with an introduction, body paragraphs, and conclusion.
    Use the research tool available to you to gather accurate information on topics.
    Incorporate the research seamlessly into your writing while maintaining your voice.
    """"""
    
    # Convert the research agent into a tool
    research_tool = research_agent.as_tool(
        tool_name=""research_topic"",
        tool_description=""Research a specific topic to gather accurate information. Provide a clear, specific topic or question to research.""
    )
    
    return Agent(
        name=""BlogWriter"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[research_tool]
    )
",openai-agents-examples/08_agent_with_agent_as_tool.py,,1,7
survived,"def generate_blog_outline(topic: str, research: str) -> str:
    """"""
    Generate an outline for a blog post based on research.
    
    Args:
        topic: The blog topic
        research: The research information to incorporate
        
    Returns:
        A string containing a structured blog outline
    """"""
    # This is a simplified implementation - in a real application, this would use more sophisticated logic
    # Extract key points from research
    research_lines = research.strip().split('\n')
    key_points = [line.strip() for line in research_lines if line.strip() and not line.strip().startswith('#')]
    
    # Create a basic outline structure
    outline = f""""""
        # Blog Outline: {topic}
        
        ## Introduction
        - Hook: Engaging opening to capture reader interest
        - Context: Brief background on {topic}
        - Thesis: Main point or argument of the blog post
        
        ## Main Section 1: Overview and Background
        - Historical context
        - Current relevance
        - Key concepts and definitions
        
        ## Main Section 2: Key Aspects and Analysis
    """"""
    
    # Add research points to the outline
    for i, point in enumerate(key_points[:5]):
        if len(point) > 100:  # Only use shorter points
            continue
        outline += f""\n        - Point {i+1}: {point}""
    
    # Complete the outline
    outline += f""""""
        
        ## Main Section 3: Implications and Applications
        - Practical applications
        - Future developments
        - Challenges and opportunities
        
        ## Conclusion
        - Summary of key points
        - Final thoughts
        - Call to action or next steps
    """"""
    
    return outline.strip()
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"def create_research_agent() -> Agent:
    """"""
    Create a research agent that gathers and analyzes information.
    
    Returns:
        An Agent instance specialized in research.
    """"""
    instructions = """"""
    You are a research specialist who excels at gathering and analyzing information on various topics.
    Your task is to:
    1. Understand the research request
    2. Use the search_for_information tool to gather relevant information
    3. Use the analyze_topic tool to identify key aspects for research
    4. Synthesize the information into a comprehensive, well-organized research report
    5. Include relevant facts, statistics, and context
    6. Ensure the research is accurate, balanced, and thorough
    
    Your research should be detailed enough to serve as the foundation for content creation.
    """"""
    
    # Create the research agent with function tools
    return Agent(
        name=""ResearchSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[search_for_information, analyze_topic],
        handoff_description=""Use this agent to conduct thorough research on a topic.""
    )
",openai-agents-examples/13_research_blog_system.py,,1,7
survived,"def test_run_multi_agent_system():
    """"""Test that the multi-agent system can run and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a simple test query that should go to the tech specialist
    response = asyncio.run(run_multi_agent_system(""What is machine learning?""))
    
    # Verify we got a non-empty response
    assert response
    assert len(response) > 0
",openai-agents-examples/02_multi_agent.py,,0,7
survived,"def test_run_customer_support_system():
    """"""Test that the customer support system can run and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a test query that should go to the billing specialist
    response = asyncio.run(run_customer_support_system(""I have a question about my recent invoice""))
    
    # Verify we got a non-empty response
    assert response
    assert len(response) > 0
",openai-agents-examples/07_agent_with_handoffs.py,,1,6
survived,"async def run_traced_agent(prompt: str, tracer) -> str:
    """"""
    Run an agent with tracing for the given prompt.
    
    Args:
        prompt: The user's query or prompt
        tracer: The OpenTelemetry tracer to use
        
    Returns:
        The agent's response as a string
    """"""
    # Create a span for the entire agent execution
    with tracer.start_as_current_span(""agent_execution"") as span:
        # Add attributes to the span
        span.set_attribute(""prompt"", prompt)
        
        # Create the agent
        with tracer.start_as_current_span(""create_agent""):
            agent = create_geography_agent()
            span.set_attribute(""agent_name"", agent.name)
        
        # Run the agent with the prompt
        with tracer.start_as_current_span(""Runner.run""):
            result = await Runner.run(agent, prompt)
            # Note: In the current version, RunResult doesn't have usage attribute
            # We'll just record the response length as a basic metric
            span.set_attribute(""response_length"", len(result.final_output))
            span.set_attribute(""response_first_chars"", result.final_output[:30])
        
        # Return the response
        return result.final_output
",openai-agents-examples/04_agent_with_tracing.py,,1,7
survived,"def create_triage_agent(specialists: List[Agent]) -> Agent:
    """"""
    Create a triage agent that can delegate to specialist agents.
    
    Args:
        specialists: List of specialist agents to which tasks can be delegated
        
    Returns:
        An Agent instance that triages customer inquiries
    """"""
    instructions = """"""
    You are a customer support triage agent. Your job is to:
    1. Understand the customer's issue
    2. Determine which specialist would be best suited to help
    3. Hand off the conversation to that specialist
    
    Be polite and professional. If you're unsure which specialist to choose, ask clarifying questions.
    """"""
    
    # Create handoffs to specialist agents
    handoffs = [handoff(agent) for agent in specialists]
    
    return Agent(
        name=""TriageAgent"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoffs=handoffs
    )
",openai-agents-examples/07_agent_with_handoffs.py,,1,7
survived,"async def run_blog_writer_system(prompt: str) -> str:
    """"""
    Run the blog writer system with the given prompt.
    
    Args:
        prompt: The topic or request for a blog post
        
    Returns:
        The blog post content
    """"""
    # Create the research agent
    research_agent = create_research_agent()
    
    # Create the blog writer agent with the research agent as a tool
    blog_writer = create_blog_writer_agent(research_agent)
    
    # Run the blog writer agent with the prompt
    result = await Runner.run(blog_writer, prompt)
    
    # Return the blog post
    return result.final_output
",openai-agents-examples/08_agent_with_agent_as_tool.py,,1,7
survived,"def create_research_agent() -> Agent:
    """"""
    Create a research agent that can gather information on topics.
    
    Returns:
        An Agent instance specialized in research.
    """"""
    instructions = """"""
    You are a research specialist who excels at gathering accurate information on various topics.
    Your responses should be factual, well-organized, and comprehensive.
    Include relevant details, statistics, and context when available.
    Always cite your sources if you're providing specific facts or quotes.
    Focus on providing high-quality, reliable information that would be useful for content creation.
    """"""
    
    return Agent(
        name=""ResearchSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
    )
",openai-agents-examples/08_agent_with_agent_as_tool.py,,1,7
survived,"    def get_rejection_message(self, input_str: str) -> str:
        """"""
        Get a message explaining why the input was rejected.
        
        Args:
            input_str: The rejected input string
            
        Returns:
            A message explaining the rejection
        """"""
        if len(input_str) < self.min_length:
            return f""Your input is too short. Please provide at least {self.min_length} characters.""
        
        if len(input_str) > self.max_length:
            return f""Your input is too long. Please limit your request to {self.max_length} characters.""
        
        return ""Your input does not meet the format requirements.""
",openai-agents-examples/10_agent_with_guardrails.py,FormatValidationGuardrail,1,7
survived,"def test_anthropic_message_formatting():
    # Test when first message is system
    llm = LLM(model=""anthropic/claude-3-sonnet"")
    messages = [{""role"": ""system"", ""content"": ""test""}]
    formatted = llm._format_messages_for_provider(messages)
    assert len(formatted) == 2
    assert formatted[0][""role""] == ""user""
    assert formatted[0][""content""] == "".""
    assert formatted[1][""role""] == ""system""
    assert formatted[1][""content""] == ""test""

    # Test when first message is already user
    messages = [{""role"": ""user"", ""content"": ""test""}]
    formatted = llm._format_messages_for_provider(messages)
    assert len(formatted) == 1
    assert formatted[0][""role""] == ""user""
    assert formatted[0][""content""] == ""test""

    # Test with empty message list
    messages = []
    formatted = llm._format_messages_for_provider(messages)
    assert len(formatted) == 1
    assert formatted[0][""role""] == ""user""
    assert formatted[0][""content""] == "".""

    # Test with non-Anthropic model (should not modify messages)
    llm = LLM(model=""gpt-4"")
    messages = [{""role"": ""system"", ""content"": ""test""}]
    formatted = llm._format_messages_for_provider(messages)
    assert len(formatted) == 1
    assert formatted[0][""role""] == ""system""
    assert formatted[0][""content""] == ""test""
",tests/llm_test.py,,1,7
survived,"async def test_get_files_endpoint():
    payload = {
        ""iat"": datetime.datetime.utcnow(),
        ""exp"": datetime.datetime.utcnow() + datetime.timedelta(days=1),
        ""subdomain"": ""abcd1234"",
    }
    token = jwt.encode(payload, ""test-secret"", algorithm=""HS256"")
    
    file_data = {
        ""index.html"": {
            ""raw"": ""SGVsbG8gV29ybGQ="",  # base64 encoded ""Hello World""
            ""headers"": [
                {""header"": ""Content-Type"", ""value"": ""text/plain""},
                {""header"": ""X-Custom"", ""value"": ""test""},
            ],
            ""status_code"": 200,
        },
        ""test.js"": {
            ""raw"": ""Y29uc29sZS5sb2coJ0hlbGxvJyk7"",  # base64 encoded ""console.log('Hello');""
            ""headers"": [
                {""header"": ""Content-Type"", ""value"": ""application/javascript""},
            ],
            ""status_code"": 200,
        }
    }
    
    mock_redis.get.return_value = json.dumps(file_data)
    
    with patch(""backend.app.config.jwt_secret"", ""test-secret""):
        response = client.get(""/api/files"", params={""token"": token})
        
        assert response.status_code == 200
        assert response.json() == file_data
        
        mock_redis.get.assert_called_with(""files:abcd1234"")
",backend/tests/test_endpoints.py,,1,7
survived,"    def get_debug_messages(self, session_type: str) -> list[dict]:
        """"""èŽ·å–è°ƒè¯•æ¶ˆæ¯åŽ†å²""""""
        session_key = f'webchat{session_type}'
        return self.debug_messages.get(session_key, [])
",pkg/platform/sources/webchat.py,WebChatAdapter,1,7
survived,"    def hide_results(self):
        """"""Hide search results.""""""
        self.show_results = False
",pcweb/components/docpage/navbar/typesense.py,TypesenseSearchState,1,6
survived,"    def __init__(self):
        self.md = Markdown(extensions=['meta', 'toc'])
",scripts/typesense_indexer.py,MarkdownProcessor,1,7
survived,"def typesense_search() -> rx.Component:
    """"""Create the Typesense search component.""""""
    return rx.box(
        rx.input(
            placeholder=""Search docs..."",
            value=TypesenseSearchState.search_query,
            on_change=TypesenseSearchState.search_docs,
            on_blur=TypesenseSearchState.hide_results,
            style={
                ""display"": ""flex"",
                ""max_height"": ""32px"",
                ""min_height"": ""32px"",
                ""padding"": ""6px 12px"",
                ""min_width"": ""256px"",
                ""border_radius"": ""10px"",
                ""border"": ""1px solid var(--c-slate-5, #E0E1E6)"",
                ""background"": ""var(--c-slate-1)"",
                ""font_family"": ""Instrument Sans"",
                ""font_size"": ""14px"",
                ""font_weight"": ""500"",
                ""line_height"": ""20px"",
                ""letter_spacing"": ""-0.0125em"",
                ""color"": ""var(--c-slate-9, #8B8D98)"",
                ""box_shadow"": ""0px 24px 12px 0px rgba(28, 32, 36, 0.02), 0px 8px 8px 0px rgba(28, 32, 36, 0.02), 0px 2px 6px 0px rgba(28, 32, 36, 0.02)"",
                ""transition"": ""background-color 0.1s linear"",
                ""outline"": ""none""
            },
            _hover={""background_color"": ""var(--c-slate-3, #F0F0F3)""},
            _focus={""border_color"": ""var(--c-violet-7)""}
        ),
        rx.cond(
            TypesenseSearchState.show_results & (TypesenseSearchState.search_results.length() > 0),
            rx.box(
                rx.foreach(
                    TypesenseSearchState.search_results,
                    search_result_item
                ),
                position=""absolute"",
                top=""100%"",
                left=""0"",
                right=""0"",
                background=""var(--c-slate-1)"",
                border=""1px solid var(--c-slate-5)"",
                border_radius=""10px"",
                box_shadow=""0px 24px 12px 0px rgba(28, 32, 36, 0.02), 0px 8px 8px 0px rgba(28, 32, 36, 0.02), 0px 2px 6px 0px rgba(28, 32, 36, 0.02)"",
                max_height=""400px"",
                overflow_y=""auto"",
                z_index=""1000"",
                margin_top=""4px""
            )
        ),
        rx.cond(
            TypesenseSearchState.is_searching,
            rx.box(
                rx.text(
                    ""Searching..."",
                    color=""var(--c-slate-9)"",
                    font_size=""12px""
                ),
                position=""absolute"",
                top=""100%"",
                left=""0"",
                right=""0"",
                background=""var(--c-slate-1)"",
                border=""1px solid var(--c-slate-5)"",
                border_radius=""10px"",
                padding=""12px"",
                margin_top=""4px"",
                z_index=""1000""
            )
        ),
        position=""relative"",
        class_name=""search-container""
    )
",pcweb/components/docpage/navbar/typesense.py,,1,7
survived,"def test_result_as_answer_in_tool_decorator():
    @tool(""Tool with result as answer"", result_as_answer=True)
    def my_tool_with_result_as_answer(question: str) -> str:
        """"""This tool will return its result as the final answer.""""""
        return question
    
    assert my_tool_with_result_as_answer.result_as_answer is True
    
    converted_tool = my_tool_with_result_as_answer.to_structured_tool()
    assert converted_tool.result_as_answer is True
    
    @tool(""Tool with default result_as_answer"")
    def my_tool_with_default(question: str) -> str:
        """"""This tool uses the default result_as_answer value.""""""
        return question
    
    assert my_tool_with_default.result_as_answer is False
    
    converted_tool = my_tool_with_default.to_structured_tool()
    assert converted_tool.result_as_answer is False",tests/tools/test_base_tool.py,,1,7
survived,"def login_user(username: str, password: str) -> Tuple[bool, Dict]:
    """"""
    Login a user and create an authentication token.
    
    Args:
        username: The username to authenticate
        password: The password to authenticate
        
    Returns:
        Tuple of (success, result) where result contains user data and token or error message
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""username"": username, ""password"": password},
        [""username"", ""password""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Authenticate the user
    user_data = authenticate(username, password)
    if not user_data:
        return False, {""error"": ""Invalid username or password""}
    
    # Create an authentication token
    token = create_token(user_data[""id""])
    
    return True, {
        ""user"": user_data,
        ""token"": token
    }
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,,1,8
survived,"    def __init__(self):
        """"""Initialize the processing stage.""""""
        self.data = None
        self.metadata = {
            ""stage"": ""processing"",
            ""status"": ""initialized"",
            ""errors"": [],
            ""processing_steps"": []
        }
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage,1,7
survived,"def validate_password_strength(password: str) -> Dict[str, bool]:
    """"""
    Validate password strength against multiple criteria.
    
    Args:
        password: The password to validate
        
    Returns:
        Dictionary with validation results for each criterion
    """"""
    results = {
        ""length"": len(password) >= 8,
        ""uppercase"": bool(re.search(r'[A-Z]', password)),
        ""lowercase"": bool(re.search(r'[a-z]', password)),
        ""digit"": bool(re.search(r'\d', password)),
        ""special_char"": bool(re.search(r'[!@#$%^&*(),.?"":{}|<>]', password))
    }
    
    results[""is_valid""] = all(results.values())
    return results
",codebase-architectures/atomic-composable-architecture/modules/validation.py,,1,8
survived,"    def delete_product(product_id):
        """"""Delete a product.""""""
        try:
            # Check if product exists
            product_data = db.get(""products"", product_id)
            if not product_data:
                Logger.warning(app_logger, f""Cannot delete: Product not found: {product_id}"")
                return False
            
            # Delete product
            result = db.delete(""products"", product_id)
            Logger.info(app_logger, f""Deleted product: {product_id}"")
            return result
        except Exception as e:
            Logger.error(app_logger, f""Error deleting product: {str(e)}"", exc_info=True)
            raise",codebase-architectures/layered-architecture/services/product_service.py,ProductService,1,7
survived,"def mark_all_alerts_as_read(user_id: str) -> int:
    """"""
    Mark all alerts for a user as read.
    
    Args:
        user_id: The ID of the user
        
    Returns:
        Number of alerts marked as read
    """"""
    return mark_all_notifications_as_read(user_id)
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,,0,7
survived,"    def info(logger, message):
        """"""Log an info message.""""""
        logger.info(message)
",codebase-architectures/layered-architecture/utils/logger.py,Logger,1,7
survived,"    def to_dict(self):
        """"""Convert category to dictionary.""""""
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""description"": self.description,
            ""created_at"": self.created_at,
            ""updated_at"": self.updated_at
        }
",codebase-architectures/layered-architecture/models/category.py,Category,1,7
survived,"    def delete_product(product_id):
        """"""Delete a product.""""""
        try:
            result = ProductService.delete_product(product_id)
            if not result:
                return {
                    ""success"": False,
                    ""message"": f""Product with ID {product_id} not found""
                }
            return {
                ""success"": True,
                ""message"": ""Product deleted successfully""
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in delete_product: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while deleting the product""
            }",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI,1,7
survived,"def validate_pattern(value: str, pattern: str) -> bool:
    """"""
    Validate that a string matches a regular expression pattern.
    
    Args:
        value: The string to validate
        pattern: Regular expression pattern to match
        
    Returns:
        True if the string matches the pattern, False otherwise
    """"""
    return bool(re.match(pattern, value))
",codebase-architectures/atomic-composable-architecture/modules/validation.py,,1,7
survived,"    def _execute_first_stage(self, stage_instance):
        """"""Execute the first stage of the pipeline.""""""
        # This method should be overridden in subclasses to provide
        # specific implementation for the first stage
        raise NotImplementedError(""Subclasses must implement _execute_first_stage"")
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager,1,7
survived,"    def send_system_alert(token: str, user_id: str, notification_type: str, 
                         data: Dict, email: Optional[str] = None) -> Dict:
        """"""
        Send a system notification to a user (admin function).
        
        Args:
            token: Authentication token (must be admin)
            user_id: The ID of the user to notify
            notification_type: The type of notification
            data: Data for the notification template
            email: Optional email address to send the notification to
            
        Returns:
            Response with success status and notification details or error message
        """"""
        # Validate token (in a real app, would check if user is admin)
        success, admin_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Send system notification
        success, result = send_system_notification(
            user_id=user_id,
            notification_type=notification_type,
            data=data,
            email=email
        )
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""System notification sent successfully"",
                ""data"": result
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": result.get(""error"", ""Failed to send system notification""),
                ""data"": None
            }",codebase-architectures/atomic-composable-architecture/endpoints/alerts_api.py,AlertsAPI,1,7
survived,"def main():
    """"""Run the application.""""""
    display_header(""Vertical Slice Architecture Example"")
    
    # Create users
    display_header(""Creating Users"")
    user1 = UserAPI.create_user(""johndoe"", ""john@example.com"", ""John Doe"")
    display_result(user1)
    
    user2 = UserAPI.create_user(""janedoe"", ""jane@example.com"", ""Jane Doe"")
    display_result(user2)
    
    # Try to create a user with an existing username
    duplicate_user = UserAPI.create_user(""johndoe"", ""another@example.com"")
    display_result(duplicate_user)
    
    # Get all users
    display_header(""All Users"")
    all_users = UserAPI.get_all_users()
    for user in all_users:
        display_result(user)
    
    # Create tasks
    display_header(""Creating Tasks"")
    task1 = TaskAPI.create_task(""Complete project"", ""Finish the architecture example"", user1[""id""])
    display_result(task1)
    
    task2 = TaskAPI.create_task(""Review code"", ""Check for bugs and improvements"", user2[""id""])
    display_result(task2)
    
    task3 = TaskAPI.create_task(""Write documentation"", ""Document the architecture"", user1[""id""])
    display_result(task3)
    
    # Get user tasks
    display_header(f""Tasks for {user1['name']}"")
    user1_tasks = TaskAPI.get_user_tasks(user1[""id""])
    for task in user1_tasks:
        display_result(task)
    
    # Update a task
    display_header(""Updating a Task"")
    updated_task = TaskAPI.update_task(task1[""id""], {""status"": ""completed""})
    display_result(updated_task)
    
    # Delete a task
    display_header(""Deleting a Task"")
    delete_result = TaskAPI.delete_task(task2[""id""])
    display_result(delete_result)
    
    # Get all remaining tasks
    display_header(""All Remaining Tasks"")
    all_tasks = TaskAPI.get_all_tasks()
    for task in all_tasks:
        display_result(task)
",codebase-architectures/vertical-slice-architecture/main.py,,1,7
survived,"def display_response(response):
    """"""Display an API response.""""""
    status = response[""status""]
    message = response[""message""]
    data = response[""data""]
    
    if status == ""success"":
        print(f""âœ… {message}"")
    else:
        print(f""âŒ {message}"")
    
    if data:
        if isinstance(data, dict):
            for key, value in data.items():
                if key == ""user"":
                    print(""\nUser:"")
                    for user_key, user_value in value.items():
                        print(f""  {user_key}: {user_value}"")
                elif key == ""alerts"":
                    print(""\nAlerts:"")
                    for i, alert in enumerate(value):
                        print(f""\nAlert {i+1}:"")
                        print(f""  Message: {alert['message']}"")
                        print(f""  Type: {alert['type']}"")
                        print(f""  Level: {alert['data'].get('level', 'N/A')}"")
                        print(f""  Read: {'Yes' if alert['is_read'] else 'No'}"")
                else:
                    print(f""\n{key.capitalize()}:"")
                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            print(f""  {sub_key}: {sub_value}"")
                    else:
                        print(f""  {value}"")
",codebase-architectures/atomic-composable-architecture/main.py,,1,7
survived,"    def get_product(product_id):
        """"""Get a product by ID.""""""
        try:
            product = ProductService.get_product(product_id)
            if not product:
                return {
                    ""success"": False,
                    ""message"": f""Product with ID {product_id} not found""
                }
            return {
                ""success"": True,
                ""data"": product
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in get_product: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while retrieving the product""
            }
",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI,1,7
survived,"def validate_string_length(value: str, min_length: int = 0, max_length: Optional[int] = None) -> bool:
    """"""
    Validate that a string's length is within the specified range.
    
    Args:
        value: The string to validate
        min_length: Minimum allowed length
        max_length: Maximum allowed length, or None for no maximum
        
    Returns:
        True if the string length is valid, False otherwise
    """"""
    if not isinstance(value, str):
        return False
    
    if len(value) < min_length:
        return False
    
    if max_length is not None and len(value) > max_length:
        return False
    
    return True
",codebase-architectures/atomic-composable-architecture/modules/validation.py,,1,7
survived,"    def get_all_tasks():
        """"""Get all tasks.""""""
        return TaskService.get_all_tasks()
",codebase-architectures/vertical-slice-architecture/features/tasks/api.py,TaskAPI,1,7
survived,"    def delete_user(user_id):
        """"""Delete a user.""""""
        success = UserService.delete_user(user_id)
        if not success:
            return {""error"": f""User with ID {user_id} not found""}
        return {""message"": f""User with ID {user_id} deleted successfully""}",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI,1,6
survived,"    def get_user(user_id):
        """"""Get a user by ID.""""""
        user_data = db.get(""users"", user_id)
        if not user_data:
            return None
        return user_data
",codebase-architectures/vertical-slice-architecture/features/users/service.py,UserService,1,7
survived,"    def __init__(self):
        """"""
        Initialize the file editor pipeline.
        """"""
        # Create pipeline stages
        self.input_stage = InputStage()
        self.processing_stage = ProcessingStage()
        self.output_stage = OutputStage()
        
        # Create and configure pipeline
        self.pipeline = Pipeline(""File Editor Pipeline"")
        
        # Add stages
        self.pipeline.add_stage(""input"", self.input_stage)
        self.pipeline.add_stage(""processing"", self.processing_stage)
        self.pipeline.add_stage(""output"", self.output_stage)
        
        console.log(""[file_editor_pipeline] Initialized file editor pipeline"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/file_editor_pipeline.py,FileEditorPipeline,1,7
survived,"    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """"""
        Process the input data and return the result.
        
        Args:
            data: The input data to process
            
        Returns:
            The processed data
        """"""
        raise NotImplementedError(""Pipeline stages must implement the process method"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/pipeline_manager.py,PipelineStage,1,7
survived,"    def error(logger_name: str, message: str, exc_info: bool = False) -> None:
        """"""
        Log an error message.
        
        Args:
            logger_name: Name of the logger
            message: Message to log
            exc_info: Whether to include exception info
        """"""
        console.log(f""[{logger_name}] [error] {message}"")
        
        if exc_info:
            import traceback
            console.log(traceback.format_exc())",example-agent-codebase-arch/layered-architecture/utils/logger.py,Logger,1,6
survived,"    def to_response(self) -> Dict[str, Any]:
        """"""
        Convert the result to a response for Claude.
        
        Returns:
            Dictionary with result or error to send back to Claude
        """"""
        if self.success:
            return {""result"": self.data if self.data is not None else self.message}
        else:
            return {""error"": self.message}",example-agent-codebase-arch/layered-architecture/models/tool_models.py,FileOperationResult,1,7
survived,"def display_token_usage(input_tokens: int, output_tokens: int) -> None:
    """"""
    Display token usage in a table.

    Args:
        input_tokens: Number of input tokens used
        output_tokens: Number of output tokens used
    """"""
    table = Table(title=""Token Usage"")
    table.add_column(""Type"", style=""cyan"")
    table.add_column(""Count"", style=""green"")
    
    table.add_row(""Input Tokens"", str(input_tokens))
    table.add_row(""Output Tokens"", str(output_tokens))
    table.add_row(""Total Tokens"", str(input_tokens + output_tokens))
    
    console.print(table)
",example-agent-codebase-arch/pipeline-architecture/main.py,,1,7
survived,"def display_token_usage(input_tokens: int, output_tokens: int) -> None:
    """"""
    Display token usage in a table.

    Args:
        input_tokens: Number of input tokens used
        output_tokens: Number of output tokens used
    """"""
    table = Table(title=""Token Usage"")
    table.add_column(""Type"", style=""cyan"")
    table.add_column(""Count"", style=""green"")
    
    table.add_row(""Input Tokens"", str(input_tokens))
    table.add_row(""Output Tokens"", str(output_tokens))
    table.add_row(""Total Tokens"", str(input_tokens + output_tokens))
    
    console.print(table)
",example-agent-codebase-arch/layered-architecture/main.py,,1,7
survived,"def test_create_folder_structure_strips_multiple_trailing_slashes():
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        folder_path, folder_name, class_name = create_folder_structure(""hello///"")
        
        assert folder_name == ""hello""
        assert class_name == ""Hello""
        assert folder_path.name == ""hello""
        assert folder_path.exists()
",tests/cli/test_create_crew.py,,1,7
survived,"def temp_dir():
    temp_path = tempfile.mkdtemp()
    yield temp_path
    shutil.rmtree(temp_path)
",tests/cli/test_create_crew.py,,1,6
survived,"def test_multimodal_agent_with_image_url():
    """"""
    Test that a multimodal agent can process images without validation errors.
    This test reproduces the scenario from issue #2475.
    """"""
    OPENAI_API_KEY = os.getenv(""OPENAI_API_KEY"")
    if not OPENAI_API_KEY:
        pytest.skip(""OPENAI_API_KEY environment variable not set"")

    llm = LLM(
        model=""openai/gpt-4o"",  # model with vision capabilities
        api_key=OPENAI_API_KEY,
        temperature=0.7
    )

    expert_analyst = Agent(
        role=""Visual Quality Inspector"",
        goal=""Perform detailed quality analysis of product images"",
        backstory=""Senior quality control expert with expertise in visual inspection"",
        llm=llm,
        verbose=True,
        allow_delegation=False,
        multimodal=True
    )

    inspection_task = Task(
        description=""""""
        Analyze the product image at https://www.us.maguireshoes.com/collections/spring-25/products/lucena-black-boot with focus on:
        1. Quality of materials
        2. Manufacturing defects
        3. Compliance with standards
        Provide a detailed report highlighting any issues found.
        """""",
        expected_output=""A detailed report highlighting any issues found"",
        agent=expert_analyst
    )

    crew = Crew(agents=[expert_analyst], tasks=[inspection_task])",tests/test_multimodal_validation.py,,0,7
survived,"    def decorator(method: Callable[..., T]) -> Callable[..., T]:
        """"""Decorator that handles both sync and async methods.""""""
        if asyncio.iscoroutinefunction(method):
            @functools.wraps(method)
            async def async_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original async method
                result = await method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
            return cast(Callable[..., T], async_wrapper)
        else:
            @functools.wraps(method)
            def sync_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original sync method
                result = method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
            return cast(Callable[..., T], sync_wrapper)
",src/crewai/flow/persistence/decorators.py,,1,7
survived,"    def save_state(
        self,
        flow_uuid: str,
        method_name: str,
        state_data: Union[Dict[str, Any], BaseModel],
    ) -> None:
        """"""Save the current flow state to SQLite.
        
        Args:
            flow_uuid: Unique identifier for the flow instance
            method_name: Name of the method that just completed
            state_data: Current state data (either dict or Pydantic model)
        """"""
        # Convert state_data to dict, handling both Pydantic and dict cases
        if isinstance(state_data, BaseModel):
            state_dict = dict(state_data)  # Use dict() for better type compatibility
        elif isinstance(state_data, dict):
            state_dict = state_data
        else:
            raise ValueError(
                f""state_data must be either a Pydantic BaseModel or dict, got {type(state_data)}""
            )
            
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(""""""
            INSERT INTO flow_states (
                flow_uuid,
                method_name,
                timestamp,
                state_json
            ) VALUES (?, ?, ?, ?)
            """""", (
                flow_uuid,
                method_name,
                datetime.utcnow().isoformat(),
                json.dumps(state_dict),
            ))
",src/crewai/flow/persistence/sqlite.py,SQLiteFlowPersistence,1,7
survived,"    def update_member_units(self, wallet_client: EVMWalletClient, parameters: dict):
        try:
            hash_result = wallet_client.send_transaction(
                {
                    ""to"": parameters[""poolAddress""],
                    ""abi"": POOL_ABI,
                    ""functionName"": ""updateMemberUnits"",
                    ""args"": [parameters[""memberAddr""], parameters[""newUnits""]],
                }
            )
            return hash_result[""hash""]
        except Exception as error:
            raise Exception(f""Failed to update member units: {error}"")
",python/src/plugins/superfluid/goat_plugins/superfluid/service.py,SuperfluidService,1,6
survived,"    def __init__(self, options: Optional[SuperfluidPluginOptions] = None):
        super().__init__(""superfluid"", [SuperfluidService()])
",python/src/plugins/superfluid/goat_plugins/superfluid/__init__.py,SuperfluidPlugin,1,7
survived,"def jsonrpc(options: JSONRpcPluginOptions) -> JSONRpcPlugin:
    return JSONRpcPlugin(options)",python/src/plugins/jsonrpc/goat_plugins/jsonrpc/__init__.py,,1,6
survived,"    def validate_name(cls, v):
        if v.upper() != v:
            raise ValueError(
                ""Name should have all letters in uppercase. Make sure to use the `uppercase` form of the name""
            )
        return v
",tests/llm/test_xai/test_raw_response.py,UserValidated,1,6
