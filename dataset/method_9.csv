status,method,filepath,class_name
survived,"def parse_signature(args: ast.arguments) -> Signature:
    """"""Convert ast.arguments to a Signature dataclass for easier processing.""""""
    parameters_positional: list[Parameter] = []
    parameters_keyword_only: list[Parameter] = []

    # Process positional-only parameters
    for i, arg in enumerate(args.posonlyargs):
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=i,
                is_required=True,  # All positional-only are required
                is_positional_only=True,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process regular positional parameters
    offset = len(args.posonlyargs)
    first_optional_idx = len(args.posonlyargs + args.args) - len(args.defaults)

    for i, arg in enumerate(args.args):
        pos = offset + i
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=pos,
                is_required=pos < first_optional_idx,
                is_positional_only=False,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process keyword-only parameters
    for arg, default in zip(args.kwonlyargs, args.kw_defaults):
        parameters_keyword_only.append(
            Parameter(
                name=arg.arg,
                position=None,
                is_required=default is None,
                is_positional_only=False,
                is_keyword_only=True,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    return Signature(
        positional=parameters_positional,
        keyword_only=parameters_keyword_only,
        has_var_positional=args.vararg is not None,
        has_var_keyword=args.kwarg is not None,
    )
",dev/check_function_signatures.py,
survived,"def parse_functions(content: str) -> dict[str, ast.FunctionDef | ast.AsyncFunctionDef]:
    tree = ast.parse(content)
    extractor = FunctionSignatureExtractor()
    extractor.visit(tree)
    return extractor.functions
",dev/check_function_signatures.py,
survived,"def test_new_optional_keyword_only_allowed():
    old_code = ""def func(*, a): pass""
    new_code = ""def func(*, a, b=1): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 0
",tests/dev/test_check_function_signatures.py,
survived,"def test_only_first_positional_rename_flagged():
    old_code = ""def func(a, b, c, d): pass""
    new_code = ""def func(x, y, z, w): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert ""Positional param order/name changed: 'a' -> 'x'."" in errors[0].message
",tests/dev/test_check_function_signatures.py,
survived,"def test_positional_param_renamed():
    old_code = ""def func(a, b): pass""
    new_code = ""def func(x, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert ""Positional param order/name changed: 'a' -> 'x'."" in errors[0].message
    assert errors[0].param_name == ""x""
",tests/dev/test_check_function_signatures.py,
survived,"def index():
    return render_template(""index.html"")
",triton_viz/visualizer/interface.py,
deleted,"def update_global_data():
    global global_data, raw_tensor_data, precomputed_c_values
    analysis_data = analyze_records()
    viz_data = get_visualization_data()
    global_data = {
        ""ops"": {
            ""visualization_data"": viz_data[""visualization_data""],
            ""failures"": viz_data[""failures""],
            ""kernel_src"": viz_data[""kernel_src""],
        }
    }
    raw_tensor_data = viz_data[""raw_tensor_data""]

    # Precompute C values for each Dot operation
    precomputed_c_values = {}
    for uuid, op_data in raw_tensor_data.items():
        if ""input_data"" in op_data and ""other_data"" in op_data:
            precomputed_c_values[uuid] = precompute_c_values(op_data)

    df = pd.DataFrame(analysis_data, columns=[""Metric"", ""Value""])
    analysis_with_tooltip = get_tooltip_data(df)
    global_data[""analysis""] = analysis_with_tooltip
",triton_viz/visualizer/interface.py,
survived,"    def test_rolling_1d_array_raises_error(self, move_func):
        """"""Test that 1D arrays raise an appropriate error for rolling functions.""""""
        data_1d = np.array([1, 2, 3, 4, 5], dtype=np.float64)

        with pytest.raises(ValueError, match=""requires at least a 2D array""):
            move_func(data_1d, window=3)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"    def __init__(
        self,
        func: Callable,
        signature: tuple[list[tuple], str],
        **kwargs,
    ):
        self.signature = signature
        super().__init__(func, **kwargs)
",numbagg/decorators.py,ndmovematrix
survived,"def test_move_matrix_pandas_comp(array, func, window, min_count):
    """"""Test matrix functions against pandas with various parameters.""""""
    if array.ndim < 2:
        pytest.skip(""Matrix functions require at least 2D input"")

    c = COMPARISONS[func]

    if min_count == ""window"":
        min_count = window

    # Get numbagg result
    result = c[""numbagg""](array, window=window, min_count=min_count)()

    # Get pandas result - need to handle the different output format
    pandas_callable = c[""pandas""](array, window=window, min_count=min_count)
    pandas_result = pandas_callable()

    # Convert pandas MultiIndex DataFrame to 3D array for comparison
    n_obs = array.shape[-1]
    n_vars = array.shape[-2]
    expected_pandas = np.full((n_obs, n_vars, n_vars), np.nan)

    # Only include windows where we have at least min_count observations
    actual_min_count = min_count if min_count is not None else window
    for t in range(n_obs):
        # Check if we have enough observations in this window
        window_size = min(t + 1, window)
        if (
            window_size >= actual_min_count
            and t in pandas_result.index.get_level_values(0)
        ):
            expected_pandas[t] = pandas_result.loc[t].values

    assert_allclose(result, expected_pandas)
",numbagg/test/test_moving.py,
survived,"    def test_sparse_valid_data(self):
        """"""Test with very sparse non-NaN observations.""""""
        # Create data where only every 5th observation is valid
        data = np.full((2, 20), np.nan, dtype=np.float64)
        data[0, ::5] = [1, 2, 3, 4]  # Only positions 0, 5, 10, 15
        data[1, ::5] = [2, 4, 6, 8]

        result = move_exp_nancorrmatrix(data, alpha=0.3, min_weight=0.1)

        # Should produce some valid results eventually
        assert not np.all(np.isnan(result))

        # Check that results are reasonable where they exist
        finite_mask = np.isfinite(result)
        if np.any(finite_mask):
            finite_values = result[finite_mask]
            assert np.all(finite_values >= -1.0)
            assert np.all(finite_values <= 1.0)
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"    def test_correlation_with_nans(self):
        """"""Test correlation consistency with NaN values.""""""
        np.random.seed(789)

        # Create two time series with some NaN values
        n_obs = 30
        a1 = np.random.randn(n_obs)
        a2 = np.random.randn(n_obs) * 1.2 + 0.3

        # Add some NaN values
        a1[3:6] = np.nan
        a2[12:15] = np.nan

        alpha = 0.4

        # Compute using non-matrix function
        corr_nonmatrix = move_exp_nancorr(a1, a2, alpha=alpha)

        # Compute using matrix function
        data_matrix = np.array([a1, a2])
        corr_matrix_result = move_exp_nancorrmatrix(data_matrix, alpha=alpha)
        corr_from_matrix = corr_matrix_result[:, 0, 1]

        # They should match
        assert_allclose(corr_nonmatrix, corr_from_matrix, rtol=1e-10)
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency
survived,"    def test_extreme_alpha_values(self):
        """"""Test behavior with alpha values very close to 0 and 1.""""""
        # Use data that's not perfectly correlated to see differences
        np.random.seed(42)
        data = np.random.randn(2, 20)
        data[1] = 0.7 * data[0] + 0.5 * np.random.randn(20)  # Partial correlation

        # Test with alpha very close to 0 (almost no decay)
        result_low = move_exp_nancorrmatrix(data, alpha=1e-6)

        # Test with alpha close to 1 (very fast decay)
        result_high = move_exp_nancorrmatrix(data, alpha=0.99)

        # Both should produce valid results
        assert not np.all(np.isnan(result_low[-1]))
        assert not np.all(np.isnan(result_high[-1]))

        # With different correlation patterns, results should differ
        # Check the off-diagonal correlation values
        assert not np.allclose(result_low[-5:, 0, 1], result_high[-5:, 0, 1], rtol=1e-2)
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"def move_exp_nancovmatrix(a, alpha, min_weight, out):
    """"""
    Exponential moving window covariance matrix gufunc.

    For 2D input, computes covariance between variables (rows) across observations (columns) with exponential decay.
    """"""
    n_vars = a.shape[0]
    n_obs = a.shape[1]

    # Initialize pairwise statistics - each (i,j) pair tracks its own statistics
    # This is necessary for consistency with non-matrix exponential functions
    sums_i = np.zeros(
        (n_vars, n_vars), dtype=a.dtype
    )  # sum of variable i for pair (i,j)
    sums_j = np.zeros(
        (n_vars, n_vars), dtype=a.dtype
    )  # sum of variable j for pair (i,j)
    prods = np.zeros((n_vars, n_vars), dtype=a.dtype)  # sum of products for pair (i,j)
    pair_weights = np.zeros(
        (n_vars, n_vars), dtype=a.dtype
    )  # accumulated alpha weights
    pair_sum_weights = np.zeros((n_vars, n_vars), dtype=a.dtype)  # count of valid pairs
    pair_sum_weights_sq = np.zeros(
        (n_vars, n_vars), dtype=a.dtype
    )  # sum of squared weights

    for t in range(n_obs):
        alpha_t = alpha[t]
        decay = 1.0 - alpha_t

        # Apply exponential decay to all pairwise statistics
        for i in range(n_vars):
            for j in range(n_vars):
                sums_i[i, j] *= decay
                sums_j[i, j] *= decay
                prods[i, j] *= decay
                pair_weights[i, j] *= decay
                pair_sum_weights[i, j] *= decay
                pair_sum_weights_sq[i, j] *= decay**2

        # Add new values - track pairwise statistics for consistency
        for i in range(n_vars):
            for j in range(n_vars):
                new_val_i = a[i, t]
                new_val_j = a[j, t]

                # Only update if BOTH values are non-NaN (consistent with non-matrix functions)
                if not (np.isnan(new_val_i) or np.isnan(new_val_j)):
                    # Update pairwise statistics
                    sums_i[i, j] += new_val_i
                    sums_j[i, j] += new_val_j
                    prods[i, j] += new_val_i * new_val_j
                    pair_weights[i, j] += alpha_t
                    pair_sum_weights[i, j] += 1.0
                    pair_sum_weights_sq[i, j] += 1.0

        # Compute covariance matrix for current time step
        for i in range(n_vars):
            for j in range(n_vars):
                # Check if we have sufficient weight for a meaningful covariance calculation
                bias = (
                    1 - pair_sum_weights_sq[i, j] / (pair_sum_weights[i, j] ** 2)
                    if pair_sum_weights[i, j] > 0
                    else 0.0
                )

                if pair_weights[i, j] >= min_weight and bias > 0:
                    # Compute covariance using pairwise statistics
                    n = pair_sum_weights[i, j]
                    mean_i = sums_i[i, j] / n
                    mean_j = sums_j[i, j] / n

                    # Compute biased covariance
                    cov_biased = (prods[i, j] / n) - mean_i * mean_j

                    # Apply bias correction
                    out[t, i, j] = cov_biased / bias
                else:
                    out[t, i, j] = np.nan",numbagg/moving_matrix.py,
survived,"    def _format_yaml(self, data: Any) -> str:
        """"""Format as YAML""""""
        return yaml.dump(data, default_flow_style=False, allow_unicode=True)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def analyze_all(self) -> Dict[str, Any]:
        """"""Analyze entire directory structure""""""
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'base_path': str(self.base_path),
            'categories': defaultdict(list),
            'providers': defaultdict(list),
            'model_formats': defaultdict(int),
            'total_models': 0,
            'total_size': 0,
            'insights': []
        }
        
        # Scan directory structure
        for root, dirs, files in self.base_path.walk():
            root_path = Path(root)
            
            # Check if this is a model directory
            if self._is_model_directory(root_path, files):
                model_info = self._analyze_model_directory(root_path, files)
                
                if model_info:
                    category = model_info['category']
                    provider = model_info['provider']
                    
                    analysis['categories'][category].append(model_info)
                    analysis['providers'][provider].append(model_info['name'])
                    analysis['total_models'] += 1
                    analysis['total_size'] += model_info.get('size', 0)
                    
                    # Track model formats
                    for fmt in model_info.get('formats', []):
                        analysis['model_formats'][fmt] += 1
        
        # Generate insights
        analysis['insights'] = self._generate_insights(analysis)
        
        # Convert defaultdicts to regular dicts
        analysis['categories'] = dict(analysis['categories'])
        analysis['providers'] = dict(analysis['providers'])
        analysis['model_formats'] = dict(analysis['model_formats'])
        
        return analysis
",src/haconiwa/scan/analyzer.py,ModelAnalyzer
survived,"    def test_whitelist(self, temp_model_dir):
        """"""Test whitelist functionality""""""
        scanner = ModelScanner(
            temp_model_dir,
            whitelist=[""*/openai/*"", ""*/anthropic/*""]
        )
        
        # Should find openai and anthropic models
        results = scanner.search_by_model_name(""gpt-4"")
        assert results['total_files'] > 0
        
        results = scanner.search_by_model_name(""claude-3-opus"")
        assert results['total_files'] > 0
        
        # Should not find meta models (not in whitelist)
        results = scanner.search_by_model_name(""llama-2-70b"")
        assert results['total_files'] == 0
",tests/test_scan/test_scanner.py,TestModelScanner
survived,"    def test_generate_from_scan_results_with_files(self):
        """"""Test generation from directory analysis results""""""
        scan_results = {
            'files': {
                'src/utils/helper.py': {'category': 'utils'},
                'src/api/routes.py': {'category': 'api'},
                'src/models/user.py': {'category': 'model'}
            }
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=2
        )
        
        assert len(config['tasks']) == 2
        assert config['metadata']['action'] == 'refactor'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _dict_to_text(self, data: Dict[str, Any], indent: int = 0) -> str:
        """"""Convert dictionary to formatted text""""""
        lines = []
        indent_str = ""  "" * indent
        
        for key, value in data.items():
            if isinstance(value, dict):
                lines.append(f""{indent_str}{key}:"")
                lines.append(self._dict_to_text(value, indent + 1))
            elif isinstance(value, list):
                lines.append(f""{indent_str}{key}:"")
                for item in value:
                    if isinstance(item, dict):
                        lines.append(self._dict_to_text(item, indent + 1))
                    else:
                        lines.append(f""{indent_str}  - {item}"")
            else:
                lines.append(f""{indent_str}{key}: {value}"")
        
        return ""\n"".join(lines)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def _normalize_model_name(self, model_name: str) -> str:
        """"""Normalize model name by stripping common prefixes if enabled""""""
        if not self.strip_prefix:
            return model_name.lower()
        
        normalized = model_name.lower()
        for prefix in self.model_prefixes:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):]
                break
        
        return normalized
",src/haconiwa/scan/scanner.py,ModelScanner
survived,"    def test_metadata_generation(self):
        """"""Test metadata is properly generated""""""
        config = self.generator.create_example_yaml()
        
        assert 'metadata' in config
        assert 'generated_at' in config['metadata']
        assert 'source' in config['metadata']
        
        # Check timestamp format
        timestamp = config['metadata']['generated_at']
        # Should be able to parse it
        datetime.fromisoformat(timestamp)
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def search_by_model_name(self, 
                           model_name: str, 
                           include_content: bool = False) -> Dict[str, Any]:
        """"""Search for files and directories related to a model name""""""
        normalized_name = self._normalize_model_name(model_name)
        results = {
            'model_name': model_name,
            'normalized_name': normalized_name,
            'matches': defaultdict(list),
            'total_files': 0,
            'categories': set()
        }
        
        # Search patterns
        patterns = [
            normalized_name,
            model_name.lower(),
            model_name.replace('-', '_'),
            normalized_name.replace('-', '_')
        ]
        
        for root, dirs, files in os.walk(self.base_path):
            root_path = Path(root)
            
            # Filter directories
            dirs[:] = [d for d in dirs if not self._should_ignore(root_path / d)]
            
            # Check directory names
            for pattern in patterns:
                if pattern in root_path.name.lower():
                    category = self._determine_category(root_path)
                    results['categories'].add(category)
                    
                    # Process files in matching directory
                    for file in files:
                        file_path = root_path / file
                        if not self._should_ignore(file_path):
                            file_info = self._get_file_info(file_path, include_content)
                            results['matches'][category].append(file_info)
                            results['total_files'] += 1
            
            # Check file names
            for file in files:
                file_path = root_path / file
                if self._should_ignore(file_path):
                    continue
                
                for pattern in patterns:
                    if pattern in file.lower():
                        category = self._determine_category(root_path)
                        results['categories'].add(category)
                        file_info = self._get_file_info(file_path, include_content)
                        results['matches'][category].append(file_info)
                        results['total_files'] += 1
                        break
        
        results['categories'] = list(results['categories'])
        results['matches'] = dict(results['matches'])
        return results
",src/haconiwa/scan/scanner.py,ModelScanner
survived,"    def test_save_yaml(self):
        """"""Test YAML file saving""""""
        config = {
            'provider': 'claude',
            'tasks': [
                {'file': 'test.py', 'prompt': 'Add tests'}
            ],
            'options': {
                'max_concurrent': 1,
                'timeout': 60
            }
        }
        
        output_path = self.temp_dir / 'test-output.yaml'
        saved_path = self.generator.save_yaml(config, output_path)
        
        assert saved_path == output_path
        assert output_path.exists()
        
        # Load and verify saved content
        with open(output_path, 'r') as f:
            loaded_config = yaml.safe_load(f)
        
        assert loaded_config['provider'] == 'claude'
        assert len(loaded_config['tasks']) == 1
        assert loaded_config['tasks'][0]['file'] == 'test.py'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _format_summary(self, data: Any) -> str:
        """"""Format as a concise summary""""""
        if not isinstance(data, dict):
            return str(data)
        
        lines = [""="" * 50]
        
        # Handle model search results
        if 'model_name' in data:
            lines.append(f""Model Search: {data['model_name']}"")
            lines.append(""="" * 50)
            lines.append(f""Total files found: {data.get('total_files', 0)}"")
            
            if 'matches' in data:
                lines.append(""\nMatches by category:"")
                for category, files in data['matches'].items():
                    lines.append(f""  {category}: {len(files)} files"")
        
        # Handle content search results
        elif 'pattern' in data and 'matches' in data:
            lines.append(f""Content Search: {data['pattern']}"")
            lines.append(""="" * 50)
            lines.append(f""Total matches: {data.get('total_matches', 0)}"")
            lines.append(f""Files searched: {data.get('files_searched', 0)}"")
            
            if data['matches']:
                lines.append(""\nTop matches:"")
                for match in data['matches'][:5]:
                    lines.append(f""  {match['file']}:{match['line_number']}"")
        
        # Handle analysis results
        elif 'categories' in data and 'providers' in data:
            lines.append(""Model Analysis Summary"")
            lines.append(""="" * 50)
            lines.append(f""Base path: {data.get('base_path', 'Unknown')}"")
            lines.append(f""Total models: {data.get('total_models', 0)}"")
            
            if data.get('total_size', 0) > 0:
                size_gb = data['total_size'] / (1024 ** 3)
                lines.append(f""Total size: {size_gb:.2f} GB"")
            
            if 'insights' in data:
                lines.append(""\nInsights:"")
                for insight in data['insights']:
                    lines.append(f""  • {insight}"")
        
        # Handle list results
        elif isinstance(data, list) and data:
            lines.append(f""Results: {len(data)} items"")
            lines.append(""="" * 50)
            for i, item in enumerate(data[:10], 1):
                if isinstance(item, dict):
                    name = item.get('name', item.get('path', 'Unknown'))
                    lines.append(f""{i}. {name}"")
                else:
                    lines.append(f""{i}. {item}"")
            
            if len(data) > 10:
                lines.append(f""... and {len(data) - 10} more"")
        
        lines.append(""="" * 50)
        return ""\n"".join(lines)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def test_scan_generate_parallel_config_from_model(self, runner, temp_model_dir):
        """"""Test generate-parallel-config from model search""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""parallel-dev.yaml""
            
            # Change to temp_model_dir before running command
            import os
            original_cwd = os.getcwd()
            try:
                os.chdir(temp_model_dir)
                result = runner.invoke(
                    scan_app,
                    [""generate-parallel-config"", 
                     ""--source"", ""model:o1-mini"", 
                     ""--action"", ""add_tests"",
                     ""--output"", str(output_path)]
                )
            finally:
                os.chdir(original_cwd)
            
            assert result.exit_code == 0
            assert ""Generated parallel-dev YAML"" in result.stdout
            assert output_path.exists()
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_full_workflow(self):
        """"""Test complete workflow from scan results to YAML generation""""""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Create test files
            model_dir = temp_path / 'models'
            model_dir.mkdir()
            (model_dir / 'user.py').write_text('class User: pass')
            (model_dir / 'product.py').write_text('class Product: pass')
            
            api_dir = temp_path / 'api'
            api_dir.mkdir()
            (api_dir / 'routes.py').write_text('def get_users(): pass')
            
            # Create generator
            generator = ParallelYAMLGenerator(base_path=temp_path)
            
            # Simulate scan results
            scan_results = {
                'matches': {
                    'model': [
                        {'path': 'models/user.py', 'type': 'python'},
                        {'path': 'models/product.py', 'type': 'python'}
                    ],
                    'api': [
                        {'path': 'api/routes.py', 'type': 'python'}
                    ]
                }
            }
            
            # Generate YAML
            config = generator.generate_from_scan_results(
                scan_results,
                action='add_type_hints',
                max_files=10
            )
            
            # Save YAML
            output_path = temp_path / 'parallel-dev.yaml'
            generator.save_yaml(config, output_path)
            
            # Verify
            assert output_path.exists()
            
            with open(output_path, 'r') as f:
                loaded = yaml.safe_load(f)
            
            assert loaded['provider'] == 'claude'
            assert len(loaded['tasks']) == 3
            assert loaded['metadata']['action'] == 'add_type_hints'
            
            # Check task content
            file_paths = [task['file'] for task in loaded['tasks']]
            assert 'models/user.py' in file_paths
            assert 'models/product.py' in file_paths
            assert 'api/routes.py' in file_paths",tests/test_scan/test_generate_parallel.py,TestGenerateParallelIntegration
survived,"    def test_scan_generate_parallel_config_project_wide(self, runner, temp_model_dir):
        """"""Test generate-parallel-config for project-wide changes""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""project-wide.yaml""
            
            # Change to temp_model_dir before running command
            import os
            original_cwd = os.getcwd()
            try:
                os.chdir(temp_model_dir)
                result = runner.invoke(
                    scan_app,
                    [""generate-parallel-config"",
                     ""--project-wide"", ""*.py"",
                     ""--action"", ""add_type_hints"",
                     ""--output"", str(output_path)]
                )
            finally:
                os.chdir(original_cwd)
            
            assert result.exit_code == 0
            assert ""Generated project-wide YAML"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def generate_for_model_migration(self,
                                   old_model: str,
                                   new_model: str,
                                   files: List[str]) -> Dict[str, Any]:
        """"""Generate YAML for model migration tasks""""""
        
        tasks = []
        
        for file_path in files:
            prompt = f""Migrate code from {old_model} to {new_model}. Update import statements, "" \
                    f""API calls, method names, and parameters. Ensure compatibility with {new_model} "" \
                    f""while maintaining existing functionality. Add migration comments where significant changes are made.""
            
            tasks.append({
                'file': file_path,
                'prompt': prompt
            })
        
        config = {
            'provider': 'claude',
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'source': 'haconiwa scan generate-parallel-config',
                'migration': f""{old_model} -> {new_model}"",
                'total_tasks': len(tasks)
            },
            'tasks': tasks,
            'options': {
                'max_concurrent': 3,
                'timeout': 180,  # 3 minutes for migration tasks
                'allowed_tools': ['Read', 'Write', 'Edit', 'MultiEdit'],
                'permission_mode': 'confirmEach',
                'output_dir': f'./migration-{old_model}-to-{new_model}'
            }
        }
        
        return config
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator
survived,"    def __init__(self):
        self.format_handlers = {
            'text': self._format_text,
            'json': self._format_json,
            'yaml': self._format_yaml,
            'summary': self._format_summary,
            'table': self._format_table,
            'tree': self._format_tree
        }
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def process_dimension_data(
        summary_df, func, dimension, all_libs, matrix_shape_exclusions
    ):
        """"""Process data for a single dimension (1D or 2D) for a specific function.""""""
        if summary_df.empty:
            return {f""{dimension}_{lib}"": ""n/a"" for lib in all_libs}

        return {
            f""{dimension}_{lib}"": get_column_value(
                summary_df, func, lib, dimension, matrix_shape_exclusions
            )
            for lib in all_libs
        }
",numbagg/test/run_benchmarks.py,
survived,"    def test_with_nans(self, func):
        """"""Test with NaN values.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array(
            [[1, 2, np.nan], [2, 4, 1], [np.nan, 6, 2], [4, np.nan, 3]],
            dtype=np.float64,
        )
        alpha = 0.3
        result = func(data, alpha=alpha)

        # Check shape
        assert result.shape == (4, 3, 3)

        # Should handle NaN gracefully - check that we get some finite values
        assert np.any(np.isfinite(result))
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Context manager exit - ensure connections are closed.""""""
        self.close_all_connections()
",ocode_python/core/context_manager.py,ContextManager
survived,"    def test_command_sanitizer_unix_only(self, mock_platform):
        """"""Test that Windows patterns are not applied on Unix.""""""
        sanitizer = CommandSanitizer()

        # Windows-specific patterns should not be in Unix sanitizer
        assert (
            len([p for p in sanitizer.forbidden_patterns if ""format.*[cC]:"" in p]) == 0
        )
        assert len([p for p in sanitizer.forbidden_patterns if ""taskkill"" in p]) == 0
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility
survived,"def deploy_staticfiles(release: Release) -> bool:
    """"""Deploy static files to CDN.""""""
    print(""Deploying static files to cdn"")
    cc = f""public, max-age={int(datetime.timedelta(days=365).total_seconds())}""

    if not release.static_key:
        print(""No static files to deploy"")
        return True

    with tempfile.NamedTemporaryFile(suffix=os.path.basename(release.static_key)) as f:
        download_release_fileobj(release.static_key, f)
        f.flush()
        with DeploymentJob(f.name, ""ce-cdn.net"", version=release.version, cache_control=cc) as job:
            return job.run()
",bin/lib/builds_core.py,
survived,"def notify_sentry_deployment(cfg: Config, release: Release) -> None:
    """"""Notify Sentry about a deployment. Failures are logged but don't stop deployment.""""""
    try:
        print(""Marking as a release in sentry..."")
        token = get_ssm_param(""/compiler-explorer/sentryAuthToken"")
        result = requests.post(
            f""https://sentry.io/api/0/organizations/compiler-explorer/releases/{release.version}/deploys/"",
            data=dict(environment=cfg.env.value),
            headers=dict(Authorization=f""Bearer {token}""),
            timeout=30,
        )
        if not result.ok:
            print(f""Warning: Failed to notify sentry: {result.status_code}"")
            # Don't fail deployment for sentry notification failure
        else:
            print(""...done"")
    except Exception as e:
        print(f""Warning: Failed to notify sentry: {e}"")
",bin/lib/builds_core.py,
survived,"def old_deploy_staticfiles(branch: Optional[str], versionfile: str) -> None:
    """"""Deploy static files using the old method (for releases without static_key).""""""
    print(""Deploying static files"")
    downloadfile = versionfile
    filename = ""deploy.tar.xz""
    remotefile = (branch + ""/"" if branch else """") + downloadfile
    download_release_file(remotefile[1:], filename)
    os.mkdir(""deploy"")
    subprocess.call([""tar"", ""-C"", ""deploy"", ""-Jxf"", filename])
    os.remove(filename)
    subprocess.call([""aws"", ""s3"", ""sync"", ""deploy/out/dist/dist"", ""s3://compiler-explorer/dist/cdn""])
    subprocess.call([""rm"", ""-Rf"", ""deploy""])
",bin/lib/builds_core.py,
survived,"    def test_run_with_uv_with_requirements(self, mock_run):
        """"""Test run_with_uv with requirements file.""""""
        mock_run.return_value = Mock(returncode=0)
        req_path = Path(""requirements.txt"")

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(""server.py"", with_requirements=req_path)

        assert exc_info.value.code == 0

        cmd = mock_run.call_args[0][0]
        expected = [
            ""uv"",
            ""run"",
            ""--with"",
            ""fastmcp"",
            ""--with-requirements"",
            ""requirements.txt"",
            ""fastmcp"",
            ""run"",
            ""server.py"",
        ]
        assert cmd == expected
",tests/cli/test_run_with_uv.py,TestRunWithUv
survived,"def _build_tree_structure(
    node: ClusterTreeNode,
    node_id_to_cluster: dict[str, ClusterTreeNode],
    level: int = 0,
    is_last: bool = True,
    prefix: str = """",
) -> str:
    """"""Build a text representation of the hierarchical cluster tree.
    
    This is a recursive helper function used by visualise_clusters().
    
    Args:
        node: Current tree node
        node_id_to_cluster: Dictionary mapping node IDs to nodes
        level: Current depth in the tree (for indentation)
        is_last: Whether this is the last child of its parent
        prefix: Current line prefix for tree structure
        
    Returns:
        String representation of the tree structure
    """"""
    # Current line prefix (used for tree visualization symbols)
    current_prefix = prefix

    # Add the appropriate connector based on whether this is the last child
    if level > 0:
        if is_last:
            current_prefix += ""╚══ ""
        else:
            current_prefix += ""╠══ ""

    # Print the current node
    result = (
        current_prefix + node.name + "" ("" + str(node.count) + "" conversations)\n""
    )

    # Calculate the prefix for children (continue vertical lines for non-last children)
    child_prefix = prefix
    if level > 0:
        if is_last:
            child_prefix += ""    ""  # No vertical line needed for last child's children
        else:
            child_prefix += ""║   ""  # Continue vertical line for non-last child's children

    # Process children
    children = node.children
    for i, child_id in enumerate(children):
        child = node_id_to_cluster[child_id]
        is_last_child = i == len(children) - 1
        result += _build_tree_structure(
            child, node_id_to_cluster, level + 1, is_last_child, child_prefix
        )

    return result
",kura/v1/visualization.py,
survived,"    def __len__(self) -> int:
        """"""Number of steps in the pipeline.""""""
        return len(self.steps)
",bayesianbandits/pipelines/_learner.py,LearnerPipeline
survived,"    def test_indexing(self):
        """"""Test step indexing.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling())
        transform1 = FunctionTransformer(lambda x: x * 2)
        transform2 = StandardScaler()
        steps = [(""double"", transform1), (""scale"", transform2)]

        pipeline = ContextualAgentPipeline(steps, agent)

        # Test string indexing
        assert pipeline[""double""] is transform1
        assert pipeline[""scale""] is transform2

        # Test integer indexing
        assert pipeline[0] == (""double"", transform1)
        assert pipeline[1] == (""scale"", transform2)

        # Test invalid access
        with pytest.raises(KeyError):
            _ = pipeline[""invalid""]

        with pytest.raises(IndexError):
            _ = pipeline[10]
",tests/test_agent_pipeline.py,TestContextualAgentPipeline
survived,"    def transform(self, X: Any) -> Any:
        """"""Apply all transformers to input data.""""""
        return _transform_data(X, self.steps)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def test_contextual_agent_dispatch(self):
        """"""Test factory dispatches to ContextualAgentPipeline for ContextualAgent.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling())
        steps = [(""identity"", FunctionTransformer())]

        pipeline = AgentPipeline(steps, agent)

        assert isinstance(pipeline, ContextualAgentPipeline)
        assert pipeline._agent is agent
",tests/test_agent_pipeline.py,TestAgentPipelineFactory
survived,"    def test_predict_method(self):
        """"""Test predict method delegates correctly.""""""
        X = np.array([[1, 2], [3, 4]])

        # Train the pipeline first
        self.pipeline.partial_fit(X, np.array([1, 2]))

        # Now predict
        self.pipeline.predict(X)

        assert len(self.mock_learner.predict_calls) == 1
        received_X = self.mock_learner.predict_calls[0]
        assert received_X.shape == X.shape
",tests/test_learner_pipeline.py,TestLearnerPipelineInterface
survived,"    def test_sklearn_transformer_integration(self):
        """"""Test integration with sklearn transformers.""""""
        # Pre-fit scaler
        scaler = StandardScaler()
        historical_data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
        scaler.fit(historical_data)

        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling(), random_seed=42)

        steps = [(""scale"", scaler)]
        pipeline = ContextualAgentPipeline(steps, agent)

        X = np.array([[2.0, 3.0], [4.0, 5.0]])

        # Should work with pre-fitted transformer
        actions = pipeline.pull(X)
        assert len(actions) == 2

        y = np.array([1.0, 2.0])
        pipeline.update(X, y)
",tests/test_agent_pipeline.py,TestTransformationFlow
survived,"    def test_random_state_property(self):
        """"""Test random_state property delegation.""""""
        # Test getting random_state
        self.mock_learner.random_state = 42  # type: ignore
        assert self.pipeline.random_state == 42

        # Test setting random_state
        self.pipeline.random_state = 123
        assert self.mock_learner.random_state == 123
",tests/test_learner_pipeline.py,TestLearnerPipelineInterface
survived,"    def test_complex_pipeline(self):
        """"""Test complex pipeline with multiple transformers.""""""
        # Pre-fit transformers
        scaler = StandardScaler()
        pca = PCA(n_components=5)

        # Fit on dummy high-dimensional data
        dummy_data = np.random.randn(100, 20)
        scaler.fit(dummy_data)
        pca.fit(scaler.transform(dummy_data))

        pipeline = LearnerPipeline(
            steps=[(""scale"", scaler), (""pca"", pca)],
            learner=NormalRegressor(alpha=0.1, beta=1.0)
        )

        # High-dimensional data
        X = np.random.randn(50, 20)
        y = np.random.randn(50)

        # Should handle the full pipeline
        pipeline.partial_fit(X, y)

        # Test inference
        X_test = np.random.randn(10, 20)
        samples = pipeline.sample(X_test, size=1)
        assert samples.shape == (1, 10)  # (size, n_samples)

        predictions = pipeline.predict(X_test)
        assert predictions.shape == (10,)
",tests/test_learner_pipeline.py,TestLearnerPipelineIntegration
survived,"    def test_basic_construction(self):
        """"""Test basic non-contextual pipeline construction.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        steps = [(""identity"", FunctionTransformer())]

        pipeline = NonContextualAgentPipeline(steps, agent)

        assert len(pipeline) == 1
        assert pipeline._agent is agent
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline
survived,"    def update(
        self,
        y: NDArray[np.float64],
        sample_weight: Optional[NDArray[np.float64]] = None,
    ) -> None:
        """"""Update the wrapped agent with observed reward(s).

        Parameters
        ----------
        y : NDArray[np.float64]
            Reward(s) to use for updating the arm.
        sample_weight : Optional[NDArray[np.float64]], default=None
            Sample weights to use for updating the arm.
        """"""
        self._agent.update(y, sample_weight=sample_weight)
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline
survived,"    def create_implementation_blueprint(self, feature_request: str, context_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """"""Create a detailed implementation blueprint based on context analysis.""""""
        blueprint = {
            ""feature"": feature_request,
            ""implementation_steps"": self._generate_implementation_steps(feature_request, context_analysis),
            ""file_modifications"": self._identify_file_modifications(feature_request, context_analysis),
            ""new_files_required"": self._identify_new_files(feature_request, context_analysis),
            ""dependencies"": self._identify_dependencies(feature_request, context_analysis),
            ""testing_strategy"": self._generate_testing_strategy(feature_request, context_analysis),
            ""integration_points"": self._identify_integration_points(feature_request, context_analysis)
        }
        return blueprint
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def setup_agents(self):
        """"""Setup the multi-agent team with Context Engineering support.""""""
        
        # 1. Product Manager Agent - Defines requirements
        self.product_manager = Agent(
            name=""Product Manager"",
            role=""Product Requirements Specialist"",
            goal=""Define clear, comprehensive product requirements and user stories"",
            backstory=""""""You are an experienced product manager who specializes in 
            creating detailed, actionable requirements. You understand that clear 
            requirements are the foundation of successful implementation."""""",
            instructions=""""""
            When defining requirements:
            1. Be specific and measurable
            2. Include user acceptance criteria
            3. Consider edge cases and error scenarios
            4. Define success metrics
            5. Specify technical constraints
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 2. Context Engineering Agent - Generates comprehensive context
        self.context_engineer = create_context_agent(
            llm=self.llm,
            name=""Context Engineering Specialist"",
            verbose=True
        )
        
        # 3. Software Architect Agent - Designs implementation using context
        self.architect = Agent(
            name=""Software Architect"",
            role=""System Architecture Specialist"", 
            goal=""Design robust, scalable system architecture based on comprehensive context"",
            backstory=""""""You are a senior software architect with expertise in 
            designing systems that follow established patterns and best practices. 
            You excel at creating architectures that integrate seamlessly with 
            existing codebases."""""",
            instructions=""""""
            When designing architecture:
            1. Follow the patterns identified in the context analysis
            2. Ensure compatibility with existing systems
            3. Design for scalability and maintainability
            4. Consider security and performance implications
            5. Document architectural decisions and rationale
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 4. Senior Developer Agent - Implements using context-enhanced guidance
        self.developer = Agent(
            name=""Senior Developer"",
            role=""Implementation Specialist"",
            goal=""Implement features following context-guided best practices"",
            backstory=""""""You are a senior developer who excels at implementing 
            features that follow established codebase patterns. You understand 
            that consistency and quality are paramount."""""",
            instructions=""""""
            When implementing features:
            1. Follow the codebase patterns identified in context analysis
            2. Maintain consistency with existing code style
            3. Implement comprehensive error handling
            4. Write clean, maintainable code
            5. Follow the implementation blueprint exactly
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 5. QA Engineer Agent - Validates using context-generated criteria
        self.qa_engineer = Agent(
            name=""QA Engineer"",
            role=""Quality Assurance Specialist"",
            goal=""Ensure implementation meets all quality criteria and requirements"",
            backstory=""""""You are an experienced QA engineer who specializes in 
            comprehensive testing and validation. You understand that quality 
            is built in, not bolted on."""""",
            instructions=""""""
            When validating implementations:
            1. Use the validation criteria from context analysis
            2. Test both happy path and edge cases
            3. Verify integration with existing systems
            4. Check for security vulnerabilities
            5. Validate performance requirements
            """""",
            llm=self.llm,
            verbose=True
        )
",examples/python/concepts/context-engineering-workflow.py,ContextEngineeringWorkflow
survived,"    def _analyze_import_patterns(self, project_path: str, file_patterns: List[str]) -> Dict[str, Any]:
        """"""Analyze import patterns and dependencies.""""""
        imports = {""relative"": [], ""absolute"": [], ""external"": [], ""patterns"": []}
        # Implementation would analyze actual import statements
        return imports
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _generate_expected_outcome(self, criterion: str) -> str:
        """"""Generate expected outcome for a criterion.""""""
        return f""Criterion '{criterion}' passes validation""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _analyze_readme_style(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze README style and structure.""""""
        return {""style"": ""standard"", ""sections"": []}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def check(node: ast.Call, resolver: Resolver) -> bool:
        """"""
        Returns True if the call is threading.Thread() without a name parameter.
        """"""
        return (
            (resolved := resolver.resolve(node))
            and resolved == [""threading"", ""Thread""]
            and not any(keyword.arg == ""name"" for keyword in node.keywords)
        )",dev/clint/src/clint/rules/unnamed_thread.py,UnnamedThread
survived,"    def __init__(self, function_name: str, unknown_args: set[str]) -> None:
        self.function_name = function_name
        self.unknown_args = unknown_args
",dev/clint/src/clint/rules/unknown_mlflow_arguments.py,UnknownMlflowArguments
survived,"    def check(cls, rules: set[str]) -> ""DoNotDisable"":
        if s := rules.intersection(DoNotDisable.DO_NOT_DISABLE):
            return cls(s)
",dev/clint/src/clint/rules/do_not_disable.py,DoNotDisable
survived,"    def __init__(self, rules: set[str]) -> None:
        self.rules = rules
",dev/clint/src/clint/rules/do_not_disable.py,DoNotDisable
survived,"    def _message(self) -> str:
        if correct_hint := self.MAPPING.get(self.type_hint):
            return f""Did you mean `{correct_hint}` instead of `{self.type_hint}`?""

        raise ValueError(
            f""Unexpected type: {self.type_hint}. It must be one of {list(self.MAPPING)}.""
        )",dev/clint/src/clint/rules/incorrect_type_annotation.py,IncorrectTypeAnnotation
survived,"    def _message(self) -> str:
        return ""Builtin modules must be imported at the top level.""",dev/clint/src/clint/rules/lazy_builtin_import.py,LazyBuiltinImport
survived,"    def check(node: ast.expr, resolver: Resolver) -> bool:
        """"""
        Returns True if the `@experimental` decorator from mlflow.utils.annotations is used
        incorrectly.
        """"""
        resolved = resolver.resolve(node)
        if not resolved:
            return False

        if resolved != [""mlflow"", ""utils"", ""annotations"", ""experimental""]:
            return False

        if not isinstance(node, ast.Call):
            return True

        version = next((k.value for k in node.keywords if k.arg == ""version""), None)
        if version is None:
            # No `version` argument, invalid usage
            return True

        if not isinstance(version, ast.Constant) or not isinstance(version.value, str):
            # `version` is not a string literal, invalid usage
            return True

        if not _is_valid_version(version.value):
            # `version` is not a valid semantic version, # invalid usage
            return True

        return False",dev/clint/src/clint/rules/invalid_experimental_decorator.py,InvalidExperimentalDecorator
survived,"    def check(node: ast.Name) -> bool:
        return node.id in IncorrectTypeAnnotation.MAPPING
",dev/clint/src/clint/rules/incorrect_type_annotation.py,IncorrectTypeAnnotation
survived,"    def example(cls) -> ""ModelVersionTagDeletedPayload"":
        return cls(
            name=""example_model"",
            version=""1"",
            key=""example_key"",
        )
",mlflow/webhooks/types.py,ModelVersionTagDeletedPayload
survived,"    def example(cls) -> ""ModelVersionAliasCreatedPayload"":
        return cls(
            name=""example_model"",
            alias=""example_alias"",
            version=""1"",
        )
",mlflow/webhooks/types.py,ModelVersionAliasCreatedPayload
survived,"    def test_webhook(
        self, webhook_id: str, event: Optional[WebhookEvent] = None
    ) -> WebhookTestResult:
        """"""
        Test a webhook by sending a test event to the specified URL.

        Args:
            webhook_id: Webhook ID.
            event: Optional event type to test. If not specified, uses the first event from webhook.

        Returns:
            WebhookTestResult indicating success/failure and response details
        """"""
        raise NotImplementedError(f""{self.__class__.__name__} does not support test_webhook"")",mlflow/store/model_registry/abstract_store.py,AbstractStore
survived,"    def w_GET_color(vm: 'SPyVM', wop_x: 'W_OpArg',
                    wop_attr: 'W_OpArg') -> 'W_OpImpl':
        from spy.vm.builtin import builtin_func
        from spy.vm.str import W_Str

        @builtin_func(W_OpArg._w.fqn, 'get_color')
        def w_get_color(vm: 'SPyVM', w_oparg: W_OpArg) -> W_Str:
            return vm.wrap(w_oparg.color)  # type: ignore

        return W_OpImpl(w_get_color, [wop_x])
",spy/vm/opimpl.py,W_OpArg
survived,"            def w_get_null(vm: 'SPyVM', w_cls: W_Type) -> W_OpImpl:
                return W_OpImpl.NULL
",spy/vm/opimpl.py,W_OpImpl
survived,"        def w_get_color(vm: 'SPyVM', w_oparg: W_OpArg) -> W_Str:
            return vm.wrap(w_oparg.color)  # type: ignore
",spy/vm/opimpl.py,W_OpArg
survived,"    def test_definition(self, mock_definition):
        """"""Test the implementation of the definition method""""""
        # Setup the mock return value
        mock_definition.return_value = ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated.""
        
        # Test definition retrieval
        definition = self.oi.definition(""GO:0005634"")
        self.assertEqual(definition, ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated."")
        
        # Verify the mock was called correctly
        mock_definition.assert_called_with(""GO:0005634"")
",tests/test_implementations/test_ols.py,TestOlsImplementation
survived,"    def test_simple_correlation_matrix(self):
        # Simple 2x2 correlation matrix
        data = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        result = nancorrmatrix(data)

        # Perfect correlation since second row is 2x first row
        expected = np.array([[1.0, 1.0], [1.0, 1.0]])
        assert_allclose(result, expected, rtol=1e-10)
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix
deleted,"def create_connection_pool() -> AsyncConnectionPool:
    """"""Create and return a PostgreSQL connection pool with configured settings.""""""
    conn_string = get_postgres_connection_string()
    
    # Create connection pool with settings from config
    pool = AsyncConnectionPool(
        conn_string,
        min_size=settings.POSTGRES_MIN_SIZE,
        max_size=settings.POSTGRES_POOL_SIZE,
        max_idle=settings.POSTGRES_MAX_IDLE,
    )
    
    logger.info(
        f""Created PostgreSQL connection pool: min_size={settings.POSTGRES_MIN_SIZE}, ""
        f""max_size={settings.POSTGRES_POOL_SIZE}, max_idle={settings.POSTGRES_MAX_IDLE}""
    )
    
    return pool
",src/memory/postgres.py,
survived,"    def test_resource() -> str:
        return ""test resource""
",tests/server/middleware/test_middleware.py,
survived,"    async def _apply_middleware(
        self,
        context: MiddlewareContext[Any],
        call_next: Callable[[MiddlewareContext[Any]], Awaitable[Any]],
    ) -> Any:
        """"""Builds and executes the middleware chain.""""""
        chain = call_next
        for mw in reversed(self.middleware):
            chain = partial(mw, call_next=chain)
        return await chain(context)
",src/fastmcp/server/server.py,FastMCP
deleted,"    async def _list_tools(self, apply_middleware: bool = True) -> list[Tool]:
        """"""
        List all available tools.
        """"""

        if (tools := self._cache.get(""tools"")) is self._cache.NOT_FOUND:
            tools: list[Tool] = []

            # iterate such that new mounts overwrite older ones
            for mounted_server in self._mounted_servers:
                try:
                    if apply_middleware:
                        server_tools = (
                            await mounted_server.server._middleware_list_tools()
                        )
                    else:
                        server_tools = await mounted_server.server._list_tools()
                    # Apply prefix to each tool key if prefix exists and is not empty
                    if mounted_server.prefix:
                        for tool in server_tools:
                            tool = tool.with_key(f""{mounted_server.prefix}_{tool.key}"")
                            tools.append(tool)
                    else:
                        tools.extend(server_tools)
                except Exception as e:
                    logger.warning(
                        f""Failed to get tools from mounted server '{mounted_server.prefix}': {e}""
                    )
                    continue
            tools.extend(self._tool_manager.get_tools().values())
            self._cache.set(""tools"", tools)
        return tools
",src/fastmcp/server/server.py,FastMCP
survived,"    def __getattribute__(self, name: str) -> Callable:
        """"""Dynamically create recording methods for any on_* method.""""""
        if name.startswith(""on_""):

            async def record_and_call(
                context: MiddlewareContext, call_next: Callable
            ) -> Any:
                result = await call_next(context)

                self.calls.append(Recording(hook=name, context=context, result=result))

                return result

            return record_and_call

        return super().__getattribute__(name)
",tests/server/middleware/test_middleware.py,RecordingMiddleware
survived,"    def nested_middleware():
        return RecordingMiddleware(name=""nested_middleware"")
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    def test_prompt(x: str) -> str:
        return f""test prompt with {x}""
",tests/server/middleware/test_middleware.py,
survived,"    def test_resource_with_path(x: int) -> str:
        return f""test resource with {x}""
",tests/server/middleware/test_middleware.py,
survived,"        def add(a: int, b: int) -> int:
            return a + b
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
deleted,"    async def _middleware_get_prompt(
        self,
        name: str,
        arguments: dict[str, Any] | None = None,
    ) -> GetPromptResult:
        """"""
        Get a prompt with middleware.
        """"""

        async def _handler(
            context: MiddlewareContext[mcp.types.GetPromptRequestParams],
        ) -> GetPromptResult:
            return await self._get_prompt(
                name=context.message.name,
                arguments=context.message.arguments,
            )

        mw_context = MiddlewareContext(
            message=mcp.types.GetPromptRequestParams(name=name, arguments=arguments),
            source=""client"",
            type=""request"",
            method=""prompts/get"",
            fastmcp_context=fastmcp.server.dependencies.get_context(),
        )
        return await self._apply_middleware(mw_context, _handler)
",src/fastmcp/server/server.py,FastMCP
survived,"def test_compile_simple_query():
    """"""Test compiling a simple Wvlet query""""""
    try:
        compiler = WvletCompiler()
    except NotImplementedError:
        pytest.skip(""Neither native library nor wvlet executable is available"")
    
    # Test a simple query
    query = ""from users select name""
    try:
        sql = compiler.compile(query)
        assert sql is not None
        assert len(sql) > 0
        # The exact SQL output depends on the target, but it should contain 'users'
        assert 'users' in sql.lower()
    except ValueError as e:
        # If compilation fails, it might be due to missing catalog/schema
        # This is acceptable for the test environment
        assert ""Failed to compile"" in str(e)
",sdks/python/tests/test_compiler.py,
survived,"def _process_single_dataset(
    dataset: DataSet,
    source_conn: AtomicConnection,
    target_conn: AtomicConnection,
    export_path: Path,
    target_exp_id: int,
) -> str:
    """"""
    Process a single dataset: export to NetCDF and create metadata-only version
    or copy as-is if export fails.
    
    Returns:
        Status string indicating what was done with the dataset
    """"""
    run_id = dataset.run_id
    
    # Check if dataset is already in target database
    existing_run_id = get_runid_from_guid(target_conn, dataset.guid)
    if existing_run_id is not None:
        log.info(f""Dataset {run_id} (GUID: {dataset.guid}) already exists in target database"")
        return ""already_exists""
    
    # Check if dataset is completed
    if not dataset.completed:
        log.warning(f""Dataset {run_id} is not completed, copying as-is"")
        return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
    
    try:
        # Try to export to NetCDF
        log.info(f""Attempting to export dataset {run_id} to NetCDF"")
        netcdf_path = dataset.export(""netcdf"", path=export_path)
        
        if netcdf_path is None:
            log.warning(f""Failed to export dataset {run_id} to NetCDF, copying as-is"")
            return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
            
        # Load from NetCDF to create metadata-only dataset
        log.info(f""Loading dataset {run_id} from NetCDF to create metadata-only version"")
        netcdf_dataset = load_from_netcdf(netcdf_path)
        
        # Insert metadata-only version into target database
        with atomic(target_conn) as target_conn_atomic:
            _, _, target_table_name = _add_run_to_runs_table(
                netcdf_dataset, target_conn_atomic, target_exp_id
            )
            
            # Note: We deliberately don't populate the results table to keep only metadata
            log.info(f""Successfully created metadata-only version of dataset {run_id}"")
        
        return ""exported""
        
    except Exception as e:
        log.warning(f""Failed to export dataset {run_id} to NetCDF: {e}, copying as-is"")
        return _copy_dataset_as_is(dataset, target_conn, target_exp_id)
",src/qcodes/dataset/database_extract_runs.py,
survived,"def test_export_datasets_default_export_path(simple_dataset):
    """"""Test that default export path is used when none provided""""""
    source_db_path, run_id = simple_dataset
    
    with tempfile.TemporaryDirectory() as temp_dir:
        target_db_path = Path(temp_dir) / ""target.db""
        
        # Run the export function without explicit export path
        result = export_datasets_and_create_metadata_db(
            source_db_path=source_db_path,
            target_db_path=target_db_path,
            # export_path=None  # Use default
        )
        
        # Should still work
        assert isinstance(result, dict)
        assert run_id in result
",tests/dataset/test_export_datasets_and_create_metadata_db.py,
survived,"def export_datasets_and_create_metadata_db(
    source_db_path: str | Path,
    target_db_path: str | Path,
    export_path: str | Path | None = None,
    upgrade_source_db: bool = False,
    upgrade_target_db: bool = False,
) -> dict[int, str]:
    """"""
    Export all datasets from a source database to NetCDF files and create
    a new database file containing only metadata (no raw data) for those exported
    datasets. Datasets that cannot be exported to NetCDF will be transferred
    as-is to the new database.

    This function is useful for reducing the size of database files by offloading
    raw data to NetCDF files while preserving all metadata and structural information.

    Args:
        source_db_path: Path to the source database file
        target_db_path: Path to the target database file. Will be created if it doesn't exist.
        export_path: Optional path where NetCDF files should be exported. If None,
            uses the default export path from QCoDeS configuration.
        upgrade_source_db: If the source DB is found to be in a version that is
            not the newest, should it be upgraded?
        upgrade_target_db: If the target DB is found to be in a version that is
            not the newest, should it be upgraded?

    Returns:
        A dictionary mapping run_id to status ('exported' or 'copied_as_is')

    Raises:
        ValueError: If there are issues with the database files or datasets
    """"""
    # Convert paths to Path objects
    source_db_path = Path(source_db_path)
    target_db_path = Path(target_db_path)
    
    if export_path is None:
        export_path = get_data_export_path()
    else:
        export_path = Path(export_path)
    
    log.info(f""Starting export process from {source_db_path} to {target_db_path}"")
    log.info(f""NetCDF files will be exported to {export_path}"")
    
    # Check database versions
    (s_v, new_v) = get_db_version_and_newest_available_version(source_db_path)
    if s_v < new_v and not upgrade_source_db:
        warn(
            f""Source DB version is {s_v}, but this function needs it to be""
            f"" in version {new_v}. Run this function again with ""
            ""upgrade_source_db=True to auto-upgrade the source DB file.""
        )
        return {}

    if target_db_path.exists():
        (t_v, new_v) = get_db_version_and_newest_available_version(target_db_path)
        if t_v < new_v and not upgrade_target_db:
            warn(
                f""Target DB version is {t_v}, but this function needs it to ""
                f""be in version {new_v}. Run this function again with ""
                ""upgrade_target_db=True to auto-upgrade the target DB file.""
            )
            return {}

    # Create export directory if it doesn't exist
    export_path.mkdir(parents=True, exist_ok=True)
    
    source_conn = connect(source_db_path)
    target_conn = connect(target_db_path)
    
    try:
        # Get all run IDs from the source database
        run_ids = get_runs(source_conn)
        log.info(f""Found {len(run_ids)} datasets to process"")
        
        if not run_ids:
            log.warning(""No datasets found in source database"")
            return {}
        
        # Process datasets by experiment to preserve structure
        result_status = {}
        processed_experiments = {}  # Map source exp_id to target exp_id
        
        for run_id in run_ids:
            try:
                dataset = DataSet(run_id=run_id, conn=source_conn)
                exp_id = dataset.exp_id
                
                # Create experiment in target DB if not already done
                if exp_id not in processed_experiments:
                    exp_attrs = get_experiment_attributes_by_exp_id(source_conn, exp_id)
                    
                    with atomic(target_conn) as target_conn_atomic:
                        target_exp_id = _create_exp_if_needed(
                            target_conn_atomic,
                            exp_attrs[""name""],
                            exp_attrs[""sample_name""],
                            exp_attrs[""format_string""],
                            exp_attrs[""start_time""],
                            exp_attrs[""end_time""],
                        )
                    processed_experiments[exp_id] = target_exp_id
                    log.info(f""Created experiment '{exp_attrs['name']}' in target database"")
                else:
                    target_exp_id = processed_experiments[exp_id]
                
                # Try to export dataset to NetCDF and create metadata-only version
                status = _process_single_dataset(
                    dataset, source_conn, target_conn, export_path, target_exp_id
                )
                result_status[run_id] = status
                
            except Exception as e:
                log.error(f""Failed to process dataset {run_id}: {e}"")
                result_status[run_id] = f""failed: {str(e)}""
        
        log.info(f""Processing complete. Status summary: {result_status}"")
        return result_status
        
    finally:
        source_conn.close()
        target_conn.close()
",src/qcodes/dataset/database_extract_runs.py,
survived,"async def no_sleep(_: float) -> None:
    return None
",tests/test_register_mesh_backoff.py,
survived,"def run_claude(
    prompt: str,
    output_format: str = ""json"",
    allowed_tools: Optional[List[str]] = None,
    cli: str = ""claude"",
) -> str:
    """"""Run Claude Code in headless mode with the given output format.""""""
    cmd = [cli, ""-p"", prompt, ""--output-format"", output_format]
    if allowed_tools:
        cmd.extend([""--allowedTools"", *allowed_tools])
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )
    if result.returncode != 0:
        raise RuntimeError(f""claude failed: {result.stderr}"")
    return result.stdout
",claude_testing_v1.py,
survived,"    def _args(self):
        return argparse.Namespace(
            agents=""A,B"",
            port=123,
            metrics_port=456,
            a2a_port=789,
            cycle=5,
            loglevel=""DEBUG"",
            version=False,
            list_agents=False,
        )
",alpha_factory_v1/tests/test_edge_runner_main.py,EdgeRunnerMainInvokesRun
survived,"    def fail_commit(self, *a, **k):
        raise RuntimeError(""boom"")
",tests/test_self_improver.py,
survived,"    def start_merkle_task(self, *_a, **_kw) -> None:
        pass
",tests/test_self_improver.py,DummyLedger
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}):
        """"""Initialize your AiForce provider with custom settings! ⚙️

        Args:
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
        """"""
        self.api_endpoint = ""https://api.airforce/imagine2""
        self.headers = {
            ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"",
            ""Accept-Language"": ""en-US,en;q=0.5"",
            ""Accept-Encoding"": ""gzip, deflate"",
            ""User-Agent"": agent.random()
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
",webscout/Provider/TTI/aiforce.py,AiForceimager
survived,"    def save(
        self,
        response: List[str],
        name: str = None,
        dir: str = os.getcwd(),
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire images! 💾

        Args:
            response (List[str]): Your image URLs to save
            name (str, optional): Custom name (default: uses prompt)
            dir (str, optional): Where to save (default: current directory)
            filenames_prefix (str, optional): Add prefix to filenames

        Returns:
            List[str]: Where your images were saved
        """"""
        assert isinstance(response, list), f""Response gotta be a list, not {type(response)} 🤔""
        name = self.prompt if name is None else name

        filenames = []
        count = 0

        for img_url in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            try:
                img_response = requests.get(img_url, stream=True, timeout=self.timeout)
                img_response.raise_for_status()

                with open(absolute_path_to_file, ""wb"") as fh:
                    for chunk in img_response.iter_content(chunk_size=8192):
                        fh.write(chunk)

            except requests.exceptions.RequestException as e:
                raise

        return filenames",webscout/Provider/TTI/artbit.py,ArtbitImager
survived,"    def __init__(
        self, 
        timeout: int = 60, 
        proxies: Optional[dict] = None
    ):
        """"""Initialize your ImgSys provider with custom settings

        Examples:
            >>> provider = ImgSys(timeout=30)
            >>> provider = ImgSys(proxies={""http"": ""http://proxy:8080""})

        Args:
            timeout (int): HTTP request timeout in seconds (default: 60)
            proxies (dict, optional): Proxy configuration for requests
        """"""
        self.request_id_endpoint = ""https://imgsys.org/api/initiate""
        self.image_response_endpoint = ""https://imgsys.org/api/get""
        self.image_provider_endpoint = ""https://imgsys.org/api/submit""
        
        self.headers = {
            ""Accept"": ""application/json"",
            ""Content-Type"": ""application/json"",
            ""User-Agent"": agent.random(),
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        if proxies:
            self.session.proxies.update(proxies)
            
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""jpeg""
",webscout/Provider/TTI/imgsys.py,ImgSys
survived,"            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(save_dir, name + count_value + ""."" + self.image_extension)
",webscout/Provider/TTI/aiforce.py,AiForceimager
survived,"    def test_matrix_grad_torch(self):
        klong = KlongInterpreter()
        klong('A::˙[2 2]:^!4')
        klong('B::[2 2]:^!4')
        r = klong('(A ∇ {+/(+/ (A*B)) })')

        A = torch.arange(4, dtype=torch.float64, requires_grad=True).reshape(2,2)
        B = torch.arange(4, dtype=torch.float64).reshape(2,2)
        loss = (A * B).sum()
        loss.backward()
        self.assertTrue(np.allclose(r, A.grad.numpy(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"    def test_array_grad_torch(self):
        klong = KlongInterpreter()
        klong('x::˙!5')
        klong('loss::{+/x*x}')
        r = klong('x ∇ loss')

        x = torch.arange(5, dtype=torch.float64, requires_grad=True)
        loss = (x * x).sum()
        loss.backward()
        self.assertTrue(np.allclose(r, x.grad.numpy(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"def test_with_retry_fail(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    calls = {""n"": 0}

    def func() -> str:
        calls[""n""] += 1
        raise ValueError(""fail"")

    wrapped = retry.with_retry(func, max_tries=2)
    with pytest.raises(ValueError):
        wrapped()
    assert calls[""n""] == 2",tests/test_retry_wrapper.py,
survived,"def test_insight_aggregates_results() -> None:
    _setup_simulations()
    client = _make_client()
    headers = {""Authorization"": ""Bearer test-token""}
    resp = client.post(""/insight"", json={""ids"": [""a"", ""b""]}, headers=headers)
    assert resp.status_code == 200
    assert resp.json() == {""forecast"": [{""year"": 1, ""capability"": 0.5}]}
",tests/test_insight_endpoint.py,
survived,"def test_custom_node_styles():
    """"""Custom node style directives are included in output.""""""
    generator = DiagramGenerator()
    styles = {""AGENT"": ""fill:#fff""}
    diagram = generator.generate(MINIMAL_SPEC, node_styles=styles)

    assert ""style AGENT fill:#fff"" in diagram",tests/ux/test_diagram_generator.py,
survived,"    def test_get_agent_health_queue(self):
        from alpha_factory_v1.backend.agents import _HEALTH_Q

        class WrapAgent(AgentBase):
            NAME = ""wrap""

            async def step(self):
                return ""ok""

        register_agent(AgentMetadata(name=WrapAgent.NAME, cls=WrapAgent))

        while not _HEALTH_Q.empty():
            _HEALTH_Q.get()

        agent = get_agent(WrapAgent.NAME)
        asyncio.run(agent.step())
        name, latency, ok = _HEALTH_Q.get(timeout=1)
        self.assertEqual(name, WrapAgent.NAME)
        self.assertTrue(ok)
        self.assertIsInstance(latency, float)
",tests/test_agents_registry.py,TestAgentRegistryFunctions
survived,"    def test_stub_producer(self):
        class Stub:
            def __init__(self, bootstrap_servers=None, value_serializer=None, linger_ms=None):
                self.args = (bootstrap_servers, value_serializer, linger_ms)
        os.environ[""ALPHA_KAFKA_BROKER""] = ""a:1,b:2 ,""
        orig = base_mod.KafkaProducer
        base_mod.KafkaProducer = Stub
        prod = base_mod._kafka_producer()
        self.assertIsInstance(prod, Stub)
        self.assertEqual(prod.args[0], [""a:1"", ""b:2""])
        base_mod.KafkaProducer = orig
        os.environ.pop(""ALPHA_KAFKA_BROKER"", None)
",tests/test_base_helpers.py,TestKafkaProducer
survived,"    def test_lower_version_ignored(self):
        class AgentV1(AgentBase):
            NAME = ""dup""
            VERSION = ""1.0""

            async def step(self):
                return None

        class AgentOld(AgentBase):
            NAME = ""dup""
            VERSION = ""0.9""

            async def step(self):
                return None

        register_agent(AgentMetadata(name=""dup"", cls=AgentV1, version=""1.0""))
        register_agent(AgentMetadata(name=""dup"", cls=AgentOld, version=""0.9""))

        self.assertIs(AGENT_REGISTRY[""dup""].cls, AgentV1)
        self.assertEqual(AGENT_REGISTRY[""dup""].version, ""1.0"")
",tests/test_agents_registry.py,TestVersionOverride
survived,"    def test_readmes_min_lines(self) -> None:
        """"""``validate_demos`` succeeds for shipped demos.""""""
        exit_code = validate_demos.main(validate_demos.DEFAULT_DIR, min_lines=3)
        self.assertEqual(exit_code, 0)
",tests/test_demos.py,TestDemos
survived,"    def __init__(self) -> None:
        self.committed = False
",tests/test_alpha_agi_business_3_v1.py,DummyModel
survived,"    def commit(self, weight_update: dict[str, object]) -> None:  # type: ignore[override]
        self.committed = True
        super().commit(weight_update)
",tests/test_alpha_agi_business_3_v1.py,DummyModel
survived,"    def test_stream_macro_events_offline(self):
        async def get_one():
            it = data_feeds.stream_macro_events(live=False)
            return await anext(it)

        evt = asyncio.run(get_one())
        self.assertIn(""fed_speech"", evt)
        self.assertIn(""yield_10y"", evt)
        self.assertIn(""yield_3m"", evt)
        self.assertIn(""stable_flow"", evt)
        self.assertIn(""es_settle"", evt)
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"    def load_weights(self, path: str) -> None:
        """"""Load updated model weights from *path*.

        Subclasses may override this to implement hot-swapping of
        learning artefacts.  The default implementation simply stores the
        path for later use.
        """"""
        self._weights_path = path
",alpha_factory_v1/backend/agents/base.py,AgentBase
survived,"async def get_order_item_order(
    order_item_id: int, ctx: EnrichContext
) -> Optional[""OrderEnrichModel""]:
    """"""Get the order for a specific order item.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        item = await session.get(OrderItem, order_item_id)
        if not item:
            return None

        # Load the order
        await session.refresh(item, [""order""])
        order = item.order

        return OrderEnrichModel(
            id=order.id,
            order_number=order.order_number,
            user_id=order.user_id,
            status=order.status,
            total_amount=order.total_amount,
            created_at=order.created_at,
            updated_at=order.updated_at,
            shipping_address=order.shipping_address,
            notes=order.notes,
        )
",examples/sqlalchemy_shop/app.py,
survived,"    def test_model_with_no_docstring(self):
        """"""Test model without docstring gets a default one.""""""

        class Base(DeclarativeBase):
            pass

        class NoDoc(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""no_doc""
            id: Mapped[int] = mapped_column(primary_key=True)

        NoDocEnrichModel = NoDoc.__enrich_model__()
        assert NoDocEnrichModel.__doc__ == ""NoDoc entity""
",tests/test_sqlalchemy_integration.py,TestEdgeCases
survived,"    def __enter__(self):
        if resource:
            resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_sec, self.cpu_sec))
            resource.setrlimit(resource.RLIMIT_AS, (self.mem_mb*1024*1024, self.mem_mb*1024*1024))
        return self
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,SafeExec
survived,"            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] ← %s"", cls_name, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Stub
survived,"            def _safe_call(self,prompt:str,timeout:int=15)->str:
                with concurrent.futures.ThreadPoolExecutor() as ex:
                    fut=ex.submit(lambda:openai.ChatCompletion.create(
                        model=""gpt-4o-mini"",
                        messages=[{""role"":""user"",""content"":prompt}],
                        timeout=timeout))
                    return fut.result().choices[0].message.content
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,LLMPlanner
survived,"    def run(self, prompt: str, context: Optional[Iterable[Dict[str,str]]]=None, **kw) -> Dict[str,Any]:
        ctx: List[Dict[str,str]] = list(context or [])
        ctx.append({""role"":""user"", ""content"": prompt})
        t0 = time.perf_counter()
        output = self.lm.chat(ctx, **kw)
        latency = time.perf_counter()-t0
        tokens_in = _str_tkn(prompt)
        tokens_out = _str_tkn(output)
        cost = self._estimate_cost(tokens_in,tokens_out)
        carbon = cost*0.00015 # placeholder multiplier (avg kgCO2 per $ cloud)
        risk = self._risk_assess(prompt, output)
        metrics = dict(latency=latency, cost=cost, carbon=carbon, risk=risk)
        score = self.objectives.score(metrics)
        self.tracer.log(""run"", prompt=prompt[:120], response=output[:120], metrics=metrics, score=score)
        return {""response"": output, ""metrics"": metrics, ""score"": score}
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,Agent
survived,"    def publish(cls, topic: str, msg: dict):
        with cls._lock:
            for cb in list(cls._subs.get(topic, [])):
                try:
                    cb(msg)
                except Exception as exc:  # pragma: no cover
                    LOG.error(""[A2A] handler error on %s: %s"", topic, exc)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,A2ABus
survived,"def _load_cfg() -> Config:
    cfg = Config()
    # yaml config file optional
    if yaml:
        for p in (Path.cwd() / ""config.yaml"", Path.cwd() / ""alpha_asi.yaml""):
            if p.exists():
                try:
                    cfg.update(**yaml.safe_load(p.read_text()))
                    LOG.info(""Loaded config from %s"", p)
                except Exception as e:
                    LOG.warning(""Failed to parse %s: %s"", p, e)
    # env overrides
    for k in cfg.__dict__.keys():
        env_key = ""ALPHA_ASI_"" + k.upper()
        if env_key in os.environ:
            val = os.environ[env_key]
            try:
                val = type(getattr(cfg, k))(val)
            except Exception:
                pass
            setattr(cfg, k, val)
    return cfg
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def acquire(self, cost: float = 1.0):
        while True:
            now = time.perf_counter()
            elapsed = now - self._last
            self._last = now
            self._allow = min(self._tps, self._allow + elapsed * self._tps)
            if self._allow >= cost:
                self._allow -= cost
                return
            sleep = (cost - self._allow) / self._tps + 1e-3
            time.sleep(sleep)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,RateLimiter
survived,"def _main():
    p=argparse.ArgumentParser(prog=""alpha_asi_world_model_demo"")
    p.add_argument(""--demo"",action=""store_true"")
    p.add_argument(""--emit-docker"",action=""store_true"")
    p.add_argument(""--emit-helm"",action=""store_true"")
    p.add_argument(""--emit-notebook"",action=""store_true"")
    p.add_argument(""--host"",default=""127.0.0.1"")
    p.add_argument(""--port"",type=int,default=7860)
    args=p.parse_args()
    if args.emit_docker: emit_docker()
    elif args.emit_helm: emit_helm()
    elif args.emit_notebook: emit_notebook()
    elif args.demo:
        uvicorn.run(""alpha_asi_world_model_demo:app"",host=args.host,port=args.port,log_level=""info"")
    else: p.print_help()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"async def ws_endpoint(sock:WebSocket):
    await sock.accept(); q:List[dict]=[]
    A2ABus.subscribe(""ui"", lambda m:q.append(m))
    try:
        while True:
            if q: await sock.send_text(json.dumps(q.pop(0)))
            await asyncio.sleep(0.1)
    except Exception: pass
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def __init__(self, cpu_sec:int=2, mem_mb:int=128):
        self.cpu_sec = cpu_sec
        self.mem_mb = mem_mb
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,SafeExec
survived,"    def idx():
        return VIEW_HTML
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,
survived,"    def score(self, metrics: Dict[str,float]) -> float:
        return (
            self.latency * (1/ (1+metrics.get(""latency"",0))) +
            self.cost    * (1/ (1+metrics.get(""cost"",0))) +
            self.carbon  * (1/ (1+metrics.get(""carbon"",0))) +
            self.risk    * (1- metrics.get(""risk"",0))
        )
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,ObjectiveWeights
survived,"        def handle(self, _msg):
            LOG.debug(""[Fallback%d] ← %s"", idx, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Fallback
survived,"    def run(self, prompt: str, context: Optional[Iterable[Dict[str,str]]]=None, **kw) -> Dict[str,Any]:
        ctx: List[Dict[str,str]] = list(context or [])
        ctx.append({""role"":""user"", ""content"": prompt})
        t0 = time.perf_counter()
        output = self.lm.chat(ctx, **kw)
        latency = time.perf_counter()-t0
        tokens_in = _str_tkn(prompt)
        tokens_out = _str_tkn(output)
        cost = self._estimate_cost(tokens_in,tokens_out)
        carbon = cost*0.00015 # placeholder multiplier (avg kgCO2 per $ cloud)
        risk = self._risk_assess(prompt, output)
        metrics = dict(latency=latency, cost=cost, carbon=carbon, risk=risk)
        score = self.objectives.score(metrics)
        self.tracer.log(""run"", prompt=prompt[:120], response=output[:120], metrics=metrics, score=score)
        return {""response"": output, ""metrics"": metrics, ""score"": score}
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent
survived,"    def __init__(self):
        super().__init__()
        self.sub1 = DummySub()
        self.sub2 = DummySub()
",tests/test_multi_contributor.py,DummyModel
survived,"def test_commit_roundtrip(tmp_path: Path):
    data = {""input_data"": [1, 2, 3, 4]}
    path = tmp_path / ""acts.json""
    with open(path, ""w"") as f:
        json.dump(data, f)

    commit = commit_activations(str(path))
    assert verify_commitment(str(path), commit)",tests/test_poly_commit.py,
survived,"    def __call__(self, text, return_tensors=None):
        return {""input_ids"": torch.tensor([[1]])}
",tests/test_multi_contributor.py,DummyTokenizer
survived,"def commit_activations(activations_path: str, challenge: int = CHALLENGE) -> str:
    """"""Return polynomial commitment of activations stored in JSON file.""""""
    with open(activations_path, ""r"") as f:
        data = json.load(f)
    arr = np.array(data[""input_data""], dtype=np.int64).reshape(-1)
    val = _poly_eval(arr, challenge, PRIME)
    return hex(val)
",src/zklora/polynomial_commit.py,
survived,"    def init_request(self):
        return [self.name]
",tests/test_multi_contributor.py,FakeComm
survived,"def test_log_to_section_training():
    """"""Test log_to_section writes to the training file""""""
    with tempfile.TemporaryDirectory() as tmpdir:
        logger = Logger(""test_module"", base_dir=tmpdir)
        data = {""foo"": ""bar""}
        logger.log_to_section(data, section=""training"")

        with open(logger.training_file, ""r"", encoding=""utf-8"") as f:
            lines = f.readlines()
            assert len(lines) == 1
            entry = json.loads(lines[0])
            assert entry[""foo""] == ""bar""
            assert isinstance(entry[""timestamp""], float)",tests/test_logger.py,
survived,"def test_lm_param_override_and_restore():
    """"""Test that lm_params temporarily override LM settings""""""
    # Reset instances so we get a fresh caller with patched LM
    from simpledspy.module_caller import BaseCaller, Predict
    BaseCaller._instances = {}

    with patch('simpledspy.module_caller.dspy.LM') as MockLM:
        mock_lm = MockLM.return_value
        mock_lm.temperature = 0.2
        captured = {}

        class MockModule(dspy.Module):
            def forward(self, **_):
                captured['temp'] = mock_lm.temperature
                return dspy.Prediction(result='ok')

        with patch('simpledspy.module_caller.Predict._create_module', return_value=MockModule()):
            caller = Predict()
            result = caller('text', inputs=['text'], outputs=['result'], lm_params={'temperature': 0.9})
            assert result == 'ok'
            assert captured['temp'] == 0.9
            assert caller.lm.temperature == 0.2",tests/test_predict.py,
survived,"            def _invoke(self, msgs, temperature, max_tokens, stream, stop):
                answer = ""[offline] "" + msgs[-1][""content""][:400]
                if stream:
                    for tok in answer.split():
                        yield tok + "" ""
                else:
                    return answer
",alpha_factory_v1/backend/utils/llm_provider.py,_Stub
survived,"def test_load_corpus(tmp_path):
    corpus_file = tmp_path / ""corpus.csv""
    with open(corpus_file, ""w"", encoding=""utf-8"", newline="""") as f:
        writer = csv.writer(f)
        writer.writerow([""0 hello"", ""1 world""])
        writer.writerow([""1 world"", ""0 hello ""])

    corpus = sampler.load_corpus(str(corpus_file))

    assert corpus == [[0, 1], [1, 0]]",tests/test_sampler_io.py,
survived,"    def __init__(self, settings: config.Settings) -> None:
        self.settings = settings
        self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_codegen_safety.py,DummyBus
survived,"def test_allows_normal_message() -> None:
    agent = _make_agent()
    env = messaging.Envelope(
        sender=""market"",
        recipient=""safety"",
        payload={""analysis"": ""hold position""},
        ts=0.0,
    )
    asyncio.run(agent.handle(env))
    assert agent.bus.published[-1][1].payload[""status""] == ""ok""",tests/test_codegen_safety.py,
survived,"def _venv_pip(venv: Path) -> Path:
    """"""Return the path to the pip executable inside *venv*.""""""
    if os.name == ""nt"":
        return venv / ""Scripts"" / ""pip.exe""
    return venv / ""bin"" / ""pip""
",alpha_factory_v1/quickstart.py,
survived,"                def locate_file(self, path):
                    return init_file
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest.Dist
survived,"    def test_load_real_package_when_available(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            pkg = Path(tmpdir) / ""requests""
            pkg.mkdir()
            init_file = pkg / ""__init__.py""
            init_file.write_text(""value = 42\n"")

            class Dist:
                def locate_file(self, path):
                    return init_file

            original = im.distribution
            def fake_distribution(name):
                self.assertEqual(name, ""requests"")
                return Dist()
            im.distribution = fake_distribution
            sys.path.insert(0, tmpdir)
            try:
                mod = importlib.import_module(""requests"")
                self.assertEqual(getattr(mod, ""value"", None), 42)
                self.assertEqual(Path(mod.__file__).resolve(), init_file.resolve())
            finally:
                sys.path.remove(tmpdir)
                im.distribution = original
                sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest
survived,"    def __init__(self):
        self.next_ts = 0
        self.period = 1
        self.last_beat = time.time()
        self.inst = SimpleNamespace()
        self.spec = None
",alpha_factory_v1/tests/test_orchestrator_rest.py,DummyRunner
survived,"    def check_chart_file(self, chart_path: Path):
        self.assertTrue(chart_path.is_file(), f""{chart_path} missing"")
        text = chart_path.read_text()
        for key in [""apiVersion:"", ""name:"", ""version:"", ""appVersion:""]:
            self.assertIn(key, text, f""{key} not found in {chart_path}"")
",alpha_factory_v1/tests/test_helm_charts.py,HelmChartTests
survived,"    def test_import_success(self):
        with NamedTemporaryFile('w', delete=False) as tmp:
            json.dump({'title': 't'}, tmp)
            tmp_path = tmp.name
        with mock.patch.dict(os.environ, {'GRAFANA_TOKEN': 'tok', 'GRAFANA_HOST': 'http://h'}, clear=True):
            with mock.patch.object(sys, 'argv', ['script', tmp_path]):
                with mock.patch('alpha_factory_v1.scripts.import_dashboard.post') as post:
                    post.return_value = mock.Mock(raise_for_status=lambda: None)
                    import_dashboard.main()
                    post.assert_called_once()
                    args, kwargs = post.call_args
                    self.assertEqual(args[0], 'http://h/api/dashboards/import')
                    self.assertEqual(kwargs['headers']['Authorization'], 'Bearer tok')
",alpha_factory_v1/tests/test_import_dashboard.py,ImportDashboardTest
survived,"def example7():
    x = fetch_()
    if x == ""cheese"":
        None
    else:
        if someCondition():
            None
",tests/rosetta/transpiler/Python/conditional-structures-7.py,
survived,"def timeStr(sec):
    wks = sec // 604800
    sec = sec % 604800
    ds = sec // 86400
    sec = sec % 86400
    hrs = sec // 3600
    sec = sec % 3600
    mins = sec // 60
    sec = sec % 60
    res = """"
    comma = False
    if wks != 0:
        res = res + str(wks) + "" wk""
        comma = True
    if ds != 0:
        if comma:
            res = res + "", ""
        res = res + str(ds) + "" d""
        comma = True
    if hrs != 0:
        if comma:
            res = res + "", ""
        res = res + str(hrs) + "" hr""
        comma = True
    if mins != 0:
        if comma:
            res = res + "", ""
        res = res + str(mins) + "" min""
        comma = True
    if sec != 0:
        if comma:
            res = res + "", ""
        res = res + str(sec) + "" sec""
    return res
",tests/rosetta/transpiler/Python/convert-seconds-to-compound-duration.py,
survived,"def main():
    print(""For primes < 1 million:\n"")
    for dir in [""ascending"", ""descending""]:
        longestSeq(dir)
",tests/rosetta/transpiler/Python/consecutive-primes-with-ascending-or-descending-differences.py,
survived,"def randN(n):
    global seed
    seed = (seed * 1664525 + 1013904223) % 2147483647
    return seed % n
",tests/rosetta/transpiler/Python/conways-game-of-life.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/concurrent-computing-2.py,
survived,"def fetchSomething():
    return 0
",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"    def _act(x: torch.Tensor) -> torch.Tensor:
        nonlocal calls
        calls += 1
        return x
",tests/test_evo_net_activation.py,
survived,"    def test_main_offline_skips_network(self) -> None:
        with mock.patch.multiple(
            preflight,
            check_python=lambda: True,
            check_cmd=lambda cmd: True,
            check_docker_daemon=lambda: True,
            check_docker_compose=lambda: True,
            check_pkg=lambda pkg: True,
            ensure_dir=lambda p: None,
            banner=lambda *a, **k: None,
        ):
            with mock.patch.object(preflight, ""check_network"") as cn:
                preflight.main([""--offline""])
                cn.assert_not_called()
",alpha_factory_v1/tests/test_preflight.py,PreflightTest
survived,"    def _replace(match: re.Match) -> str:
        for iast, slp in _IAST_TO_SLP1:
            if match.group(0) == iast:
                return slp
        return match.group(0)
",atroposlib/envs/reward_fns/chandas_meter_reward.py,
survived,"    def test_concurrent_writes(self) -> None:
        from alpha_factory_v1.demos.cross_industry_alpha_factory import (
            cross_alpha_discovery_stub as stub,
        )

        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""thread_log.json""

            def worker(seed: int) -> None:
                stub.discover_alpha(num=1, seed=seed, ledger=ledger, model=""gpt-4o-mini"")

            threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]
            for t in threads:
                t.start()
            for t in threads:
                t.join()

            data = json.loads(ledger.read_text())
            self.assertEqual(len(data), 5)
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"def available_scenarios(directory: str | Path | None = None) -> list[str]:
    """"""Return the list of available scenario names.""""""

    dir_path = Path(directory or BASE_DIR)
    names = {p.stem for p in dir_path.glob(""*.yaml"")}
    names.update(p.stem for p in dir_path.glob(""*.yml""))
    return sorted(names)
",src/simulation/replay.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/dataset_where_filter.py,Person
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/cast_struct.py,Todo
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/outer_join.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/json_builtin.py,Auto1
survived,"def _find_gitignore_files_for_dir(dir_path: Path, root_path: Path) -> List[Path]:
    """"""Finds all .gitignore files from root_path down to dir_path.""""""
    gitignore_files = []
    current = dir_path.resolve()
    root = root_path.resolve()

    if not (current == root or root in current.parents):
         logger.warning(f""Directory {current} is not within the root {root}. Cannot find gitignore files."")
         return []

    paths_to_check = []
    temp_path = current
    while temp_path >= root:
        paths_to_check.append(temp_path)
        if temp_path == root:
            break
        parent = temp_path.parent
        if parent == temp_path:
            break
        temp_path = parent

    for p in reversed(paths_to_check):
        ignore_file = p / GITIGNORE_FILENAME
        if ignore_file.is_file():
            gitignore_files.append(ignore_file)
            logger.debug(f""Found gitignore file: {ignore_file}"")

    return gitignore_files
",jinni/utils.py,
survived,"def test_colon_replacement():
    assert remove_chars('Title: Subtitle') == 'Title - Subtitle'
    assert remove_chars('A:B') == 'A - B'
    assert remove_chars('Multi:part:colon') == 'Multi - part - colon'
    assert remove_chars('Title:Subtitle : Another') == 'Title - Subtitle - Another'
",tests/test_remove_chars.py,
deleted,"def test_question_mark_meridiem(now):
    assert timefhuman(
        'Are you free this Wed at 3p? Or maybe Fri at 5p?',
        tfhConfig(now=now)
    ) == [
        datetime.datetime(2018, 8, 8, 15, 0),
        datetime.datetime(2018, 8, 10, 17, 0)
    ]",tests/test_e2e.py,
survived,"            def __call__(self, *_a, **_k):
                return """"
",tests/test_macro_adk_integration.py,_OpenAI
survived,"            def __init__(self, *a, **kw):
                self.name = kw.get(""name"", ""agent"")
",tests/test_macro_adk_integration.py,_Agent
survived,"def non_network(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Disable outbound networking for the duration of a test.""""""

    def _blocked(*_a: Any, **_k: Any) -> None:
        raise OSError(""network disabled"")

    monkeypatch.setattr(socket.socket, ""connect"", _blocked)
    yield
",tests/conftest.py,
survived,"        def parse_dataset_iter(it: ast.expr) -> tuple[str, str | None, str | None, str | None]:
            sort = None
            skip = None
            take = None
            cur = it

            # handle take
            if isinstance(cur, ast.Subscript) and isinstance(cur.slice, ast.Slice):
                sl = cur.slice
                if (
                    sl.lower is None
                    and sl.step is None
                    and isinstance(sl.upper, ast.Call)
                    and getattr(sl.upper.func, ""id"", None) == ""max""
                    and len(sl.upper.args) == 2
                    and isinstance(sl.upper.args[1], ast.Constant)
                    and sl.upper.args[1].value == 0
                ):
                    take = self.convert_expr(sl.upper.args[0])
                    cur = cur.value

            # handle skip
            if isinstance(cur, ast.Subscript) and isinstance(cur.slice, ast.Slice):
                sl = cur.slice
                if (
                    sl.upper is None
                    and sl.step is None
                    and isinstance(sl.lower, ast.Call)
                    and getattr(sl.lower.func, ""id"", None) == ""max""
                    and len(sl.lower.args) == 2
                    and isinstance(sl.lower.args[1], ast.Constant)
                    and sl.lower.args[1].value == 0
                ):
                    skip = self.convert_expr(sl.lower.args[0])
                    cur = cur.value

            # handle sort
            if isinstance(cur, ast.Call) and isinstance(cur.func, ast.Name) and cur.func.id == ""sorted"":
                if cur.keywords:
                    for kw in cur.keywords:
                        if kw.arg == ""key"" and isinstance(kw.value, ast.Lambda):
                            body = kw.value.body
                            if (
                                isinstance(body, ast.Call)
                                and isinstance(body.func, ast.Name)
                                and body.func.id == ""_sort_key""
                                and body.args
                            ):
                                sort = self.convert_expr(body.args[0])
                            else:
                                sort = self.convert_expr(body)
                if cur.args:
                    cur = cur.args[0]

            # remove trivial list comp wrappers
            if (
                isinstance(cur, ast.ListComp)
                and len(cur.generators) == 1
                and isinstance(cur.generators[0].target, ast.Name)
                and isinstance(cur.elt, ast.Name)
                and cur.elt.id == cur.generators[0].target.id
                and not cur.generators[0].ifs
            ):
                cur = cur.generators[0].iter

            return self.convert_expr(cur), sort, skip, take
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def visit_If(self, node: ast.If) -> None:
        test = self.convert_expr(node.test)
        self.emit(f""if {test} {{"")
        self.indent += 1
        for stmt in node.body:
            self.visit(stmt)
        self.indent -= 1
        if node.orelse:
            self.emit(""} else {"")
            self.indent += 1
            for stmt in node.orelse:
                self.visit(stmt)
            self.indent -= 1
            self.emit(""}"")
        else:
            self.emit(""}"")
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def __init__(self, src: str):
        self.lines: list[str] = []
        self.indent = 0
        self.src_lines = src.splitlines()
        self.dataclasses: set[str] = set()
        self.seen_assigns: set[str] = set()
        self.assign_values: dict[str, str] = {}
        self.current_callable: tuple[list[str], str] | None = None
        self.structs: dict[str, tuple[list[tuple[str, str]], list[ast.FunctionDef]]] = {}
        self.unions: dict[str, list[tuple[str, list[tuple[str, str]]]]] = {}
        self.name_map = {""_next"": ""next""}
",tools/any2mochi/py/py2mochi.py,Converter
deleted,"def get_tool_stats() -> dict[str, dict[str, int]]:
    with _lock:
        return {name: entry.to_dict() for name, entry in _tool_stats.items()}
",src/serena/analytics.py,
survived,"    def Markdown(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyBlocks
survived,"        def __init__(self, *a, **kw):
            agent_args.update(kw)
",tests/test_selfheal_env.py,FakeAgent
survived,"    def __exit__(self, exc_type, exc, tb):
        pass
",tests/test_selfheal_env.py,DummyBlocks
survived,"            async def ingest_loop() -> None:
                async for evt in demo.experience_stream():
                    await queue.put(evt)
                    break
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def test_policy_uses_tools(self) -> None:
        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = MagicMock()

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Tool = _tool

        with patch.dict(sys.modules, {""openai_agents"": stub}):
            sys.modules.pop(
                ""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge"",
                None,
            )
            mod = importlib.import_module(""alpha_factory_v1.demos.cross_industry_alpha_factory.openai_agents_bridge"")
            agent = mod.CrossIndustryAgent()

            with (
                patch.object(mod, ""discover"", new=AsyncMock(return_value=""disc"")),
                patch.object(mod, ""recent_log"", new=AsyncMock(return_value=""recent"")),
                patch.object(mod, ""list_samples"", new=AsyncMock(return_value=""samples"")),
            ):
                result = asyncio.run(agent.policy({""action"": ""discover""}, None))
                self.assertEqual(result, ""disc"")

                result = asyncio.run(agent.policy({""action"": ""recent""}, None))
                self.assertEqual(result, ""recent"")

                result = asyncio.run(agent.policy({}, None))
                self.assertEqual(result, ""samples"")
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime
survived,"def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""OpenAI Agents bridge for the α‑AGI Insight demo"")
    parser.add_argument(""--episodes"", type=int, default=5, help=""Search episodes when offline"")
    parser.add_argument(""--target"", type=int, default=3, help=""Target sector index when offline"")
    parser.add_argument(""--model"", type=str, help=""Model name override"")
    parser.add_argument(
        ""--rewriter"",
        choices=[""random"", ""openai"", ""anthropic""],
        help=""Rewrite strategy"",
    )
    parser.add_argument(""--sectors"", type=str, help=""Comma-separated sector names"")
    parser.add_argument(
        ""--enable-adk"",
        action=""store_true"",
        help=""Enable the Google ADK gateway"",
    )
    parser.add_argument(
        ""--verify-env"",
        action=""store_true"",
        help=""Check runtime dependencies before launching"",
    )
    args = parser.parse_args(argv)

    if args.verify_env:
        verify_environment()

    if args.enable_adk:
        os.environ.setdefault(""ALPHA_FACTORY_ENABLE_ADK"", ""true"")

    _run_runtime(args.episodes, args.target, args.model, args.rewriter)
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,
survived,"    def freeze(self) -> None:
        self._frozen = True
",weave/trace/weave_client.py,AttributesDict
survived,"    def __delitem__(self, key: Any) -> None:
        if self.__dict__.get(""_frozen"", False):
            raise TypeError(""Cannot modify attributes after call start"")
        super().__delitem__(key)
",weave/trace/weave_client.py,AttributesDict
survived,"def _parse_file(path: Path) -> Iterable[ArchiveEntry]:
    """"""Yield archive entries from ``path``.""""""
    for line in path.read_text(encoding=""utf-8"").splitlines():
        if not line.strip():
            continue
        try:
            rec = json.loads(line)
        except Exception:  # noqa: BLE001 - skip invalid lines
            continue
        yield ArchiveEntry(
            hash=rec[""hash""],
            parent=rec.get(""parent""),
            score=float(rec.get(""score"", 0.0)),
            novelty=float(rec.get(""novelty"", 0.0)),
            is_live=bool(rec.get(""is_live"", True)),
            ts=float(rec.get(""ts"", 0.0)),
        )
",alpha_factory_v1/core/tools/dgm_import.py,
survived,"def test_template_metadata_valid() -> None:
    meta = TemplateMetadata(
        slug=""basic-chat"",
        title=""Basic Chat Bot"",
        description=""Minimal conversational agent"",
        category=TemplateCategory.CONVERSATION,
        subcategory=""qa"",
        complexity=TemplateComplexity.BASIC,
        tags=[""chat""],
    )
    assert meta.slug == ""basic-chat""
    assert meta.category is TemplateCategory.CONVERSATION
    assert meta.complexity is TemplateComplexity.BASIC
    assert meta.tags == [""chat""]
",tests/test_template_schema.py,
survived,"def test_template_metadata_invalid_category() -> None:
    try:
        TemplateMetadata(
            slug=""bad"",
            title=""Bad"",
            description=""Bad"",
            category=""invalid"",  # type: ignore[arg-type]
            subcategory=""x"",
            complexity=TemplateComplexity.BASIC,
        )
    except ValidationError:
        pass
    else:  # pragma: no cover - should not succeed
        assert False, ""ValidationError not raised""",tests/test_template_schema.py,
survived,"def load_vocab(file_name):
    with open(file_name, 'rb') as f:
        vocab = []
        reader = csv.reader(f)
        for row in reader:
            idx, word = row
            stripped = word.strip()
            vocab.append(stripped)
        return vocab
",src/hlda/sampler.py,
survived,"def _run_script(tmp_path: Path) -> dict:
    script = Path(""alpha_factory_v1/demos/cross_industry_alpha_factory/deploy_alpha_factory_cross_industry_demo.sh"")
    compose = tmp_path / ""docker-compose.yml""
    compose.write_text(""services: {}\n"")

    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(
        """"""#!/usr/bin/env bash
if [ ""$1"" = ""info"" ] || [ ""$1"" = ""compose"" ]; then exit 0; fi
if [ ""$1"" = ""run"" ]; then
  while [ ""$1"" != ""ghcr.io/mikefarah/yq"" ] && [ $# -gt 0 ]; do shift; done
  shift
  yq ""$@""
  exit $?
fi
exit 0
""""""
    )
    docker_stub.chmod(0o755)

    for cmd in [""git"", ""curl"", ""openssl"", ""ssh-keygen"", ""cosign"", ""rekor"", ""k6"", ""locust""]:
        _write_executable(bin_dir / cmd, ""#!/usr/bin/env bash\nexit 0\n"")

    env = os.environ.copy()
    env.update(
        {
            ""PATH"": f""{bin_dir}:{env.get('PATH', '')}"",
            ""COMPOSE_FILE"": str(compose),
            ""PROJECT_DIR"": str(tmp_path),
            ""SKIP_BENCH"": ""1"",
        }
    )

    subprocess.run([""bash"", str(script)], check=True, env=env, timeout=10)
    first = yaml.safe_load(compose.read_text())
    subprocess.run([""bash"", str(script)], check=True, env=env, timeout=10)
    second = yaml.safe_load(compose.read_text())
    return {""first"": first, ""second"": second}
",tests/test_cross_industry_patch.py,
survived,"def test_devicon_xdg_trailing_slash(monkeypatch):
    monkeypatch.setenv('XDG_PICTURES_DIR', '/tmp/Pictures/')
    devicons = reload_devicons('es')
    file = MockFile('Pictures', is_directory=True)
    assert devicons.devicon(file) == ''",tests/test_devicons.py,
survived,"def test_notebook_conversion(tmp_path):
    """"""With the notebook flag enabled code cells are converted.""""""
    nb = tmp_path / ""t.ipynb""
    _write_notebook(nb)
    result = _fstringify_file(str(nb), State(process_notebooks=True))
    assert result and result.n_changes == 1
    with open(nb) as fh:
        data = json.load(fh)
    assert ""f'{1}'"" in """".join(data[""cells""][0][""source""])",test/integration/test_api.py,
survived,"def set_language():
    lang = request.form.get(""language"")
    if lang in app.config[""BABEL_SUPPORTED_LOCALES""]:
        session[""lang""] = lang
    return redirect(request.referrer or url_for(""index""))
",app.py,
survived,"    def padded(start, stop):
        pos = hax.arange(Pos, dtype=jnp.int32, start=start)
        return hax.where(pos >= stop, -1, pos)
",tests/test_attention.py,
survived,"    def test_qcli_and_qclic(self):
        klong = KlongInterpreter()
        klong('c::.qcli(1234)')
        proxy = klong('c')
        self.assertTrue(proxy.connection.is_open())
        r = klong('c(""1+1"")')
        self.assertEqual(r, 'EXEC: 1+1')
        self.assertEqual(proxy.connection.conn.queries[-1], ' 1+1')
        klong('.qclic(c)')
        self.assertFalse(proxy.connection.is_open())
",tests/test_sys_fn_kdb.py,TestKdbIPC
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""Return order as plain dictionary.""""""
        return asdict(self)
",alpha_factory_v1/backend/trade_broker.py,Order
survived,"    def test_broadcast_error(self) -> None:
        led = self._ledger()
        env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
        led.log(env)
        captured, DummyClient, DummyTx, DummyInstr, DummyPk = self._dummy_classes(True)
        with (
            mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
            mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
            mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
            mock.patch.object(insight_logging, ""_log"") as log,
        ):
            asyncio.run(led.broadcast_merkle_root())
        log.warning.assert_called()  # ensure warning emitted",tests/test_merkle_broadcast.py,TestMerkleBroadcast
survived,"    def geompath_route():
        args = request.args
        data = rs.geompath(
            lat1=float(args[""lat1""]),
            lon1=float(args[""lon1""]),
            lat2=float(args[""lat2""]),
            lon2=float(args[""lon2""]),
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"    def __init__(self) -> None:
        self.dim = _DIM
        self.index = faiss.IndexFlatIP(self.dim) if faiss else None
        self.mean = np.zeros(self.dim, dtype=""float32"")
        self.count = 0
",src/evaluators/novelty.py,NoveltyIndex
survived,"def embed(text: str) -> np.ndarray:
    """"""Return the MiniLM embedding for ``text``.""""""
    model = _get_model()
    vec = model.encode([text], normalize_embeddings=True)
    return np.asarray(vec, dtype=""float32"")
",src/evaluators/novelty.py,
survived,"def test_novelty_divergence_for_elites() -> None:
    def fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    idx = NoveltyIndex()
    idx.add(""0.0,0.0"")

    pop = mats.run_evolution(
        fn,
        2,
        population_size=6,
        generations=1,
        seed=1,
        novelty_index=idx,
    )
    front = mats.pareto_front(pop)
    novelties = [ind.fitness[-1] for ind in front]
    assert sum(n > 0.3 for n in novelties) >= len(novelties) - 1",tests/test_mats.py,
survived,"def _crowding(values: Sequence[Sequence[float]], fronts: Iterable[Iterable[int]]) -> list[float]:
    n = len(values)
    m = len(values[0]) if n else 0
    crowd = [0.0] * n
    for front in fronts:
        f = list(front)
        if not f:
            continue
        for idx in f:
            crowd[idx] = 0.0
        for i in range(m):
            f.sort(key=lambda idx: values[idx][i])
            crowd[f[0]] = float(""inf"")
            crowd[f[-1]] = float(""inf"")
            fmin = values[f[0]][i]
            fmax = values[f[-1]][i]
            span = fmax - fmin or 1.0
            for j in range(1, len(f) - 1):
                prev_v = values[f[j - 1]][i]
                next_v = values[f[j + 1]][i]
                crowd[f[j]] += (next_v - prev_v) / span
    return crowd
",src/simulation/surrogate_fitness.py,
survived,"def aggregate(
    values: Sequence[Sequence[float]],
    *,
    weights: dict[str, float | Sequence[float]] | None = None,
    weights_path: str | Path | None = None,
) -> list[float]:
    """"""Return scalar surrogate scores for ``values``.""""""
    cfg = weights if weights is not None else load_weights(weights_path)
    rank_w = float(cfg.get(""rank"", 1.0))
    crowd_w = float(cfg.get(""crowd"", 0.0))
    obj_w = cfg.get(""objectives"", [])
    if not isinstance(obj_w, Sequence):
        obj_w = []
    obj_w = list(obj_w) + [0.0] * (len(values[0]) - len(obj_w))
    ranks, fronts = _non_dominated_sort(values)
    crowds = _crowding(values, fronts)
    scores = []
    for idx, vec in enumerate(values):
        s = rank_w * ranks[idx] + crowd_w * crowds[idx]
        s += sum(w * v for w, v in zip(obj_w, vec))
        scores.append(float(s))
    return scores",src/simulation/surrogate_fitness.py,
survived,"def test_invalid_token(monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(monkeypatch)
    resp = client.get(""/agents"", headers={""Authorization"": ""Bearer wrong""})
    assert resp.status_code == 403
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,
survived,"def check_node() -> bool:
    """"""Return True if Node.js is available and warn when outdated.""""""
    if not shutil.which(""node""):
        banner(""node missing"", ""RED"")
        return False
    try:
        out = subprocess.check_output([""node"", ""--version""], text=True).strip()
    except Exception:
        banner(""failed to run node --version"", ""RED"")
        return False
    banner(f""Node {out} detected"", ""GREEN"")
    try:
        major = int(out.lstrip(""v"").split(""."")[0])
        if major < 22:
            banner(""Node 22 or newer recommended"", ""YELLOW"")
    except ValueError:
        banner(""Unable to parse Node version"", ""YELLOW"")
    return True
",alpha_factory_v1/scripts/preflight.py,
survived,"def test_empty_wheelhouse_fallback(tmp_path, monkeypatch, capsys):
    """"""Ensure empty wheelhouse is ignored and network install is used.""""""
    _no_missing(monkeypatch)
    empty = tmp_path / ""wheels""
    empty.mkdir()
    monkeypatch.setattr(check_env, ""has_network"", lambda: True)

    monkeypatch.setattr(subprocess, ""run"", lambda *a, **k: subprocess.CompletedProcess([], 0, """", """"))
    rc = check_env.main([""--auto-install"", ""--wheelhouse"", str(empty)])
    out = capsys.readouterr().out.lower()
    assert rc == 0
    assert ""falling back to network"" in out",tests/test_check_env_wheelhouse.py,
survived,"def test_validate_template_success() -> None:
    ok, err = validate_template(""hello {{ name }}"")
    assert ok and err == """"
",tests/test_template_creator.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Lineitem
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q12.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Lineitem
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q4.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q14.py,Part
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Lineitem
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q16.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Lineitem
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q3.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Part
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto7
survived,"def test_Q27_selects_minimal_company__link_and_title():
    assert result == Auto1(
        producing_company=""Best Film"",
        link_type=""follows"",
        complete_western_sequel=""Western Sequel"",
    )
",tests/dataset/job/compiler/py/q27.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto1
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q23.py,
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q30.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto3
survived,"def test_Q32_finds_movie_link_for_10_000_mile_club():
    assert result == Auto1(
        link_type=""sequel"", first_movie=""Movie A"", second_movie=""Movie C""
    )
",tests/dataset/job/compiler/py/q32.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto4
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q23.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto12
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q3.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto1
survived,"def test_Q24_finds_voiced_action_movie_with_actress_named_An():
    assert result == [
        Auto1(
            voiced_char_name=""Hero Character"",
            voicing_actress_name=""Ann Actress"",
            voiced_action_movie_jap_eng=""Heroic Adventure"",
        )
    ]
",tests/dataset/job/compiler/py/q24.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto4
survived,"def test_Q16_finds_series_named_after_a_character_between_episodes_50_and_99():
    assert result == [
        Auto1(cool_actor_pseudonym=""Alpha"", series_named_after_char=""Hero Bob"")
    ]
",tests/dataset/job/compiler/py/q16.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto1
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q19.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto6
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q16.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q67.py,Reason
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Auto4
survived,"def _q0():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = Auto2(item=ss.item)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(item=g.key[""item""], revenue=sum([x.price for x in g])) for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q65.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto2
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q19.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q45.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q73.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Item
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q10.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,StoreReturn
survived,"def _q0():
    _groups = {}
    _order = []
    for b in base:
        _k = Auto3(
            item_id=b.i_item_id,
            item_desc=b.i_item_desc,
            s_store_id=b.s_store_id,
            s_store_name=b.s_store_name,
        )
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(b)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            i_item_id=g.key[""item_id""],
            i_item_desc=g.key[""item_desc""],
            s_store_id=g.key[""s_store_id""],
            s_store_name=g.key[""s_store_name""],
            store_sales_quantity=sum([x.ss_quantity for x in g]),
            store_returns_quantity=sum([x.sr_return_quantity for x in g]),
            catalog_sales_quantity=sum([x.cs_quantity for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q29.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,StoreSale
survived,"def _q2():
    _groups = {}
    _order = []
    for ctr in customer_total_return:
        _k = ctr.ctr_state
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ctr)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto4(
            state=g.key,
            avg_return=(
                sum([x.ctr_total_return for x in g])
                / len([x.ctr_total_return for x in g])
                if [x.ctr_total_return for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q30.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,DateDim
survived,"def _q1():
    _groups = {}
    _order = []
    for r in by_customer:
        _k = Auto4(seg=int(r.revenue / 50))
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            segment=g.key[""seg""], num_customers=len(g), segment_base=g.key[""seg""] * 50
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q54.py,
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q94.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,CustomerDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,CustomerAddres
survived,"def test_TPCDS_Q62_simplified():
    assert result == 62
",tests/dataset/tpc-ds/compiler/py/q62.py,
survived,"def _q0():
    _src = t
    _rows = _query(_src, [], {""select"": lambda x: x})
    _groups = _group_by(_rows, lambda x: x.ss_customer_sk)
    _items1 = _groups
    _items1 = sorted(
        _items1, key=lambda g: _sort_key([sum([y.act_sales for y in g]), g.key])
    )
    return [
        Auto1(ss_customer_sk=g.key, sumsales=sum([y.act_sales for y in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q93.py,
survived,"def test_TPCDS_Q1_result():
    assert result == [Auto1(c_customer_id=""C2"")]
",tests/dataset/tpc-ds/compiler/py/q1.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Auto4
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q76.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,WebReturn
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q39.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Warehouse
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q81.py,CatalogReturn
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q89.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Auto2
survived,"def test_TPCDS_Q98_revenue():
    assert result == [
        Auto1(
            i_item_id=""I1"",
            i_item_desc=""desc1"",
            i_category=""CatA"",
            i_class=""Class1"",
            i_current_price=100.0,
            itemrevenue=50.0,
            revenueratio=33.333333333333336,
        ),
        Auto1(
            i_item_id=""I2"",
            i_item_desc=""desc2"",
            i_category=""CatB"",
            i_class=""Class1"",
            i_current_price=200.0,
            itemrevenue=100.0,
            revenueratio=66.66666666666667,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q98.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q35.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,S
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Result
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q97.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,CustomerDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,W
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Auto1
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q45.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,StoreSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q19.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,Store
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q7.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group
survived,"def test_TPCDS_Q69_simplified():
    assert result == 69
",tests/dataset/tpc-ds/compiler/py/q69.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q25.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto2
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q29.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Auto2
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q65.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q90.py,HouseholdDemographic
survived,"def test_TPCDS_Q92_threshold():
    assert result == 4.0
",tests/dataset/tpc-ds/compiler/py/q92.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CatalogSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q91.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Auto3
survived,"def test_TPCDS_Q73_simplified():
    assert result == [
        Auto1(
            c_last_name=""Smith"",
            c_first_name=""Alice"",
            c_salutation=""Ms."",
            c_preferred_cust_flag=""Y"",
            ss_ticket_number=1,
            cnt=1,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q73.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,StoreSale
survived,"def test_TPCDS_Q21_inventory_ratio():
    assert result == [
        Auto1(
            w_warehouse_name=""Backup"", i_item_id=""ITEM2"", inv_before=20, inv_after=20
        ),
        Auto1(w_warehouse_name=""Main"", i_item_id=""ITEM1"", inv_before=30, inv_after=40),
    ]
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,HouseholdDemographic
survived,"def _q0():
    _groups = {}
    _order = []
    for s in store_sales:
        _k = s.item
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(s)
    _items1 = [_groups[k] for k in _order]
    return [Auto2(item=g.key, total=sum([x.price for x in g])) for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q56.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"def test_TPCDS_Q75_simplified():
    assert result == [
        Auto1(
            prev_year=2000,
            year=2001,
            i_brand_id=1,
            i_class_id=2,
            i_category_id=3,
            i_manufact_id=4,
            prev_yr_cnt=100,
            curr_yr_cnt=80,
            sales_cnt_diff=-20,
            sales_amt_diff=-200.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q48.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Warehouse
survived,"def _q0():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = Auto1(customer_sk=ss.ss_customer_sk, item_sk=ss.ss_item_sk)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(customer_sk=g.key[""customer_sk""], item_sk=g.key[""item_sk""])
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q97.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q57.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,HouseholdDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,CustomerDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,CrossItem
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q14.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q19.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q92.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q66.py,WebSale
survived,"def test_TPCDS_Q9_result():
    assert result == [
        Auto1(bucket1=7.0, bucket2=15.0, bucket3=30.0, bucket4=35.0, bucket5=50.0)
    ]
",tests/dataset/tpc-ds/compiler/py/q9.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,SalesYear1
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q39.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q56.py,StoreSale
survived,"def test_TPCDS_Q86_sample():
    assert result == 86.0
",tests/dataset/tpc-ds/compiler/py/q86.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Auto2
survived,"def test_TPCDS_Q13_averages():
    assert result == [
        Auto1(
            avg_ss_quantity=10.0,
            avg_ss_ext_sales_price=100.0,
            avg_ss_ext_wholesale_cost=50.0,
            sum_ss_ext_wholesale_cost=50.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q13.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,WebSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,DateDim
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group
survived,"def test_TPCDS_Q51_simplified():
    assert result == [Auto2(item_sk=1, d_date=1), Auto2(item_sk=1, d_date=2)]
",tests/dataset/tpc-ds/compiler/py/q51.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,Customer
survived,"def test_TPCDS_Q52_simplified():
    assert result == [
        Auto1(d_year=2001, brand_id=1, ext_price=30.0),
        Auto1(d_year=2001, brand_id=2, ext_price=22.0),
    ]
",tests/dataset/tpc-ds/compiler/py/q52.py,
survived,"def _q1():
    _groups = {}
    _order = []
    for x in bucket2:
        _k = x[""ss_list_price""]
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(x)
    _items1 = [_groups[k] for k in _order]
    return [g.key for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q28.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q9.py,Reason
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q37.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q36.py,_Group
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Auto1
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Auto3
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q26.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q24.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,DateDim
survived,"def _q0():
    _src = web_sales
    _rows = _query(
        _src,
        [
            {""items"": item, ""on"": lambda ws, i: ws.ws_item_sk == i.i_item_sk},
            {
                ""items"": date_dim,
                ""on"": lambda ws, i, d: ws.ws_sold_date_sk == d.d_date_sk,
            },
        ],
        {
            ""select"": lambda ws, i, d: (ws, i, d),
            ""where"": lambda ws, i, d: (
                i.i_category in [""A"", ""B"", ""C""] and d.d_date >= ""2001-01-15""
            )
            and d.d_date <= ""2001-02-14"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda ws, i, d: Auto3(
            id=i.i_item_id,
            desc=i.i_item_desc,
            cat=i.i_category,
            _class=i.i_class,
            price=i.i_current_price,
        ),
    )
    _items1 = _groups
    return [
        Auto2(
            i_item_id=g.key[""id""],
            i_item_desc=g.key[""desc""],
            i_category=g.key[""cat""],
            i_class=g.key[""_class""],
            i_current_price=g.key[""price""],
            itemrevenue=sum([x[0].ws_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q12.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q53.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Customer
survived,"def test_TPCDS_Q36_simplified():
    assert result == [
        Auto1(i_category=""Books"", i_class=""C1"", gross_margin=0.2),
        Auto1(i_category=""Books"", i_class=""C2"", gross_margin=0.25),
        Auto1(i_category=""Electronics"", i_class=""C3"", gross_margin=0.2),
    ]
",tests/dataset/tpc-ds/compiler/py/q36.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,DateDim
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q17.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q87.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto3
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q46.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,Auto1
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": date_dim,
                ""on"": lambda ss, d: (ss.ss_sold_date_sk == d.d_date_sk and d.d_qoy == 1)
                and d.d_year == 1998,
            },
            {""items"": store, ""on"": lambda ss, d, s: ss.ss_store_sk == s.s_store_sk},
            {
                ""items"": customer_address,
                ""on"": lambda ss, d, s, ca: s.s_zip[0:2] == ca.ca_zip[0:2],
            },
            {
                ""items"": customer,
                ""on"": lambda ss, d, s, ca, c: ca.ca_address_sk == c.c_current_addr_sk
                and c.c_preferred_cust_flag == ""Y"",
            },
        ],
        {
            ""select"": lambda ss, d, s, ca, c: (ss, d, s, ca, c),
            ""where"": lambda ss, d, s, ca, c: ca.ca_zip[0:5] in zip_list,
        },
    )
    _groups = _group_by(_rows, lambda ss, d, s, ca, c: s.s_store_name)
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: g.key)
    return [
        Auto1(s_store_name=g.key, net_profit=_sum([x[0].ss_net_profit for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto4
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q57.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q32.py,DateDim
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q56.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q64.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q38.py,CatalogSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q48.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,CatalogSale
survived,"    def test_env_override(self) -> None:
        env = os.environ.copy()
        env[""ALPHA_AGI_SECTORS""] = ""Finance,Healthcare,Space""
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.alpha_agi_insight_v0.insight_demo"",
                ""--episodes"",
                ""1"",
            ],
            capture_output=True,
            text=True,
            env=env,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        match = re.search(r""Best sector:\s*(\w+)"", result.stdout)
        self.assertIsNotNone(match, result.stdout)
        self.assertIn(match.group(1), {""Finance"", ""Healthcare"", ""Space""})
",tests/test_alpha_agi_insight_env.py,TestAlphaAgiInsightEnv
survived,"    def create(self, *args, **kwargs):  # pragma: no cover - stub method
        raise NotImplementedError
",openai/__init__.py,_ChatCompletions
survived,"def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Refresh offline CSV snapshots"")
    parser.add_argument(
        ""--revision"",
        required=True,
        help=""demo-assets commit SHA"",
    )
    return parser.parse_args()
",alpha_factory_v1/demos/macro_sentinel/refresh_offline_data.py,
deleted,"    def __init__(
        self, system_message: str, user_input: str, thinking_instructions: str | None
    ) -> None:
        super().__init__(system_message, user_input, thinking_instructions)
        if self.thinking_instructions is None:
            raise ValueError(
                ""thinking_instructions are required when strategy is final_and_intermediate""
            )
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,FinalAndIntermediateFormatter
survived,"    def json(self):
        return json.loads(self.text)
",alpha_factory_v1/requests.py,Response
survived,"    def test_main_registers_agent(self) -> None:
        os.environ[""ALPHA_FACTORY_ENABLE_ADK""] = ""true""
        from alpha_factory_v1.backend import adk_bridge as _adk_bridge
        adk_bridge = importlib.reload(_adk_bridge)

        runtime = MagicMock()
        with patch(""openai_agents.AgentRuntime"", return_value=runtime) as rt_cls, \
                patch.object(adk_bridge, ""auto_register"") as auto_reg, \
                patch.object(adk_bridge, ""maybe_launch"") as maybe_launch:
            mod = importlib.reload(importlib.import_module(
                ""alpha_factory_v1.demos.alpha_asi_world_model.openai_agents_bridge""
            ))
            mod.main()

            rt_cls.assert_called_once_with(api_key=None)
            runtime.register.assert_called_once()
            agent_arg = runtime.register.call_args.args[0]
            self.assertIsInstance(agent_arg, mod.InspectorAgent)
            auto_reg.assert_called_once_with([agent_arg])
            maybe_launch.assert_called_once_with()

        os.environ.pop(""ALPHA_FACTORY_ENABLE_ADK"", None)
",tests/test_inspector_bridge_runtime.py,TestInspectorBridgeRuntime
survived,"def _build_local_site(repo_root: Path) -> bool:
    """"""Return ``True`` if the gallery was built successfully.""""""
    script = repo_root / ""scripts"" / ""build_gallery_site.sh""
    if not script.is_file():
        return False
    try:
        subprocess.run([str(script)], check=True)
    except Exception:
        return False
    return True
",scripts/open_gallery.py,
survived,"    async def stop(self) -> None:
        """"""Stop the underlying manager.""""""
        await self.manager.stop()
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler
survived,"    def query(
        self,
        sector: str | None = None,
        approach: str | None = None,
        band: int | None = None,
    ) -> list[Solution]:
        clauses: list[str] = []
        params: list[Any] = []
        if sector is not None:
            clauses.append(""sector=?"")
            params.append(sector)
        if approach is not None:
            clauses.append(""approach=?"")
            params.append(approach)
        if band is not None:
            clauses.append(""band=?"")
            params.append(band)
        sql = ""SELECT sector, approach, score, data, ts FROM solutions""
        if clauses:
            sql += "" WHERE "" + "" AND "".join(clauses)
        cur = self.conn.execute(sql, params)
        rows = cur.fetchall()
        result = [
            Solution(
                sector=row[0],
                approach=row[1],
                score=float(row[2]),
                data=json.loads(row[3]),
                ts=float(row[4]),
            )
            for row in rows
        ]
        return result
",src/archive/solution_archive.py,SolutionArchive
survived,"def _make_repo(tmp_path: Path) -> Path:
    repo = tmp_path / ""repo""
    repo.mkdir()
    (repo / ""metric.txt"").write_text(""1\n"", encoding=""utf-8"")
    (repo / ""test_dummy.py"").write_text(""def test_ok():\n    assert True\n"", encoding=""utf-8"")
    return repo
",tests/test_meta_refinement_agent.py,
survived,"def Tool(*_a: object, **_k: object) -> Callable[[F], F]:
    def dec(f: F) -> F:
        return f

    return dec",tests/resources/openai_agents.py,
survived,"def boom():
    print(""boom"")
    return True
",tests/human/python/bool_chain.py,
survived,"def boom():
    print(""boom"")
    return True
",tests/human/x/python/bool_chain.py,
survived,"def request_with_proxy_fallback(
    session: requests.Session,
    method: str,
    url: str,
    *,
    timeout: Optional[int] = None,
    retries: int = 3,
    **kwargs,
) -> requests.Response:
    """"""Perform a request using rotating proxies.

    The request first uses the session's current proxy configuration. If the
    request fails due to connectivity issues, a new proxy is fetched using
    :func:`get_auto_proxy` and the request is retried. This continues up to
    ``retries`` times before raising an exception.
    """"""

    if retries < 1:
        retries = 1

    last_error: Exception | None = None
    for attempt in range(retries):
        try:
            resp = session.request(method, url, timeout=timeout, **kwargs)
            resp.raise_for_status()
            return resp
        except (
            requests.exceptions.ConnectionError,
            requests.exceptions.ProxyError,
            requests.exceptions.Timeout,
        ) as exc:
            last_error = exc
            try:
                proxy = get_auto_proxy()
                session.proxies.update({""http"": proxy, ""https"": proxy})
            except Exception as fetch_err:  # pragma: no cover - network dependent
                last_error = fetch_err
        except Exception:
            # Other errors should not trigger proxy retry
            raise

    raise RuntimeError(f""All proxy attempts failed: {last_error}"")
",webscout/Provider/TTI/utils.py,
survived,"def test_thermodynamic_trigger_edges() -> None:
    sec = sector.Sector(""x"", energy=1.0, entropy=2.0)
    assert not forecast.thermodynamic_trigger(sec, 0.5)
    assert forecast.thermodynamic_trigger(sec, 0.50001)
    sec2 = sector.Sector(""y"", energy=0.0, entropy=1.0)
    assert not forecast.thermodynamic_trigger(sec2, 0.0)
    assert forecast.thermodynamic_trigger(sec2, 0.1)
",tests/test_forecast.py,
survived,"def test_innovation_gain_positive() -> None:
    gain = forecast._innovation_gain(pop_size=2, generations=1)
    assert gain > 0.0
    assert gain < 0.1",tests/test_forecast.py,
survived,"    def setUp(self) -> None:
        self._backup = AGENT_REGISTRY.copy()
        AGENT_REGISTRY.clear()
",tests/test_demo_registration.py,TestRegisterDemoAgents
survived,"    def test_cli_args_override_env(self) -> None:
        env = {""PORT"": ""1111"", ""METRICS_PORT"": ""2222"", ""A2A_PORT"": ""3333"", ""CYCLE"": ""4""}
        argv = [""--port"", ""9000"", ""--metrics-port"", ""9001"", ""--agents"", ""X,Y""]
        with patch.dict(os.environ, env, clear=True):
            args = edge_runner.parse_args(argv)
        self.assertEqual(args.port, 9000)
        self.assertEqual(args.metrics_port, 9001)
        self.assertEqual(args.agents, ""X,Y"")
        # Unspecified flags fall back to environment defaults
        self.assertEqual(args.a2a_port, 3333)
        self.assertEqual(args.cycle, 4)
",tests/test_edge_runner_cli.py,TestParseArgs
survived,"def test_serde_jsonplus_numpy_array(arr: np.ndarray) -> None:
    serde = JsonPlusSerializer()

    dumped = serde.dumps_typed(arr)
    assert dumped[0] == ""msgpack""
    result = serde.loads_typed(dumped)
    assert isinstance(result, np.ndarray)
    assert result.dtype == arr.dtype
    assert np.array_equal(result, arr)
",libs/checkpoint/tests/test_jsonplus.py,
survived,"def test_serde_jsonplus_numpy_array_json_hook(arr: np.ndarray) -> None:
    serde = JsonPlusSerializer(__unpack_ext_hook__=_msgpack_ext_hook_to_json)
    dumped = serde.dumps_typed(arr)
    assert dumped[0] == ""msgpack""
    result = serde.loads_typed(dumped)
    assert isinstance(result, list)
    assert result == arr.tolist()
",libs/checkpoint/tests/test_jsonplus.py,
survived,"def test_maybe_await_async():
    result = asyncio.run(maybe_await(_async_fn, 5))
    assert result == 10
",tests/test_agent_runner_utils.py,
survived,"    async def _run_input_guardrails_with_queue(
        cls,
        agent: Agent[Any],
        guardrails: list[InputGuardrail[TContext]],
        input: str | list[TResponseInputItem],
        context: RunContextWrapper[TContext],
        streamed_result: RunResultStreaming,
        parent_span: Span[Any],
    ):
        queue = streamed_result._input_guardrail_queue

        # We'll run the guardrails and push them onto the queue as they complete
        guardrail_tasks = [
            asyncio.create_task(
                RunImpl.run_single_input_guardrail(agent, guardrail, input, context)
            )
            for guardrail in guardrails
        ]
        guardrail_results = []
        try:
            for done in asyncio.as_completed(guardrail_tasks):
                result = await done
                if result.output.tripwire_triggered:
                    _error_tracing.attach_error_to_span(
                        parent_span,
                        SpanError(
                            message=""Guardrail tripwire triggered"",
                            data={
                                ""guardrail"": result.guardrail.get_name(),
                                ""type"": ""input_guardrail"",
                            },
                        ),
                    )
                queue.put_nowait(result)
                guardrail_results.append(result)
        except Exception:
            for t in guardrail_tasks:
                t.cancel()
            raise

        streamed_result.input_guardrail_results = guardrail_results
",src/agents/run.py,DefaultAgentRunner
survived,"def main() -> None:
    signals = gather_signals()
    choice = best_alpha(signals)
    print(""\nAlpha signals:"")
    for k, v in signals.items():
        print(f""- {k}: {v}"")
    print(""\nBest current alpha →"", choice)
",alpha_factory_v1/demos/era_of_experience/alpha_report.py,
survived,"def test_get_resource_type_from_arn():
    assert ""ec2:instance"" == rgta.get_resource_type_from_arn(
        ""arn:aws:ec2:us-east-1:1234:instance/i-01""
    )
    assert ""s3"" == rgta.get_resource_type_from_arn(""arn:aws:s3:::bucket-1"")
    assert ""elasticloadbalancing:loadbalancer/app"" == rgta.get_resource_type_from_arn(
        ""arn:aws:elasticloadbalancing:us-east-1:1234:loadbalancer/app/foo/123""
    )
",tests/unit/cartography/intel/aws/test_resourcegroupstaggingapi.py,
survived,"def test_csp_meta_tag() -> None:
    index_file = BROWSER / ""dist/index.html""
    html = index_file.read_text()
    match = re.search(r'<meta[^>]*http-equiv=[""\']Content-Security-Policy[""\'][^>]*>', html)
    assert match, ""Content Security Policy meta tag missing""
    tag = match.group(0)
    content = re.search(r'content=""([^""]+)""', tag)
    assert content, ""content attribute missing""
    policy = content.group(1)
    assert ""script-src 'self' 'wasm-unsafe-eval'"" in policy, ""CSP missing script-src 'self' 'wasm-unsafe-eval'""",tests/test_integrity.py,
survived,"    def boom(*_a, **_kw):
        raise FileNotFoundError(""docker"")
",tests/test_start_aiga_demo.py,
survived,"def test_start_aiga_demo_help() -> None:
    """"""--help prints usage information.""""""
    result = subprocess.run([
        sys.executable,
        str(SCRIPT),
        ""--help"",
    ], capture_output=True, text=True)
    assert result.returncode == 0
    assert ""usage"" in result.stdout.lower()
",tests/test_start_aiga_demo.py,
survived,"def test_postgres_merkle_root(tmp_path) -> None:
    params = {
        ""host"": os.getenv(""PGHOST"", ""localhost""),
        ""port"": int(os.getenv(""PGPORT"", ""5432"")),
        ""user"": os.getenv(""PGUSER"", ""postgres""),
        ""password"": os.getenv(""PGPASSWORD"", """"),
        ""dbname"": os.getenv(""PGDATABASE"", ""postgres""),
    }
    try:
        conn = psycopg2.connect(**params)
    except Exception:
        pytest.skip(""postgres unavailable"")
    with conn:
        with conn.cursor() as cur:
            cur.execute(""DROP TABLE IF EXISTS messages"")
    conn.close()

    ledger = Ledger(tmp_path / ""ignore.db"", db=""postgres"", broadcast=False)
    env = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
    ledger.log(env)
    assert ledger.compute_merkle_root() == _expected_root([env])
    ledger.close()",tests/test_ledger_backends.py,
survived,"    def burn(self, agent_id: str, fraction: float) -> None:
        """"""Burn ``fraction`` of ``agent_id``'s stake if present.""""""
        if agent_id in self.stakes:
            self.stakes[agent_id] = max(0.0, self.stakes[agent_id] * (1.0 - fraction))
",src/governance/stake_registry.py,StakeRegistry
survived,"def main() -> None:
    out = Path(""docs/bench_history.csv"")
    for name in replay.available_scenarios():
        scn = replay.load_scenario(name)
        traj = replay.run_scenario(scn)
        replay.score_trajectory(name, traj, csv_path=out)
",scripts/run_replay_bench.py,
survived,"    def history(self, start_hash: str) -> Iterator[ArchiveEntry]:
        current = self.get(start_hash)
        while current is not None:
            yield current
            if not current.parent:
                break
            current = self.get(current.parent)",src/archive/db.py,ArchiveDB
survived,"def test_archive_crud(tmp_path) -> None:
    db = ArchiveDB(tmp_path / ""arch.db"")
    root = ArchiveEntry(""h1"", None, 0.1, 0.0, True, 1.0)
    child = ArchiveEntry(""h2"", ""h1"", 0.2, 0.0, False, 2.0)
    db.add(root)
    db.add(child)

    assert db.get(""h2"") == child
    history = list(db.history(""h2""))
    assert [e.hash for e in history] == [""h2"", ""h1""]
",tests/test_archive.py,
survived,"def plot_histogram(counts: Iterable[int], out_file: str | Path = DEFAULT_OUT) -> None:
    """"""Save histogram of ``counts`` to ``out_file``.""""""
    df = pd.DataFrame({""backtracks"": list(counts)})
    fig = px.histogram(df, x=""backtracks"")
    path = Path(out_file)
    path.parent.mkdir(parents=True, exist_ok=True)
    fig.write_image(str(path))
",src/tools/analyse_backtrack.py,
survived,"    def exception(self, message: str):
        exc = sys.exc_info()
        formatted = f""{message}\n"" + """".join(traceback.format_exception(*exc))
        self.error(formatted)
",webscout/litlogger/logger.py,Logger
survived,"    def debug(self, message: str):
        self.log(LogLevel.DEBUG, message)
",webscout/litlogger/logger.py,Logger
survived,"    async def _log_async(self, level: LogLevel, message: str):
        if not self._should_log(level):
            return
        record = self._format(level, message)
        tasks = []
        for h in self.handlers:
            if level >= h.level:
                if asyncio.iscoroutinefunction(h.emit):
                    tasks.append(h.emit(record, level))
                else:
                    tasks.append(asyncio.to_thread(h.emit, record, level))
        if tasks:
            await asyncio.gather(*tasks)
",webscout/litlogger/logger.py,Logger
survived,"    def __enter__(self):
        return self
",webscout/litlogger/logger.py,Logger
survived,"    async def fetch_and_replace() -> None:
        try:
            models = await asyncio.to_thread(load_from_url, url)
            built_in_models[:] = models
        except Exception:
            pass
",libs/core/kiln_ai/adapters/remote_config.py,
survived,"def deserialize_config(path: str | Path) -> List[KilnModel]:
    raw = json.loads(Path(path).read_text())
    model_data = raw.get(""model_list"", raw if isinstance(raw, list) else [])
    return [KilnModel.model_validate(item) for item in model_data]
",libs/core/kiln_ai/adapters/remote_config.py,
survived,"async def test_load_remote_models_failure(monkeypatch):
    original = built_in_models.copy()

    def fake_fetch(url):
        raise RuntimeError(""fail"")

    monkeypatch.setattr(""kiln_ai.adapters.remote_config.load_from_url"", fake_fetch)

    load_remote_models(""http://example.com/models.json"")
    await asyncio.sleep(0.01)
    assert built_in_models == original",libs/core/kiln_ai/adapters/test_remote_config.py,
survived,"        def run(self) -> None:
            print(""OpenAI Agents bridge disabled."")
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,AgentRuntime
survived,"        def register(self, *_: object, **__: object) -> None:
            pass
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,AgentRuntime
survived,"def test_save_and_load(tmp_path):
    scraper = AutoScraper()
    scraper.build(html=HTML, wanted_list=[""Banana""])
    file_path = tmp_path / ""model.json""
    scraper.save(file_path)
    new_scraper = AutoScraper()
    new_scraper.load(file_path)
    assert new_scraper.get_result_exact(html=HTML) == scraper.get_result_exact(html=HTML)
",tests/unit/test_features.py,
survived,"def test_build_with_regex():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX, wanted_list=[re.compile(""Ban.*"")])
    result = scraper.get_result_exact(html=HTML_COMPLEX)
    assert ""Banana"" in result[0]
",tests/integration/test_complex_features.py,
survived,"    async def broadcast_merkle_root(self) -> None:
        root = self.compute_merkle_root()
        if AsyncClient is None:
            _log.info(""Merkle root %s"", root)
            return
        try:
            client = AsyncClient(""https://api.testnet.solana.com"")
            memo_prog = PublicKey(""MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr"")
            tx = Transaction().add(
                TransactionInstruction(program_id=memo_prog, data=root.encode(), keys=[])
            )
            await client.send_transaction(tx)
            _log.info(""Broadcasted Merkle root %s"", root)
        except Exception as exc:  # pragma: no cover - network errors
            _log.warning(""Failed to broadcast Merkle root: %s"", exc)
        finally:
            try:
                await client.close()
            except Exception:  # pragma: no cover - ignore close errors
                pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"def test_labels_too_long():
    runner = ScenarioRunner(uri=GMT_DIR, uri_type='folder', filename='tests/data/usage_scenarios/labels_stress_forbidden.yml', skip_system_checks=True, dev_no_metrics=True, dev_no_phase_stats=True, dev_no_sleeps=True, dev_cache_build=True)
    with pytest.raises(RuntimeError) as e:
        with Tests.RunUntilManager(runner) as context:
            context.run_until('setup_services')

    assert ""- value of label 'LABEL_TOO_LONG' is too long 1025 (max allowed length is 1024) - Maybe consider using --allow-unsafe or --skip-unsafe"" == str(e.value), Tests.assertion_info('Label value is too long', str(e.value))
",tests/test_usage_scenario.py,
survived,"def append_pkg_resource_path_KLONGPATH() -> None:
    with importlib.resources.as_file(importlib.resources.files('klongpy')) as pkg_path:
        pkg_lib_path = os.path.join(pkg_path, 'lib')
        klongpath = os.environ.get('KLONGPATH', '.:lib')
        klongpath = f""{klongpath}:{pkg_lib_path}"" if klongpath else str(pkg_lib_path)
        os.environ['KLONGPATH'] = klongpath
",klongpy/repl.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q8.py,
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q8.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q7.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q9.py,
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q7.py,
survived,"def adk_server(monkeypatch: pytest.MonkeyPatch) -> Iterator[Tuple[str, str]]:
    """"""Launch the ADK gateway on a free port and yield the base URL and token.""""""

    port = _free_port()
    token = ""test-token""

    os.environ[""ALPHA_FACTORY_ENABLE_ADK""] = ""1""
    os.environ[""ALPHA_FACTORY_ADK_TOKEN""] = token
    os.environ[""ALPHA_FACTORY_ADK_HOST""] = ""127.0.0.1""
    os.environ[""ALPHA_FACTORY_ADK_PORT""] = str(port)

    from alpha_factory_v1.backend import adk_bridge as _bridge

    # Reload so env vars take effect
    adk_bridge = importlib.reload(_bridge)

    class DummyAgent:
        name = ""dummy""

        def run(self, _prompt: str) -> str:
            return ""ok""

    server: Any | None = None
    thread: threading.Thread | None = None

    def patched_run(app: Any, host: str, port: int, log_level: str = ""info"", **kw: Any) -> None:
        nonlocal server, thread
        config = uvicorn.Config(app, host=host, port=port, log_level=log_level, **kw)
        server = uvicorn.Server(config)
        thread = threading.Thread(target=server.run, daemon=True)
        thread.start()
        for _ in range(50):
            if server.started:
                break
            time.sleep(0.1)

    monkeypatch.setattr(uvicorn, ""run"", patched_run)

    adk_bridge.auto_register([DummyAgent()])
    adk_bridge.maybe_launch()

    assert thread is not None and server is not None

    yield f""http://127.0.0.1:{port}"", token

    server.should_exit = True
    thread.join(timeout=5)

    for var in (
        ""ALPHA_FACTORY_ENABLE_ADK"",
        ""ALPHA_FACTORY_ADK_TOKEN"",
        ""ALPHA_FACTORY_ADK_HOST"",
        ""ALPHA_FACTORY_ADK_PORT"",
    ):
        os.environ.pop(var, None)
",tests/test_adk_gateway.py,
survived,"def register(cls=None, *, condition=True):  # type: ignore
    """"""Decorator adding an :class:`AgentBase` subclass to the registry.""""""

    def decorator(inner_cls):
        from_backend_base = _agent_base()
        if not issubclass(inner_cls, from_backend_base):
            raise TypeError(""register() only allowed on AgentBase subclasses"")

        cond_result = condition() if callable(condition) else bool(condition)
        if cond_result:
            meta = AgentMetadata(
                name=getattr(inner_cls, ""NAME"", inner_cls.__name__),
                cls=inner_cls,
                version=getattr(inner_cls, ""__version__"", ""0.1.0""),
                capabilities=list(getattr(inner_cls, ""CAPABILITIES"", [])),
                compliance_tags=list(getattr(inner_cls, ""COMPLIANCE_TAGS"", [])),
                requires_api_key=getattr(inner_cls, ""REQUIRES_API_KEY"", False),
            )
            _register(meta, overwrite=False)
        else:
            logger.info(
                ""Agent %s not registered (condition=false)"",
                getattr(inner_cls, ""NAME"", inner_cls.__name__),
            )
        return inner_cls

    return decorator(cls) if cls is not None else decorator
",alpha_factory_v1/backend/agents/registry.py,
survived,"def _register(meta: AgentMetadata, *, overwrite: bool = False) -> None:
    if not _should_register(meta):
        return
    with _REGISTRY_LOCK:
        if meta.name in AGENT_REGISTRY and not overwrite:
            existing = AGENT_REGISTRY[meta.name]
            try:
                if _parse_version(meta.version) > _parse_version(existing.version):
                    logger.info(
                        ""Overriding agent %s with newer version %s > %s"",
                        meta.name,
                        meta.version,
                        existing.version,
                    )
                    overwrite = True
                else:
                    logger.error(""Duplicate agent name '%s' ignored"", meta.name)
                    return
            except Exception:  # pragma: no cover - version parse failed
                logger.error(""Duplicate agent name '%s' ignored"", meta.name)
                return

        AGENT_REGISTRY[meta.name] = meta
        for cap in meta.capabilities:
            CAPABILITY_GRAPH.add(cap, meta.name)

    logger.info(""\u2713 agent %-18s caps=%s"", meta.name, "","".join(meta.capabilities))
    _emit_kafka(""agent.manifest"", meta.to_json())
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def _parse_version(v: str):
        return tuple(int(p) for p in v.split(""."") if p.isdigit())
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def instantiate(self, **kw):
        return self.cls(**kw)  # type: ignore[arg-type]
",alpha_factory_v1/backend/agents/registry.py,AgentMetadata
survived,"    async def step(self) -> None:
        await asyncio.sleep(self.SLEEP)
",alpha_factory_v1/backend/agents/registry.py,StubAgent
survived,"    def test_int(self):
        r = self.klong(',1')
        self.assertTrue(kg_equal(r, np.asarray([1])))
",tests/test_eval_monad_list.py,TestEvalMonadList
survived,"    def foo(x: int) -> int:
        return x
",tests/test_function_schema.py,
survived,"def apply_env(args: argparse.Namespace) -> None:
    if args.dev:
        os.environ[""DEV_MODE""] = ""true""
    if args.port is not None:
        os.environ[""PORT""] = str(args.port)
    if args.metrics_port is not None:
        os.environ[""METRICS_PORT""] = str(args.metrics_port)
    if args.a2a_port is not None:
        os.environ[""A2A_PORT""] = str(args.a2a_port)
    if args.enabled is not None:
        os.environ[""ALPHA_ENABLED_AGENTS""] = args.enabled
    if args.loglevel:
        os.environ[""LOGLEVEL""] = args.loglevel.upper()
",alpha_factory_v1/run.py,
survived,"async def summarize_thread(thread_ts: str, conversation: list[ModelMessage]) -> str:
    """"""Summarize a Slack conversation and store the result.""""""

    summary = await summarize_async(""\n\n"".join(m.content for m in conversation))

    add_asset_metadata(
        THREAD_SUMMARY,
        {
            ""thread_ts"": thread_ts,
            ""message_count"": len(conversation),
            ""timestamp"": datetime.now().isoformat(),
        },
    )

    return summary",examples/slackbot/src/slackbot/assets.py,
survived,"    def __init__(self, dim: int | None = None) -> None:
        super().__init__()
        self.dim = dim
",src/raglite/_typing.py,DuckDBVec
deleted,"    def _is_postgres(self) -> bool:
        return isinstance(self.type, HalfVec)
",src/raglite/_typing.py,EmbeddingComparator
survived,"def test_docs_bundle_integrity() -> None:
    bundle = DOCS_DIR / ""insight.bundle.js""
    if not bundle.is_file():
        pytest.skip(""insight.bundle.js missing"")
    html = (DOCS_DIR / ""index.html"").read_text()
    match = re.search(r""<script[^>]*src=['\""]insight.bundle.js['\""][^>]*>"", html)
    assert match, ""insight.bundle.js script tag missing""
    tag = match.group(0)
    integrity = re.search(r""integrity=['\""]([^'\""]+)['\""]"", tag)
    assert integrity, ""integrity attribute missing""
    expected = _sha384(bundle)
    assert integrity.group(1) == expected",tests/test_docs_bundle_hash.py,
survived,"    def close(cls) -> None:
        rt = cls._runtime
        if not rt:
            return
        fn = getattr(rt, ""shutdown"", None)
        if not callable(fn):
            fn = getattr(rt, ""close"", None)
        if callable(fn):
            try:
                fn()
            except Exception:  # noqa: BLE001
                log.exception(""OpenAI runtime shutdown failed"")
",alpha_factory_v1/backend/orchestrator.py,_OAI
survived,"def _run_deploy_script(tmp_path: Path, env_vars: dict[str, str]) -> str:
    script = Path(
        ""alpha_factory_v1/demos/alpha_asi_world_model/deploy_alpha_asi_world_model_demo.sh""
    )
    assert script.exists(), script
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()
    capture = tmp_path / ""env.txt""
    _write_executable(
        bin_dir / ""python"",
        ""#!/usr/bin/env bash\nprintenv > \""$CAPTURE\""\n"",
    )
    env = os.environ.copy()
    env.update(env_vars)
    env.update({""PATH"": f""{bin_dir}:{os.environ.get('PATH', '')}"", ""CAPTURE"": str(capture)})
    subprocess.run([""bash"", str(script)], env=env, check=True, timeout=5)
    return capture.read_text()
",tests/test_world_model_safety.py,
survived,"    def test_summary_with_openai_mock(self) -> None:
        completion = SimpleNamespace(
            choices=[SimpleNamespace(message=SimpleNamespace(content=""ok""))]
        )
        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""sk-test""}):
            with patch(""openai.OpenAI"") as mock_client:
                mock_client.return_value.chat.completions.create.return_value = completion
                text = summarise_with_agent(
                    0.9,
                    agents=5,
                    rounds=10,
                    delta=0.8,
                    stake=1.0,
                )
        self.assertEqual(text, ""ok"")
",tests/test_governance_sim.py,TestGovernanceSim
survived,"    async def restart(self, bus: object, ledger: object) -> None:
        if self.task:
            self.task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self.task
        try:
            close = getattr(self.agent, ""close"")
        except AttributeError:
            pass
        else:
            close()
        self.agent = self.cls(bus, ledger)
        self.error_count = 0
        self.restarts += 1
        self.restart_streak += 1
        self.start(bus, ledger)
",alpha_factory_v1/backend/agent_supervisor.py,AgentRunner
survived,"    async def drive() -> float:
        guard = asyncio.create_task(orchestrator._regression_guard(runners))
        start = time.time()
        for v in [1.0, 0.7, 0.5, 0.3]:
            metrics.dgm_best_score.set(v)
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.5)
        duration = time.time() - start
        guard.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await guard
        return duration
",tests/test_governance.py,
survived,"def _pareto_ranks(pop: Sequence[Any]) -> list[int]:
    metrics = [_metrics(p) for p in pop]
    ranks = [1 for _ in pop]
    for i, a in enumerate(metrics):
        for j, b in enumerate(metrics):
            if i == j:
                continue
            if all(bk <= ak for bk, ak in zip(b, a)) and any(bk < ak for bk, ak in zip(b, a)):
                ranks[i] += 1
    return ranks
",src/simulation/selector.py,
survived,"    def critique(self, text: str) -> float:
        """"""Return a score in [0,1] based on word overlap with :data:`_WORDS`.""""""
        tokens = re.findall(r""[a-zA-Z']+"", text.lower())
        if not tokens:
            return 0.0
        good = sum(1 for t in tokens if t in self._WORDS)
        return good / len(tokens)",src/agents/reviewer_agent.py,ReviewerAgent
survived,"    async def set_stake(req: StakeRequest, _: None = Depends(verify_token)) -> StakeResponse | JSONResponse:
        """"""Register ``req.agent_id`` with ``req.amount`` tokens.""""""

        start = time.perf_counter()
        status = ""200""
        try:
            orch = cast(Any, app_f.state.orchestrator)
            if orch is None:
                raise HTTPException(status_code=503, detail=""Orchestrator not running"")
            orch.registry.set_stake(req.agent_id, req.amount)
            return StakeResponse(status=""ok"")
        except HTTPException as exc:
            status = str(exc.status_code)
            return problem_response(exc)
        finally:
            REQ_COUNT.labels(""POST"", ""/stake"", status).inc()
            REQ_LAT.labels(""POST"", ""/stake"").observe(time.perf_counter() - start)
",src/interface/api_server.py,
survived,"    def __init__(
        self,
        name: str,
        cycle_seconds: int,
        max_cycle_sec: int,
        publish: callable,
        inst: object | None = None,
    ) -> None:
        self.name = name
        self.inst = inst or get_agent(name)
        self.period = getattr(self.inst, ""CYCLE_SECONDS"", cycle_seconds)
        self.spec = getattr(self.inst, ""SCHED_SPEC"", None)
        self.next_ts = 0.0
        self.last_beat = time.time()
        self.task: Optional[asyncio.Task] = None
        self._max_cycle_sec = max_cycle_sec
        self._publish = publish
        self._calc_next()

        with contextlib.suppress(ModuleNotFoundError):
            from openai.agents import AgentContext  # type: ignore[attr-defined]

            if isinstance(self.inst, AgentContext):
                from .telemetry import tracer  # avoid circular import
                from openai.agents import AgentRuntime  # type: ignore[attr-defined]

                runtime = AgentRuntime()
                runtime.register(self.inst)
                atexit.register(runtime.close)
",alpha_factory_v1/backend/agent_runner.py,AgentRunner
survived,"                    def edges(self, *, keys=False, data=False):
                        if keys and data:
                            return list(self._edges)
                        if keys:
                            return [(u, v, k) for u, v, k, _ in self._edges]
                        if data:
                            return [(u, v, d) for u, v, _, d in self._edges]
                        return [(u, v) for u, v, _, _ in self._edges]
",alpha_factory_v1/backend/memory_graph.py,GraphMemory._Stub
survived,"async def list_products(ctx: EnrichContext) -> list[Product]:
    client = await _client(ctx)
    resp = await client.get(""/products"")
    resp.raise_for_status()
    return [Product(**p) for p in resp.json()]
",examples/shop_api_gateway/app.py,
survived,"async def get_user(user_id: int):
    user = next((u for u in USERS if u[""id""] == user_id), None)
    if not user:
        raise HTTPException(status_code=404, detail=""User not found"")
    return user
",examples/shop_api_gateway/server.py,
survived,"async def list_users():
    return USERS
",examples/shop_api_gateway/server.py,
survived,"def parse_agents_table(path: Path) -> set[str]:
    text = path.read_text().splitlines()
    try:
        start = text.index(""### Key Environment Variables"")
    except ValueError:
        return set()

    table_vars: set[str] = set()
    for line in text[start + 1 :]:
        if line.startswith(""|""):
            match = re.search(r""`([^`]+)`"", line)
            if match:
                table_vars.add(match.group(1))
            continue
        if table_vars:
            break
    return table_vars
",tools/check_env_table.py,
survived,"def display(nums):
    s = ""[""
    i = 0
    while i < len(nums):
        if i > 0:
            s = s + "", ""
        s = s + str(nums[i])
        i = i + 1
    s = s + ""]""
    return s
",tests/rosetta/transpiler/Python/boyer-moore-string-search.py,
survived,"def encipher(s, k):
    out = """"
    i = 0
    while i < len(s):
        out = out + shiftRune(s[i:i + 1], k)
        i = i + 1
    return out
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"def getBrilliant(digits, limit, countOnly):
    brilliant = []
    count = 0
    pow = 1
    next = 999999999999999
    k = 1
    while k <= digits:
        s = []
        for p in primes:
            if p >= pow * 10:
                break
            if p > pow:
                s = s + [p]
        i = 0
        while i < len(s):
            j = i
            while j < len(s):
                prod = s[i] * s[j]
                if prod < limit:
                    if countOnly:
                        count = count + 1
                    else:
                        brilliant = brilliant + [prod]
                else:
                    if prod < next:
                        next = prod
                    break
                j = j + 1
            i = i + 1
        pow = pow * 10
        k = k + 1
    if countOnly:
        return {""bc"": count, ""next"": next}
    return {""bc"": brilliant, ""next"": next}
",tests/rosetta/transpiler/Python/brilliant-numbers.py,
survived,"def absf(x):
    if x < 0.0:
        return -x
    return x
",tests/rosetta/transpiler/Python/calculating-the-value-of-e.py,
survived,"def mysum(x, y):
    return x + y
",tests/rosetta/transpiler/Python/call-a-function-12.py,
survived,"def bar(a, b, c):
    print(str(a) + "", "" + str(b) + "", "" + str(c))
",tests/rosetta/transpiler/Python/call-a-function-6.py,
survived,"def chr(n):
    upper = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""
    lower = ""abcdefghijklmnopqrstuvwxyz""
    if n >= 65 and n < 91:
        return upper[n - 65:n - 64]
    if n >= 97 and n < 123:
        return lower[n - 97:n - 96]
    return ""?""
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"def main():
    kinds = ["" "", "" odd "", "" prime ""]
    for kind in kinds:
        print(""First 20"" + kind + ""Brazilian numbers:"")
        c = 0
        n = 7
        while True:
            if isBrazilian(n):
                print(str(n) + "" "")
                c = c + 1
                if c == 20:
                    print(""\n"")
                    break
            if kind == "" "":
                n = n + 1
            else:
                if kind == "" odd "":
                    n = n + 2
                else:
                    while True:
                        n = n + 2
                        if isPrime(n):
                            break
    n = 7
    c = 0
    while c < 100000:
        if isBrazilian(n):
            c = c + 1
        n = n + 1
    print(""The 100,000th Brazilian number: "" + str(n - 1))
",tests/rosetta/transpiler/Python/brazilian-numbers.py,
survived,"def main():
    pt = ""The five boxing wizards jump quickly""
    print(""Plaintext: "" + pt)
    for key in [0, 1, 7, 25, 26]:
        if key < 1 or key > 25:
            print(""Key "" + str(key) + "" invalid"")
            continue
        ct = encipher(pt, key)
        print(""Key "" + str(key))
        print(""  Enciphered: "" + ct)
        print(""  Deciphered: "" + decipher(ct, key))
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"def import_osm(graphdb_filename, osmdb_filename, namespace, slog_strings, profiledb_filename):
    """"""Import an OSM database into a graph database.""""""
    slogs = {}
    for slog_string in slog_strings:
        highway_type, slog_penalty = slog_string.split("":"")
        slogs[highway_type] = float(slog_penalty)
    profiledb = ProfileDB(profiledb_filename) if profiledb_filename else None
    osmdb = OSMDB(osmdb_filename)
    gdb = GraphDatabase(graphdb_filename, overwrite=False)
    gdb_import_osm(gdb, osmdb, namespace, slogs, profiledb)
",pygs/graphserver/cli.py,
survived,"def _select_softmax(pop: list[_Candidate]) -> _Candidate:
    return select_parent(pop, beta=1.0, gamma=0.0)
",experiments/ablate_selector.py,
survived,"    def __init__(self, genome: float, fitness: float, novelty: float) -> None:
        self.genome = genome
        self.fitness = fitness
        self.novelty = novelty
",experiments/ablate_selector.py,_Candidate
survived,"    def fake_sleep(sec):
        called.append(sec)
",tests/test_thread_retry.py,
survived,"def test_maybe_launch_starts_uvicorn(stub_adk, monkeypatch):
    """"""maybe_launch should call uvicorn.run when ADK is enabled.""""""
    uvicorn = pytest.importorskip(""uvicorn"")
    from alpha_factory_v1.backend import adk_bridge as module
    module = importlib.reload(module)

    called = {}

    def fake_run(app, host, port, log_level=""info"", **kw):
        called[""app""] = app
        called[""host""] = host
        called[""port""] = port

    monkeypatch.setattr(uvicorn, ""run"", fake_run)

    class DummyThread:
        def __init__(self, target, *a, **k):
            self.target = target

        def start(self):
            self.target()

    monkeypatch.setattr(module.threading, ""Thread"", DummyThread)

    module.maybe_launch(host=""1.2.3.4"", port=1234)
    assert called == {""app"": module._ensure_router().app, ""host"": ""1.2.3.4"", ""port"": 1234}",tests/test_adk_gateway_startup.py,
survived,"def set_global_setting(key, value):
    conn = get_db()
    c = conn.cursor()
    c.execute('INSERT OR REPLACE INTO global_settings (key, value) VALUES (?, ?)',
              (key, json.dumps(value)))
    conn.commit()
    conn.close()
",users_db.py,
survived,"def fib(n):
    if n < 2:
        return n
    a = 0
    b = 1
    i = n
    i = i - 1
    while i > 0:
        tmp = a + b
        a = b
        b = tmp
        i = i - 1
    return b
",tests/rosetta/transpiler/Python/fibonacci-sequence-2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fasta-format.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-sequence-4.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    old = fs.get(""input.txt"")
    print(""mod time was: "" + str(old))
    mtime = _now()
    mtime = _now()
    fs[""input.txt""] = int(mtime)
    print(""mod time now: "" + str(mtime))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/file-modification-time.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/execute-hq9+.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/erd-s-selfridge-categorization-of-primes.py,
survived,"def lastIndexOf(s, sub):
    idx = 0 - 1
    i = 0
    while i <= len(s) - len(sub):
        if s[i:i + len(sub)] == sub:
            idx = i
        i = i + 1
    sys.exit(idx)
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/file-modification-time.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""Hello World!"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/execute-snusp.py,
survived,"def foo():
    print(""let's foo..."")
    a = []
    if 12 >= len(a):
        sys.exit(""runtime error: index out of range [12] with length "" + str(len(a)))
    a[12] = 0
    sys.exit("""")
",tests/rosetta/transpiler/Python/exceptions.py,
survived,"def dfs(n, m, i):
    global esths
    if i >= n and i <= m:
        esths = esths + [i]
    if i == 0 or i > m:
        sys.exit()
    d = i % 10
    i1 = i * 10 + d - 1
    i2 = i1 + 2
    if d == 0:
        dfs(n, m, i2)
    else:
        if d == 9:
            dfs(n, m, i1)
        else:
            dfs(n, m, i1)
            dfs(n, m, i2)
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    r = eulerSum()
    print(str(r[0]) + "" "" + str(r[1]) + "" "" + str(r[2]) + "" "" + str(r[3]) + "" "" + str(r[4]))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/eulers-sum-of-powers-conjecture.py,
survived,"def commatize(n):
    s = str(n)
    res = """"
    i = 0
    while i < len(s):
        if i > 0 and (len(s) - i) % 3 == 0:
            res = res + "",""
        res = res + """".join(s[i:i + 1])
        i = i + 1
    sys.exit(res)
",tests/rosetta/transpiler/Python/file-size-distribution.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def cosApprox(x):
    term = 1.0
    sum = 1.0
    n = 1
    while n <= 10:
        denom = float(((2 * n - 1) * (2 * n)))
        term = -term * x * x / denom
        sum = sum + term
        n = n + 1
    sys.exit(sum)
",tests/rosetta/transpiler/Python/eulers-identity.py,
survived,"def eulerSum():
    pow5 = []
    i = 0
    while i < 250:
        pow5 = pow5 + [i * i * i * i * i]
        i = i + 1
    sums = {}
    x2 = 2
    while x2 < 250:
        x3 = 1
        while x3 < x2:
            s = pow5[x2] + pow5[x3]
            if not (s in sums):
                sums[s] = [x2, x3]
            x3 = x3 + 1
        x2 = x2 + 1
    x0 = 4
    while x0 < 250:
        x1 = 3
        while x1 < x0:
            y = x0 + 1
            while y < 250:
                rem = pow5[y] - pow5[x0] - pow5[x1]
                if rem in sums:
                    pair = sums[rem]
                    a = pair[0]
                    b = pair[1]
                    if x1 > a and a > b:
                        sys.exit([x0, x1, a, b, y])
                y = y + 1
            x1 = x1 + 1
        x0 = x0 + 1
    sys.exit([0, 0, 0, 0, 0])
",tests/rosetta/transpiler/Python/eulers-sum-of-powers-conjecture.py,
survived,"def fmt8(x):
    y = floorf(x * 1e+08 + 0.5) / 1e+08
    s = str(y)
    dot = s.find(""."")
    if dot == 0 - 1:
        s = s + "".00000000""
    else:
        decs = len(s) - dot - 1
        while decs < 8:
            s = s + ""0""
            decs = decs + 1
    sys.exit(s)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,
survived,"def eulerStep(f, x, y, h):
    sys.exit(y + h * f(x, y))
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def indexOfSub(s, sub):
    if len(sub) == 0:
        sys.exit(0)
    i = 0
    while i + len(sub) <= len(s):
        if s[i:i + len(sub)] == sub:
            sys.exit(i)
        i = i + 1
    sys.exit(0 - 1)
",tests/rosetta/transpiler/Python/execute-a-markov-algorithm.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/ethiopian-multiplication.py,
survived,"def p(x, e):
    r = 1.0
    i = 0
    while i < (int(e)):
        r = r * x
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/exponentiation-with-infix-operators-in-or-operating-on-the-base.py,
survived,"def test_http_app_sets_mcp_server_state():
    server = FastMCP(name=""StateTest"")
    app = server.http_app()
    assert app.state.fastmcp_server is server
",tests/server/test_app_state.py,
survived,"def _write_stub(directory: Path) -> None:
    mod = directory / ""openai_agents""
    mod.mkdir()
    mod_init = mod / ""__init__.py""
    mod_init.write_text(
        """"""
import asyncio
import json
import os
from http.server import BaseHTTPRequestHandler, HTTPServer

class Agent:
    name: str = """"
    tools = []

class OpenAIAgent:
    def __init__(self, *a, base_url=None, **kw):
        self.base_url = base_url

def Tool(*_a, **_k):
    def dec(f):
        return f
    return dec

class AgentRuntime:
    def __init__(self, *a, port=5001, llm=None, api_key=None, **k):
        self.port = int(os.getenv('AGENTS_RUNTIME_PORT', port))
        self._agent = None

    def register(self, agent):
        self._agent = agent

    def run(self):
        agent = self._agent
        if agent is None:
            raise RuntimeError('no agent registered')
        port = self.port

        class Handler(BaseHTTPRequestHandler):
            def do_POST(self):
                if self.path == f'/v1/agents/{agent.name}/invoke':
                    length = int(self.headers.get('content-length', '0'))
                    body = self.rfile.read(length)
                    try:
                        payload = json.loads(body or '{}')
                    except json.JSONDecodeError:
                        payload = {}
                    result = asyncio.run(agent.policy(payload, None))
                    data = json.dumps(result).encode()
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    self.wfile.write(data)
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, *_):
                pass

        server = HTTPServer(('127.0.0.1', port), Handler)
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass
        finally:
            server.server_close()
""""""
    )
",tests/test_aiga_workflow.py,
survived,"def test_agents_import_fallback(monkeypatch, present):
    """"""Ensure modules import with either package name.""""""
    missing = ""agents"" if present == ""openai_agents"" else ""openai_agents""

    stub = types.ModuleType(present)
    stub.Agent = object
    stub.AgentRuntime = object

    class DummyAgent:
        pass

    stub.OpenAIAgent = DummyAgent

    def _tool(*_a, **_k):
        def _decorator(func):
            return func

        return _decorator

    stub.Tool = _tool

    monkeypatch.setitem(sys.modules, present, stub)
    monkeypatch.delitem(sys.modules, missing, raising=False)

    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == missing:
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)

    for mod_name in MODULES:
        mod = importlib.reload(importlib.import_module(mod_name))
        assert mod.OpenAIAgent is stub.OpenAIAgent
        if mod_name.endswith(""utils""):
            llm = mod.build_llm()
            assert isinstance(llm, stub.OpenAIAgent)",tests/test_agents_fallback.py,
survived,"def test_hello_world_template_registered() -> None:
    reg = TemplateRegistry()
    templates = {t[""slug""] for t in reg.list_templates()}
    assert ""hello-world"" in templates
    content = reg.load_template(""hello-world"")
    assert content and ""Hello"" in content
",tests/test_hello_world_template.py,
survived,"    def do_rollout(self) -> list[RolloutGroup]:  # pragma: no cover - abstract
        """"""Produce one or more rollout groups.

        Subclasses implement their rollout logic here as a regular function
        without needing to worry about ``async``/``await`` semantics.
        """"""

        raise NotImplementedError
",marin/rl/env.py,SimpleEnv
survived,"def iter_rollout_groups(root_path: str) -> Iterator[RolloutGroup]:
    """"""Yield :class:`RolloutGroup` objects stored under *root_path*.

    Groups are reconstructed on a *best-effort* basis using the serialized group
    metadata.  If multiple rollouts share identical ``group_metadata_json`` they
    will be packed into the same :class:`RolloutGroup`.
    """"""

    fs, dataset_root = pafs.FileSystem.from_uri(root_path)

    dataset = ds.dataset(dataset_root, format=""parquet"", filesystem=fs)

    # We'll accumulate rows with identical group metadata together.
    pending: dict[str, RolloutGroup] = {}

    # Iterate over record batches to avoid loading the full dataset in memory.
    for batch in dataset.to_batches():
        for record in batch.to_pylist():
            gid: str = record[""id""]
            source: str = record[""source""]
            created: float = record[""created""]
            group_meta_json: str = record[""group_metadata_json""]
            rollout_meta_json: str = record[""rollout_metadata_json""]

            turns = []
            for t in record[""turns""]:
                turns.append(
                    Turn(
                        message=t[""message""],
                        role=t[""role""],
                        logprobs=t.get(""logprobs""),
                        reward=t.get(""reward""),
                        inference_metadata=json.loads(t[""inference_metadata_json""]),
                    )
                )

            rollout = Rollout(
                turns=turns,
                metadata=json.loads(rollout_meta_json),
            )

            if gid not in pending:
                pending[gid] = RolloutGroup(
                    id=gid,
                    source=source,
                    created=created,
                    rollouts=[rollout],
                    metadata=json.loads(group_meta_json),
                )
            else:
                grp = pending[gid]
                pending[gid] = RolloutGroup(
                    id=gid,
                    source=source,
                    created=created,
                    rollouts=[*grp.rollouts, rollout],
                    metadata=grp.metadata,
                )

    yield from pending.values()",marin/rl/parquet_store.py,
survived,"    def __iter__(self):
        return iter(self.turns)
",marin/rl/types.py,Rollout
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = WorkflowAgent()
    runtime.register(agent)
    print(""Registered WorkflowAgent with runtime"")

    if ADK_AVAILABLE:
        auto_register([agent])
        maybe_launch()
        print(""WorkflowAgent exposed via ADK gateway"")

    runtime.run()
",alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py,
survived,"async def fetch_logs() -> list[str]:
    """"""Retrieve the latest orchestrator logs via the REST API.""""""
    resp = requests.get(f""{HOST}/api/logs"", timeout=5)
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bell-numbers.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap.py,
survived,"def _substr(s, start, end):
    return s[start:end]
",tests/rosetta/transpiler/Python/bitmap-read-a-ppm-file.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-b-zier-curves-quadratic.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bioinformatics-base-count.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-flood-fill.py,
survived,"    def __str__(self) -> str:  # pragma: no cover - simple formatting
        """"""Return a human-readable Markdown summary.""""""
        lines = [f""# {self.title}""]
        if self.description:
            lines.append(self.description)
        lines.append("""")
        lines.append(f""**Entity count:** {self.entity_count}"")
        if self.entities:
            lines.append("""")
            lines.append(""## Entities"")
            for name in sorted(self.entities):
                lines.append(f""- {name}"")
        lines.append("""")
        lines.append(self.model)
        lines.append("""")
        lines.append(self.usage_hint)
        return ""\n"".join(lines)",src/enrichmcp/datamodel.py,DataModelSummary
survived,"def load_df(db_path: str | Path) -> pd.DataFrame:
    """"""Return archive contents as a DataFrame.""""""
    arch = Archive(db_path)
    rows = []
    for a in arch.all():
        rows.append(
            {
                ""id"": a.id,
                ""parent"": a.meta.get(""parent""),
                ""patch"": a.meta.get(""diff"") or a.meta.get(""patch""),
                ""score"": a.score,
            }
        )
    return pd.DataFrame(rows)
",src/interface/lineage_dashboard.py,
survived,"    def heartbeat(self) -> None:
        """"""Invoke a trivial call if available.""""""
        ping = getattr(self._client, ""ping"", None)
        if callable(ping):
            ping()",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_adapter.py,ADKAdapter
survived,"    def list_profiles(path: str | None = None):
        """"""Return available profile names.""""""
        _path = os.path.expanduser(path or ""~/.dhapi/credentials"")
        if not os.path.exists(_path):
            raise FileNotFoundError(f""{_path} 파일을 찾을 수 없습니다."")

        with open(_path, ""r"", encoding=""UTF-8"") as f:
            config = tomli.loads(f.read())

        return list(config.keys())",src/dhapi/port/credentials_provider.py,CredentialsProvider
survived,"def load_tools() -> List[Type[BaseTool]]:
    tools: List[Type[BaseTool]] = []
    pkg_dir = Path(__file__).parent / ""tools""
    for module_info in pkgutil.iter_modules([str(pkg_dir)]):
        module = importlib.import_module(f""{__package__}.tools.{module_info.name}"")
        for attr in module.__dict__.values():
            if isinstance(attr, type) and issubclass(attr, BaseTool) and attr is not BaseTool:
                tools.append(attr)
    return tools
",servers/server_clear_thought/app.py,
survived,"def test_performance_drop() -> None:
    rng1 = random.Random(1)
    op_good = SelfRewriteOperator(steps=1, rng=rng1, templates=[""meme""], reuse_rate=1.0)
    score_good = len(op_good(""improve quick test""))

    rng2 = random.Random(1)
    op_bad = SelfRewriteOperator(steps=1, rng=rng2, templates=[""meme""], reuse_rate=0.0)
    score_bad = len(op_bad(""improve quick test""))
    assert score_good > score_bad",tests/test_meme_reuse.py,
survived,"    def execute_in_sandbox(self, code: str) -> tuple[str, str]:
        """"""Run ``code`` inside a subprocess with resource limits.""""""

        def _apply_limits() -> None:  # pragma: no cover - platform dependent
            try:
                import resource

                resource.setrlimit(resource.RLIMIT_CPU, (2, 2))
                mem = 128 * 1024 * 1024
                resource.setrlimit(resource.RLIMIT_AS, (mem, mem))
            except Exception:
                pass

        with tempfile.NamedTemporaryFile(""w"", suffix="".py"", delete=False) as fh:
            fh.write(code)
            code_path = fh.name

        helper = tempfile.NamedTemporaryFile(""w"", suffix="".py"", delete=False)
        helper.write(
            ""import json,sys,contextlib,io,textwrap,resource\n""
            ""code=open(sys.argv[1]).read()\n""
            ""try:\n""
            ""    resource.setrlimit(resource.RLIMIT_CPU,(2,2))\n""
            ""    resource.setrlimit(resource.RLIMIT_AS,(256*1024*1024,256*1024*1024))\n""
            ""except Exception:\n""
            ""    pass\n""
            ""wrapped='def snippet():\\n'+textwrap.indent(code,'    ')\n""
            ""env={'__builtins__':{'print':print,'range':range,'len':len}}\n""
            ""loc={}\n""
            ""out,err=io.StringIO(),io.StringIO()\n""
            ""with contextlib.redirect_stdout(out), contextlib.redirect_stderr(err):\n""
            ""    try:\n""
            ""        exec(compile(wrapped,'<agent>','exec'),env,loc)\n""
            ""        loc['snippet']()\n""
            ""    except Exception as e:\n""
            ""        err.write(type(e).__name__)\n""
            ""print(json.dumps({'stdout':out.getvalue(),'stderr':err.getvalue()}))\n""
        )
        helper.flush()
        helper_path = helper.name
        helper.close()

        try:
            proc = subprocess.run(
                [sys.executable, helper_path, code_path],
                text=True,
                capture_output=True,
                timeout=3,
                preexec_fn=_apply_limits if os.name == ""posix"" else None,
            )
            try:
                data = json.loads(proc.stdout or ""{}"")
                out = data.get(""stdout"", """")
                err = data.get(""stderr"", """")
            except json.JSONDecodeError:
                out, err = proc.stdout, proc.stderr
        except Exception as exc:  # pragma: no cover - runtime errors
            out, err = """", str(exc)
        finally:
            os.unlink(code_path)
            os.unlink(helper_path)

        env = messaging.Envelope(self.name, ""exec"", {""stdout"": out, ""stderr"": err}, time.time())
        self.ledger.log(env)
        return out, err",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/codegen_agent.py,CodeGenAgent
survived,"    def __init__(self, **data: Any) -> None:  # pragma: no cover - exercised in tests
        super().__init__(**data)
        if not self.openai_api_key:
            _log.warning(""OPENAI_API_KEY missing – offline mode enabled"")
            self.offline = True
        if self.offline:
            self.broadcast = False
        if not self.solana_wallet and self.solana_wallet_file:
            try:
                self.solana_wallet = Path(self.solana_wallet_file).read_text(encoding=""utf-8"").strip()
            except Exception as exc:  # pragma: no cover - optional
                _log.warning(""Failed to load wallet file %s: %s"", self.solana_wallet_file, exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/config.py,Settings
survived,"    def test_cli_patches_repo(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            repo = Path(tmp) / ""repo""
            (repo / ""tests"").mkdir(parents=True)
            # buggy source file
            (repo / ""calc.py"").write_text(""def add(a, b):\n    return a - b\n"", encoding=""utf-8"")
            # failing test
            (repo / ""tests"" / ""test_calc.py"").write_text(
                ""from calc import add\n\n"" ""def test_add():\n    assert add(1, 2) == 3\n"",
                encoding=""utf-8"",
            )

            patch_file = Path(tmp) / ""patch.diff""
            patch_file.write_text(
                """"""--- a/calc.py
+++ b/calc.py
@@ -1,2 +1,2 @@
-def add(a, b):
-    return a - b
+def add(a, b):
+    return a + b
\\ No newline at end of file
"""""",
                encoding=""utf-8"",
            )

            stub_dir = Path(tmp) / ""stubs""
            stub_pkg = stub_dir / ""openai_agents""
            stub_pkg.mkdir(parents=True)
            (stub_pkg / ""__init__.py"").write_text(
                """"""import os
from pathlib import Path

class OpenAIAgent:
    def __init__(self, *a, **k):
        self.patch_file = os.environ.get('PATCH_FILE')

    def __call__(self, _prompt):
        return Path(self.patch_file).read_text() if self.patch_file else ''
"""""",
                encoding=""utf-8"",
            )

            env = os.environ.copy()
            env[""PATCH_FILE""] = str(patch_file)
            env[""PYTHONPATH""] = f""{stub_dir}:{env.get('PYTHONPATH', '')}""

            result = subprocess.run(
                [
                    sys.executable,
                    ""-m"",
                    ""alpha_factory_v1.demos.self_healing_repo.patcher_core"",
                    ""--repo"",
                    str(repo),
                ],
                capture_output=True,
                text=True,
                env=env,
            )

            self.assertEqual(result.returncode, 0, result.stdout + result.stderr)
            combined = result.stdout + result.stderr
            self.assertIn(""Patch fixed the tests"", combined)
",tests/test_patcher_cli.py,TestPatcherCLI
survived,"            async def __aexit__(self_inner, exc_type, exc, tb):
                pass
",src/aiohttp/__init__.py,ClientSession._RespCtx
survived,"    def __init__(self, cost_cap: float = 0.5) -> None:
        self.cost_cap = cost_cap
        self.token_count = 0
        self.cost = 0.0
        self.guardrail_hits = 0
        self.latency = 0.0
        self._start: Optional[float] = None
        self.logger = logging.getLogger(__name__)
",src/meta_agent/telemetry.py,TelemetryCollector
survived,"    def add_usage(self, prompt_tokens: int, response_tokens: int, model: str = ""default"") -> None:
        """"""Record token usage and update cost.""""""
        tokens = prompt_tokens + response_tokens
        self.token_count += tokens
        rate = self.COST_TABLE.get(model, self.COST_TABLE[""default""])
        self.cost += rate * tokens / 1000.0
        if self.cost >= self.cost_cap:
            self.logger.warning(""Cost cap exceeded: $%.2f >= $%.2f"", self.cost, self.cost_cap)
            raise RuntimeError(""cost cap exceeded"")
",src/meta_agent/telemetry.py,TelemetryCollector
survived,"    def increment_guardrail_hits(self) -> None:
        """"""Increment guardrail hit counter and record an event.""""""
        self.guardrail_hits += 1
        self.record_event(
            self.Category.GUARDRAIL,
            ""guardrail violation"",
            severity=self.Severity.WARNING,
        )
",src/meta_agent/telemetry.py,TelemetryCollector
survived,"def test_collector_with_db(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    collector = TelemetryCollector(db=db, include_sensitive=False)
    collector.start_timer()
    collector.stop_timer()
    line = collector.summary_line()
    assert ""<redacted>"" in line
    assert db.fetch_all()
    db.close()",tests/unit/test_telemetry_db.py,
survived,"def test_purge_old(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(1, 0.01, 0.1, 0)
    # update timestamp to old date
    old_ts = ""2000-01-01T00:00:00""
    db.conn.execute(""UPDATE telemetry SET timestamp=?"", (old_ts,))
    db.conn.commit()
    db.purge_old()
    assert db.fetch_all() == []
    db.close()
",tests/unit/test_telemetry_db.py,
survived,"def test_with_retry_async(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    orig_sleep = asyncio.sleep
    monkeypatch.setattr(retry.asyncio, ""sleep"", lambda *_: orig_sleep(0))
    calls = {""n"": 0}

    async def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 2:
            raise ValueError(""boom"")
        return ""ok""

    wrapped = retry.with_retry(func, max_tries=2)
    result = asyncio.run(wrapped())
    assert result == ""ok""
    assert calls[""n""] == 2",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_retry.py,
survived,"  def _log_command(self, name: str, **kwargs) -> None:
    params = "", "".join(f""{k}={self._format_param(v)}"" for k, v in kwargs.items())
    logger.debug(""%s(%s)"", name, params)
",pylabrobot/liquid_handling/liquid_handler.py,LiquidHandler
survived,"def test_bridge_launch() -> None:
    """"""Start ``openai_agents_bridge.main`` and confirm registration.""""""
    proc = subprocess.Popen(
        [
            sys.executable,
            ""-m"",
            ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"",
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
    )
    try:
        time.sleep(2)
        proc.terminate()
        out, _ = proc.communicate(timeout=5)
    finally:
        if proc.poll() is None:
            proc.kill()
            proc.wait(timeout=5)
    assert ""EvolverAgent"" in out
",tests/test_aiga_agents_bridge.py,
survived,"def _print_summary(found_files: int, changed_files: int, total_time: float) -> None:
    """"""Print a concise conversion summary.""""""
    if changed_files:
        print(f""Modified {changed_files} of {found_files} files in {total_time:.2f}s"")
    else:
        print(f""No changes made to {found_files} files in {total_time:.2f}s"")
",src/flynt/api.py,
survived,"async def test_orchestrator_lifecycle(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Start the orchestrator, verify health, then shut down cleanly.""""""

    # Allocate random ports
    with socket.socket() as s:
        s.bind(("""", 0))
        rest_port = s.getsockname()[1]
    with socket.socket() as s:
        s.bind(("""", 0))
        grpc_port = s.getsockname()[1]

    monkeypatch.setenv(""DEV_MODE"", ""true"")
    monkeypatch.setenv(""API_TOKEN"", ""t"")
    monkeypatch.setenv(""NEO4J_PASSWORD"", ""x"")
    monkeypatch.setenv(""PORT"", str(rest_port))
    monkeypatch.setenv(""A2A_PORT"", str(grpc_port))

    # Prepare stub packages before importing orchestrator
    agents_stub = types.ModuleType(""backend.agents"")
    setattr(agents_stub, ""list_agents"", lambda _detail=False: [""dummy""])
    setattr(agents_stub, ""get_agent"", lambda name: DummyAgent())
    setattr(agents_stub, ""start_background_tasks"", lambda: None)

    fin_stub = types.ModuleType(""alpha_factory_v1.backend.agents.finance_agent"")
    setattr(fin_stub, ""metrics_asgi_app"", lambda: None)

    mem_stub = types.ModuleType(""backend.memory_fabric"")
    setattr(
        mem_stub,
        ""mem"",
        types.SimpleNamespace(
            vector=types.SimpleNamespace(
                recent=lambda *a, **k: [],
                search=lambda *a, **k: [],
            )
        ),
    )

    monkeypatch.setitem(sys.modules, ""backend.agents"", agents_stub)
    monkeypatch.setitem(sys.modules, ""alpha_factory_v1.backend.agents"", agents_stub)
    monkeypatch.setitem(sys.modules, ""backend.memory_fabric"", mem_stub)
    monkeypatch.setitem(sys.modules, ""alpha_factory_v1.backend.agents.finance_agent"", fin_stub)
    monkeypatch.setitem(sys.modules, ""backend.finance_agent"", fin_stub)

    orch_mod = importlib.import_module(""alpha_factory_v1.backend.orchestrator"")

    # Provide minimal protobuf stubs so gRPC server starts
    pb2 = types.SimpleNamespace(
        StreamReply=object,
        Ack=object,
        AgentStat=object,
        StatusReply=object,
    )
    pb2_grpc = types.SimpleNamespace(
        PeerServiceServicer=object,
        add_PeerServiceServicer_to_server=lambda serv, server: None,
    )
    proto_pkg = types.ModuleType(""backend.proto"")
    setattr(proto_pkg, ""a2a_pb2"", pb2)
    setattr(proto_pkg, ""a2a_pb2_grpc"", pb2_grpc)
    monkeypatch.setitem(sys.modules, ""backend.proto"", proto_pkg)
    monkeypatch.setitem(sys.modules, ""backend.proto.a2a_pb2"", pb2)
    monkeypatch.setitem(sys.modules, ""backend.proto.a2a_pb2_grpc"", pb2_grpc)

    stop = asyncio.Event()
    orch = orch_mod.Orchestrator()

    run_task = asyncio.create_task(orch.run(stop))
    await asyncio.sleep(0.2)  # allow servers to start

    assert orch._rest_task is not None
    assert orch._grpc_server is not None

    import httpx

    async with httpx.AsyncClient() as client:
        res = await client.get(
            f""http://localhost:{rest_port}/healthz"",
            headers={""Authorization"": ""Bearer t""},
        )
    assert res.status_code == 200 and res.text == ""ok""

    stop.set()
    await run_task

    assert orch._rest_task.done()
    for r in orch.manager.runners.values():
        assert r.task is None or r.task.done()",tests/test_orchestrator_lifecycle.py,
survived,"    async def run_cycle(self) -> None:  # pragma: no cover - simple stub
        return None
",tests/test_orchestrator_lifecycle.py,DummyAgent
deleted,"    def repl(match: re.Match[str]) -> str:
        char = match.group(0)
        escapes = mapping.get(char)
        if escapes:
            return escapes.pop(0)
        return char
",src/flynt/utils/utils.py,
survived,"def evaluate(agents: List[int]) -> float:
    """"""Return a pseudo reward for the agents.""""""
    distance = sum(abs(a - TARGET) for a in agents)
    noise = random.random() * 0.1
    return -distance + noise
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/evaluators.py,
survived,"    def __init__(self, root: Node, exploration: float = 1.4) -> None:
        self.root = root
        self.exploration = exploration
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/tree.py,Tree
survived,"    def __init__(self, target: int = 5) -> None:
        self.target = target
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/env.py,NumberLineEnv
survived,"                async def policy(self, obs, _ctx):  # type: ignore[override]
                    cand = obs.get(""policy"", []) if isinstance(obs, dict) else obs
                    return await improve_policy(list(cand))
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/meta_rewrite.py,RewriterAgent
survived,"            def call_ctrans(prompt: str, s: Settings) -> str:
                return cast(str, cast(Any, _MODEL)(prompt, temperature=s.temperature))
",alpha_factory_v1/common/utils/local_llm.py,
survived,"    def bus(self) -> EventBus:
        return self._bus",alpha_factory_v1/backend/services/kafka_service.py,KafkaService
survived,"    def __init__(
        self,
        runners: Dict[str, Any],
        model_max_bytes: int,
        mem: Any,
        rest_port: int,
        grpc_port: int,
        loglevel: str,
        ssl_disable: bool,
    ) -> None:
        self._runners = runners
        self._model_max_bytes = model_max_bytes
        self._mem = mem
        self._rest_port = rest_port
        self._grpc_port = grpc_port
        self._loglevel = loglevel
        self._ssl_disable = ssl_disable
        self._rest_task: Optional[asyncio.Task] = None
        self._grpc_server: Optional[Any] = None
",alpha_factory_v1/backend/services/api_server_service.py,APIServer
survived,"def test_metrics_exporter_start(monkeypatch):
    called = []
    monkeypatch.setattr(
        ""alpha_factory_v1.backend.services.metrics_service.init_metrics"",
        lambda port: called.append(port),
    )
    exporter = MetricsExporter(9999)
    exporter.start()
    assert called == [9999]",tests/test_metrics_service.py,
survived,"    async def _run() -> None:
        async with bus, ledger:
            for env in envs:
                await agent.handle(env)
",tests/test_memory_agent_file_persistence.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q1.py,Lineitem
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Auto2
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q21.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q12.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q8.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q1.py,Auto2
survived,"            def register(self, *_a, **_k) -> None:
                pass
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,AgentRuntime
survived,"    def test_edge_runner_help(self) -> None:
        result = subprocess.run(
            [sys.executable, ""-m"", ""alpha_factory_v1.edge_runner"", ""--help""],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0)
        self.assertIn(""usage"", result.stdout.lower())
",tests/test_edge_runner_cli.py,TestEdgeRunnerCLI
survived,"    def test_condition_true(self):
        from alpha_factory_v1.backend.agents import register, _agent_base
        Base = _agent_base()

        @register
        class OkAgent(Base):
            NAME = ""ok""

            async def step(self):
                return None

        self.assertIn(""ok"", AGENT_REGISTRY)
",tests/test_agents_registry.py,TestRegisterDecorator
survived,"def list_capabilities():
    """"""Return sorted list of all capabilities currently registered.""""""
    return sorted(CAPABILITY_GRAPH.keys())
",alpha_factory_v1/backend/agents/__init__.py,
survived,"def test_replay_existing(tmp_path) -> None:
    path = tmp_path / ""led.db""
    path.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", path):
        with (
            patch.object(cli.logging, ""Ledger"") as led_cls,
            patch.object(
                cli.time,
                ""sleep"",
                return_value=None,
            ),
        ):
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 0.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            out = CliRunner().invoke(cli.main, [""replay""])
            assert ""a -> b"" in out.output",tests/test_cli.py,
survived,"    async def run_cycle(self) -> None:
        self.calls += 1
",tests/test_agent_runner.py,DummyAgent
survived,"def test_simulate_export_json() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--pop-size"",
                    ""1"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""json"",
                ],
            )
    assert res.exit_code == 0
    assert res.output.startswith(""["")
",tests/test_cli.py,
survived,"    def fake_push(self):
        pushed.append(True)
        return ""branch""
",tests/test_self_healer_pipeline.py,
survived,"  def valid(self, current_nanos: int, bus_timeout: bool) -> bool:
    if self.ignore_alive:
      return True
    if not self.timestamps:
      return False
    if self.timeout_threshold > 0 and (current_nanos - self.timestamps[-1]) > self.timeout_threshold:
      return False
    return True
",opendbc/can/parser.py,MessageState
survived,"def test_setup_config_creates_file(tmp_path: Path) -> None:
    sample = tmp_path / ""config.env.sample""
    sample.write_text(""SECRET=1\n"")
    path, created = setup_config.ensure_config(tmp_path)
    assert created is True
    assert path == tmp_path / ""config.env""
    assert path.read_text() == ""SECRET=1\n""",tests/test_setup_config.py,
survived,"def test_single_batched_selector():
    B, S, V = Axis(""batch"", 4), Axis(""seq"", 3), Axis(""vocab"", 7)
    x = hax.arange((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = x[""vocab"", idx]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(x, V, idx))
",tests/test_scatter_gather.py,
survived,"    async def init_async(self) -> None:
        """"""Launch background tasks after instantiation.""""""
        asyncio.create_task(self.kg.load())
",alpha_factory_v1/backend/agents/biotech_agent.py,BiotechAgent
survived,"    def _decode(self, seq: Sequence, cache, page_table, step: int):
        prev_token = jnp.array([seq.last_token], dtype=jnp.int32)
        seq_named = hax.named([seq.seq_id], ""seq"")
        temps = hax.full((), seq.sampling_params.temperature, dtype=jnp.float32)
        key = jrandom.PRNGKey(step)
        start = jnp.array(step, dtype=jnp.int32)
        tok, page_table, cache = do_generate(
            self.model, cache, page_table, prev_token, self.sampler, seq_named, start, temps, key
        )
        return int(tok.array), cache, page_table
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"def linear_conflicts(start_list,goal_list):
    """"""
    calculates number of moves to add to the estimate of
    the moves to get from start to goal based on the number
    of conflicts on a given row or column. start_list
    represents the current location and goal_list represnts
    the final goal.
    """"""

    # Find which of the tiles in start_list have their goals on this line
    # build a pattern to use in a lookup table of this form:
    # g0, g1, g3, g3 fill in x where there is no goal for this line

    # all 'x' until we file a tile whose goal is in this line

    goal_pattern = ['x', 'x', 'x', 'x']

    for g in range(4):
        for s in range(4):
            start_tile_num = start_list[s]
            if start_tile_num == goal_list[g] and start_tile_num != 0:
                goal_pattern[s] = 'g' + str(g) # i.e. g0

    global conflict_table

    tup_goal_pattern = tuple(goal_pattern)

    if tup_goal_pattern in conflict_table:
        return conflict_table[tuple(goal_pattern)]
    else:
        return 0
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,
survived,"    def __init__(self, goal):
        """"""
        Preprocess goal position to setup internal data structures
        that can be used to speed up heuristic.
        """"""

        build_conflict_table()

        self.goal_map = []
        for i in range(16):
            self.goal_map.append(i)

        self.goal_lists = goal.tiles

        # preprocess for manhattan distance

        for row in range(4):
            for col in range(4):
                self.goal_map[goal.tiles[row][col]] = (row, col)

        # make access faster by changing to a tuple

        self.goal_map = tuple(self.goal_map)

        # preprocess for linear conflicts

        self.row_conflicts = []
        for row in range(4):
            t = goal.tiles[row]
            conf_dict = listconflicts([t[0],t[1],t[2],t[3]])
            self.row_conflicts.append(conf_dict)

        self.col_conflicts = []
        for col in range(4):
            col_list =[]
            for row in range(4):
                col_list.append(goal.tiles[row][col])
            conf_dict = listconflicts(col_list)
            self.col_conflicts.append(conf_dict)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,HeuristicObj
survived,"    def push(self, new_object):
        """""" save object in heapq """"""
        heapq.heappush(self.qheap,(new_object.fscore,new_object.tiles))
        self.queue_length += 1
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    req_txt = repo_root / ""alpha_factory_v1"" / ""demos"" / ""aiga_meta_evolution"" / ""requirements.txt""
    lock_file = repo_root / ""alpha_factory_v1"" / ""demos"" / ""aiga_meta_evolution"" / ""requirements.lock""

    with tempfile.TemporaryDirectory() as tmpdir:
        out_path = Path(tmpdir) / ""requirements.lock""
        pip_compile = shutil.which(""pip-compile"")
        if pip_compile:
            cmd = [pip_compile]
        else:
            cmd = [sys.executable, ""-m"", ""piptools"", ""compile""]
        wheelhouse = os.getenv(""WHEELHOUSE"")
        cmd += [""--quiet""]
        if wheelhouse:
            cmd += [""--no-index"", ""--find-links"", wheelhouse]
        cmd += [""--generate-hashes"", str(req_txt), ""-o"", str(out_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        sys.stdout.write(result.stdout)
        sys.stderr.write(result.stderr)
        if result.returncode != 0:
            return result.returncode
        if not lock_file.exists() or out_path.read_bytes() != lock_file.read_bytes():
            extra = """"
            if wheelhouse:
                extra = f""--no-index --find-links {wheelhouse} ""
            msg = (
                ""alpha_factory_v1/demos/aiga_meta_evolution/requirements.lock is outdated. Run 'pip-compile ""
                f""{extra}--quiet --generate-hashes alpha_factory_v1/demos/aiga_meta_evolution/requirements.txt -o ""
                ""alpha_factory_v1/demos/aiga_meta_evolution/requirements.lock'\n""
            )
            sys.stderr.write(msg)
            return 1
    return 0
",scripts/verify_aiga_requirements_lock.py,
survived,"def test_visualize_shardings_inside_jit(capsys):
    mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))

    @named_jit(out_axis_resources={""dim1"": ResourceAxis.DATA})
    def fn(x):
        visualize_shardings(x)
        return x

    with axis_mapping({""dim1"": ResourceAxis.DATA}), mesh:
        x = hax.ones(Dim1)
        fn(x)

    out = capsys.readouterr().out
    assert ""dim1"" in out
",tests/test_visualize_sharding.py,
survived,"            def __init__(self, *a, **k):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"            def reset(self):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"            def latest_log(self):
                return """"
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"def main() -> None:
    alpha = discover_alpha()
    print(""Discovered alpha:"")
    print(json.dumps(alpha, indent=2))
    print(f""Logged to {LEDGER}"")
",alpha_factory_v1/demos/omni_factory_demo/alpha_discovery_stub.py,
survived,"    def test_apply_patch_failure_rollback(self):
        with tempfile.TemporaryDirectory() as repo:
            file_path = os.path.join(repo, ""hello.txt"")
            with open(file_path, ""w"") as fh:
                fh.write(""hello\n"")
            bad_patch = ""invalid diff""
            with self.assertRaises(RuntimeError):
                patcher_core.apply_patch(bad_patch, repo_path=repo)
            with open(file_path) as fh:
                self.assertEqual(fh.read(), ""hello\n"")
",tests/test_self_healing_patcher.py,TestPatcherCore
survived,"    def Tool(*_, **__):  # type: ignore[misc]
        def wrapper(func):
            return func

        return wrapper
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,
survived,"        def __call__(self, *_: str) -> str:
            return ""LLM commentary unavailable.""
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,OpenAIAgent
survived,"    def test_notebook_valid(self) -> None:
        nb_path = Path(""alpha_factory_v1/demos/alpha_agi_business_v1/colab_alpha_agi_business_v1_demo.ipynb"")
        self.assertTrue(nb_path.exists(), ""Notebook missing"")
        data = json.loads(nb_path.read_text(encoding=""utf-8""))
        self.assertIn(""cells"", data)
        self.assertIn(""nbformat"", data)
        self.assertGreaterEqual(data.get(""nbformat"", 0), 4)
",tests/test_business_notebook.py,TestBusinessNotebook
survived,"    def test_notebook_v1_valid(self) -> None:
        self._check_notebook(Path(""alpha_factory_v1/demos/meta_agentic_agi/colab_meta_agentic_agi.ipynb""))
",tests/test_meta_agentic_notebook.py,TestMetaAgenticNotebook
survived,"    async def step(self) -> None:
        """"""Post a short market insight using OpenAI Agents if available.""""""
        insight = ""LLM unavailable""
        try:
            from openai_agents import OpenAIAgent

            agent = OpenAIAgent(
                model=os.getenv(""MODEL_NAME"", ""gpt-4o-mini""),
                api_key=os.getenv(""OPENAI_API_KEY""),
                base_url=None
                if os.getenv(""OPENAI_API_KEY"")
                else ""http://ollama:11434/v1"",
            )
            insight = await agent(""One sentence on today's market outlook"")
        except Exception as exc:  # noqa: BLE001
            logging.getLogger(__name__).warning(""LLM fallback: %s"", exc)
        await self.publish(""alpha.insight"", {""insight"": insight})
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,LLMCommentAgent
survived,"    def test_detect_yield_curve_alpha(self) -> None:
        msg = alpha_detection.detect_yield_curve_alpha()
        self.assertIsInstance(msg, str)
        self.assertTrue(""Yield curve spread"" in msg)
",tests/test_alpha_detection.py,TestAlphaDetection
survived,"async def run_episode(max_steps: int = 500) -> dict:
    mu = MiniMu(env_id=ENV_ID)
    frames, reward = minimuzero.play_episode(mu, render=False, max_steps=max_steps)
    return {""reward"": reward}
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,
survived,"def boom(a: int, b: int) -> bool:
    print(""boom"")
    return True
",tests/machine/x/python/short_circuit.py,
survived,"    def adder(x):
        return x + n
",tests/machine/x/python/closure.py,
survived,"def sum_rec(n: int, acc: int) -> int:
    if n == 0:
        return acc
    return sum_rec(n - 1, acc + n)
",tests/machine/x/python/tail_recursion.py,
survived,"    def _align_gpu(self, seq1: str, seq2: str) -> tuple[int, ""np.ndarray""]:
        len1, len2 = len(seq1), len(seq2)
        seq1_buf = np.frombuffer(seq1.encode(""ascii""), dtype=np.int8)
        seq2_buf = np.frombuffer(seq2.encode(""ascii""), dtype=np.int8)
        H = np.zeros((len1 + 1) * (len2 + 1), dtype=np.int32)
        d_seq1 = cl.Buffer(self._ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=seq1_buf)
        d_seq2 = cl.Buffer(self._ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=seq2_buf)
        d_H = cl.Buffer(self._ctx, cl.mem_flags.READ_WRITE | cl.mem_flags.COPY_HOST_PTR, hostbuf=H)
        for diag in range(2, len1 + len2 + 1):
            start_i = max(1, diag - len2)
            end_i = min(len1, diag - 1)
            diag_size = max(0, end_i - start_i + 1)
            if diag_size == 0:
                continue
            self._prog.sw_diag(
                self._queue,
                (diag_size,),
                None,
                d_seq1,
                d_seq2,
                d_H,
                np.int32(len1),
                np.int32(len2),
                np.int32(diag),
                np.int32(start_i),
                np.int32(self.match),
                np.int32(self.mismatch),
                np.int32(self.gap),
            )
        cl.enqueue_copy(self._queue, H, d_H)
        H = H.reshape((len1 + 1, len2 + 1))
        return int(H.max()), H
",src/python/gpu_smith_waterman.py,SmithWatermanGPU
survived,"    def align(self, seq1: str, seq2: str):
        """"""Return Smith-Waterman max score and matrix.""""""
        if _GPU_AVAILABLE:
            return self._align_gpu(seq1, seq2)
        return self._align_cpu(seq1, seq2)
",src/python/gpu_smith_waterman.py,SmithWatermanGPU
survived,"    def generate_from_file(self, df_infile, vocab_infile):
        """"""Load a document-term matrix and vocabulary from disk.

        Column names are kept as words. When using this matrix with the
        sampler, map the words to integer token IDs before constructing the
        corpus.
        """"""

        # read data frame with word columns intact
        df = pd.read_csv(df_infile, index_col=0)

        vocab = np.genfromtxt(vocab_infile, dtype=""str"")
        return df, vocab
",examples/synthetic_data.py,HldaDataGenerator
survived,"def test_transform_acm_certificates():
    result = acm.transform_acm_certificates(
        [test_data.DESCRIBE_CERTIFICATE[""Certificate""]],
        ""us-east-1"",
    )
    assert len(result) == 1
    cert = result[0]
    assert cert[""Arn""] == ""arn:aws:acm:us-east-1:000000000000:certificate/test-cert""
    assert cert[""DomainName""] == ""example.com""
    assert cert[""Status""] == ""ISSUED""
    assert cert[""InUseBy""] == [
        ""arn:aws:elasticloadbalancing:us-east-1:000000000000:listener/app/test-lb/abcd/efgh""
    ]
    assert cert[""ELBV2ListenerArns""] == [
        ""arn:aws:elasticloadbalancing:us-east-1:000000000000:listener/app/test-lb/abcd/efgh""
    ]",tests/unit/cartography/intel/aws/test_acm.py,
survived,"def sum_tree(t):
    return (0 if t == Leaf else (sum_tree(t[""left""]) + t[""value""] + sum_tree(t[""right""]) if t != None else None))
",tests/transpiler/x/py/tree_sum.py,
survived,"    def open_logs():
        try:
            open_logs_folder()
            return {""message"": ""opened""}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))",app/desktop/studio_server/settings_api.py,
survived,"    async def invoke(
        self,
        prompt: str,
        *,
        model: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        for guard in self.input_guardrails:
            await guard(prompt)

        adapter = self.adapters.get(model or self.default_model)
        if adapter is None:
            raise ValueError(f""Unknown model '{model}'"")

        result = await adapter.invoke(prompt, context)

        for guard in self.output_guardrails:
            await guard(result)

        return result
",src/meta_agent/services/guardrail_router.py,GuardrailModelRouter
survived,"def _apply_grad(fn, x, backend_name=""numpy""):
    backend.set_backend(backend_name)
    b = backend.current()
    g = b.grad(fn)
    out = g(b.array(x, requires_grad=True))
    out = to_numpy(out)
    return float(out) if np.ndim(out) == 0 else out
",tests/kgtests/autograd/helpers.py,
survived,"    def test_scalar_grad_numpy(self):
        self._check_scalar_square_grad(""numpy"")
",tests/test_autograd.py,TestAutograd
survived,"def test_apply_patch_missing_patch_binary(tmp_path: Path, monkeypatch: mock.MagicMock) -> None:
    (tmp_path / ""hello.txt"").write_text(""hello\n"", encoding=""utf-8"")
    monkeypatch.setattr(shutil, ""which"", lambda _: None)
    with pytest.raises(RuntimeError) as exc:
        patcher_core.apply_patch(_DEF_DIFF, repo_path=str(tmp_path))
    assert ""patch` command not found"" in str(exc.value)
",tests/test_patcher_core_additional.py,
survived,"def test_get_secret_env(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""MY_SECRET"", ""value"")
    monkeypatch.delenv(""AGI_INSIGHT_SECRET_BACKEND"", raising=False)
    assert cfg.get_secret(""MY_SECRET"") == ""value""
    monkeypatch.delenv(""MY_SECRET"", raising=False)
    assert cfg.get_secret(""MY_SECRET"", ""default"") == ""default""
",tests/test_config_utils.py,
survived,"    def agents_status(*_a: object, **_kw: object) -> None:
        raise click.ClickException(""Insight demo not installed"")
",alpha_factory_v1/core/interface/cli.py,
survived,"    def test_eviction(self) -> None:
        llm._cache_put(""a"", ""1"", ""p"")
        llm._cache_put(""b"", ""2"", ""p"")
        llm._cache_put(""c"", ""3"", ""p"")
        self.assertEqual(len(llm._cache_mem), 2)
        self.assertNotIn(""a"", llm._cache_mem)
        llm._cache_get(""b"")
        llm._cache_put(""d"", ""4"", ""p"")
        self.assertEqual(len(llm._cache_mem), 2)
        self.assertIn(""b"", llm._cache_mem)
        self.assertIn(""d"", llm._cache_mem)
        self.assertNotIn(""c"", llm._cache_mem)
",tests/test_llm_cache.py,TestLLMCacheLRU
survived,"def _sync_run(coro: Awaitable[str]) -> str:
    """"""Run ``coro`` synchronously regardless of event loop state.""""""
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)

    result: list[str] = []

    def _worker() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        task = loop.create_task(coro)
        try:
            result.append(asyncio.get_event_loop().run_until_complete(task))
        finally:
            loop.close()

    t = threading.Thread(target=_worker)
    t.start()
    t.join()
    return result[0]
",alpha_factory_v1/backend/agents/energy_agent.py,
survived,"def evolve_cmd(max_cost: float, wallclock: float | None, backtrack_rate: float) -> None:
    """"""Run the minimal asynchronous evolution demo.""""""
    from src import evolve as _evolve

    async def _eval(genome: float) -> tuple[float, float]:
        await asyncio.sleep(0)
        return random.random(), 0.01

    archive = _evolve.InMemoryArchive()
    asyncio.run(
        _evolve.evolve(
            lambda g: g,
            _eval,
            archive,
            max_cost=max_cost,
            wallclock=wallclock,
            backtrack_rate=backtrack_rate,
        )
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"        def hook(p: Phase) -> None:
            nonlocal current
            current = p
",tests/test_phase_order.py,TestPhaseOrder
survived,"def _run_bench(repo: Path, flags: Dict[str, bool]) -> float:
    """"""Return SWE pass rate for the given repo and feature flags.""""""

    env = os.environ.copy()
    env[""PYTHONPATH""] = str(repo)
    for name, enabled in flags.items():
        env[f""ENABLE_{name.upper()}""] = ""1"" if enabled else ""0""
    proc = subprocess.run(
        [sys.executable, str(repo / ""benchmarks"" / ""run_benchmarks.py"")],
        capture_output=True,
        text=True,
        env=env,
        check=True,
    )
    results = json.loads(proc.stdout)
    metrics = compute_fitness(results)
    score = metrics.get(""swe_mini"", {}).get(""pass_rate"", 0.0)
    # simple synthetic penalty when features disabled
    for enabled in flags.values():
        if not enabled:
            score = max(0.0, score - 0.05)
    return score
",src/tools/ablation_runner.py,
survived,"def _clone_repo(dest: Path) -> None:
    """"""Copy source tree needed for benchmarks into ``dest``.""""""

    shutil.copytree(ROOT / ""benchmarks"", dest / ""benchmarks"")
    shutil.copytree(ROOT / ""src"", dest / ""src"")
",src/tools/ablation_runner.py,
survived,"def test_fsm_cycles_three() -> None:
    result = loop.run_loop(cost_budget=3.0, cost_per_cycle=1.0)
    assert result.cycles == 3
    assert result.state is loop.State.SELECT",tests/test_loop_fsm.py,
survived,"    def test_index_negative_row(self):
        """"""Use negative index for last row""""""
        self.assert_eval_cmp('[[1 2 3] [4 5 6]]:@[-1 []]', '[4 5 6]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior
survived,"def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Alpha-AGI Business dashboard"")
    parser.add_argument(
        ""--token"",
        help=""REST API bearer token (defaults to API_TOKEN env var)"",
    )
    return parser.parse_args(argv)
",alpha_factory_v1/demos/alpha_agi_business_v1/gradio_dashboard.py,
survived,"    def needs_rebuild(self) -> bool:
        """"""Return True if stored checksums differ from source files.""""""
        if not self.index_path.exists():
            return True
        if not self._index:
            self.load()
        for item in self._index:
            template_path = self.registry.templates_dir / item[""path""]
            try:
                content = template_path.read_text(encoding=""utf-8"")
            except OSError:  # file removed
                return True
            checksum = sha256(content.encode(""utf-8"")).hexdigest()
            if checksum != item.get(""checksum""):
                return True
        # check for new templates not in index
        seen = {(i[""slug""], i[""version""]) for i in self._index}
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                if (slug, version_info[""version""]) not in seen:
                    return True
        return False
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.index_path = self.registry.templates_dir / self.INDEX_FILE_NAME
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def __iter__(self) -> Iterable[Any]:  # pragma: no cover - convenience
        yield self.default",src/enrichmcp/parameter.py,EnrichParameter
survived,"    def from_json(js: str | dict) -> ""Genome"":
        return Genome(**(json.loads(js) if isinstance(js, str) else js))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,Genome
survived,"    def population_sha(self) -> str:
        concat = """".join(sorted(g.sha for g in self.population))
        return hashlib.sha256(concat.encode()).hexdigest()[:16]
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def to_json(self) -> str:
        return json.dumps(dc.asdict(self), separators=(',', ':'))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,Genome
survived,"    def run_forever(self) -> None:
        """"""Start the orchestrator and block until interrupted.""""""
        asyncio.run(_main())
",alpha_factory_v1/backend/orchestrator.py,Orchestrator
survived,"def ensure_dir(path: Path) -> None:
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)
        banner(f""Created {path}"", 'YELLOW')
    else:
        banner(f""Using {path}"", 'GREEN')
",alpha_factory_v1/scripts/preflight.py,
survived,"    def test_reproducibility(self):
        a = run_sim(agents=5, rounds=50, delta=0.9, stake=2.0, seed=123)
        b = run_sim(agents=5, rounds=50, delta=0.9, stake=2.0, seed=123)
        self.assertAlmostEqual(a, b)
",alpha_factory_v1/tests/test_governance_sim.py,GovernanceSimTest
survived,"def powf(base, exp):
    return expf(exp * ln(base))
",tests/rosetta/transpiler/Python/gamma-function.py,
survived,"def swap(a, b):
    return [b, a]
",tests/rosetta/transpiler/Python/generic-swap.py,
survived,"def expf(x):
    term = 1.0
    sum = 1.0
    i = 1
    while i < 20:
        term = term * x / float(i)
        sum = sum + term
        i = i + 1
    return sum
",tests/rosetta/transpiler/Python/gamma-function.py,
survived,"def test_pbm(h, f):
    """"""Verify if the image is a PBM (portable bitmap).""""""
    if len(h) >= 3 and \
        h[0] == ord(b'P') and h[1] in b'14' and h[2] in b' \t\n\r':
        return 'pbm'
",metaflow/_vendor/imghdr/__init__.py,
survived,"def test_gif(h, f):
    """"""Verify if the image is a GIF ('87 or '89 variants).""""""
    if h[:6] in (b'GIF87a', b'GIF89a'):
        return 'gif'
",metaflow/_vendor/imghdr/__init__.py,
survived,"def test_pgm(h, f):
    """"""Verify if the image is a PGM (portable graymap).""""""
    if len(h) >= 3 and \
        h[0] == ord(b'P') and h[1] in b'25' and h[2] in b' \t\n\r':
        return 'pgm'
",metaflow/_vendor/imghdr/__init__.py,
survived,"def run_action(service_var, action_var, csv_entry):
    service = service_var.get()
    action = action_var.get()
    csv_path = csv_entry.get()

    cfg = load_config()

    if action == ""Import"" and not csv_path:
        messagebox.showerror(""Error"", ""Please select a CSV file for import"")
        return

    try:
        if service == ""Radarr"" and action == ""Import"":
            radarr_import(csv_path, cfg)
        elif service == ""Radarr"" and action == ""Export"":
            radarr_export(cfg)
        elif service == ""Sonarr"" and action == ""Import"":
            sonarr_import(csv_path, cfg)
        elif service == ""Sonarr"" and action == ""Export"":
            sonarr_export(cfg)
        elif service == ""Lidarr"" and action == ""Import"":
            lidarr_import(csv_path, cfg)
        elif service == ""Lidarr"" and action == ""Export"":
            lidarr_export(cfg)
        else:
            messagebox.showerror(""Error"", ""Invalid selection"")
            return
        messagebox.showinfo(""Success"", ""Operation completed"")
    except Exception as exc:  # simplistic error handling
        messagebox.showerror(""Error"", str(exc))
",arr_gui.py,
survived,"def normalize_sql_content(content: str) -> str:
    """"""Return content with all whitespace at beginning or end removed.""""""
    return content.strip()
",.hacking/dialect_sqlfluff_catchup/main.py,
survived,"    def fake_post(url: str, json=None, timeout=None):
        called[""url""] = url
        called[""json""] = json
        return DummyResp()
",tests/test_selfheal_entrypoint_offline.py,
survived,"        async def run() -> None:
            async with bus:
                env = messaging.Envelope(""a"", ""x"", {""ok"": True}, 0.0)
                bus.publish(""x"", env)
                await asyncio.sleep(0)
",tests/test_message_bus.py,
survived,"    def test_init_fallback_backend(self):
        mem = mv.VectorMemory()
        self.assertEqual(mem.backend, ""numpy"")
",tests/test_memory_vector.py,TestVectorMemoryOffline
survived,"    def test_aiga_bridge_import_paths(self, monkeypatch):
        """"""Import the AI‑GA bridge with both `openai_agents` and `agents`.""""""

        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = object
        stub.OpenAIAgent = object

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Tool = _tool

        # Import using openai_agents module
        monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
        sys.modules.pop(""agents"", None)
        mod = importlib.reload(
            importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
        )
        self.assertIs(mod.Agent, stub.Agent)

        # Import using agents fallback
        monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
        sys.modules.pop(""openai_agents"", None)

        stub_agents = types.ModuleType(""agents"")
        stub_agents.Agent = object
        stub_agents.AgentRuntime = object
        stub_agents.OpenAIAgent = object
        stub_agents.Tool = _tool
        monkeypatch.setitem(sys.modules, ""agents"", stub_agents)

        orig_import = builtins.__import__

        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)

        monkeypatch.setattr(builtins, ""__import__"", fake_import)
        mod = importlib.reload(
            importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
        )
        self.assertIs(mod.Agent, stub_agents.Agent)
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"  async def set_racks(self, racks: List[PlateCarrier]):
    await super().set_racks(racks)
    warnings.warn(""Cytomat racks need to be configured with the exe software"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"def cytomat_rack_95mm_5(name: str):
  return _cytomat_rack(name=name, site_height=95, num_sites=5, model=""cytomat_rack_95mm_5"")",pylabrobot/storage/cytomat/racks.py,
survived,"  async def fetch_plate_to_loading_tray(self, plate: Plate):
    site = plate.parent
    assert isinstance(site, PlateHolder)
    await self.action_storage_to_transfer(site)
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"def cytomat_rack_33mm_15(name: str):
  return _cytomat_rack(name=name, site_height=33, num_sites=15, model=""cytomat_rack_33mm_15"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def close_door(self):
    pass
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def shovel_in(self):
    return await self.send_action(""ll"", ""sp"", ""001"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"def cytomat_rack_17mm_28(name: str):
  return _cytomat_rack(name=name, site_height=17, num_sites=28, model=""cytomat_rack_17mm_28"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  def find_random_site(self, plate: Plate) -> PlateHolder:
    return random.choice(self._find_available_sites_sorted(plate))
",pylabrobot/storage/incubator.py,Incubator
survived,"  def deserialize(cls, data: dict, allow_marshal: bool = False):
    backend = IncubatorBackend.deserialize(data.pop(""backend""))
    return cls(
      backend=backend,
      name=data[""name""],
      size_x=data[""size_x""],
      size_y=data[""size_y""],
      size_z=data[""size_z""],
      racks=[PlateCarrier.deserialize(rack) for rack in data[""racks""]],
      loading_tray_location=cast(Coordinate, deserialize(data[""loading_tray_location""])),
      rotation=Rotation.deserialize(data[""rotation""]),
      category=data[""category""],
      model=data[""model""],
    )",pylabrobot/storage/incubator.py,Incubator
survived,"  def _assemble_command(self, command_type: str, command: str, params: str):
    carriage_return = ""\r"" if self.model == CytomatType.C2C_425 else ""\r\n""
    command = f""{command_type}:{command} {params}"".strip() + carriage_return
    return command
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def set_racks(self, racks: List[PlateCarrier]):
    self._racks = racks
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  def serialize(self) -> dict:
    return {
      **IncubatorBackend.serialize(self),
      ""model"": self.model.value,
      ""port"": self.io.port,
    }
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def setup(self) -> Serial:
    """"""
    1. Open serial port (9600 8E1, RTS/CTS) via the Serial wrapper.
    2. Send >200 ms break, wait 150 ms, flush buffers.
    3. Handshake: CR → wait for CC<CR><LF>
    4. Activate handling: ST 1801 → expect OK<CR><LF>
    5. Poll ready-flag: RD 1915 → wait for ""1""<CR><LF>
    """"""
    try:
      await self.io.setup()
    except serial.SerialException as e:
      raise RuntimeError(f""Could not open {self.io.port}: {e}"")

    await self.io.send_break(duration=0.2)  # >100 ms required
    await asyncio.sleep(0.15)
    await self.io.reset_input_buffer()
    await self.io.reset_output_buffer()

    await self.io.write(b""CR\r"")
    deadline = time.time() + self.init_timeout
    while time.time() < deadline:
      resp = await self.io.readline()  # reads through LF
      if resp.strip() == b""CC"":
        break
    else:
      await self.io.stop()
      raise TimeoutError(f""No CC response from PLC within {self.init_timeout} seconds"")

    await self.io.write(b""ST 1801\r"")
    resp = await self.io.readline()
    if resp.strip() != b""OK"":
      await self.io.stop()
      raise RuntimeError(f""Unexpected reply to ST 1801: {resp!r}"")

    deadline = time.time() + self.start_timeout
    while time.time() < deadline:
      await self.io.write(b""RD 1915\r"")
      flag = await self.io.readline()
      if flag.strip() == b""1"":
        return self.io
      await asyncio.sleep(self.poll_interval)

    await self.io.stop()
    raise TimeoutError(f""PLC did not signal ready within {self.start_timeout} seconds"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"    def dependency_graph(
        self, slug: str, *, version: str = ""latest""
    ) -> Dict[str, List[str]]:
        """"""Return a mapping of template to templates it references via ``extends`` or ``include``.""""""

        visited: Dict[str, List[str]] = {}
        pattern = re.compile(r""{%\s*(?:extends|include)\s+'([^']+)'"")

        def _walk(name: str) -> None:
            if name in visited:
                return
            s, v = _split_name(name)
            source = self.registry.load_template(s, v) or """"
            deps = pattern.findall(source)
            visited[name] = deps
            for dep in deps:
                _walk(dep)

        root = slug if version == ""latest"" else f""{slug}@{version}""
        _walk(root)
        return visited",src/meta_agent/template_mixer.py,TemplateMixer
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
    )
",tests/test_template_mixer.py,
survived,"    def from_string(self, source: str) -> Template:
        """"""Create a template from a string after validation.""""""
        self.parse(source)
        return Template(source, globals=self.globals)",src/jinja2/__init__.py,Environment
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
    )
",tests/test_template_sharing.py,
survived,"        async def dispatch(
            self, request: Request, call_next: RequestResponseEndpoint
        ) -> Response:
            ip = request.client.host
            now = time.time()
            async with self.lock:
                count, start = self.counters.get(ip, (0, now))
                if now - start > self.window:
                    count = 0
                    start = now
                count += 1
                self.counters[ip] = (count, start)
                if count > self.limit:
                    return Response(""Too Many Requests"", status_code=429)
            return await call_next(request)
",src/interface/api_server.py,SimpleRateLimiter
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return s.getsockname()[1]
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_subprocess.py,
survived,"def test_simulate_sectors_file_json(tmp_path: Path) -> None:
    """"""Run simulate with a sectors file and export JSON.""""""
    src = Path(""alpha_factory_v1/demos/alpha_agi_insight_v1/docs/sectors.sample.json"")
    sectors_file = tmp_path / ""sectors.json""
    sectors_file.write_text(src.read_text(encoding=""utf-8""), encoding=""utf-8"")

    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--sectors-file"",
                    str(sectors_file),
                    ""--pop-size"",
                    ""2"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""json"",
                ],
            )

    assert res.exit_code == 0
    data = json.loads(res.output)
    assert isinstance(data, list)
    assert data
    assert {""year"", ""capability"", ""affected""} <= set(data[0])",tests/test_demo_cli.py,
survived,"def test_vmap_multiple_axes():
    Batch1 = Axis(""Batch1"", 4)
    Batch2 = Axis(""Batch2"", 3)
    Width = Axis(""Width"", 2)
    Depth = Axis(""Depth"", 5)

    named = hax.random.uniform(PRNGKey(0), (Batch1, Batch2, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, (Batch1, Batch2))(named)

    expected = jnp.sum(named.array, axis=2)

    assert jnp.allclose(selected.array, expected)
    assert selected.axes == (Batch1, Batch2, Depth)",tests/test_hof.py,
survived,"def percent_conditional(line):
    return f""{line}\n"" if not line.endswith('\\') or line.endswith('\\\\') else f""{line[:-1]}""
",test/integration/expected_out_single_line/issue192.py,
survived,"def format_values(node, values):
    return '{}({})'.format(node.__class__.__name__, ',\n    '.join(values))
",test/integration/expected_out_single_line/issue192.py,
survived,"def unique_inverse(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]:
    """"""Shortcut for :func:`unique` that also returns inverse indices.""""""

    values, inverse = typing.cast(
        tuple[NamedArray, NamedArray],
        unique(
            array,
            Unique,
            return_inverse=True,
            axis=axis,
            fill_value=fill_value,
        ),
    )
    return values, inverse
",src/haliax/ops.py,
survived,"def test_unique_shortcuts():
    Height = Axis(""Height"", 3)
    Width = Axis(""Width"", 2)

    arr2d = hax.named([[1, 2], [2, 3], [1, 2]], (Height, Width))
    U = Axis(""U"", 3)

    # unique_values
    uv = hax.unique_values(arr2d, U)
    uv_expected = hax.unique(arr2d, U)
    assert jnp.all(uv.array == uv_expected.array)

    # unique_counts
    vc, cc = hax.unique_counts(arr2d, U)
    vc_exp, cc_exp = hax.unique(arr2d, U, return_counts=True)
    assert jnp.all(vc.array == vc_exp.array)
    assert jnp.all(cc.array == cc_exp.array)

    # unique_inverse
    Height1 = Axis(""Height1"", 5)
    arr1d = hax.named([3, 4, 1, 3, 1], (Height1,))
    U2 = Axis(""U2"", 3)
    vi, ii = hax.unique_inverse(arr1d, U2)
    vi_exp, ii_exp = hax.unique(arr1d, U2, return_inverse=True)
    assert jnp.all(vi.array == vi_exp.array)
    assert jnp.all(ii.array == ii_exp.array)

    # unique_all
    U3 = Axis(""U3"", 2)
    va, ia, ina, ca = hax.unique_all(arr2d, U3, axis=Height)
    va_exp, ia_exp, ina_exp, ca_exp = typing.cast(
        tuple[NamedArray, NamedArray, NamedArray, NamedArray],
        hax.unique(
            arr2d,
            U3,
            axis=Height,
            return_index=True,
            return_inverse=True,
            return_counts=True,
        ),
    )
    assert jnp.all(va.array == va_exp.array)
    assert jnp.all(ia.array == ia_exp.array)
    assert jnp.all(ina.array == ina_exp.array)
    assert jnp.all(ca.array == ca_exp.array)",tests/test_ops.py,
survived,"        def _render_source(source: str) -> str:
            # handle extends directive
            match = re.search(r""{%\s*extends\s+'([^']+)'\s*%}"", source)
            if match:
                parent_slug = match.group(1)
                parent = self.registry.load_template(parent_slug, version) or """"
                source = source.replace(match.group(0), """")
                base = _render_source(parent)
                block_re = re.compile(
                    r""{%\s*block\s+(\w+)\s*%}(.*?){%\s*endblock\s*%}"", re.S
                )
                parent_blocks = {n: c for n, c in block_re.findall(base)}
                child_blocks = {n: c for n, c in block_re.findall(source)}
                for name, content in parent_blocks.items():
                    if name in child_blocks:
                        child = child_blocks[name].replace(""{{ super() }}"", content)
                        pattern = (
                            r""{%\s*block\s+""
                            + re.escape(name)
                            + r""\s*%}.*?{%\s*endblock\s*%}""
                        )
                        base = re.sub(pattern, child, base, flags=re.S)
                source = base
            # handle include directive
            include_re = re.compile(r""{%\s*include\s+'([^']+)'\s*%}"")

            def _replace_include(m: re.Match[str]) -> str:
                inc = self.registry.load_template(m.group(1), version) or """"
                return _render_source(inc)

            source = include_re.sub(_replace_include, source)
            return source
",src/meta_agent/template_mixer.py,TemplateMixer
survived,"    def update(val):
        try:
            config.update_settings({""int_property"": val})
        except Exception as e:
            exceptions.append(e)
",libs/core/kiln_ai/utils/test_config.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-octal-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-octal-2.py,
survived,"def _lambda6():
    draw.get(20)()
    draw.get(60)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def _lambda2():
    draw.get(2)()
    draw.get(6)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fractran.py,
survived,"def _lambda4(n):
    if n == 0:
        return 0.0
    return extract(cos, n - 1) / (float(n))
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def join(xs, sep):
    res = """"
    i = 0
    while i < len(xs):
        if i > 0:
            res = res + sep
        res = res + xs[i]
        i = i + 1
    return res
",tests/rosetta/transpiler/Python/function-frequency.py,
survived,"def c(nums):
    pass
",tests/rosetta/transpiler/Python/function-prototype.py,
survived,"def state(v):
    return State(entry=v == 0, inc=v < 10, dec=v > 0)
",tests/rosetta/transpiler/Python/gui-enabling-disabling-of-controls.py,
survived,"def chance(prob):
    threshold = int(prob * 1000.0)
    return _now() % 1000 < threshold
",tests/rosetta/transpiler/Python/forest-fire.py,
survived,"def padLeftZeros(s, width):
    out = s
    while len(out) < width:
        out = ""0"" + out
    return out
",tests/rosetta/transpiler/Python/formatted-numeric-output.py,
survived,"def greToDay(d, m, y):
    yy = y
    mm = m
    if mm < 3:
        yy = yy - 1
        mm = mm + 12
    return (yy * 36525 // 100) - (yy // 100) + (yy // 400) + 306 * (mm + 1) // 10 + d - 654842
",tests/rosetta/transpiler/Python/french-republican-calendar.py,
survived,"def b2i(b):
    if b:
        return 1
    return 0
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,
survived,"def step(n, program):
    i = 0
    while i < len(program):
        num = program[i][0]
        den = program[i][1]
        if n % den == 0:
            n = (n // den) * num
            return StepResult(n=n, ok=True)
        i = i + 1
    return StepResult(n=n, ok=False)
",tests/rosetta/transpiler/Python/fractran.py,
survived,"    async def get_latest(_: None = Depends(verify_token)) -> ResultsResponse:
        if _latest_id is None:
            raise HTTPException(status_code=404)
        result = _simulations.get(_latest_id)
        if result is None:
            raise HTTPException(status_code=404)
        return result
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"def _reset_demo_globals() -> Iterator[None]:
    """"""Reset global state altered by the demo.""""""
    yield
    if MODULE in sys.modules:
        sys.modules[MODULE]._A2A = None
",tests/test_alpha_agi_business_3_v1.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/if_else.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/join_multi.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/partial_application.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/if_then_else_nested.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/sort_stable.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_by_conditional_sum.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/membership.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/exists_builtin.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_membership.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/string_compare.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/len_map.py,
survived,"def test_view_command_invalid_password(mock_get_connection, runner):
    mock_get_connection.side_effect = InvalidPassword()

    result = runner.invoke(cli, ['view', 'bad_conn', '--table', 'users'])

    assert result.exit_code != 0
    assert ""Error: Unable to decrypt saved connection. Invalid password provided."" in result.output
",peepdb/tests/test_cli.py,
survived,"def test_service_worker_exists() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist""
    assert (dist / ""service-worker.js"").is_file()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_service_worker_present.py,
survived,"def test_download_with_retry_auth_message(
    tmp_path: Path, requests_mock: requests_mock.Mocker, capsys: pytest.CaptureFixture[str]
) -> None:
    path = tmp_path / ""out""
    url_primary = f""{fa.GATEWAY}/CID""
    requests_mock.get(url_primary, status_code=401)
    with pytest.raises(RuntimeError) as exc:
        fa.download_with_retry(""CID"", path, attempts=1, label=""wasm_llm/wasm-gpt2.tar"")
    out = capsys.readouterr().out
    assert ""WASM_GPT2_URL"" in out
    msg = str(exc.value)
    assert ""CID"" in msg and ""http"" in msg
    assert ""authentication"" in msg.lower()
",tests/test_fetch_assets.py,
survived,"def triple(x: int) -> int:
    return x * 3
",tests/human/x/python/pure_fold.py,
survived,"def test_fetch_assets_failure(monkeypatch, capsys):
    monkeypatch.setattr(fa, ""ASSETS"", {""dummy.txt"": ""cid""})

    def boom(*args, **kwargs):
        raise RuntimeError(""boom"")

    monkeypatch.setattr(fa, ""download_with_retry"", boom)

    with pytest.raises(SystemExit) as exc:
        fa.main()

    out = capsys.readouterr().out
    assert ""Download failed for dummy.txt"" in out
    assert ""ERROR: Unable to retrieve dummy.txt"" in out
    assert exc.value.code == 1",tests/test_fetch_assets.py,
survived,"    def _get_thread(self, service, thread_id: str, include_spam_trash: bool) -> dict:
        thread = (
            service.users()
            .threads()
            .get(
                userId=""me"",
                id=thread_id,
                format=""full"",
                includeSpamTrash=include_spam_trash,
            )
            .execute()
        )

        parsed_messages = []
        for msg in thread.get(""messages"", []):
            headers = {
                h[""name""].lower(): h[""value""]
                for h in msg.get(""payload"", {}).get(""headers"", [])
            }
            body = self._get_email_body(msg)
            attachments = self._get_attachments(service, msg)
            email = Email(
                threadId=msg.get(""threadId"", thread_id),
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg.get(""snippet"", """"),
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=body,
                sizeEstimate=msg.get(""sizeEstimate"", 0),
                attachments=attachments,
            )
            parsed_messages.append(email.dict())

        thread[""messages""] = parsed_messages
        return thread
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailGetThreadBlock
survived,"    def __init__(self, bus: orchestrator.messaging.A2ABus, ledger: orchestrator.Ledger) -> None:
        super().__init__(""fail"", bus, ledger)
",tests/test_orchestrator_backoff.py,FailingAgent
survived,"    async def skill_test(self, payload: dict) -> dict:
        return {""ok"": True}
",tests/test_orchestrator_rest.py,DummyAgent
survived,"    async def skill_test(self, payload: dict) -> dict:
        """"""Execute a diagnostic skill test.

        Agents may override this method to provide custom behaviour.
        The default implementation returns ``{""ok"": True}``.
        """"""
        return {""ok"": True}
",alpha_factory_v1/backend/agents/base.py,AgentBase
survived,"def test_seed_iteration():
    cfg = make_cfg()
    es = EnvStateManager(cfg, mode='train')
    es.reset()
    first_seed = es.envs[0]['status'].seed
    es.reset()
    second_seed = es.envs[0]['status'].seed
    assert first_seed == 7
    assert second_seed == 8",tests/es_manager/test_seed_iteration.py,
survived,"def body_checksum(address: int, sig: Signal, d: bytearray) -> int:
    crc = 0xFF
    poly = 0xD5
    for i in range(len(d) - 2, -1, -1):
        crc ^= d[i]
        for _ in range(8):
            if crc & 0x80:
                crc = ((crc << 1) ^ poly) & 0xFF
            else:
                crc = (crc << 1) & 0xFF
    return crc
",opendbc/can/packer.py,
survived,"def hkg_can_fd_checksum(address: int, sig: Signal, d: bytearray) -> int:
    crc = 0
    for i in range(2, len(d)):
        crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ d[i]]) & 0xFFFF
    crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ ((address >> 0) & 0xFF)]) & 0xFFFF
    crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ ((address >> 8) & 0xFF)]) & 0xFFFF
    if len(d) == 8:
        crc ^= 0x5F29
    elif len(d) == 16:
        crc ^= 0x041D
    elif len(d) == 24:
        crc ^= 0x819D
    elif len(d) == 32:
        crc ^= 0x9F5B
    return crc
",opendbc/can/packer.py,
survived,"    def test_import_system_blocked(self):
        agent_base.resource = None
        agent_base.signal = None
        se = SafeExec()
        code = """"""\

def transform(x):
    return __import__('os').system('echo hi')
""""""
        with self.assertRaises(NameError):
            se.run(code, ""transform"", 0)
",tests/test_safe_exec_security.py,TestSafeExecSecurity
survived,"def test_merkle_root_tracking(tmp_path: Path) -> None:
    db = tmp_path / ""arch.db""
    h1 = hashlib.sha256(json.dumps({""parent"": ""p"", ""child"": ""c1"", ""metrics"": {""s"": 1}}, sort_keys=True).encode()).hexdigest()
    insert(""p"", ""c1"", {""s"": 1}, db_path=db)
    h2 = hashlib.sha256(json.dumps({""parent"": ""c1"", ""child"": ""c2"", ""metrics"": {""s"": 2}}, sort_keys=True).encode()).hexdigest()
    insert(""c1"", ""c2"", {""s"": 2}, db_path=db)
    root = merkle_root(db_path=db)
    assert root == _manual_root([h1, h2])
",tests/test_archive_cron.py,
survived,"def utc_now() -> str:
    return datetime.now(timezone.utc).isoformat(timespec=""milliseconds"")
",alpha_factory_v1/backend/agent_manager.py,
survived,"    async def _trigger(name: str) -> Dict[str, bool]:  # noqa: D401
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        runners[name].next_ts = 0
        return {""queued"": True}
",alpha_factory_v1/backend/api_server.py,
survived,"async def regression_guard(runners: Dict[str, AgentRunner]) -> None:
    history: deque[float] = deque(maxlen=3)
    while True:
        await asyncio.sleep(1)
        try:
            sample = metrics.dgm_best_score.collect()[0].samples[0]
            score = float(sample.value)
        except Exception:  # pragma: no cover - metrics optional
            continue
        history.append(score)
        if len(history) == 3 and history[1] <= history[0] * 0.8 and history[2] <= history[1] * 0.8:
            runner = runners.get(""aiga_evolver"")
            if runner and runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
            alerts.send_alert(""Evolution paused due to metric regression"")
            history.clear()",alpha_factory_v1/backend/agent_manager.py,
survived,"        def set(self, *_a: Any) -> None:
            ...
",alpha_factory_v1/backend/telemetry.py,_Metric
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_left_join.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/left_join.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/left_join_multi.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/inner_join.py,Order
survived,"        def __init__(self, *a, **kw) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AgentRuntime
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",tests/test_bus_ssl_gen.py,
survived,"    def fake_setrlimit(res: int, limits: tuple[int, int]) -> None:
        recorded.append((res, limits))
",tests/test_codegen_agent.py,
survived,"def test_invalid_seed_fallback(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
    """"""Invalid ALPHA_ASI_SEED should trigger fallback to default.""""""
    module = ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    mod = importlib.import_module(module)
    monkeypatch.setenv(""ALPHA_ASI_SEED"", ""bad"")
    caplog.set_level(""WARNING"")
    cfg = mod._load_cfg()
    assert mod._SEED == 42
    assert any(""Invalid seed"" in rec.message for rec in caplog.records)
    assert isinstance(cfg, mod.Config)",tests/test_world_model_demo.py,
survived,"    def _run_command(command: str, cwd: str) -> None:
        if PlatformUtils.get_platform_id().value.startswith(""win""):
            subprocess.run(
                command,
                shell=True,
                check=True,
                cwd=cwd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
        else:
            import pwd

            user = pwd.getpwuid(os.getuid()).pw_name
            subprocess.run(
                command,
                shell=True,
                check=True,
                user=user,
                cwd=cwd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection
survived,"    def for_current_platform(self) -> list[RuntimeDependency]:
        return self.for_platform(PlatformUtils.get_platform_id().value)
",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = int(s.getsockname()[1])
    s.close()
    return port
",tests/test_orchestrator_bus_tls_env.py,
survived,"    def __init__(self, *args: object, **kwargs: object) -> None:
        super().__init__(*args)
",tests/conftest.py,APITimeoutError
survived,"def bigTrim(a):
    n = len(a)
    while n > 1 and a[n - 1] == 0:
        a = a[0:n - 1]
        n = n - 1
    return a
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,
survived,"def toInt(c):
    return int(c(incr)(0))
",tests/rosetta/transpiler/Python/church-numerals-1.py,
survived,"def printNumeral():
    i = 0
    while i < 15:
        line = """"
        j = 0
        while j < 11:
            line = line + n[i][j] + "" ""
            j = j + 1
        print(line)
        i = i + 1
    print("""")
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def trimRightStr(s):
    end = len(s)
    while end > 0 and s[end - 1:end] == "" "":
        end = end - 1
    return s[0:end]
",tests/rosetta/transpiler/Python/composite-numbers-k-with-no-single-digit-factors-whose-factors-are-all-substrings-of-k.py,
survived,"def isPrime(n):
    if n < 2:
        return False
    if n % 2 == 0:
        return n == 2
    if n % 3 == 0:
        return n == 3
    d = 5
    while d * d <= n:
        if n % d == 0:
            return False
        d = d + 2
        if n % d == 0:
            return False
        d = d + 4
    return True
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,
survived,"def printSym(m):
    printMat(unpackSym(m))
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,
survived,"def toStr(x):
    s = """"
    def fCounter(f):
        global s
        s = s + ""|""
        return f
    x(fCounter)(id)
    return s
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def printMat(m):
    i = 0
    while i < len(m):
        line = """"
        j = 0
        while j < len(m[i]):
            line = line + str(m[i][j])
            if j < len(m[i]) - 1:
                line = line + "" ""
            j = j + 1
        print(line)
        i = i + 1
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,
survived,"def mul(c, d):
    return lambda f: lambda x: c(d(f))(x)
",tests/rosetta/transpiler/Python/church-numerals-1.py,
survived,"def test_ask_interrupt(monkeypatch):
    def raise_interrupt(_):
        raise EOFError

    monkeypatch.setattr(""builtins.input"", raise_interrupt)
    inter = Interactive()
    with pytest.raises(InteractiveError):
        inter.ask(""Question"")",tests/ux/test_interactive.py,
survived,"def test_health_endpoint() -> None:
    """"""Verify /health returns expected metrics.""""""
    module = importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.agent_aiga_entrypoint"")
    client = TestClient(cast(Any, module.app))

    resp = client.get(""/health"")
    assert resp.status_code == 200
    data = resp.json()
    assert set(data) >= {""status"", ""generations"", ""best_fitness""}",tests/test_aiga_service.py,
survived,"    def post(path: Path, meta) -> None:
        calls.append(""post"")
",tests/test_bundle_api.py,
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
        self._metadata: BundleMetadata | None = None
",src/meta_agent/bundle.py,Bundle
survived,"    def pre(path: Path) -> None:
        calls.append(""pre"")
",tests/test_bundle_api.py,
survived,"def _small_population() -> list[mats.Individual]:
    """"""Return a tiny population with known fitness values.""""""

    fits = [(1.0, 3.0), (2.0, 2.0), (3.0, 1.0), (4.0, 5.0), (5.0, 4.0)]
    return [mats.Individual([], fitness=f) for f in fits]
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_mats.py,
survived,"def _make_agent(monkeypatch):
    """"""Return a ResearchAgent wired with dummy bus/ledger.""""""
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.agents import research_agent
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils import config, messaging

    class DummyBus:
        def __init__(self, settings: config.Settings) -> None:
            self.settings = settings
            self.published: list[tuple[str, messaging.Envelope]] = []

        def publish(self, topic: str, env: messaging.Envelope) -> None:
            self.published.append((topic, env))

        def subscribe(self, _t: str, _h):
            pass

    class DummyLedger:
        def __init__(self) -> None:
            self.logged: list[messaging.Envelope] = []

        def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
            self.logged.append(env)

        def start_merkle_task(self, *_a, **_kw):
            pass

        async def stop_merkle_task(self) -> None:  # pragma: no cover - interface
            pass

        def close(self) -> None:
            pass

    settings = config.Settings(bus_port=0)
    bus = DummyBus(settings)
    agent = research_agent.ResearchAgent(bus, DummyLedger())
    return agent, bus
",tests/test_adapters.py,
survived,"    def generate_text(self, prompt: str) -> str:
        """"""Generate text using ``adk.Client`` if the method exists.""""""
        gen_fn = getattr(self._client, ""generate"", None)
        if not callable(gen_fn):
            raise AttributeError(""generate not available"")
        return gen_fn(prompt)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_adapter.py,ADKAdapter
survived,"        def __init__(self) -> None:
            self.called: list[str] = []
",tests/test_adapters.py,StubADK
survived,"    async def patched_run_cycle(self) -> None:
        res = await self.mcp.invoke_tool(""echo"", {""t"": 1})
        await self.emit(""strategy"", res)
",tests/test_adapters.py,
survived,"    async def fake_call_tool(self, name: str, args: dict[str, object]) -> object:
        calls[""call""] = (name, args)
        return {""done"": True}
",tests/test_adapters.py,
survived,"def test_fuzz_envelope_blocks_malicious(sender: str, recipient: str, ts: float, payload: dict[str, object]) -> None:
    code = payload[""code""]
    assume(""import os"" in code)
    bus = DummyBus(config.Settings(bus_port=0))
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(sender, recipient, payload, ts)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_safety_guardian_property.py,
survived,"def pad(
    array: NamedArray,
    pad_width: Mapping[AxisSelector, tuple[int, int]],
    *,
    mode: str = ""constant"",
    constant_values: NamedOrNumeric = 0,
    **kwargs,
) -> NamedArray:
    """"""Version of ``jax.numpy.pad`` that works with ``NamedArray``.

    ``pad_width`` should be a mapping from axis (or axis name) to a ``(before, after)``
    tuple specifying how much padding to add on each side of that axis. Any axis
    not present in ``pad_width`` will not be padded.
    """"""

    padding = []
    new_axes = []
    for ax in array.axes:
        left_right = pad_width.get(ax)
        if left_right is None:
            left_right = pad_width.get(axis_name(ax))  # type: ignore[arg-type]
        if left_right is None:
            left_right = (0, 0)
        left, right = left_right
        padding.append((left, right))
        new_axes.append(ax.resize(ax.size + left + right))

    result = jnp.pad(
        array.array,
        padding,
        mode=mode,
        constant_values=raw_array_or_scalar(constant_values),
        **kwargs,
    )

    return NamedArray(result, tuple(new_axes))
",src/haliax/ops.py,
survived,"def Tool(*_args, **_kwargs):
    def decorator(func):
        return func

    return decorator
",openai_agents/__init__.py,
survived,"            async def __call__(self, text: str) -> str:
                return ""ok""
",alpha_factory_v1/demos/aiga_meta_evolution/utils.py,OpenAIAgent
survived,"    def test_entry_point_resolves(self) -> None:
        eps = im.entry_points().select(group=""console_scripts"")
        match = [ep for ep in eps if ep.name == ""mats-bridge""]
        self.assertTrue(match, ""mats-bridge entry point not found"")
        self.assertEqual(
            match[0].value,
            ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge:main"",
        )
",tests/test_mats_bridge_entrypoint.py,TestMatsBridgeEntryPoint
survived,"    async def get_resource(ctx: EnrichContext, **kwargs: int) -> enrich_model | None:  # type: ignore[name-defined]
        entity_id = kwargs[param_name]
        session_factory = ctx.request_context.lifespan_context[session_key]
        async with session_factory() as session:  # type: AsyncSession
            obj = await session.get(sa_model, entity_id)
            return _sa_to_enrich(obj, enrich_model) if obj else None
",src/enrichmcp/sqlalchemy/auto.py,
survived,"    async def run() -> None:
        await asyncio.gather(
            orch.evolve(""a"", fn, 1, experiment_id=""exp1"", population_size=2, generations=1),
            orch.evolve(""b"", fn, 1, experiment_id=""exp2"", population_size=2, generations=1),
        )
",tests/test_experiments.py,
survived,"def test_ragged_paged_attention_incremental_multi_seq():
    rng = jr.PRNGKey(3)
    seq_lens = [10, 37, 64]
    k_lens = [1, 3, 9]
    q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs = _build_incremental_case(rng, seq_lens, k_lens)

    ragged = default_ragged_paged_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs, sm_scale=SM_SCALE)
    ref = _reference_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, k_lens)

    assert ragged.axes == ref.axes
    assert_trees_all_close(ragged.array, ref.array, atol=1e-3, rtol=1e-3)",tests/test_paged_attention.py,
survived,"    def test_run_demo_anthropic_rewriter(self) -> None:
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--rewriter"",
                ""anthropic"",
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def health() -> dict[str, str]:
    """"""Return service health status.""""""
    return {""status"": ""ok""}",test_repo/backend/main.py,
survived,"def test_openai_link_head() -> None:
    url = dg.model_urls(""124M"")[0]
    resp = requests.head(url, timeout=10)
    assert resp.status_code == 200",tests/test_download_openai_gpt2.py,
survived,"def print_banner() -> None:
    """"""Display the default banner.""""""
    print(banner())
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,
survived,"  def supports_active_cooling(self) -> bool:
    """"""Whether this backend can actively cool below ambient temperature.""""""
    raise NotImplementedError
",pylabrobot/temperature_controlling/backend.py,TemperatureControllerBackend
survived,"def test_simulate_save_plots(tmp_path: Path) -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            with runner.isolated_filesystem(temp_dir=tmp_path):
                res = runner.invoke(
                    cli.main,
                    [
                        ""simulate"",
                        ""--horizon"",
                        ""1"",
                        ""--offline"",
                        ""--sectors"",
                        ""1"",
                        ""--pop-size"",
                        ""1"",
                        ""--generations"",
                        ""1"",
                        ""--save-plots"",
                    ],
                )
    assert res.exit_code == 0
    assert Path(""pareto.png"").exists()
    assert Path(""pareto.json"").exists()",tests/test_demo_cli.py,
survived,"        def search(self, *_):
            class Limiter:
                def limit(self, *_):
                    class Selector:
                        def select(self, *_):
                            return self

                        def to_list(self):
                            return [{""_distance"": 0.1, ""index"": 0, ""pdf_name"": ""x.pdf"", ""pdf_page"": 1}]

                    return Selector()

            return Limiter()
",no-ocr-api/tests/test_ingest_search.py,FakeTable
survived,"    def fake_from_list(lst):
        return FakeDataset(lst)
",no-ocr-api/tests/test_ingest_search.py,
survived,"def test_large_payloads_delivered_intact(
    sender: str, recipient: str, payload_text: str, ts: float
) -> None:  # type: ignore[misc]
    """"""Envelopes with huge strings should round-trip through the bus.""""""

    bus = messaging.A2ABus(config.Settings(bus_port=0))
    received: list[messaging.Envelope] = []

    async def handler(env: messaging.Envelope) -> None:
        received.append(env)

    bus.subscribe(recipient, handler)
    env = messaging.Envelope(sender=sender, recipient=recipient, ts=ts)
    env.payload[""data""] = payload_text

    async def run() -> None:
        bus.publish(recipient, env)
        await asyncio.sleep(0)

    asyncio.run(run())

    assert received
    assert received[0].sender == sender
    assert received[0].recipient == recipient
    assert received[0].payload[""data""] == payload_text
    assert received[0].ts == ts
",tests/test_bus_large_payloads_property.py,
survived,"def scenario_2001_genome() -> replay.Scenario:
    return replay.load_scenario(""2001_genome"")
",tests/conftest.py,
survived,"        def __init__(self) -> None:
            self.status_code = 200
",tests/test_start_alpha_business.py,Resp
survived,"        def poll(self) -> None:
            return None
",tests/test_start_alpha_business.py,DummyProc
survived,"    def _make_client(self):
        runner = DummyRunner()
        runner.inst = DummyAgent()
        app = _build_rest({""foo"": runner})
        return TestClient(app), runner
",alpha_factory_v1/tests/test_orchestrator_rest.py,UpdateModelTest
survived,"    def register(
        self,
        metadata: TemplateMetadata,
        content: str,
        version: str = ""0.1.0"",
    ) -> Optional[str]:
        slug = metadata.slug
        slug_sanitized = slug.replace("" "", ""_"").lower()
        version_sanitized = ""v"" + version.replace(""."", ""_"")
        version_dir = self.templates_dir / slug_sanitized / version_sanitized
        version_dir.mkdir(parents=True, exist_ok=True)
        template_path = version_dir / TEMPLATE_FILE_NAME
        template_path.write_text(content, encoding=""utf-8"")
        checksum = sha256(content.encode(""utf-8"")).hexdigest()
        metadata_dict = (
            metadata.model_dump()
            if hasattr(metadata, ""model_dump"")
            else metadata.dict()
        )
        meta_data = {
            **metadata_dict,
            ""version"": version,
            ""checksum"": checksum,
        }
        with open(version_dir / METADATA_FILE_NAME, ""w"", encoding=""utf-8"") as f:
            json.dump(meta_data, f, indent=2)
        manifest = self._load_manifest()
        entry = manifest.setdefault(slug_sanitized, {""versions"": {}})
        entry[""versions""][version] = {
            ""path"": f""{slug_sanitized}/{version_sanitized}/{TEMPLATE_FILE_NAME}"",
            ""checksum"": checksum,
            ""created_at"": datetime.utcnow().isoformat(),
        }
        entry[""current_version""] = version
        self._save_manifest(manifest)
        return str(template_path)
",src/meta_agent/template_registry.py,TemplateRegistry
survived,"def test_duplicate_disclaimer_fails(tmp_path: Path) -> None:
    repo = _create_repo(tmp_path, SNIPPET_TEXT + ""\n"" + SNIPPET_TEXT)
    missing, duplicates = verify_disclaimer_snippet.check_repo(repo)
    assert duplicates == [repo / ""README.md""]",tests/test_verify_disclaimer_snippet.py,
survived,"def sha384(path: Path) -> str:
    """"""Return the SHA-384 digest of ``path`` in SRI format.""""""
    digest = hashlib.sha384(path.read_bytes()).digest()
    return ""sha384-"" + base64.b64encode(digest).decode()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build/common.py,
survived,"    def _build_prompt(self, n: int) -> str:
        examples = (
            ""\n\n"".join(
                f""```python\n{t.program}```\n```json\n{t.inp}```\n```json\n{t.out}```""
                for t in self._rng.sample(self.buffer, k=min(3, len(self.buffer)))
            )
            or ""(buffer empty)""
        )

        return self._PROMPT.format(
            n=n,
            max_loc=MAX_PROG_LOC,
            buf=len(self.buffer),
            examples=examples,
        )
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"    async def _monitor(self) -> None:
        while True:
            await asyncio.sleep(2)
            now = time.time()
            for r in list(self.runners.values()):
                if r.task and r.task.done():
                    await r.restart(self.bus, self.ledger)
                elif now - r.last_beat > r.period * 5:
                    logging._log.warning(""%s unresponsive – restarting"", r.agent.name)
                    await r.restart(self.bus, self.ledger)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator
survived,"def _simulate(horizon: int, curve: str, pop_size: int, generations: int) -> list[Any]:
    """"""Run the disruption forecast and return the trajectory.""""""

    secs = [sector.Sector(f""s{i:02d}"") for i in range(pop_size)]
    return forecast.forecast_disruptions(
        secs,
        horizon,
        curve,
        pop_size=pop_size,
        generations=generations,
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/web_app.py,
survived,"    def vmap(self, init, *extra_args, **extra_kwargs):
        """"""Apply each block independently using :func:`haliax.vmap`.

        This maps ``init`` through every block in parallel, so each block
        receives the same ``init`` but its own parameters.  Extra ``args`` and
        ``kwargs`` are also mapped over the block axis by default.

        Returns the stacked outputs of each block.
        """"""

        if haliax.is_named_array(init):
            init = init.broadcast_axis(self.Block)
        elif haliax.jax_utils.is_jax_array_like(init):
            init = jnp.broadcast_to(init, (self.Block.size,) + init.shape)
        else:
            init = tuple(init for _ in range(self.Block.size))

        arg_spec = (0, 0) + (0,) * len(extra_args)
        kwarg_spec = {k: 0 for k in extra_kwargs}

        do_vmap = haliax.vmap(
            Stacked._do_block,
            self.Block,
            default=0,
            args=arg_spec,
            kwargs=kwarg_spec,
        )
        return do_vmap(init, self.stacked, *extra_args, **extra_kwargs)
",src/haliax/nn/scan.py,Stacked
survived,"def test_dtype_category_annotation_and_check():
    def baz(x: Float[""b""]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(baz, include_extras=True)[""x""])[1]
    assert str(spec.dtype) == ""float""

    B = Axis(""b"", 1)
    arr = NamedArray(jnp.ones((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float[""b""])  # type: ignore
    assert not arr.matches_axes(Int[""b""])  # type: ignore",tests/test_dtype_typing.py,
survived,"    def bar(x: i32[""batch""]):  # type: ignore  # noqa: F722
        pass
",tests/test_dtype_typing.py,
survived,"    def __init__(self, payload=None, text=""ok"", status_code=200):
        self._payload = payload
        self.text = text
        self.status_code = status_code
",tests/test_openai_bridge_integration.py,DummyResponse
survived,"def call(code: str, func: str, *args: Any, mochi_bin: str = ""mochi"") -> Any:
    """"""Call ``func`` defined in ``code`` with ``args`` and return the result.

    ``code`` should contain the Mochi function definition. The result is
    obtained by wrapping the call with the ``json`` builtin and decoding the
    output.
    """"""
    args_literal = "", "".join(_to_mochi(a) for a in args)
    snippet = f""{code}\njson({func}({args_literal}))\n""
    out = _run(snippet, mochi_bin)
    return json.loads(out.strip())
",tools/libmochi/python/libmochi.py,
survived,"        def get_tracer(self, _name: str) -> DummyTracer:
            return self.tracer
",tests/test_metrics.py,DummyTrace
survived,"        def start_as_current_span(self, name: str) -> Any:
            self.spans.append(name)
            return nullcontext()
",tests/test_metrics.py,DummyTracer
survived,"def test_router_init_requires_valid_default():
    with pytest.raises(ValueError):
        GuardrailModelRouter({""a"": MockAdapter()}, default_model=""b"")
",tests/test_guardrail_router.py,
survived,"def test_guardrail_config_from_dict():
    data = {""rules"": [{""name"": ""pii"", ""pattern"": ""ssn""}]}
    cfg = GuardrailConfig.from_dict(data)
    assert len(cfg.rules) == 1
    assert cfg.rules[0].name == ""pii""
",tests/test_guardrail_generator.py,
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return s.getsockname()[1]
",tests/test_api_server_subprocess.py,
survived,"def test_strip_color_codes():
    text = ""\x1b[31merror\x1b[0m""
    assert strip_color_codes(text) == 'error'
",tests/test_whois_perms.py,
survived,"        def start_merkle_task(self, *_a, **_kw) -> None:
            pass
",tests/test_agents.py,DummyLedger
survived,"        def log(self, env) -> None:  # type: ignore[override]
            events.append(env.payload.get(""event""))
",tests/test_agents.py,DummyLedger
survived,"    def test_no_pydantic_available(self):
        global memf
        sys.modules.pop(""alpha_factory_v1.backend.memory_fabric"", None)
        importlib.invalidate_caches()
        with mock.patch.dict(
            sys.modules,
            {""pydantic"": None, ""pydantic_settings"": None},
        ):
            memf = importlib.import_module(""alpha_factory_v1.backend.memory_fabric"")
            self.assertEqual(memf.CFG.PGDATABASE, ""memdb"")
            self.assertEqual(memf.BaseSettings.__module__, memf.__name__)
        # reload original module for other tests
        sys.modules.pop(""alpha_factory_v1.backend.memory_fabric"", None)
        memf = importlib.import_module(""alpha_factory_v1.backend.memory_fabric"")
",alpha_factory_v1/tests/test_memory_provider.py,SettingsFallbackTest
survived,"    def close(self) -> None:
        """"""Close any open database connections.""""""
        if getattr(self, ""_driver"", None):
            try:
                self._driver.close()
            except Exception as exc:  # pragma: no cover - defensive
                logger.warning(""GraphStore: driver close failed → %s"", exc)
            finally:
                self._driver = None
",alpha_factory_v1/backend/memory_fabric.py,_GraphStore
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Partsupp
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q2.py,Partsupp
survived,"def test_Q2_returns_only_supplier_with_min_cost_in_Europe_for_brass_part():
    assert result == [
        {
            ""s_acctbal"": 1000,
            ""s_name"": ""BestSupplier"",
            ""n_name"": ""FRANCE"",
            ""p_partkey"": 1000,
            ""p_mfgr"": ""M1"",
            ""s_address"": ""123 Rue"",
            ""s_phone"": ""123"",
            ""s_comment"": ""Fast and reliable"",
            ""ps_supplycost"": 10,
        }
    ]
",tests/machine/x/python/q2.py,
survived,"        def step(self, _a: int):
            return None, self.reward, True, {}
",tests/test_world_model_demo.py,DummyEnv
survived,"        def train_once(self) -> float:
            return self.loss
",tests/test_world_model_demo.py,DummyLearner
survived,"def run_transfer_test(
    models: Iterable[str],
    top_n: int,
    *,
    archive_path: str | Path = DEFAULT_ARCHIVE,
    out_file: str | Path = DEFAULT_RESULTS,
) -> None:
    """"""Evaluate the top ``top_n`` agents on each model.

    Appends the results to ``out_file``.
    """"""

    arch = Archive(archive_path)
    agents = sorted(arch.all(), key=lambda a: a.score, reverse=True)[:top_n]

    path = Path(out_file)
    path.parent.mkdir(parents=True, exist_ok=True)
    exists = path.exists()
    with path.open(""a"", newline="""", encoding=""utf-8"") as fh:
        writer = csv.writer(fh)
        if not exists:
            writer.writerow([""id"", ""model"", ""score""])
        for agent in agents:
            for model in models:
                score = evaluate_agent(agent, model)
                writer.writerow([agent.id, model, f""{score:.3f}""])
",src/tools/transfer_test.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""10""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(10)",benchmarks/poly_mini/task_010.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""16""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(16)",benchmarks/poly_mini/task_016.py,
survived,"def run() -> None:
    n = 9
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_009.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""14""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(14)",benchmarks/poly_mini/task_014.py,
survived,"def run() -> None:
    n = 13
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_013.py,
survived,"def run() -> None:
    n = 25
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_025.py,
survived,"def test_improve_repo_invalid_patch(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)

    patch_file = tmp_path / ""patch.diff""
    patch_file.write_text("""")
    log_file = tmp_path / ""log.json""

    with pytest.raises(ValueError):
        self_improver.improve_repo(
            str(repo_dir), str(patch_file), ""metric.txt"", str(log_file)
        )
",tests/test_self_improver.py,
survived,"def test_improve_repo_requires_git(monkeypatch, tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text(""dummy"")
    log_file = tmp_path / ""log.json""

    monkeypatch.setattr(self_improver, ""git"", None)
    with pytest.raises(RuntimeError):
        self_improver.improve_repo(
            str(repo_dir), str(patch_file), ""metric.txt"", str(log_file)
        )",tests/test_self_improver.py,
survived,"def test_cached_header_and_timeout(monkeypatch: pytest.MonkeyPatch) -> None:
    _clear_env_cache()
    monkeypatch.delenv(""LANGCHAIN_API_KEY"", raising=False)
    with patch.dict(""os.environ"", {}, clear=True):
        client = Client(
            api_url=""http://localhost:1984"",
            api_key=""123"",
            timeout_ms=(2000, 4000),
            auto_batch_tracing=False,
        )
        assert client._timeout == (2.0, 4.0)
        assert client._headers[""x-api-key""] == ""123""
        # Changing API key should update headers
        client.api_key = ""abc""
        assert client._headers[""x-api-key""] == ""abc""

        mock_response = MagicMock()
        client.session.request.return_value = mock_response
        with patch(""langsmith.client.ls_utils.raise_for_status_with_text""):
            client.request_with_retries(""GET"", ""/test"")
        args, kwargs = client.session.request.call_args
        assert kwargs[""timeout""] == client._timeout
        assert kwargs[""headers""][""x-api-key""] == ""abc""
",python/tests/unit_tests/test_client.py,
survived,"def test_add():
    assert calc.add(1, 1) == 2",tests/fixtures/self_heal_repo/test_calc.py,
survived,"        def _get(self: struct_pb2.Struct, key: str, default=None):
            try:
                return self[key]
            except Exception:
                return default
",tests/test_safety_block.py,
survived,"    async def run() -> None:
        async with bus, ledger:
            await chaos.run_cycle()
            await asyncio.sleep(0)
",tests/test_safety_block.py,
survived,"    def schedule(self, jobs: List[List[Dict[str, Any]]], horizon: int) -> Dict[str, Any]:
        """"""Synchronous wrapper around :meth:`_build_async` returning a dict.""""""
        req = {""jobs"": jobs, ""horizon"": horizon}
        loop = asyncio.get_event_loop()
        res = loop.run_until_complete(self._build_async(req))
        try:
            return json.loads(res)[""payload""]
        except Exception:  # noqa: BLE001
            return json.loads(res)
",alpha_factory_v1/backend/agents/manufacturing_agent.py,ManufacturingAgent
survived,"def ema(prices: Sequence[float], span: int = 20) -> float:
    """"""Return the exponential moving average over ``span`` periods.""""""

    if span <= 0:
        raise ValueError(""span must be positive"")
    if not prices:
        return 0.0

    alpha = 2 / (span + 1)
    ema_val = float(prices[0])
    for p in prices[1:]:
        ema_val = (float(p) - ema_val) * alpha + ema_val
    return ema_val
",alpha_factory_v1/backend/alpha_model.py,
survived,"    def test_stochastic_zero_noise(self):
        genes = {""temperature"": 0.7, ""top_p"": 0.9, ""max_tokens"": 128}
        self.assertAlmostEqual(
            gt.stochastic_fitness(genes, noise=0.0), gt.toy_fitness(genes), places=6
        )
",alpha_factory_v1/tests/test_genetic_tests.py,GeneticTestsTest
survived,"    def test_geneconfig_roundtrip(self):
        cfg = gt.GeneConfig(0.8, 0.95, 256)
        d = cfg.as_dict()
        self.assertEqual(d[""temperature""], 0.8)
        self.assertEqual(d[""top_p""], 0.95)
        self.assertEqual(d[""max_tokens""], 256)
",alpha_factory_v1/tests/test_genetic_tests.py,GeneticTestsTest
survived,"    def step(self, action: str) -> Tuple[float, float, bool]:
        """"""Execute ``action`` and return (price, reward, done).""""""
        self.price = self.sample_next_price(self.price)
        reward = 0.0
        done = False
        return self.price, reward, done
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv
survived,"    def book(self) -> Dict[str, float]:
        """"""Return a copy of the current position book.""""""
        return dict(self._positions)
",alpha_factory_v1/backend/portfolio.py,Portfolio
survived,"def main() -> None:
    ap = argparse.ArgumentParser(description=""Run Meta-Agentic α-AGI demo"")
    ap.add_argument(""--gens"", type=int, default=6, help=""number of generations"")
    ap.add_argument(
        ""--provider"",
        default=os.getenv(""LLM_PROVIDER"", ""mistral:7b-instruct.gguf""),
        help=""openai:gpt-4o | anthropic:claude-3-sonnet | mistral:7b-instruct.gguf"",
    )
    ap.add_argument(
        ""--ui"",
        action=""store_true"",
        help=""launch Streamlit lineage UI after the search loop"",
    )
    ap.add_argument(
        ""--db"",
        type=Path,
        default=DB,
        help=""path to lineage SQLite DB"",
    )
    args = ap.parse_args()

    os.environ[""METAAGI_DB""] = str(args.db)

    try:
        asyncio.run(meta_loop(args.gens, args.provider))
    except KeyboardInterrupt:
        return

    if args.ui:
        ui_path = Path(__file__).parent / ""ui"" / ""lineage_app.py""
        print(f""\nStarting Streamlit UI → {ui_path}\n"")
        subprocess.call([""streamlit"", ""run"", str(ui_path)])
",alpha_factory_v1/demos/meta_agentic_agi/app.py,
survived,"    def step(self, act:int):
        dx,dy = [(0,1),(1,0),(0,-1),(-1,0)][act%4]
        nx,ny = self._clip(self.agent[0]+dx), self._clip(self.agent[1]+dy)
        if (nx,ny) in self.obstacles: nx,ny = self.agent
        self.agent=(nx,ny)
        done = self.agent==self.goal
        reward = 1.0 if done else -0.01
        return self._obs(), reward, done, {}
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MiniWorld
survived,"            def handle(self,msg):
                if ""ask_plan"" in msg:
                    try:
                        plan=self._safe_call(msg[""ask_plan""])
                        self.emit(""planning_agent"",{""llm_plan"":plan})
                    except Exception as e:
                        LOG.warning(""LLMPlanner error: %s"",e)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,LLMPlanner
survived,"            def __init__(self): super().__init__(""llm_planner"")
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,LLMPlanner
survived,"    def __init__(self):
        self.gen=POETGenerator()
        self.envs=[self.gen.propose() for _ in range(CFG.env_batch)]
        self.learners=[Learner(e) for e in self.envs]
        self.stop=False
        A2ABus.subscribe(""orch"",self._on_cmd)
        LOG.info(""Orchestrator online with %d envs"", CFG.env_batch)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Orchestrator
survived,"    async def get_cash(self) -> float:
        return self.cash
",alpha_factory_v1/backend/broker/broker_sim.py,SimulatedBroker
survived,"def delete(
    url: str,
    *,
    params: dict | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    """"""HTTP DELETE request.""""""
    return _call(""DELETE"", url, params=params, headers=headers, timeout=timeout)
",alpha_factory_v1/requests.py,
survived,"    def text(self) -> str:
        try:
            return self.content.decode()
        except UnicodeDecodeError:
            return self.content.decode(""latin1"", errors=""replace"")
",alpha_factory_v1/requests.py,Response
survived,"    def ok(self) -> bool:
        return self.status_code < 400
",alpha_factory_v1/requests.py,Response
survived,"def _print_console(logs: list[str]) -> None:
    if logs:
        print(""--- Browser console logs ---"", file=sys.stderr)
        for line in logs:
            print(line, file=sys.stderr)
",scripts/verify_insight_offline.py,
survived,"    def _folder_name_to_id(name: str, cache: dict[str, str]) -> str | None:
        return cache.get(name)
",app/services/media/jellyfin.py,JellyfinClient
survived,"    def _password_for_db(self, password: str) -> str:
        """"""Return placeholder password for local DB.""""""
        return ""emby-user""",app/services/media/emby.py,EmbyClient
survived,"    def join(
        self, username: str, password: str, confirm: str, email: str, code: str
    ) -> tuple[bool, str]:
        if not EMAIL_RE.fullmatch(email):
            return False, ""Invalid e-mail address.""
        if not 8 <= len(password) <= 20:
            return False, ""Password must be 8–20 characters.""
        if password != confirm:
            return False, ""Passwords do not match.""

        ok, msg = is_invite_valid(code)
        if not ok:
            return False, msg

        existing = User.query.filter(
            or_(User.username == username, User.email == email)
        ).first()
        if existing:
            return False, ""User or e-mail already exists.""

        try:
            user_id = self.create_user(username, password)

            inv = Invitation.query.filter_by(code=code).first()

            if inv.libraries:
                sections = [lib.external_id for lib in inv.libraries]
            else:
                sections = [
                    lib.external_id
                    for lib in Library.query.filter_by(enabled=True).all()
                ]

            self._set_specific_folders(user_id, sections)

            expires = None
            if inv.duration:
                days = int(inv.duration)
                expires = datetime.datetime.utcnow() + datetime.timedelta(days=days)

            new_user = User(
                username=username,
                email=email,
                password=self._password_for_db(password),
                token=user_id,
                code=code,
                expires=expires,
            )
            db.session.add(new_user)
            db.session.commit()

            self._mark_invite_used(inv, new_user)
            notify(
                ""New User"",
                f""User {username} has joined your server! 🎉"",
                tags=""tada"",
            )

            return True, """"

        except Exception:  # noqa: BLE001
            logging.error(""Jellyfin join error"", exc_info=True)
            db.session.rollback()
            return False, ""An unexpected error occurred.""
",app/services/media/jellyfin.py,JellyfinClient
survived,"    def test_summary_rate_limit_error(self) -> None:
        import openai

        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""sk-test""}):
            with patch(""openai.OpenAI"") as mock_client:
                mock_client.return_value.chat.completions.create.side_effect = openai.RateLimitError(""limit"")
                text = summarise_with_agent(
                    0.5,
                    agents=2,
                    rounds=10,
                    delta=0.9,
                    stake=1.0,
                )
        self.assertIn(""offline summary"", text)
        self.assertIn(""rate limit"", text)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"def test_forward_to_real_requests() -> None:
    """"""When ``requests`` is installed, ``af_requests`` should proxy to it.""""""
    spec = importlib.util.find_spec(""requests"")
    if spec is None:
        pytest.skip(""real requests not installed"")

    sys.modules.pop(""af_requests"", None)
    af_requests = importlib.import_module(""af_requests"")
    import requests  # type: ignore

    assert af_requests.get is requests.get
    assert af_requests.post is requests.post",tests/test_af_requests.py,
survived,"def test_fallback_to_internal_shim() -> None:
    """"""``af_requests`` should expose the internal lightweight implementation when
    the real ``requests`` package is missing.""""""
    sys.modules.pop(""requests"", None)
    sys.modules.pop(""af_requests"", None)

    af_requests = importlib.import_module(""af_requests"")
    from alpha_factory_v1 import af_requests as internal

    assert af_requests.get is internal.get
    assert af_requests.post is internal.post
",tests/test_af_requests.py,
survived,"def test_cors_headers() -> None:
    async def run() -> None:
        client, _ = await make_client()
        async with client:
            headers = {
                ""Authorization"": ""Bearer test-token"",
                ""Origin"": ""http://example.com"",
            }
            r = await client.get(""/runs"", headers=headers)
            assert r.status_code == 200
            assert r.headers.get(""access-control-allow-origin"") == ""http://example.com""

    asyncio.run(run())",tests/test_api_server_cors.py,
survived,"def UseGuards(*guards):
    """"""Decorator to attach guards to a controller or route.""""""

    def decorator(obj):
        existing = list(getattr(obj, ""__guards__"", []))
        existing.extend(guards)
        setattr(obj, ""__guards__"", existing)
        return obj

    return decorator",nest/core/guards.py,
survived,"def optimize_autovacuum(
    ctx: Context,
    dry_run: bool = typer.Option(False, help=""Print SQL commands only.""),
    rollback: bool = typer.Option(False, help=""Reset to defaults instead.""),
) -> None:
    qbe = qb.QueryBuilderEnvironment()
    if rollback:
        query = qbe.build_optimize_autovacuum_rollback_query()
    else:
        query = qbe.build_optimize_autovacuum_query()

    print(query)

    async def run() -> None:
        async with yield_queries(ctx, qb.DBSettings()) as q:
            await q.optimize_autovacuum(rollback=rollback)

    if not dry_run:
        asyncio_run(run())
",pgqueuer/cli.py,
survived,"    def __call__(self, module: M_contra, carry: CarryT) -> tuple[CarryT, OutputT_co]:
        ...
",src/haliax/nn/scan.py,ScanFunction
survived,"def test_governance_bridge_offline(monkeypatch):
    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)

    with pytest.raises(SystemExit):
        importlib.reload(
            importlib.import_module(
                ""alpha_factory_v1.demos.solving_agi_governance.openai_agents_bridge""
            )
        )",tests/test_governance_bridge_offline.py,
survived,"        def __init__(self, *args, **kwargs):
            pass
",stubs/google_adk/__init__.py,Agent
survived,"def twoSum(nums, target):
    n = len(nums)
    for i in range(0, n):
        for j in range((i + 1), n):
            if ((nums[i] + nums[j]) == target):
                return [i, j]
    return [-1, -1]
",tests/transpiler/x/py/two-sum.py,
survived,"def local_search(query: str):
    results = []
    for root, _, files in os.walk('.'):
        for f in files:
            if query in f:
                path = os.path.join(root, f)
                try:
                    content = open(path).read()
                except Exception:
                    content = """"
                results.append({
                    ""title"": f,
                    ""description"": path,
                    ""url"": path,
                    ""content"": content,
                })
    results.sort(key=lambda r: r['title'], reverse=True)
    return results
",examples/python/local_search.py,
survived,"        async def post(self, url: str, **kwargs):
            return requests.post(url, **kwargs)
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AsyncClient
survived,"        def __init__(self, app: FastAPI, limit: int = 60, window: int = 60) -> None:
            super().__init__(app)
            self.limit = int(os.getenv(""API_RATE_LIMIT"", str(limit)))
            self.window = window
            # Use TTLCache so inactive IP entries expire automatically.
            self.counters: TTLCache[str, deque[float]] = TTLCache(maxsize=1024, ttl=window)
            self.lock = asyncio.Lock()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,SimpleRateLimiter
survived,"def main(argv: list[str] | None = None) -> None:
    """"""Launch the α‑AGI Insight API server.""""""

    if FastAPI is None or uvicorn is None:
        raise SystemExit(""FastAPI is required to run the α‑AGI Insight API."")

    parser = argparse.ArgumentParser(description=""Run the α‑AGI Insight API"")
    parser.add_argument(""--host"", default=""0.0.0.0"", help=""Bind host"")
    parser.add_argument(""--port"", type=int, default=8000, help=""Bind port"")
    args = parser.parse_args(argv)

    uvicorn.run(app, host=args.host, port=args.port)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    def verify(self, content: str, signature: str) -> bool:
        """"""Verify ``content`` against ``signature``.""""""
        expected = hmac.new(
            self.secret, content.encode(""utf-8""), hashlib.sha256
        ).hexdigest()
        valid = hmac.compare_digest(expected, signature)
        if valid:
            checksum = hashlib.sha256(content.encode(""utf-8"")).hexdigest()
            if checksum not in self.cache:
                self.cache[checksum] = signature
                self._save_cache()
        return valid
",src/meta_agent/template_governance.py,TemplateGovernance
survived,"def test_load_golden_specs() -> None:
    specs = load_golden_spec_fuzz_set()
    assert len(specs) >= 20
    assert all(isinstance(s, str) and s for s in specs)",tests/test_golden_spec_fuzz_set.py,
survived,"def parse_and_validate_diff(
    diff_text: str,
    repo_dir: str,
    allowed_paths: list[str] | None = None,
) -> str | None:
    """"""Verify the LLM's output is a valid unified diff and meets safety criteria.

    Diffs that exceed ``MAX_DIFF_LINES`` or ``MAX_DIFF_BYTES`` are rejected to
    avoid accidentally applying huge patches.
    """"""
    if not diff_text:
        return None

    lines = diff_text.splitlines()
    if len(lines) > MAX_DIFF_LINES or len(diff_text.encode(""utf-8"")) > MAX_DIFF_BYTES:
        logger.warning(
            ""Diff too large: %s lines, %s bytes"",
            len(lines),
            len(diff_text.encode(""utf-8"")),
        )
        return None

    # Basic unified diff check: should contain lines starting with '+++ ' and '--- '
    if ""+++"" not in diff_text or ""---"" not in diff_text:
        return None  # Not a diff format
    repo_root = Path(repo_dir).resolve()
    allowed = allowed_paths if allowed_paths is not None else ALLOWED_PATHS
    allowed_dirs = (
        [repo_root.joinpath(p).resolve() for p in allowed]
        if allowed
        else [repo_root]
    )

    for line in diff_text.splitlines():
        if line.startswith(""+++ "") or line.startswith(""--- ""):
            m = re.match(r""^[+-]{3} [ab]/(.+)$"", line)
            if m:
                file_path = m.group(1)
                target = (repo_root / file_path).resolve()
                if not target.is_relative_to(repo_root):
                    logger.warning(""Diff outside repository: %s"", file_path)
                    return None
                if not any(target.is_relative_to(d) for d in allowed_dirs):
                    logger.warning(""Diff touches disallowed path: %s"", file_path)
                    return None
    # (Additional checks: e.g., diff length, certain forbidden content can be added here.)
    return diff_text
",alpha_factory_v1/demos/self_healing_repo/agent_core/diff_utils.py,
survived,"def test_moe_linear_matches_ragged_dot_general():
    B, In, Out, E = hax.make_axes(B=3, In=4, Out=5, E=2)
    key = jrandom.PRNGKey(0)
    moe = MoELinear.init(E, In, Out, key=key)

    x = hax.random.normal(jrandom.PRNGKey(1), (B, In))
    group_sizes = hax.named(jnp.array([2, 1], dtype=jnp.int32), (E,))

    actual = moe(x, group_sizes)
    expected = _expected_moe_linear_output(moe, x, group_sizes)

    assert actual.axes == expected.axes
    assert jnp.allclose(actual.array, expected.array, rtol=1e-5, atol=1e-5)
",tests/test_moe_linear.py,
survived,"        def close(self):
            pass
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_StubEnv
survived,"            def sum(self):  # minimal numpy-like API
                from builtins import sum as _sum
                return _sum(self)
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_P
survived,"    def test_parse_numbers_helper(self) -> None:
        from alpha_factory_v1.demos.meta_agentic_tree_search_v0.mats.meta_rewrite import (
            _parse_numbers,
        )

        text = ""[1, 2, -3]""
        res = _parse_numbers(text, [0, 0, 0])
        self.assertEqual(res, [1, 2, -3])
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def check_node() -> bool:
    if not shutil.which(""node""):
        banner(""node missing"", ""RED"")
        return False
    try:
        out = subprocess.check_output([""node"", ""--version""], text=True).strip()
    except Exception:
        banner(""failed to run node --version"", ""RED"")
        return False
    banner(f""Node {out} detected"", ""GREEN"")
    if not out.lstrip(""v"").startswith(""20""):
        banner(""Node 20 recommended"", ""YELLOW"")
    return True
",scripts/setup_wizard.py,
survived,"def _dump_selected(txn: lmdb.Transaction, keys: Iterable[str]) -> List[dict[str, Any]]:
    """"""Return records for the provided keys.""""""
    result: List[dict[str, Any]] = []
    for key in keys:
        raw = txn.get(key.encode(""utf-8""))
        if raw is not None:
            result.append({""key"": key, ""value"": _decode_value(raw)})
    return result
",scripts/dump_lmdb.py,
survived,"def test_run_muzero_demo_port_in_use(tmp_path: Path) -> None:
    repo_root = Path(__file__).resolve().parents[1]
    src = repo_root / ""alpha_factory_v1""
    dst = tmp_path / ""alpha_factory_v1""
    shutil.copytree(src, dst)

    script = dst / ""demos"" / ""muzero_planning"" / ""run_muzero_demo.sh""
    log_file = tmp_path / ""docker.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(f""#!/usr/bin/env bash\necho docker >> '{log_file}'\nexit 0\n"")
    docker_stub.chmod(0o755)

    lsof_stub = bin_dir / ""lsof""
    lsof_stub.write_text(""#!/usr/bin/env bash\nexit 0\n"")
    lsof_stub.chmod(0o755)

    sock = socket.socket()
    sock.bind((""localhost"", 0))
    port = sock.getsockname()[1]

    env = os.environ.copy()
    env.update({""PATH"": f""{bin_dir}:{env.get('PATH', '')}"", ""HOST_PORT"": str(port)})

    result = subprocess.run([""bash"", str(script)], env=env, capture_output=True, text=True)
    sock.close()

    assert result.returncode == 1
    assert ""already in use"" in result.stderr
    assert not log_file.exists() or not log_file.read_text()",tests/test_run_muzero_demo.py,
survived,"def AnthropicParallelModel(typehint: type[Iterable[T]]) -> AnthropicParallelBase:
    the_types = get_types_array(typehint)
    return AnthropicParallelBase(*[model for model in the_types])",instructor/dsl/parallel.py,
survived,"async def test_async_parallel_tools_or(aclient):
    client = instructor.from_anthropic(
        aclient, mode=instructor.Mode.ANTHROPIC_PARALLEL_TOOLS
    )
    resp = await client.chat.completions.create(
        model=""claude-3-5-haiku-latest"",
        messages=[
            {""role"": ""system"", ""content"": ""You must always use tools""},
            {
                ""role"": ""user"",
                ""content"": ""What is the weather in toronto and dallas and who won the super bowl?"",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3",tests/llm/test_anthropic/test_parallel.py,
survived,"def test_time_dependent_discontinuity_equilibration(tmp_path):
    """"""Time dependent discontinuities are handled during equilibration.""""""

    from amici.antimony_import import antimony2sbml
    from amici.sbml_import import SbmlImporter
    from amici.jax.petab import DEFAULT_CONTROLLER_SETTINGS

    ant_model = """"""
    model time_disc_eq
        x' = piecewise(1, time - sin(time) - 1 < 0, -x)
        x = 0
    end
    """"""

    sbml = antimony2sbml(ant_model)
    importer = SbmlImporter(sbml, from_file=False)
    importer.sbml2jax(""time_disc_eq"", output_dir=tmp_path)

    module = amici._module_from_path(""time_disc_eq"", tmp_path / ""__init__.py"")
    model = module.Model()

    p = jnp.array([1.0])
    x0_full = model._x0(0.0, p)
    tcl = model._tcl(x0_full, p)
    x0 = model._x_solver(x0_full)

    assert len(model._root_cond_fns(p)) > 0
    assert model._known_discs(p).size == 0

    xs, _ = model._eq(
        p,
        tcl,
        x0,
        diffrax.Tsit5(),
        diffrax.PIDController(**DEFAULT_CONTROLLER_SETTINGS),
        diffrax.steady_state_event(
            rtol=1e-8, atol=1e-8, norm=lambda y: jnp.linalg.norm(y)
        ),
        1000,
    )

    assert_allclose(xs[0], 0.0, atol=1e-2)",python/tests/test_jax.py,
survived,"def test_logistic_curve_midpoint() -> None:
    assert forecast.logistic_curve(0.0) == pytest.approx(0.5)
",tests/test_forecast.py,
survived,"def get_stripped_lines(val: str):
    ## you don't want empty lines to add empty list after splitlines!
    val = val.strip()

    return [val_line.strip() for val_line in val.split(""\n"")]
",scripts/utils/lcb_runner.py,
survived,"    def readline(self, *args):
        return self._stringio.readline(*args)
",scripts/utils/lcb_runner.py,MockStdinWithBuffer
survived,"    def __exit__(self, *args):
        self.append(self._stringio.getvalue())
        del self._stringio  # free up some memory
        sys.stdout = self._stdout
",scripts/utils/lcb_runner.py,Capturing
survived,"def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + ""...(truncated) ..."" + s[-length // 2 :]
",scripts/utils/lcb_runner.py,
survived,"    def _ensure(self) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(
                ""CREATE TABLE IF NOT EXISTS agents(""
                ""id INTEGER PRIMARY KEY AUTOINCREMENT,""
                ""meta TEXT,""
                ""score REAL""
                "")""
            )
",src/archive.py,Archive
survived,"    async def _spawn_jobs(self) -> None:
        """"""Spawn new worker tasks until quotas or limits are hit.""""""
        if self.time_quota and time.time() - self.start_time >= self.time_quota:
            self.app.session.finish()
            return
        if self.tokens_quota is not None and self.tokens_used >= self.tokens_quota:
            self.app.session.finish()
            return
        while not self.queue.empty() and len(self.running) < self.max_workers:
            job = await self.queue.get()
            task = asyncio.create_task(self._run_job(job))
            self.running.add(task)
            task.add_done_callback(self.running.discard)
",src/scheduler.py,SelfImprovementScheduler
survived,"def _discover_tasks(dataset: str) -> list[tuple[str, str]]:
    """"""Return list of (task_id, module_name).""""""
    tasks = []
    base = ROOT.parent
    for path in (ROOT / dataset).glob(""task_*.py""):
        rel = path.with_suffix("""").relative_to(base)
        module_name = ""."".join(rel.parts)
        task_id = f""{dataset}/{path.stem}""
        tasks.append((task_id, module_name))
    return tasks
",benchmarks/run_benchmarks.py,
survived,"def main() -> None:
    # Ensure the repository root is on sys.path so benchmark modules import
    sys.path.insert(0, str(ROOT.parent))
    datasets = [""swebench_verified_mini"", ""polyglot_lite""]
    results = []
    for ds in datasets:
        for task_id, module in _discover_tasks(ds):
            results.append(run_task(task_id, module))
    json.dump(results, sys.stdout)
",benchmarks/run_benchmarks.py,
survived,"    async def accept(self, cand: Candidate) -> None:
        self._items.append(cand)
",src/evolve.py,InMemoryArchive
survived,"def get_component(
    dataset: Union[DataFrame, Connector],
    gid: Union[int, str] = None,
    *,
    field_specs: Optional[List[FieldSpec]] = None,
    theme_key: IThemeKey = ""g2"",
    appearance: IAppearance = ""media"",
    spec: str = """",
    spec_io_mode: ISpecIOMode = ""r"",
    kernel_computation: Optional[bool] = None,
    kanaries_api_key: str = """",
    default_tab: Literal[""data"", ""vis""] = ""vis"",
    **kwargs,
) -> rx.Component:
    """"""Get a Reflex component that renders Pygwalker.""""""
    check_expired_params(kwargs)

    walker = PygWalker(
        gid=gid,
        dataset=dataset,
        field_specs=field_specs if field_specs is not None else [],
        spec=spec,
        source_invoke_code="""",
        theme_key=theme_key,
        appearance=appearance,
        show_cloud_tool=False,
        use_preview=False,
        kernel_computation=isinstance(dataset, Connector) or kernel_computation,
        use_save_tool=""w"" in spec_io_mode,
        is_export_dataframe=False,
        kanaries_api_key=kanaries_api_key,
        default_tab=default_tab,
        cloud_computation=False,
        gw_mode=""explore"",
        **kwargs,
    )

    props = walker._get_props(""reflex"")
    props[""communicationUrl""] = BASE_URL_PATH
    comm = ReflexCommunication(str(walker.gid))
    walker._init_callback(comm)

    html = walker._get_render_iframe(props, True)
    return rx.html(html)",pygwalker/api/reflex.py,
survived,"    def _get_model(self, blank: bool = False) -> LogisticRegression:
        global _model
        if _model is not None and not blank:
            return _model
        model_path = os.path.join(self.MODEL_DIR, 'model.pkl')
        if not blank and os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                _model = pickle.load(f)
        else:
            _model = LogisticRegression(max_iter=1000)
        return _model
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"def setup_windsurf_config(host_system: str, path_to_env: str) -> bool:
    """"""Placeholder setup for Windsurf integration.""""""
    console.print(""[yellow]![/] Windsurf integration setup is not implemented yet"")
    return False",skyvern/cli/commands.py,
survived,"def convert_species_traits(subrace_path, out_path, parent_slug, start_pk):
    subrace = load_json(subrace_path)[0]
    fields = subrace[""fields""]
    traits = []
    pk = start_pk
    asi = fields.get(""asi_desc"")
    if asi:
        traits.append({
            ""model"": ""api_v2.speciestrait"",
            ""pk"": pk,
            ""fields"": {""name"": ""Ability Score Increase"", ""desc"": asi, ""type"": None, ""parent"": parent_slug},
        })
        pk += 1
    text = fields.get(""traits"", """")
    for part in filter(None, [p.strip() for p in text.split(""\n\n"")]):
        m = re.match(r""\*\*[_*]?([^.*]+)[.*_]*\*\*\s*(.*)"", part)
        if m:
            name = m.group(1).strip().rstrip('.')
            desc = m.group(2).strip()
        else:
            name, _, desc = part.partition('.')
            name = name.strip()
            desc = desc.strip()
        traits.append({
            ""model"": ""api_v2.speciestrait"",
            ""pk"": pk,
            ""fields"": {""name"": name, ""desc"": desc, ""type"": None, ""parent"": parent_slug},
        })
        pk += 1
    append_json(traits, out_path)
    return pk
",convert_missing.py,
survived,"def parse_feature_sections(text):
    sections = []
    parts = text.split(""##### "")
    for part in parts[1:]:
        if not part.strip():
            continue
        header, *rest = part.split(""\n"", 1)
        body = rest[0] if rest else """"
        sections.append((header.strip(), body.strip()))
    return sections
",convert_missing.py,
survived,"        def __init__(self, val: str) -> None:
            pass
",tests/test_ledger.py,DummyPk
survived,"    def test_yaml_dict_names(self):
        labelmap = load_labelmap(""tests/annotations/dict_names.yaml"")
        self.assertEqual(labelmap, {0: ""cat"", 1: ""dog"", 2: ""fish""})",tests/util/test_image_utils.py,TestLoadLabelmap
survived,"def test_caseinfo_save_and_update(tmp_path, env_setup, monkeypatch):
    from importlib import reload

    import np_ocr.api as api
    reload(api)
    monkeypatch.setattr(api.settings, ""CASE_INFO_FILENAME"", ""case_info.json"", raising=False)

    case_dir = tmp_path / ""case""
    case_dir.mkdir()
    case = api.CaseInfo(
        name=""mycase"",
        status=""processing"",
        number_of_pdfs=1,
        files=[""file.pdf""],
        case_dir=case_dir,
    )
    case.save()
    with open(case_dir / ""case_info.json"") as f:
        data = json.load(f)
    assert data[""status""] == ""processing""

    case.update_status(""done"")
    with open(case_dir / ""case_info.json"") as f:
        data = json.load(f)
    assert data[""status""] == ""done""
",no-ocr-api/tests/test_utils.py,
survived,"        def parse(*args, **kwargs):
            class Msg:
                parsed = search.ImageAnswer(answer=""ok"")
            class Choice:
                message = Msg()
            class Completion:
                choices = [Choice()]
            return Completion()
",no-ocr-api/tests/test_utils.py,FakeCompletions
survived,"    def test_uses_pytest_when_available(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            target = Path(tmpdir)
            with mock.patch('importlib.util.find_spec', return_value=object()):
                with mock.patch('subprocess.call', return_value=0) as call:
                    with mock.patch.object(sys, 'argv', ['run_tests.py', str(target)]):
                        with self.assertRaises(SystemExit):
                            run_tests.main()
                    call.assert_called_once()
                    self.assertIn('pytest', call.call_args[0][0])
",alpha_factory_v1/tests/test_scripts_run_tests.py,RunTestsScriptTest
survived,"    def test_path_must_exist(self):
        with self.assertRaises(SystemExit):
            with mock.patch.object(sys, 'argv', ['run_tests.py', '/nope']):
                run_tests.main()
",alpha_factory_v1/tests/test_scripts_run_tests.py,RunTestsScriptTest
survived,"    async def run(self, agent: Agent, *args, **kwargs):
        return await agent.run(*args, **kwargs)
",src/agents/__init__.py,Runner
survived,"    async def run(self, *_args, **_kwargs):
        return {""status"": ""success""}
",src/agents/__init__.py,Agent
survived,"def test_simulate_returns_trajectory() -> None:
    traj = _simulate(2, ""logistic"", 2, 1)
    assert len(traj) == 2
    assert isinstance(traj[0], forecast.TrajectoryPoint)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_web_app.py,
survived,"    def export_json(
        self,
        path: str | Path,
        *,
        start: datetime | str | None = None,
        end: datetime | str | None = None,
        metrics: Iterable[str] | None = None,
        compress: bool | None = None,
    ) -> str:
        """"""Export telemetry to a JSON file with optional compression.""""""
        compress = compress or str(path).endswith(("".gz"", "".gzip""))
        data = self.fetch_all(start=start, end=end, metrics=metrics)
        open_fn = gzip.open if compress else open
        mode = ""wt""
        with open_fn(path, mode, encoding=""utf-8"") as f:
            json.dump(data, f)
        return str(path)
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def model_urls(model: str) -> list[str]:
    base = f""https://openaipublic.blob.core.windows.net/gpt-2/models/{model}/""
    return [base + name for name in _FILE_LIST]
",scripts/download_openai_gpt2.py,
survived,"def test_guardrail_event():
    t = TelemetryCollector()
    t.increment_guardrail_hits()
    assert t.guardrail_hits == 1
    assert len(t.events) == 1
    ev = t.events[0]
    assert ev.category == TelemetryCollector.Category.GUARDRAIL
    assert ev.severity == TelemetryCollector.Severity.WARNING",tests/unit/test_telemetry_collector.py,
survived,"def test_summary_line_custom_metrics():
    t = TelemetryCollector()
    t.start_timer()
    t.stop_timer()
    line = t.summary_line([""latency""])
    assert ""Telemetry:"" in line
    assert ""latency="" in line
    assert ""cost="" not in line
",tests/unit/test_telemetry_collector.py,
survived,"async def test_llm_comment_no_openai(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""_llm_comment should rely on the local model when OpenAI is unavailable.""""""
    called = {}

    def fake_chat(prompt: str, cfg: object | None = None) -> str:
        called[""prompt""] = prompt
        return ""offline""

    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    removed = sys.modules.pop(""openai_agents"", None)
    monkeypatch.setattr(demo.local_llm, ""chat"", fake_chat)

    try:
        out = await demo._llm_comment(-0.42)
    finally:
        if removed is not None:
            sys.modules[""openai_agents""] = removed

    assert out == ""offline""
    assert called[""prompt""].startswith(""In one sentence, comment on ΔG=-0.4200"")
",tests/test_alpha_agi_business_3_v1.py,
survived,"def discover_hot_dir() -> None:
    if not _HOT_DIR.is_dir():
        return
    for wheel in _HOT_DIR.glob(""*.whl""):
        if wheel.stem.replace(""-"", ""_"") in AGENT_REGISTRY:
            continue
        try:
            if not verify_wheel(wheel):
                continue
            mod = install_wheel(wheel)
            if mod:
                meta = _inspect_module(mod)
                if meta and meta.name not in AGENT_REGISTRY:
                    _register(meta)
        except Exception:  # noqa: BLE001
            logger.exception(""Hot-dir load failed for %s"", wheel.name)
",alpha_factory_v1/backend/agents/discovery.py,
survived,"def show_memory(limit: int, export: str | None) -> None:
    """"""Display stored memory entries.""""""
    path = config.CFG.memory_path
    if not path:
        click.echo(""Memory persistence not enabled"")
        return
    mem_file = Path(path)
    if not mem_file.exists():
        click.echo(""No memory entries"")
        return
    entries = []
    for line in mem_file.read_text(encoding=""utf-8"").splitlines():
        if not line:
            continue
        try:
            entries.append(json.loads(line))
        except Exception:  # noqa: BLE001 - ignore bad records
            entries.append({""raw"": line})
    if not entries:
        click.echo(""No memory entries"")
        return
    entries = entries[-limit:]
    if export == ""json"":
        click.echo(json.dumps(entries))
    elif export == ""csv"":
        lines = [""payload""]
        for e in entries:
            lines.append(json.dumps(e).replace("","", "";""))
        click.echo(""\n"".join(lines))
    else:
        _rich_table([""payload""], [(json.dumps(e),) for e in entries])
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"        def __init__(self) -> None:
            self.coro: Any | None = None
",tests/test_alpha_agi_business_3_v1.py,DummyLoop
survived,"def outer(x):
    def inner(y):
        return x + y
    return inner(5)
",tests/transpiler/x/py/nested_function.py,
survived,"def test_simulate_seed_reproducible(tmp_path: Path) -> None:
    """"""Output should be identical when running with the same seed.""""""
    ledger = tmp_path / ""audit.db""
    args = [
        ""simulate"",
        ""--horizon"",
        ""1"",
        ""--offline"",
        ""--sectors"",
        ""1"",
        ""--pop-size"",
        ""1"",
        ""--generations"",
        ""1"",
        ""--export"",
        ""json"",
        ""--no-broadcast"",
        ""--seed"",
        ""42"",
    ]
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            with patch.object(cli.config.CFG, ""ledger_path"", ledger):
                res1 = runner.invoke(cli.main, args)
                res2 = runner.invoke(cli.main, args)

    assert res1.exit_code == 0
    assert res2.exit_code == 0
    digest1 = hashlib.sha256(res1.output.encode()).hexdigest()
    digest2 = hashlib.sha256(res2.output.encode()).hexdigest()
    assert digest1 == digest2",tests/test_demo_cli.py,
survived,"def _log_delta(delta: float, log_file: Path) -> None:
    """"""Append ``delta`` with timestamp to ``log_file`` (JSON list).""""""
    log: list[dict[str, float]]
    if log_file.exists():
        log = json.loads(log_file.read_text())
    else:
        log = []
    log.append({""ts"": time.time(), ""delta"": delta})
    log_file.write_text(json.dumps(log))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_improver.py,
survived,"async def test_llm_comment_uses_local_model(monkeypatch: pytest.MonkeyPatch) -> None:
    called = {}

    def fake_chat(prompt: str, cfg: object | None = None) -> str:
        called[""prompt""] = prompt
        return ""local""

    monkeypatch.setattr(demo, ""OpenAIAgent"", None)
    monkeypatch.setattr(demo.local_llm, ""chat"", fake_chat)

    out = await demo._llm_comment(0.1234)

    assert out == ""local""
    assert called[""prompt""].startswith(""In one sentence, comment on ΔG=0.1234"")
",tests/test_alpha_agi_business_3_v1.py,
survived,"def test_namedarray_type_syntax():
    t1 = NamedArray[""batch"", ""embed""]
    t2 = NamedArray[""batch embed""]
    assert typing.get_args(t1)[1] == typing.get_args(t2)[1]

    t3 = NamedArray[""batch embed ...""]
    axes3 = typing.get_args(t3)[1]
    assert axes3.before == (""batch"", ""embed"") and axes3.subset and axes3.after == ()

    t4 = NamedArray[{""batch"", ""embed""}]
    axes4 = typing.get_args(t4)[1]
    assert set(axes4.before) == {""batch"", ""embed""} and not axes4.ordered

    t5 = NamedArray[{""batch"", ""embed"", ...}]
    axes5 = typing.get_args(t5)[1]
    assert set(axes5.before) == {""batch"", ""embed""} and not axes5.ordered and axes5.subset

    t6 = NamedArray[""... embed""]
    axes6 = typing.get_args(t6)[1]
    assert axes6.before == () and axes6.after == (""embed"",) and axes6.subset

    t7 = NamedArray[""batch ... embed""]
    axes7 = typing.get_args(t7)[1]
    assert axes7.before == (""batch"",) and axes7.after == (""embed"",) and axes7.subset
",tests/test_namedarray_typing.py,
survived,"    def __repr__(self) -> str:
        if self.ordered:
            parts = list(self.before)
            if self.subset:
                parts.append(""..."")
            parts.extend(self.after)
            spec = "" "".join(parts)
            return f""NamedArray[{spec}]""
        else:
            part = "", "".join(self.before)
            if self.subset:
                if part:
                    part += "", ...""
                else:
                    part = ""...""
            return f""NamedArray[{{{part}}}]""
",src/haliax/core.py,NamedArrayAxes
survived,"    def test_summary_auth_error(self) -> None:
        import openai

        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""sk-test""}):
            with patch(""openai.OpenAI"") as mock_client:
                mock_client.return_value.chat.completions.create.side_effect = openai.AuthenticationError(""bad key"")
                text = summarise_with_agent(
                    0.5,
                    agents=2,
                    rounds=10,
                    delta=0.9,
                    stake=1.0,
                )
        self.assertIn(""OPENAI_API_KEY not set"", text)
        self.assertIn(""offline summary"", text)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"def tokenize(text: str) -> List[str]:
    return re.findall(r""\b\w+\b"", text.lower())
",scripts/dp_scrubber.py,
survived,"    def __init__(
        self,
        *,
        region: str = ""us-east-1"",
        budget_per_day: float = 200.0,
        price_fetcher: FetchFunc | None = None,
    ) -> None:
        self.region = region
        self.budget_per_day = budget_per_day
        self.price_fetcher = price_fetcher or _fetch_spot_price
",src/scheduler/spot_gpu.py,SpotGPUAllocator
survived,"    async def broadcast_merkle_root(self) -> None:
        try:
            root = self.compute_merkle_root()
        except Exception as exc:  # pragma: no cover - corruption
            _log.warning(""Failed to compute Merkle root: %s"", exc)
            return
        if AsyncClient is None or not self.broadcast:
            _log.info(""Merkle root %s"", root)
            return
        try:
            client = AsyncClient(self.rpc_url or ""https://api.testnet.solana.com"")
            memo_prog = PublicKey(""MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr"")
            tx = Transaction().add(TransactionInstruction(program_id=memo_prog, data=root.encode(), keys=[]))
            signer = None
            if self.wallet:
                try:  # pragma: no cover - optional dependency
                    from solana.keypair import Keypair

                    signer = Keypair.from_secret_key(bytes.fromhex(self.wallet))
                except Exception as exc:  # noqa: BLE001 - invalid key
                    _log.warning(""Invalid wallet key: %s"", exc)
            if signer:
                await client.send_transaction(tx, signer)
            else:
                await client.send_transaction(tx)
            _log.info(""Broadcasted Merkle root %s"", root)
        except Exception as exc:  # pragma: no cover - network errors
            _log.warning(""Failed to broadcast Merkle root: %s"", exc)
        finally:
            try:
                await client.close()
            except Exception:  # pragma: no cover - ignore close errors
                pass
",src/archive/service.py,ArchiveService
survived,"def _merkle_root(hashes: Iterable[str]) -> str:
    nodes: List[bytes] = [bytes.fromhex(h) for h in hashes]
    if not nodes:
        return blake3(b""\x00"").hexdigest()

    while len(nodes) > 1:
        if len(nodes) % 2 == 1:
            nodes.append(nodes[-1])
        next_lvl: List[bytes] = []
        for i in range(0, len(nodes), 2):
            next_lvl.append(blake3(nodes[i] + nodes[i + 1]).digest())
        nodes = next_lvl
    return nodes[0].hex()
",src/archive/service.py,
survived,"    def last_hash(self) -> str | None:
        cur = self.conn.execute(""SELECT hash FROM entries ORDER BY id DESC LIMIT 1"")
        row = cur.fetchone()
        return row[0] if row else None
",src/archive/service.py,ArchiveService
survived,"    def insert_entry(
        self,
        spec: Mapping[str, Any],
        scores: Mapping[str, float],
        *,
        parent: str | None = None,
    ) -> str:
        parent = parent or self.last_hash()
        record = {""parent"": parent, ""spec"": spec, ""scores"": dict(scores)}
        digest = blake3(json.dumps(record, sort_keys=True).encode()).hexdigest()
        with self.conn:
            self.conn.execute(
                ""INSERT INTO entries(parent, spec, scores, hash, ts) VALUES(?,?,?,?,?)"",
                (parent, json.dumps(spec), json.dumps(record[""scores""]), digest, time.time()),
            )
        return self.compute_merkle_root()
",src/archive/service.py,ArchiveService
survived,"def _sync_chat(prompt: str) -> str:
    """"""Synchronously invoke the async chat helper.""""""
    from alpha_factory_v1.backend.llm_provider import chat

    async def _call() -> str:
        return await chat(prompt, max_tokens=512)

    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(_call())

    result: list[str] = []

    def _worker() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        task = loop.create_task(_call())
        try:
            result.append(loop.run_until_complete(task))
        finally:
            loop.close()

    t = threading.Thread(target=_worker)
    t.start()
    t.join()
    return result[0]
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"    async def _call() -> str:
        return await chat(prompt, max_tokens=512)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"def propose_diff(repo_path: str, spec: str) -> str:
    """"""Return a git diff implementing ``spec`` inside ``repo_path``.""""""
    rel, goal = _parse_spec(spec)
    file_path = str(Path(repo_path) / rel)
    if _offline():
        return _fallback_diff(file_path, goal)
    prompt = (
        ""Generate a unified git diff for the repository at '{repo}'.\n""
        ""Apply the following change: {spec}"".format(repo=repo_path, spec=spec)
    )
    try:
        diff = _sync_chat(prompt)
        if not diff.endswith(""\n""):
            diff += ""\n""
        return diff
    except Exception:
        return _fallback_diff(file_path, goal)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"    def create_app(self) -> ""FastAPI"":
        if FastAPI is None:
            raise RuntimeError(""FastAPI not installed"")

        app = FastAPI(title=""Dual Critic Service"")

        @app.post(""/critique"")
        async def _critique(req: CritiqueRequest = Body(...)) -> Any:  # noqa: D401
            result = self.score(req.context, req.response)
            return JSONResponse(result)

        CritiqueRequest.model_rebuild()

        return app
",src/critics/dual_critic_service.py,DualCriticService
survived,"def aggregate_stats(rows: Iterable[Dict[str, float]]) -> Dict[str, float]:
    """"""Return mean and stdev for each metric in ``rows``.""""""
    stats: Dict[str, float] = {}
    metrics: Dict[str, List[float]] = {m: [] for m in _METRICS}
    for row in rows:
        for m in _METRICS:
            if m in row:
                metrics[m].append(row[m])
    for m, vals in metrics.items():
        if vals:
            stats[f""{m}_mean""] = statistics.mean(vals)
            stats[f""{m}_stdev""] = statistics.pstdev(vals) if len(vals) > 1 else 0.0
    return stats
",src/analysis/meta_foresight.py,
survived,"def test_run_simulation_smoke(capsys: pytest.CaptureFixture[str]) -> None:
    """"""Ensure _run_simulation accepts the new num_sectors argument.""""""

    web_app._run_simulation(1, ""logistic"", 2, 3, 1)
    out, _ = capsys.readouterr()
    assert ""Streamlit not installed"" in out",tests/test_web_app.py,
survived,"def test_simulate_does_not_modify_global_cfg() -> None:
    """"""CLI options should not persist on the global config.""""""
    runner = CliRunner()
    original = cli.config.CFG.model_dump()

    with patch.object(cli, ""asyncio""), patch.object(cli.orchestrator, ""Orchestrator""):
        res = runner.invoke(
            cli.main,
            [
                ""simulate"",
                ""--horizon"",
                ""1"",
                ""--offline"",
                ""--sectors"",
                ""1"",
                ""--pop-size"",
                ""1"",
                ""--generations"",
                ""1"",
                ""--model"",
                ""other"",
                ""--temperature"",
                ""0.9"",
                ""--context-window"",
                ""1024"",
            ],
        )

    assert res.exit_code == 0
    assert cli.config.CFG.model_dump() == original",tests/test_cli.py,
survived,"    def test_register_condition_false(self):
        @register(condition=False)
        class BarAgent(AgentBase):
            NAME = ""bar""
        self.assertNotIn(""bar"", AGENT_REGISTRY)
",alpha_factory_v1/tests/test_register_decorator.py,RegisterDecoratorTest
survived,"        def add_node(self, node: str, **attrs: Any) -> None:
            self.nodes[node] = attrs
",alpha_factory_v1/backend/agents/supply_chain_agent.py,_FakeGraph
survived,"        def add_edge(self, u: str, v: str, **attrs: Any) -> None:
            self.edges[(u, v)] = attrs
",alpha_factory_v1/backend/agents/supply_chain_agent.py,_FakeGraph
survived,"def _parse_with(args):
    old = sys.argv
    sys.argv = ['run.py'] + args
    try:
        return af_run.parse_args()
    finally:
        sys.argv = old
",alpha_factory_v1/tests/test_cli.py,
survived,"def __getattr__(name: str) -> Any:  # pragma: no cover - thin wrapper
    """"""Lazily import top‑level modules.

    This keeps ``import alpha_factory_v1`` fast and avoids importing heavy
    dependencies until actually needed.
    """"""

    if name in {""backend"", ""demos"", ""ui"", ""run""}:
        return importlib.import_module(f"".{name}"", __name__)
    raise AttributeError(f""module {__name__!r} has no attribute {name}"")
",alpha_factory_v1/__init__.py,
survived,"def test_cli_flags_override_env(monkeypatch) -> None:
    """"""CLI options should set env vars for the runtime helpers.""""""
    if MODULE in sys.modules:
        del sys.modules[MODULE]
    mod = importlib.import_module(MODULE)

    monkeypatch.setattr(mod, ""check_env"", types.SimpleNamespace(main=lambda *_a, **_k: None), raising=False)

    captured: dict[str, str] = {}

    async def _llm(_: float) -> str:
        captured[""api_key""] = os.getenv(""OPENAI_API_KEY"")
        return ""ok""

    class DummyADK:
        def __init__(self, host: str) -> None:  # pragma: no cover - init only
            captured[""adk_host""] = host

    class DummySock:
        def __init__(self, host: str, port: int, app_id: str) -> None:
            captured[""a2a""] = f""{host}:{port}""  # pragma: no cover - record args

        def start(self) -> None:  # pragma: no cover - unused
            pass

        def stop(self) -> None:  # pragma: no cover - unused
            pass

        def sendjson(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - unused
            pass

    monkeypatch.setattr(mod, ""_llm_comment"", _llm)
    monkeypatch.setattr(mod, ""ADKClient"", DummyADK)
    monkeypatch.setattr(mod, ""A2ASocket"", DummySock)
    monkeypatch.setattr(mod, ""_A2A"", None)

    asyncio.run(
        mod.main(
            [
                ""--cycles"",
                ""1"",
                ""--interval"",
                ""0"",
                ""--openai-api-key"",
                ""cli-key"",
                ""--adk-host"",
                ""http://cli-adk:9"",
                ""--a2a-port"",
                ""7777"",
                ""--a2a-host"",
                ""cli-host"",
            ]
        )
    )

    assert captured[""api_key""] == ""cli-key""
    assert captured[""adk_host""] == ""http://cli-adk:9""
    assert captured[""a2a""] == ""cli-host:7777""
",tests/test_alpha_agi_business_3_v1.py,
survived,"            def __init__(self, program_id: object, data: bytes, keys: list[object]):
                self.data = data
",tests/test_insight_orchestrator_features.py,TestLedger.DummyInstr
survived,"def test_assets_replaced() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    for sub in (""wasm"", ""wasm_llm"", ""lib""):
        for p in (browser_dir / sub).rglob(""*""):
            if not p.is_file():
                continue
            if ""placeholder"" in p.read_text(errors=""ignore"").lower():
                rel = p.relative_to(browser_dir)
                raise AssertionError(f""{rel} contains placeholder text"")",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_assets_replaced.py,
survived,"def run_evolution(
    fn: Callable[[List[float]], Tuple[float, float]],
    genome_length: int,
    *,
    population_size: int = 20,
    mutation_rate: float = 0.1,
    generations: int = 10,
    seed: int | None = None,
) -> Population:
    """"""Execute a complete NSGA-II evolutionary run.

    Args:
        fn: Function evaluating an individual's genome.
        genome_length: Number of float genes per individual.
        population_size: Number of individuals preserved each generation.
        mutation_rate: Probability of mutating a gene during crossover.
        generations: Number of NSGA-II steps to perform.
        seed: Optional random seed for deterministic behaviour.

    Returns:
        The final population after ``generations`` steps.
    """"""

    rng = random.Random(seed)
    pop = [Individual([rng.uniform(-1, 1) for _ in range(genome_length)]) for _ in range(population_size)]

    def _step(population: Population) -> Population:
        evaluate(population, fn)
        offspring: Population = []
        while len(offspring) < population_size:
            a, b = rng.sample(population, 2)
            cut = rng.randint(1, genome_length - 1)
            child_genome = a.genome[:cut] + b.genome[cut:]
            if rng.random() < mutation_rate:
                idx = rng.randrange(genome_length)
                child_genome[idx] += rng.uniform(-1, 1)
            offspring.append(Individual(child_genome))
        evaluate(offspring, fn)
        union = population + offspring
        fronts = _non_dominated_sort(union)
        new_pop: Population = []
        for front in fronts:
            _crowding(front)
            front.sort(key=lambda x: (-x.rank, -x.crowd))
            for ind in front:
                if len(new_pop) < population_size:
                    new_pop.append(ind)
        return new_pop

    for _ in range(generations):
        pop = _step(pop)

    return pop",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"def run_loop(
    *,
    cost_budget: float | None = None,
    wallclock: float | None = None,
    cost_per_cycle: float = 1.0,
    state_file: str = ""loop_state.json"",
    revive_rate: int = 0,
    agents: dict[str, bool] | None = None,
    rng: random.Random | None = None,
    gains: list[float] | None = None,
    early_stopper: BanditEarlyStopper | None = None,
) -> Result:
    """"""Run the FSM until budgets are exhausted.

    Args:
        cost_budget: Optional cost limit.
        wallclock: Optional wall-clock limit in seconds.
        cost_per_cycle: Cost incurred per complete cycle.
        state_file: Path used when persisting state on ``KeyboardInterrupt``.
        revive_rate: Attempt revival every ``revive_rate`` cycles (0 disables).
        agents: Mapping of agent names to active state.
        rng: Random generator for deterministic tests.

    Returns:
        :class:`Result` with final state, completed cycles, cost spent and
        the number of agents revived.
    """"""

    state = State.SELECT
    cycles = 0
    cost_spent = 0.0
    start = time.time()
    rng = rng or random.Random()
    agents = agents or {}
    revive_count = 0

    gain_iter = iter(gains or [])

    try:
        while True:
            if state is State.SELECT:
                state = State.SELF_MOD
                continue
            if state is State.SELF_MOD:
                state = State.BENCHMARK
                continue
            if state is State.BENCHMARK:
                cost_spent += cost_per_cycle
                gain = next(gain_iter, 0.0)
                if early_stopper and early_stopper.update(cost_per_cycle, gain):
                    break
                state = State.ARCHIVE
                continue
            if state is State.ARCHIVE:
                cycles += 1
                if revive_rate and cycles % revive_rate == 0:
                    inactive = [a for a, active in agents.items() if not active]
                    if inactive:
                        revived = rng.choice(inactive)
                        agents[revived] = True
                        revive_count += 1
                        metrics.dgm_revives_total.inc()
                        state = State.SELF_MOD
                        continue
                state = State.SELECT
                if cost_budget is not None and cost_spent >= cost_budget:
                    break
                if wallclock is not None and time.time() - start >= wallclock:
                    break
    except KeyboardInterrupt:  # pragma: no cover - interactive
        Path(state_file).write_text(json.dumps({""state"": state.name, ""cycles"": cycles, ""cost"": cost_spent}))
        return Result(state=state, cycles=cycles, cost=cost_spent, revives=revive_count)

    return Result(state=state, cycles=cycles, cost=cost_spent, revives=revive_count)",alpha_factory_v1/core/simulation/loop.py,
survived,"def configure() -> None:
    """"""Initialise tracing and metrics if the SDK is installed.""""""
    global tracer, meter
    if trace is None or metrics is None:
        return

    endpoint = os.getenv(""OTEL_EXPORTER_OTLP_ENDPOINT"")
    if endpoint:
        span_exporter = OTLPSpanExporter(endpoint=endpoint)
        metric_exporter = OTLPMetricExporter(endpoint=endpoint)
    else:
        span_exporter = ConsoleSpanExporter()
        metric_exporter = ConsoleMetricExporter()

    resource = Resource.create({""service.name"": ""alpha-insight""})
    provider = TracerProvider(resource=resource)
    provider.add_span_processor(BatchSpanProcessor(span_exporter))
    trace.set_tracer_provider(provider)
    tracer = trace.get_tracer(""alpha_insight"")

    meter_provider = MeterProvider(
        resource=resource,
        metric_readers=[PeriodicExportingMetricReader(metric_exporter)],
    )
    metrics.set_meter_provider(meter_provider)
    meter = metrics.get_meter(""alpha_insight"")
",alpha_factory_v1/core/utils/tracing.py,
survived,"def test_workbox_replaced():
    path = pathlib.Path(""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/lib/workbox-sw.js"")
    data = path.read_text(errors=""ignore"")
    assert ""Placeholder"" not in data",tests/test_assets_replaced.py,
survived,"  def __len__(self) -> int:
    """"""Return the number of sites on this carrier.""""""
    return len(self.sites)
",pylabrobot/resources/carrier.py,Carrier
survived,"def test_non_dict_returns_zero() -> None:
    _reset_ledger()
    value = eb.reward(None, None, ""bad"")
    assert value == 0.0",tests/test_energy_balance_reward.py,
survived,"def test_duplicate_request_id_zero() -> None:
    _reset()
    res = {""request_id"": ""r3"", ""violation"": False}
    sc.reward(None, None, res)
    assert sc.reward(None, None, res) == 0.0",tests/test_safety_compliance_reward.py,
survived,"    def start_merkle_task(self, *a, **kw) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_property.py,DummyLedger
survived,"def main():
    a = int(input())
    b = int(input())
    print(a + b)
",tests/rosetta/out/Python/a+b.py,
survived,"    def model_check_fn(self, model: Any) -> bool:
        """"""Check that the model is a causal language model.""""""
        return is_causal_lm(model)
",src/pruna/algorithms/quantization/llm_compressor.py,LLMCompressorQuantizer
survived,"def _timeline_df(traj: list[Any]) -> ""pd.DataFrame"":
    """"""Convert trajectory data into a pandas DataFrame.""""""
    import pandas as pd

    rows = []
    for point in traj:
        for sec in point.sectors:
            rows.append(
                {
                    ""year"": point.year,
                    ""sector"": sec.name,
                    ""energy"": sec.energy,
                    ""disrupted"": sec.disrupted,
                }
            )
    return pd.DataFrame(rows)
",src/interface/minimal_ui.py,
survived,"    def _fake_run(*_a, **_k):
        return subprocess.CompletedProcess([], 0, """", """")
",tests/test_check_env_network.py,
survived,"    def test_stream_rate_env_controls_interval(self) -> None:
        os.environ[""STREAM_RATE_HZ""] = ""5""

        async def grab_two() -> float:
            gen = demo.experience_stream()
            t1 = time.perf_counter()
            await anext(gen)
            t2 = time.perf_counter()
            await anext(gen)
            t3 = time.perf_counter()
            return (t2 - t1 + t3 - t2) / 2

        avg = asyncio.run(grab_two())
        del os.environ[""STREAM_RATE_HZ""]
        self.assertLess(avg, 0.4)
        self.assertGreater(avg, 0.1)
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def test_agentbase_alias(self):
        legacy = importlib.import_module(""backend.agent_base"").AgentBase
        canonical = importlib.import_module(""backend.agents.base"").AgentBase
        self.assertIs(legacy, canonical)
",tests/test_agents_alias.py,TestAgentsAlias
survived,"def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, default))
    except (TypeError, ValueError):
        return default
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/config.py,
survived,"def test_simulate_years() -> None:
    secs = [sector.Sector(""x"", 1.0, 1.0)]
    results = forecast.simulate_years(secs, 2)
    assert len(results) == 2",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_forecast.py,
survived,"    async def run_cycle(self) -> None:
        await self.emit(""research"", {""plan"": ""collect data""})
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/planning_agent.py,PlanningAgent
survived,"def setup(level: str = ""INFO"") -> None:
    if not logging.getLogger().handlers:
        logging.basicConfig(
            level=level,
            format=""%(asctime)s %(levelname)s %(name)s | %(message)s"",
            datefmt=""%Y-%m-%d %H:%M:%S"",
        )",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,
survived,"    async def run_cycle(self) -> None:  # pragma: no cover - interface
        raise NotImplementedError",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent
survived,"def nsga2_step(pop: Population, fn: Callable[[List[float]], Tuple[float, float]], mu: int = 20) -> Population:
    evaluate(pop, fn)
    offspring: Population = []
    while len(offspring) < mu:
        a, b = random.sample(pop, 2)
        cut = random.randint(1, len(a.genome) - 1)
        child_genome = a.genome[:cut] + b.genome[cut:]
        if random.random() < 0.1:
            idx = random.randrange(len(child_genome))
            child_genome[idx] += random.uniform(-1, 1)
        offspring.append(Individual(child_genome))
    evaluate(offspring, fn)
    union = pop + offspring
    fronts = _non_dominated_sort(union)
    new_pop: Population = []
    for front in fronts:
        _crowding(front)
        front.sort(key=lambda x: (-x.rank, -x.crowd))
        for ind in front:
            if len(new_pop) < mu:
                new_pop.append(ind)
    return new_pop",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"def test_jax_llh(benchmark_problem):
    jax.config.update(""jax_enable_x64"", True)
    from beartype import beartype

    problem_id, flat_petab_problem, petab_problem, amici_model = (
        benchmark_problem
    )

    amici_solver = amici_model.getSolver()
    cur_settings = settings[problem_id]
    amici_solver.setAbsoluteTolerance(1e-8)
    amici_solver.setRelativeTolerance(1e-8)
    amici_solver.setMaxSteps(10_000)

    simulate_amici = partial(
        simulate_petab,
        petab_problem=flat_petab_problem,
        amici_model=amici_model,
        solver=amici_solver,
        scaled_parameters=True,
        scaled_gradients=True,
        log_level=logging.DEBUG,
    )

    np.random.seed(cur_settings.rng_seed)

    problem_parameters = None
    if problem_id in problems_for_gradient_check:
        point = flat_petab_problem.x_nominal_free_scaled
        for _ in range(20):
            amici_solver.setSensitivityMethod(amici.SensitivityMethod.adjoint)
            amici_solver.setSensitivityOrder(amici.SensitivityOrder.first)
            amici_model.setSteadyStateSensitivityMode(
                cur_settings.ss_sensitivity_mode
            )
            point_noise = (
                np.random.randn(len(point)) * cur_settings.noise_level
            )
            point += point_noise  # avoid small gradients at nominal value

            problem_parameters = dict(
                zip(flat_petab_problem.x_free_ids, point)
            )

            r_amici = simulate_amici(
                problem_parameters=problem_parameters,
            )
            if np.isfinite(r_amici[LLH]):
                break
        else:
            raise RuntimeError(""Could not compute expected derivative."")
    else:
        r_amici = simulate_amici()
    llh_amici = r_amici[LLH]

    jax_model = import_petab_problem(
        petab_problem,
        model_output_dir=benchmark_outdir / (problem_id + ""_jax""),
        jax=True,
    )
    jax_problem = JAXProblem(jax_model, petab_problem)
    if problem_parameters:
        jax_problem = eqx.tree_at(
            lambda x: x.parameters,
            jax_problem,
            jnp.array(
                [problem_parameters[pid] for pid in jax_problem.parameter_ids]
            ),
        )

    if problem_id in problems_for_gradient_check:
        beartype(run_simulations)(jax_problem)
        (llh_jax, _), sllh_jax = eqx.filter_value_and_grad(
            run_simulations, has_aux=True
        )(jax_problem)
    else:
        llh_jax, _ = beartype(run_simulations)(jax_problem)

    np.testing.assert_allclose(
        llh_jax,
        llh_amici,
        rtol=1e-3,
        atol=1e-3,
        err_msg=f""LLH mismatch for {problem_id}"",
    )

    if problem_id in problems_for_gradient_check:
        sllh_amici = r_amici[SLLH]
        np.testing.assert_allclose(
            sllh_jax.parameters,
            np.array([sllh_amici[pid] for pid in jax_problem.parameter_ids]),
            rtol=1e-2,
            atol=1e-2,
            err_msg=f""SLLH mismatch for {problem_id}, {dict(zip(jax_problem.parameter_ids, sllh_jax.parameters))}"",
        )",tests/benchmark-models/test_petab_benchmark_jax.py,
survived,"    async def process_message(
        message: Message, tempdir: Path | None = None, tagged: bool = True
    ) -> tuple[str, list[Path | str]]:
        """"""
        Process a single message and return model input.
        """"""
        model_input = """"
        files: list[Path | str] = []
        if isinstance(message.content, str):
            # Pure text content
            model_input = message.content
        else:
            # Mixed content
            # TODO: Use Pydantic to enforce the value checking
            for item in message.content:
                if item.type == ""text"":
                    model_input = item.text or """"

                elif item.type == ""image_url"":
                    if not item.image_url:
                        raise ValueError(""Image URL cannot be empty"")
                    if url := item.image_url.get(""url"", None):
                        files.append(await save_url_to_tempfile(url, tempdir))
                    else:
                        raise ValueError(""Image URL must contain 'url' key"")

                elif item.type == ""file"":
                    if not item.file:
                        raise ValueError(""File cannot be empty"")
                    if file_data := item.file.get(""file_data"", None):
                        filename = item.file.get(""filename"", """")
                        files.append(await save_file_to_tempfile(file_data, filename, tempdir))
                    else:
                        raise ValueError(""File must contain 'file_data' key"")

        # Add role tag if needed
        if model_input and tagged:
            model_input = add_tag(message.role, model_input)

        return model_input, files
",app/services/client.py,GeminiClientWrapper
survived,"    def get_player_location_global(self):
        '''
        get_player_location_global
        '''
        scale_factor = self.cfg.localize_downscale_factor
        # Downscale both template and search image
        img_roi = self.img_frame_gray[self.cfg.camera_ceiling:self.cfg.camera_floor, :]
        img_query = cv2.resize(img_roi, (0, 0), fx=scale_factor, fy=scale_factor)

        # Get previous frame result
        if self.is_first_frame or \
            time.time() - self.t_last_camera_missed > self.cfg.localize_cached_interval:
            last_result = None
            self.t_last_camera_missed = time.time()
        else:
            last_result = (
                int(self.loc_camera[0] * scale_factor),
                int(self.loc_camera[1] * scale_factor)
            )

        loc_camera, score, is_cached = find_pattern_sqdiff(
            self.img_map_resized,
            img_query,
            last_result=last_result,
            local_search_radius=20,
            global_threshold = 0.8)
        self.loc_camera = (
            int(loc_camera[0] / scale_factor),
            int(loc_camera[1] / scale_factor)
        )
        loc_player_global = (
            self.loc_camera[0] + self.loc_player[0],
            self.loc_camera[1] + self.loc_player[1] - self.cfg.camera_ceiling)

        # Draw camera rectangle
        camera_bottom_right = (
            self.loc_camera[0] + self.img_frame.shape[1],
            self.loc_camera[1] + self.img_frame.shape[0]
        )
        cv2.rectangle(self.img_route_debug, self.loc_camera,
                      camera_bottom_right, (0, 255, 255), 2)
        cv2.putText(
            self.img_route_debug,
            f""Camera, score={round(score, 2)}, {'cached' if is_cached else 'missed'}"",
            (self.loc_camera[0], self.loc_camera[1] + 60),
            cv2.FONT_HERSHEY_SIMPLEX, 2,
            (0, 255, 0), 2
        )

        # Draw player center
        cv2.circle(self.img_route_debug,
                   loc_player_global, radius=3,
                   color=(0, 0, 255), thickness=-1)
        cv2.putText(self.img_route_debug, ""Player"",
                    (loc_player_global[0] - 30, loc_player_global[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        return loc_player_global
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def capture_frame(self):
        '''
        捕捉當前遊戲區域畫面
        '''
        img = self.capture.grab(self.region)
        frame = np.array(img)
        with self.lock:
            self.frame = frame
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"def _build_local_site(repo_root: Path) -> bool:
    script = repo_root / ""scripts"" / ""build_gallery_site.sh""
    if not script.is_file():
        return False
    try:
        subprocess.run([str(script)], check=True)
    except Exception:
        return False
    return True
",scripts/open_subdir_demo.py,
survived,"        def __init__(self, *a, llm=None, **_k) -> None:
            self.llm = llm
",tests/test_aiga_openai_bridge_offline.py,DummyEvolver
survived,"        def run_generations(self, *_a):
            pass
",tests/test_aiga_openai_bridge_offline.py,_DummyEvolver
survived,"    def start_merkle_task(self, *_a, **_kw) -> None:  # pragma: no cover - test stub
        pass
",tests/test_alert_webhook.py,DummyLedger
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - test stub
        pass
",tests/test_alert_webhook.py,DummyLedger
survived,"    def test_rect2rect_mtx(self):
        src = (0, 0, 1, 1)
        dst = (0, 0, 2, 2)
        M = common.rect2rect_mtx(src, dst)
        expected = np.array([[2.0, 0.0, 0.0],
                             [0.0, 2.0, 0.0],
                             [0.0, 0.0, 1.0]])
        np.testing.assert_array_almost_equal(M, expected)
",tests/test_common.py,TestCommonFunctions
survived,"def test_post_new_env(non_network: None) -> None:
    """"""Force a new environment via /command.""""""
    os.environ[""NO_LLM""] = ""1""
    os.environ.setdefault(""ALPHA_ASI_SILENT"", ""1"")
    os.environ.setdefault(""ALPHA_ASI_MAX_STEPS"", ""1"")

    mod = importlib.import_module(
        ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    )
    client = TestClient(cast(Any, mod.app))

    resp = client.post(""/command"", json={""cmd"": ""new_env""})
    assert resp.status_code == 200
    assert resp.json() == {""ok"": True}",tests/test_world_model_demo.py,
survived,"def eval(code: str) -> str:
    """"""Evaluate Mochi source using a Go subprocess and return output.""""""
    here = os.path.dirname(__file__)
    eval_src = os.path.join(here, ""runner"", ""main.go"")
    env = os.environ.copy()
    env[""MOCHI_CODE""] = code
    proc = subprocess.run(
        [""go"", ""run"", eval_src], capture_output=True, text=True, env=env
    )
    if proc.returncode != 0:
        raise RuntimeError(f""go run failed: {proc.stderr}"")
    result = json.loads(proc.stdout)
    if ""error"" in result and result[""error""]:
        raise RuntimeError(result[""error""])
    return result.get(""output"", """")
",tools/libmochi/python/libmochi.py,
survived,"def test_generate_mock(monkeypatch: pytest.MonkeyPatch) -> None:
    import alpha_factory_v1.demos.gpt2_small_cli.gpt2_cli as mod

    class FakeTokenizer:
        eos_token_id = 0

        def __init__(self, *args: object, **kwargs: object) -> None:
            pass

        def __call__(self, text: str, return_tensors: str = ""pt"") -> dict[str, list[int]]:
            return {""input_ids"": [0]}

        def decode(self, ids: list[int], skip_special_tokens: bool = True) -> str:
            return ""output""

    class FakeModel:
        @classmethod
        def from_pretrained(cls, name: str) -> ""FakeModel"":
            return cls()

        def generate(self, **kwargs: object) -> list[list[int]]:
            return [[0]]

    monkeypatch.setattr(""transformers.AutoTokenizer"", FakeTokenizer, raising=False)
    monkeypatch.setattr(""transformers.AutoModelForCausalLM"", FakeModel, raising=False)

    result = mod.generate(""hi"", 5)
    assert result == ""output""",tests/test_gpt2_cli_demo.py,
survived,"def convert(src: Path, dest: Path | None = None) -> None:
    dest = dest or src
    try:
        from transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch import (
            convert_gpt2_checkpoint_to_pytorch,
        )
    except Exception as exc:  # pragma: no cover
        raise SystemExit(f""transformers with PyTorch is required: {exc}"")

    ckpt = src / ""model.ckpt""
    config = src / ""hparams.json""
    convert_gpt2_checkpoint_to_pytorch(str(ckpt), str(config), str(dest))
",scripts/convert_openai_gpt2.py,
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""src"", type=Path, help=""Directory containing the OpenAI checkpoint"")
    parser.add_argument(""dest"", nargs=""?"", type=Path, help=""Output directory (defaults to src)"")
    args = parser.parse_args()
    convert(args.src, args.dest)
",scripts/convert_openai_gpt2.py,
survived,"    def test_missing_readme_fails(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            d = os.path.join(tmp, ""demo_missing"")
            os.mkdir(d)
            open(os.path.join(d, ""__init__.py""), ""w"").close()
            exit_code = validate_demos.main(tmp, min_lines=1)
            self.assertEqual(exit_code, 1)
",tests/test_demo_quality.py,TestValidateDemosFailures
survived,"    def test_ob_early_data(self):
        """"""Ensure early candles do not cause index errors in OB calculation.""""""
        short_df = pd.DataFrame(
            {
                ""open"": [1.0, 1.1, 1.2],
                ""high"": [1.05, 1.15, 1.25],
                ""low"": [0.95, 1.05, 1.15],
                ""close"": [1.02, 1.14, 1.24],
                ""volume"": [5, 6, 7],
            }
        )
        swing = smc.swing_highs_lows(short_df, swing_length=1)
        ob_df = smc.ob(short_df, swing)
        self.assertEqual(len(ob_df), len(short_df))
",tests/unit_tests.py,TestSmartMoneyConcepts
survived,"def view_lines(path: str | Path, start: int, end: Optional[int]) -> str:
    """"""Return lines ``start`` through ``end`` (1-indexed, inclusive).""""""
    s = max(0, start - 1)
    return view(path, s, end)
",src/self_edit/tools.py,
survived,"def main() -> None:
    arch = HashArchive(""audit.db"")
    success = 0
    with tempfile.TemporaryDirectory() as tmp:
        for i in range(100):
            f = Path(tmp) / f""agent_{i}.tar""
            f.write_text(str(i), encoding=""utf-8"")
            cid = arch.add_tarball(f)
            if cid:
                success += 1
    root = arch.merkle_root()
    print(f""merkle_root={root}"")
    rate = success / 100
    print(f""pin_rate={rate:.2%}"")
    if root != EXPECTED_ROOT:
        raise SystemExit(""unexpected merkle root"")
    if rate < 0.99:
        raise SystemExit(""pin success below threshold"")
",scripts/audit_archive.py,
survived,"def archive() -> None:
    """"""Manage archived agent tarballs.""""""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def tesla_checksum(address: int, sig, d: bytearray) -> int:
  checksum = (address & 0xFF) + ((address >> 8) & 0xFF)
  checksum_byte = sig.start_bit // 8
  for i in range(len(d)):
    if i != checksum_byte:
      checksum += d[i]
  return checksum & 0xFF",opendbc/car/tesla/teslacan.py,
survived,"def honda_checksum(address: int, sig, d: bytearray) -> int:
  s = 0
  extended = address > 0x7FF
  addr = address
  while addr:
    s += addr & 0xF
    addr >>= 4
  for i in range(len(d)):
    x = d[i]
    if i == len(d) - 1:
      x >>= 4
    s += (x & 0xF) + (x >> 4)
  s = 8 - s
  if extended:
    s += 3
  return s & 0xF",opendbc/car/honda/hondacan.py,
survived,"    def _convert_messages(
        self, messages: str | list[str | SamplingMessage]
    ) -> list[SamplingMessage]:
        """"""Convert plain strings to ``SamplingMessage`` objects.""""""

        if isinstance(messages, str):
            messages = [messages]

        converted: list[SamplingMessage] = []
        for msg in messages:
            if isinstance(msg, SamplingMessage):
                converted.append(msg)
            elif isinstance(msg, str):
                converted.append(
                    SamplingMessage(
                        role=""user"",
                        content=TextContent(type=""text"", text=msg),
                    )
                )
            else:
                raise TypeError(""messages must be str or SamplingMessage"")
        return converted
",src/enrichmcp/context.py,EnrichContext
survived,"        def predict(self, x: int) -> int:
            return x
",tests/trace/test_serialize.py,MyObj
survived,"def self_improve(template: str, logs: str, *, seed: int | None = None) -> str:
    """"""Return a patch proposal by querying the configured LLM.""""""
    if seed is not None:
        random.seed(seed)
    system_prompt = CFG.self_improve.system
    user_prompt = template.format(logs=logs)
    if CFG.self_improve.user:
        user_prompt = f""{CFG.self_improve.user}\n{user_prompt}""
    llm = _get_llm()
    return llm(user_prompt, system_prompt)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_edit/prompting.py,
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    docs_dir = repo_root / ""docs""
    missing: list[Path] = []

    for html in sorted(docs_dir.rglob(""index.html"")):
        if html.parent.name == ""DISCLAIMER_SNIPPET"":
            continue
        text = html.read_text(encoding=""utf-8"", errors=""ignore"")
        if SNIPPET not in text:
            missing.append(html.relative_to(repo_root))

    if missing:
        print(""Missing HTML disclaimer link:"", file=sys.stderr)
        for path in missing:
            print(f""  {path}"", file=sys.stderr)
        return 1
    return 0
",scripts/verify_html_disclaimer.py,
survived,"    def execute_and_collect(self, path: Path, timeout: int = 60) -> CollectionResult:
        """"""Run tests via the execution module and gather outputs.""""""
        start = time.perf_counter()
        result = self.execution_module.run_tests(path, timeout)
        end = time.perf_counter()
        return CollectionResult(
            exit_code=result.exit_code,
            stdout=result.stdout,
            stderr=result.stderr,
            duration=end - start,
        )",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule
survived,"    def aggregate(self, results: Iterable[CollectionResult]) -> List[SummaryReport]:
        """"""Summarize multiple results.""""""
        return [self.summarize(r) for r in results]
",src/meta_agent/evaluation/reporting.py,ReportingModule
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/emirp-primes.py,
survived,"def pow10(n):
    r = 1.0
    i = 0
    while i < n:
        r = r * 10.0
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/empty-program.py,
survived,"def neg(p):
    return Pt(x=p.x, y=-p.y, inf=p.inf)
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""Private key:\nD: 1234567890"")
    print(""\nPublic key:"")
    print(""X: 43162711582587979080031819627904423023685561091192625653251495188141318209988"")
    print(""Y: 86807430002474105664458509423764867536342689150582922106807036347047552480521"")
    print(""\nMessage: Rosetta Code"")
    print(""Hash   : 0xe6f9ed0d"")
    print(""\nSignature:"")
    print(""R: 23195197793674669608400023921033380707595656606710353926508630347378485682379"")
    print(""S: 79415614279862633473653728365954499259635019180091322566320325357594590761922"")
    print(""\nSignature verified: true"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/elliptic-curve-digital-signature-algorithm.py,
survived,"def bitAt(x, idx):
    v = x
    i = 0
    while i < idx:
        v = int((v // 2))
        i = i + 1
    return v % 2
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/entropy-2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,
survived,"def addNoCells(cells):
    l = ""O""
    r = ""O""
    if cells[0:1] == ""O"":
        l = "".""
    if cells[len(cells) - 1:len(cells)] == ""O"":
        r = "".""
    cells = l + cells + r
    cells = l + cells + r
    return cells
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,
survived,"def formatFloat(f, prec):
    scale = pow10(prec)
    scaled = (f * scale) + 0.5
    n = (int(scaled))
    digits = str(n)
    while len(digits) <= prec:
        digits = ""0"" + digits
    intPart = digits[0:len(digits) - prec]
    fracPart = digits[len(digits) - prec:len(digits)]
    return intPart + ""."" + fracPart
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def _commit_patch(repo_path: Path, message: str = ""auto‑fix: CI green 🟢"") -> str:
    if git is None:
        return ""[git unavailable ‑ simulated commit] "" + message

    try:
        repo = git.Repo(repo_path)
    except git.InvalidGitRepositoryError:
        return ""[not a git repo ‑ skipping commit]""

    branch_name = ""auto-fix""
    if branch_name in repo.heads:
        repo.git.checkout(branch_name)
    else:
        repo.git.checkout(b=branch_name)

    repo.git.add(update=True)
    repo.index.commit(message)
    commit_hash = repo.head.commit.hexsha[:7]
    return f""Committed patch {commit_hash} on branch {branch_name}""
",alpha_factory_v1/demos/self_healing_repo_cli.py,
survived,"def test_reload_restores_settings() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")

        seed_input = page.locator(""#seed"")
        seed_input.fill(""321"")
        seed_input.dispatch_event(""change"")
        page.wait_for_function(""location.hash.includes('seed=321')"")

        page.evaluate(""location.hash = ''"")
        page.reload()
        page.wait_for_selector(""#controls"")
        assert page.input_value(""#seed"") == ""321""
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"def run(cmd: list[str]) -> None:
    subprocess.run(cmd, check=True)
",alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py,
survived,"    def test_entrypoint_compiles(self):
        py_compile.compile(ENTRYPOINT, doraise=True)
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry
survived,"            async def close(self) -> None:
                calls.append((""closed"", True))
",tests/test_ledger_client_close.py,DummyClient
survived,"            def __init__(self) -> None:
                self.instructions: list[Any] = []
",tests/test_ledger_client_close.py,DummyTx
survived,"def parse_bytes(size: Union[int, str, None]) -> Optional[int]:
    """"""Convert a human friendly size string to bytes.""""""
    if size is None:
        return None
    if isinstance(size, int):
        return size
    match = re.fullmatch(r""(?i)\s*(\d+(?:\.\d+)?)\s*([kmgt]?b)?\s*"", str(size))
    if not match:
        raise ValueError(f""Invalid size value: {size}"")
    number = float(match.group(1))
    unit = (match.group(2) or ""b"").upper()
    factor = {
        ""B"": 1,
        ""KB"": 1024,
        ""MB"": 1024**2,
        ""GB"": 1024**3,
        ""TB"": 1024**4,
    }[unit]
    return int(number * factor)
",src/cachier/config.py,
survived,"    def id_to_word(cls, idx):
        return cls._words[idx]
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    def test_share_checksum(self):
        share = ""hearing echo academic acid deny bracelet playoff exact fancy various evidence standard adjust muscle parcel sled crucial amazing mansion losing""
        wallet = btcrseed.WalletSLIP39Seed.create_from_params()
        wallet.config_mnemonic(share)
        self.assertEqual(wallet.return_verified_password_or_false((btcrseed.mnemonic_ids_guess,)),
                         (btcrseed.mnemonic_ids_guess, 1))
",btcrecover/test/test_seeds.py,TestSLIP39Seed
survived,"async def test_crud_decorators_register_resources():
    app = EnrichMCP(""API"", description=""desc"")

    @app.entity
    class Item(EnrichModel):
        """"""Item entity.""""""

        id: int = Field(description=""id"")
        name: str = Field(description=""name"", mutable=True)

    @app.create
    async def create_item(name: str) -> Item:
        """"""Create item.""""""
        return Item(id=1, name=name)

    @app.update
    async def update_item(item_id: int, patch: Item.PatchModel) -> Item:
        """"""Update item.""""""
        return Item(id=item_id, name=patch.name or ""n"")

    @app.delete
    async def delete_item(item_id: int) -> bool:
        """"""Delete item.""""""
        return True

    assert ""create_item"" in app.resources
    assert ""update_item"" in app.resources
    assert ""delete_item"" in app.resources

    item = await create_item(name=""x"")
    assert item.name == ""x""
    item = await update_item(1, Item.PatchModel(name=""y""))
    assert item.name == ""y""
    assert await delete_item(1) is True",tests/test_mutability.py,
survived,"    def create(
        self,
        func: Callable[..., Any] | None = None,
        *,
        name: str | None = None,
        description: str | None = None,
    ) -> Callable[..., Any] | DecoratorCallable:
        """"""Register a create operation.""""""

        def decorator(fn: Callable[..., Any]) -> Callable[..., Any]:
            return self.resource(fn, name=name, description=description)

        if func is not None:
            return decorator(func)
        return cast(""DecoratorCallable"", decorator)
",src/enrichmcp/app.py,EnrichMCP
survived,"def get_string_prefix(code: str) -> str:
    """"""Return the leading string prefix characters (r, u, b, f).""""""
    idx = 0
    while idx < len(code) and code[idx] in ""furbFURB"":
        idx += 1
    return code[:idx]
",src/flynt/utils/format.py,
survived,"async def list_notes(page: int = 1, page_size: int = 10) -> list[NoteSummary]:
    """"""Return a paginated list of notes.""""""
    return project.list_notes(page, page_size)
",examples/basic_memory/app.py,
survived,"def _have_torch():
    try:
        import torch  # noqa: F401

        return True
    except Exception:
        return False
",tests/test_torch_backend.py,
survived,"def test_torch_backend(tmp_path):
    if not _have_torch() or os.environ.get(""RUN_ENGINE_TESTS"") != ""1"":
        print(""Skipping torch backend test"")
        return
    out_dir = tmp_path
    cmd = [
        ""./Release/Sibernetic"",
        ""-no_g"",
        ""-f"",
        ""configuration/test/test_energy"",
        ""-l_to"",
        f""lpath={out_dir}"",
        ""timelimit=0.001"",
        ""logstep=25"",
        ""backend=torch"",
    ]
    env = os.environ.copy()
    env[""PYTHONPATH""] = f""{os.getcwd()}:{env.get('PYTHONPATH', '')}""
    proc = subprocess.run(cmd, env=env)
    if proc.returncode != 0:
        print(""Torch backend run failed, skipping"")
        return

    pos = _load_matrix(""position_buffer.txt"", base=out_dir)
    base_pos = _load_matrix(""positions_step0.txt"")
    for g_row, b_row in zip(pos, base_pos):
        for gv, bv in zip(g_row, b_row):
            assert math.isfinite(gv)
            assert abs(gv - bv) < 1e-3

    vel = _load_matrix(""velocity_buffer.txt"", base=out_dir)
    base_vel = _load_matrix(""velocities_step0.txt"")
    for g_row, b_row in zip(vel, base_vel):
        for gv, bv in zip(g_row, b_row):
            assert math.isfinite(gv)
            assert abs(gv - bv) < 1e-3

    energy_file = os.path.join(out_dir, ""total_energy_distrib.txt"")
    if os.path.exists(energy_file):
        with open(energy_file) as f:
            energies = [float(line.strip()) for line in f if line.strip()]
        if len(energies) >= 2:
            start, end = energies[0], energies[-1]
            rel = abs(end - start) / (abs(start) + 1e-12)
            assert rel < 1.0",tests/test_torch_backend.py,
survived,"def gather_assets(docs_dir: Path) -> list[str]:
    base_assets = docs_dir / ""assets""
    assets: list[str] = []
    allowed = {"".js"", "".css"", "".svg"", "".json"", "".wasm"", "".tar"", "".cast""}
    pyodide_dir = base_assets / ""pyodide""
    if pyodide_dir.is_dir():
        order = [""pyodide.js"", ""pyodide.asm.wasm"", ""pyodide_py.tar""]
        for name in order:
            file = pyodide_dir / name
            if file.is_file() and file.suffix in allowed:
                rel = Path(""assets"") / ""pyodide"" / file.name
                assets.append(rel.as_posix())
    for item in sorted(docs_dir.iterdir()):
        if not item.is_dir() or item.name == ""assets"":
            continue
        a_dir = item / ""assets""
        if a_dir.is_dir():
            for file in sorted(a_dir.rglob(""*"")):
                if file.is_file() and file.suffix in allowed:
                    rel = Path("".."") / item.name / ""assets"" / file.relative_to(a_dir)
                    assets.append(rel.as_posix())
    return assets
",scripts/build_service_worker.py,
survived,"def convert_feats(v1_path, out_dir, doc_slug, start_pk):
    feats_v1 = load_json(v1_path)
    feats_v2 = []
    fb_v2 = []
    pk_counter = start_pk
    for feat in feats_v1:
        f = feat[""fields""]
        slug = feat[""pk""]
        pk = f""{doc_slug}_{slug}""
        feats_v2.append({
            ""model"": ""api_v2.feat"",
            ""pk"": pk,
            ""fields"": {
                ""name"": f[""name""],
                ""desc"": f[""desc""],
                ""prerequisite"": f.get(""prerequisite""),
                ""document"": doc_slug,
            },
        })
        eff = f.get(""effects_desc_json"")
        if eff:
            try:
                parts = json.loads(eff)
            except Exception:
                parts = []
            for part in parts:
                fb_v2.append({
                    ""model"": ""api_v2.featbenefit"",
                    ""pk"": pk_counter,
                    ""fields"": {""name"": """", ""desc"": part, ""type"": None, ""parent"": pk},
                })
                pk_counter += 1
    if feats_v2:
        save_json(feats_v2, os.path.join(out_dir, ""Feat.json""))
    if fb_v2:
        save_json(fb_v2, os.path.join(out_dir, ""FeatBenefit.json""))
    return pk_counter
",convert_missing.py,
survived,"def test_readiness() -> None:
    resp = client.get(""/readiness"")
    assert resp.status_code == 200
    assert resp.text == ""ok""",tests/test_insight_health.py,
survived,"def make_client() -> TestClient:
    return TestClient(cast(Any, api.app))
",tests/test_metrics_router.py,
survived,"def test_memory_agent_handle_appends() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = memory_agent.MemoryAgent(bus, led)
    env = messaging.Envelope(""a"", ""memory"", {""v"": 1}, 0.0)
    asyncio.run(agent.handle(env))
    assert agent.records == [{""v"": 1}]
",tests/test_agent_handle_methods.py,
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_agent_handle_methods.py,DummyLedger
survived,"def test_strategy_agent_emits_market() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = strategy_agent.StrategyAgent(bus, led)
    env = messaging.Envelope(""research"", ""strategy"", {""research"": ""foo""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published
    topic, sent = bus.published[-1]
    assert topic == ""market""
    assert ""strategy"" in sent.payload
",tests/test_agent_handle_methods.py,
survived,"    def start_socket() -> None:
        """"""Start the optional A2A socket if available.""""""
        if _A2A:
            try:
                _A2A.start()
            except Exception:  # pragma: no cover - best effort
                LOG.warning(""Failed to start A2A socket"", exc_info=True)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"async def _build_task_run_response(task_v2: TaskV2) -> TaskRunResponse:
    """"""Build TaskRunResponse object for webhook backward compatibility.""""""
    workflow_run_resp = None
    if task_v2.workflow_run_id:
        try:
            workflow_run_resp = await workflow_service.get_workflow_run_response(
                task_v2.workflow_run_id, organization_id=task_v2.organization_id
            )
        except Exception:
            LOG.warning(
                ""Failed to get workflow run response for task v2 webhook"",
                exc_info=True,
                task_v2_id=task_v2.observer_cruise_id,
            )

    app_url = None
    if task_v2.workflow_run_id and task_v2.workflow_permanent_id:
        app_url = (
            f""{settings.SKYVERN_APP_URL.rstrip('/')}/workflows/""
            f""{task_v2.workflow_permanent_id}/{task_v2.workflow_run_id}""
        )

    return TaskRunResponse(
        run_id=task_v2.observer_cruise_id,
        run_type=RunType.task_v2,
        status=task_v2.status,
        output=task_v2.output,
        failure_reason=workflow_run_resp.failure_reason if workflow_run_resp else None,
        created_at=task_v2.created_at,
        modified_at=task_v2.modified_at,
        recording_url=workflow_run_resp.recording_url if workflow_run_resp else None,
        screenshot_urls=workflow_run_resp.screenshot_urls if workflow_run_resp else None,
        downloaded_files=workflow_run_resp.downloaded_files if workflow_run_resp else None,
        app_url=app_url,
        run_request=TaskRunRequest(
            engine=RunEngine.skyvern_v2,
            prompt=task_v2.prompt or """",
            url=task_v2.url,
            webhook_url=task_v2.webhook_callback_url,
            totp_identifier=task_v2.totp_identifier,
            totp_url=task_v2.totp_verification_url,
            proxy_location=task_v2.proxy_location,
            data_extraction_schema=task_v2.extracted_information_schema,
            error_code_mapping=task_v2.error_code_mapping,
        ),
    )
",skyvern/services/task_v2_service.py,
survived,"    def _write_detailed_python_analysis(self, f):
        """"""
        写入详细的Python对象分析
        """"""
        f.write(""2. Python对象深度分析\n"")
        f.write(""-"" * 50 + ""\n"")
        
        # 强制垃圾回收
        collected = gc.collect()
        f.write(f""垃圾回收清理对象数: {collected}\n\n"")
        
        # 获取所有对象
        all_objects = muppy.get_objects()
        f.write(f""总对象数: {len(all_objects):,}\n"")
        
        # 按类型统计
        type_stats = {}
        for obj in all_objects:
            obj_type = type(obj).__name__
            if obj_type not in type_stats:
                type_stats[obj_type] = {'count': 0, 'size': 0}
            type_stats[obj_type]['count'] += 1
            type_stats[obj_type]['size'] += sys.getsizeof(obj)
        
        # 按大小排序
        sorted_types = sorted(type_stats.items(), key=lambda x: x[1]['size'], reverse=True)
        
        f.write(""对象类型统计 (按内存大小排序):\n"")
        f.write(f""{'类型':<20} {'数量':<10} {'总大小(MB)':<12} {'平均大小(B)':<12}\n"")
        f.write(""-"" * 60 + ""\n"")
        
        total_python_memory = 0
        for obj_type, stats in sorted_types[:20]:  # 只显示前20个
            size_mb = stats['size'] / 1024 / 1024
            avg_size = stats['size'] / stats['count'] if stats['count'] > 0 else 0
            total_python_memory += size_mb
            f.write(f""{obj_type:<20} {stats['count']:<10,} {size_mb:<12.2f} {avg_size:<12.1f}\n"")
        
        f.write(f""\nPython对象总内存: {total_python_memory:.2f} MB\n"")
        
        # 计算未统计内存
        process = psutil.Process()
        total_memory = process.memory_info().rss / 1024 / 1024
        unaccounted = total_memory - total_python_memory
        f.write(f""未统计内存: {unaccounted:.2f} MB ({unaccounted/total_memory*100:.1f}%)\n"")
        
        f.write(""\n"" + ""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper
survived,"    def _write_detailed_system_analysis(self, f):
        """"""
        写入详细的系统内存分析
        """"""
        f.write(""1. 系统级内存分析\n"")
        f.write(""-"" * 50 + ""\n"")
        
        process = psutil.Process()
        memory_info = process.memory_info()
        
        f.write(f""进程ID: {process.pid}\n"")
        f.write(f""进程名称: {process.name()}\n"")
        f.write(f""进程命令行: {' '.join(process.cmdline())}\n\n"")
        
        f.write(""内存使用详情:\n"")
        f.write(f""  RSS (物理内存): {memory_info.rss / 1024 / 1024:.2f} MB\n"")
        f.write(f""  VMS (虚拟内存): {memory_info.vms / 1024 / 1024:.2f} MB\n"")
        f.write(f""  共享内存: {memory_info.shared / 1024 / 1024:.2f} MB\n"")
        f.write(f""  文本段: {memory_info.text / 1024 / 1024:.2f} MB\n"")
        f.write(f""  数据段: {memory_info.data / 1024 / 1024:.2f} MB\n"")
        f.write(f""  库内存: {memory_info.lib / 1024 / 1024:.2f} MB\n"")
        f.write(f""  脏页: {memory_info.dirty / 1024 / 1024:.2f} MB\n"")
        
        # 系统内存信息
        system_memory = psutil.virtual_memory()
        f.write(f""\n系统内存:\n"")
        f.write(f""  总内存: {system_memory.total / 1024 / 1024 / 1024:.2f} GB\n"")
        f.write(f""  可用内存: {system_memory.available / 1024 / 1024 / 1024:.2f} GB\n"")
        f.write(f""  使用率: {system_memory.percent:.2f}%\n"")
        f.write(f""  缓存: {system_memory.cached / 1024 / 1024 / 1024:.2f} GB\n"")
        f.write(f""  缓冲区: {system_memory.buffers / 1024 / 1024 / 1024:.2f} GB\n"")
        
        f.write(""\n"" + ""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper
survived,"    def _get_unaccounted_memory(self) -> float:
        """"""
        计算未统计的内存（可能是C扩展、系统缓存等）
        """"""
        try:
            # 获取进程总内存
            process = psutil.Process()
            total_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # 获取Python对象总内存
            all_objects = muppy.get_objects()
            sum1 = summary.summarize(all_objects)
            
            python_total_mb = 0
            for line in summary.format_(sum1):
                if '|' in line and line.strip() and not line.startswith('=') and not line.startswith('-'):
                    parts = line.split('|')
                    if len(parts) >= 3:
                        try:
                            size_str = parts[2].strip()
                            if 'MB' in size_str:
                                size_mb = float(size_str.replace('MB', '').strip())
                                python_total_mb += size_mb
                        except:
                            pass
            
            return max(0, total_memory - python_total_mb)
        except:
            return 0.0
",app/helper/memory.py,MemoryHelper
survived,"    async def test_latest_run_with_null_metadata(
        self,
        test_api_client: AsyncClient,
        returned_run: AgentRun,
    ):
        """"""Test that null metadata is handled correctly""""""
        returned_run.metadata = None

        response = await test_api_client.get(""/v1/_/agents/bla/runs/latest"")
        assert response.status_code == 200

        response_data = response.json()
        assert response_data[""id""] == returned_run.id
        # metadata should not be present in response when it's None (due to response_model_exclude_none=True)
        assert ""metadata"" not in response_data
",api/api/routers/runs_v1_test.py,TestLatestRun
survived,"    def test_no_changes_scenario(self, mock_config, mock_logger):
        """"""Test scenario where package exists but has no changes""""""

        # Setup existing package
        existing_pkg_id = uuid4()
        existing_package = Package(
            id=existing_pkg_id,
            derived_id=""pkgx/unchanged-pkg"",
            name=""unchanged-pkg"",
            package_manager_id=mock_config.pm_config.pm_id,
            import_id=""unchanged-pkg"",
            readme=""Unchanged description"",
        )

        cache = Cache(
            package_map={""unchanged-pkg"": existing_package},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Create package data with same description
        pkg_data = create_pkgx_package()

        # Test the diff
        diff = PkgxDiff(mock_config, cache, mock_logger)
        pkg_id, pkg_obj, update_payload = diff.diff_pkg(""unchanged-pkg"", pkg_data)

        # Assertions
        assert pkg_id == existing_pkg_id
        assert pkg_obj is None  # No new package
        assert update_payload is None  # No changes
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading
deleted,"    def _is_github_url(self, url: str) -> bool:
        """"""Check if URL is a GitHub URL""""""
        from package_managers.pkgx.transformer import PkgxTransformer

        temp_transformer = PkgxTransformer(self.config, None)
        return temp_transformer.is_github(url)",package_managers/pkgx/diff.py,PkgxDiff
survived,"    def test_dependency_type_priority_with_test(self, mock_config, mock_logger):
        """"""Test priority handling with test dependencies: Runtime > Build > Test""""""

        p1_id = uuid4()
        p2_id = uuid4()
        p3_id = uuid4()
        p4_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""pkgx/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""pkgx/p2"", name=""p2"", import_id=""p2"")
        p3_pkg = Package(id=p3_id, derived_id=""pkgx/p3"", name=""p3"", import_id=""p3"")
        p4_pkg = Package(id=p4_id, derived_id=""pkgx/p4"", name=""p4"", import_id=""p4"")

        cache = Cache(
            package_map={""p1"": p1_pkg, ""p2"": p2_pkg, ""p3"": p3_pkg, ""p4"": p4_pkg},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Parsed data with overlapping dependencies across different types
        new_pkg_data = create_pkgx_package(
            dependencies=[""p2"", ""p3""],  # runtime: p2, p3
            build_deps=[""p2"", ""p4""],  # build: p2, p4
            test_deps=[""p2"", ""p3"", ""p4""],  # test: p2, p3, p4
        )

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""p1"", new_pkg_data)

        # Should create dependencies based on priority:
        # p2: runtime (highest priority among runtime/build/test)
        # p3: runtime (highest priority among runtime/test)
        # p4: build (highest priority among build/test)
        assert len(removed_deps) == 0
        assert len(new_deps) == 3

        # Sort by dependency_id for consistent testing
        new_deps_sorted = sorted(new_deps, key=lambda d: str(d.dependency_id))

        # p2 should be runtime (highest priority)
        p2_dep = next(d for d in new_deps_sorted if d.dependency_id == p2_id)
        assert p2_dep.dependency_type_id == mock_config.dependency_types.runtime

        # p3 should be runtime (highest priority)
        p3_dep = next(d for d in new_deps_sorted if d.dependency_id == p3_id)
        assert p3_dep.dependency_type_id == mock_config.dependency_types.runtime

        # p4 should be build (highest available priority)
        p4_dep = next(d for d in new_deps_sorted if d.dependency_id == p4_id)
        assert p4_dep.dependency_type_id == mock_config.dependency_types.build",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading
survived,"def launch_model(model_key: str):
    model_config = models[model_key]
    print(f""Launching {model_key} ({model_config['model_name']}) on SkyPilot…"")

    setup_script = textwrap.dedent(
        """"""
            echo 'Setting up environment...'
            apt update && apt install -y nvtop
            curl -LsSf https://astral.sh/uv/install.sh | sh
            source $HOME/.local/bin/env

            # Install project in editable mode
            uv remove openpipe-art
            uv add --editable ~/workspace

            # Sync dependencies
            uv sync
            
            # Set up tau-bench specific dependencies if needed
            cd ~/workspace/dev/tau-bench
        """"""
    )

    # Construct the run_rl.py command with all necessary arguments
    run_args = [
        f""--model-name {model_config['model_name']}"",
        f""--base-model {model_config['base_model']}"",
        f""--env {model_config['env']}"",
        f""--model {model_config['model']}"",
        f""--model-provider {model_config['model_provider']}"",
        f""--user-model {model_config['user_model']}"",
        f""--user-model-provider {model_config['user_model_provider']}"",
        f""--agent-strategy {model_config['agent_strategy']}"",
        f""--temperature {model_config['temperature']}"",
        f""--task-split {model_config['task_split']}"",
        f""--start-index {model_config['start_index']}"",
        f""--end-index {model_config['end_index']}"",
        f""--trajectories-per-group {model_config['trajectories_per_group']}"",
        f""--groups-per-step {model_config['groups_per_step']}"",
        f""--learning-rate {model_config['learning_rate']}"",
        f""--eval-steps {model_config['eval_steps']}"",
        f""--val-set-size {model_config['val_set_size']}"",
        f""--training-dataset-size {model_config['training_dataset_size']}"",
        f""--num-epochs {model_config['num_epochs']}"",
    ]

    run_script = textwrap.dedent(f""""""
        echo 'Starting tau-bench RL training...'
        cd ~/workspace/dev/tau-bench
        
        # Ensure project is installed in editable mode
        uv remove openpipe-art
        uv add --editable ~/workspace
        
        # Run the RL training
        uv run run_rl.py {' '.join(run_args)}
    """""")

    # Create a SkyPilot Task
    task = sky.Task(
        name=f""tau-bench-rl-{model_key}"",
        setup=setup_script,
        run=run_script,
        workdir=""."",  # Sync the project directory
        envs=dict(dotenv_values()),
    )
    task.set_resources(sky.Resources(accelerators=""H100-SXM:1""))
    task.set_file_mounts({""~/workspace"": "".""})

    # Generate cluster name
    cluster_name = f""tau-bench-rl-{model_key}""
    print(f""Launching task on cluster: {cluster_name}"")

    print(""Checking for existing cluster and jobs…"")
    cluster_status = sky.get(sky.status(cluster_names=[cluster_name]))
    if len(cluster_status) > 0 and cluster_status[0][""status""] == ClusterStatus.UP:
        print(f""Cluster {cluster_name} is UP. Canceling any active jobs…"")
        sky.stream_and_get(sky.cancel(cluster_name, all=True))

    # Launch the task; stream_and_get blocks until the task starts running, but
    # running this in its own thread means all models run in parallel.
    job_id, _ = sky.stream_and_get(
        sky.launch(
            task,
            cluster_name=cluster_name,
            retry_until_up=True,
            idle_minutes_to_autostop=60,
            down=True,
            fast=args.fast,
        )
    )

    print(f""Job submitted for {model_key} (ID: {job_id}). Streaming logs…"")
    exit_code = sky.tail_logs(cluster_name=cluster_name, job_id=job_id, follow=True)
    print(f""Job {job_id} for {model_key} finished with exit code {exit_code}."")
",run_training.py,
survived,"    def column_list_formats(self) -> Dict[str, str]:
        """"""Get column list formats from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_list_formats""
        ]
        if not config:
            return {}
        return config[0].get(""value"", {})
",keep/api/models/db/preset.py,PresetDto
survived,"def test_semantic_gather_with_peripheral():
    """"""Test semantic gather operation with peripheral chunks configuration.""""""
    # Create test data with more chunks for context
    df = pd.DataFrame({
        ""doc_id"": [""doc1""] * 5,
        ""chunk_num"": [1, 2, 3, 4, 5],
        ""content"": [f""Chunk {i} content"" for i in range(1, 6)]
    })
    
    result = df.semantic.gather(
        content_key=""content"",
        doc_id_key=""doc_id"",
        order_key=""chunk_num"",
        peripheral_chunks={
            ""previous"": {""head"": {""count"": 1}, ""tail"": {""count"": 1}},
            ""next"": {""head"": {""count"": 1}, ""tail"": {""count"": 1}}
        }
    )
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(df)
    assert ""content_rendered"" in result.columns
    
    # Check that middle chunks have context from surrounding chunks
    middle_chunk = result[result[""chunk_num""] == 3].iloc[0]
    rendered = middle_chunk[""content_rendered""]
    
    # Should contain previous and next context markers
    assert ""--- Previous Context ---"" in rendered
    assert ""--- Next Context ---"" in rendered
    assert ""--- Begin Main Chunk ---"" in rendered
    assert ""--- End Main Chunk ---"" in rendered
",tests/test_pandas_accessors.py,
survived,"def test_semantic_gather_basic():
    """"""Test semantic gather operation basic functionality.""""""
    # Create test data that simulates document chunks
    df = pd.DataFrame({
        ""doc_id"": [""doc1"", ""doc1"", ""doc1"", ""doc2"", ""doc2""],
        ""chunk_num"": [1, 2, 3, 1, 2],
        ""content"": [
            ""First chunk of document 1"",
            ""Second chunk of document 1"", 
            ""Third chunk of document 1"",
            ""First chunk of document 2"",
            ""Second chunk of document 2""
        ]
    })
    
    result = df.semantic.gather(
        content_key=""content"",
        doc_id_key=""doc_id"",
        order_key=""chunk_num""
    )
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(df)  # Same number of rows
    assert ""content_rendered"" in result.columns
    
    # Check that rendered content includes the original content
    for i, row in result.iterrows():
        assert row[""content""] in row[""content_rendered""]
",tests/test_pandas_accessors.py,
survived,"    def test_cost_report_endpoint_calls_core(self, mock_core_cost_report):
        """"""Test that cost_report server endpoint calls core function with correct args.""""""
        mock_core_cost_report.return_value = []
        
        # Create mock request and body
        mock_request = mock.Mock()
        mock_request.state.request_id = 'test_request_id'
        
        cost_report_body = payloads.CostReportBody(days=15)
        
        # Import and test the server function
        from sky.server import server
        
        with mock.patch('sky.server.server.executor.schedule_request') as mock_schedule:
            # Call the server endpoint
            import asyncio
            asyncio.run(server.cost_report(mock_request, cost_report_body))
            
            # Verify executor.schedule_request was called with correct parameters
            mock_schedule.assert_called_once()
            call_args = mock_schedule.call_args
            
            self.assertEqual(call_args[1]['request_id'], 'test_request_id')
            self.assertEqual(call_args[1]['request_name'], 'cost_report')
            self.assertEqual(call_args[1]['request_body'], cost_report_body)
            self.assertEqual(call_args[1]['func'], server.core.cost_report)
",tests/unit_tests/test_sky_cost_report.py,TestCostReportServer
survived,"    def test_cost_report_with_missing_instance_type(self):
        """"""Test cost report doesn't crash when historical cluster has unknown instance type.""""""
        # Mock a cluster record with an instance type that doesn't exist in catalogs
        mock_cluster_record = {
            'name': 'old-cluster',
            'status': None,  # Historical cluster
            'num_nodes': 2,
            'resources': mock.Mock(),
            'total_cost': 0.0,
            'launched_at': 1640995200,  # Some timestamp
            'duration': 3600,
            'cluster_hash': 'abc123',
            'usage_intervals': [(1640995200, 1640998800)],
            'user_hash': 'user123',
            'user_name': 'testuser',
            'workspace': 'default',
        }
        
        # Mock the resources object to have an unknown instance type
        mock_cluster_record['resources'].instance_type = 'unknown-instance-type-v1'
        mock_cluster_record['resources'].cloud = mock.Mock()
        mock_cluster_record['resources'].cloud.__str__ = lambda: 'aws'
        
        # Mock catalog functions to return None for unknown instance type
        with mock.patch('sky.catalog.get_hourly_cost', return_value=None):
            with mock.patch('sky.global_user_state.get_clusters_from_history', 
                          return_value=[mock_cluster_record]):
                
                # This should not raise an exception
                result = core.cost_report(days=30)
                
                # Should return the cluster even if cost calculation fails
                self.assertEqual(len(result), 1)
                self.assertEqual(result[0]['name'], 'old-cluster')
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness
survived,"    def diff_url(
        self, import_id: str, debian_data: DebianData, new_urls: dict[URLKey, URL]
    ) -> dict[UUID, UUID]:
        """"""Given a package's URLs, returns the resolved URL for this specific package""""""
        resolved_urls: dict[UUID, UUID] = {}

        # Generate the URLs for this package
        urls = self._generate_chai_urls(debian_data)

        # Process each URL
        for url_key in urls:
            # guard: _generate_chai_urls could be None for a url type
            if url_key is None:
                continue

            resolved_url_id: UUID

            if url_key in new_urls:
                resolved_url_id = new_urls[url_key].id
            elif url_key in self.caches.url_map:
                resolved_url_id = self.caches.url_map[url_key].id
            else:
                self.logger.debug(
                    f""URL {url_key.url} as {url_key.url_type_id} is entirely new""
                )
                new_url = URL(
                    id=uuid4(),
                    url=url_key.url,
                    url_type_id=url_key.url_type_id,
                    created_at=self.now,
                    updated_at=self.now,
                )
                resolved_url_id = new_url.id
                new_urls[url_key] = new_url

            resolved_urls[url_key.url_type_id] = resolved_url_id

        return resolved_urls
",package_managers/debian/diff.py,DebianDiff
survived,"    def test_enrich_package_missing_source_warning(self, caplog, mock_logger):
        """"""Test warning when package references missing source""""""
        from package_managers.debian.main import enrich_package_with_source

        # Create package data with source that doesn't exist in mapping
        package_data = create_debian_package(
            package=""orphan-pkg"",
            description=""An orphaned package"",
        )
        package_data.source = ""missing-source""

        # Empty source mapping
        source_mapping = {}

        # Enrich package (this should log a warning)
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # The warning should be present in the function execution output
        # Check the logged warning message directly
        # Note: The warning is logged by our function, so we check the expected behavior

        # Package should remain unchanged
        assert enriched.package == ""orphan-pkg""
        assert enriched.description == ""An orphaned package""
        assert not enriched.vcs_git
        assert not enriched.vcs_browser
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"def test_linux(linux):
    m = mock_open(read_data=linux)

    with patch(""builtins.open"", m):
        result = parse_sources_file(""dummy"")

    assert ""linux-headers-6.1.0-32-amd64"" in result[""linux""]
    assert ""linux-headers-6.1.0-32-cloud-amd64"" in result[""linux""]",package_managers/debian/scripts/test_investigate_sources.py,
survived,"def start_task():
    """"""Start a new Claude Code automation task""""""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
            
        prompt = data.get('prompt')
        repo_url = data.get('repo_url')
        branch = data.get('branch', 'main')
        github_token = data.get('github_token')
        model = data.get('model', 'claude')  # Default to claude for backward compatibility
        
        if not all([prompt, repo_url, github_token]):
            return jsonify({'error': 'prompt, repo_url, and github_token are required'}), 400
        
        # Validate model selection
        if model not in ['claude', 'codex']:
            return jsonify({'error': 'model must be either ""claude"" or ""codex""'}), 400
        
        # Generate unique task ID
        task_id = str(uuid.uuid4())
        
        # Initialize task
        tasks[task_id] = {
            'id': task_id,
            'status': TaskStatus.PENDING,
            'prompt': prompt,
            'repo_url': repo_url,
            'branch': branch,
            'github_token': github_token,
            'model': model,
            'container_id': None,
            'commit_hash': None,
            'git_diff': None,
            'error': None,
            'created_at': time.time()
        }
        
        # Save tasks after creating
        save_tasks()
        
        # Start task in background thread
        thread = threading.Thread(target=run_ai_code_task, args=(task_id,))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'status': 'success',
            'task_id': task_id,
            'message': 'Task started successfully'
        })
        
    except Exception as e:
        logger.error(f""Error starting task: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,
survived,"def create_project():
    """"""Create a new project""""""
    try:
        data = request.get_json()
        user_id = request.headers.get('X-User-ID')
        
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        # Required fields
        name = data.get('name')
        repo_url = data.get('repo_url')
        
        if not all([name, repo_url]):
            return jsonify({'error': 'name and repo_url are required'}), 400
        
        # Parse GitHub URL
        try:
            repo_owner, repo_name = parse_github_url(repo_url)
        except ValueError as e:
            return jsonify({'error': str(e)}), 400
        
        # Optional fields
        description = data.get('description', '')
        settings = data.get('settings', {})
        
        project = DatabaseOperations.create_project(
            user_id=user_id,
            name=name,
            description=description,
            repo_url=repo_url,
            repo_name=repo_name,
            repo_owner=repo_owner,
            settings=settings
        )
        
        return jsonify({
            'status': 'success',
            'project': project
        })
        
    except Exception as e:
        logger.error(f""Error creating project: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/projects.py,
survived,"    def update_project(project_id: int, user_id: str, updates: Dict) -> Optional[Dict]:
        """"""Update a project""""""
        try:
            updates['updated_at'] = datetime.utcnow().isoformat()
            result = supabase.table('projects').update(updates).eq('id', project_id).eq('user_id', user_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error updating project {project_id}: {e}"")
            raise
",server/database.py,DatabaseOperations
survived,"def combine_files(file_list: List[str]) -> Dict:
    """"""
    Combine multiple metadata files into a single merged dictionary.
    
    Args:
        file_list: List of file paths to combine
        
    Returns:
        Merged metadata dictionary
    """"""
    metadata_list = []
    for filepath in file_list:
        metadata = load_metadata_file(filepath)
        if metadata:  # Only add non-empty metadata
            metadata_list.append(metadata)
    
    return merge_metadata(metadata_list)
",combine_metadata.py,
survived,"    def test_mcp_custom_host_port(self, runner, temp_python_script):
        """"""Test MCP mode with custom host and port.""""""
        with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
            mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--mcp-transport"", ""sse"",
                ""--host"", ""0.0.0.0"",
                ""--port"", ""9000"",
                ""--verbose""
            ])
            
            # Verify custom host and port were passed
            mock_run_mcp.assert_called_once()
            run_args = mock_run_mcp.call_args
            assert run_args[1][""host""] == ""0.0.0.0""
            assert run_args[1][""port""] == 9000
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_flow_input_validation_error(self):
        """"""Test FlowInput validation with invalid data.""""""
        # Missing required field should raise validation error
        with pytest.raises(ValidationError):
            FlowInput()  # Missing input_value
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerErrorHandling
deleted,"            def decorator(func):
                registered_resources.append((uri, func))
                return func
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
survived,"    def test_run_mcp_server_sse(self, mock_fastmcp):
        """"""Test running MCP server with SSE transport.""""""
        mock_mcp_instance = MagicMock()
        mock_mcp_instance.run = MagicMock()

        run_mcp_server(
            mcp_server=mock_mcp_instance,
            transport=""sse"",
            host=""0.0.0.0"",
            port=8080
        )

        # Should call run() with transport, host, and port
        mock_mcp_instance.run.assert_called_once_with(
            transport=""sse"",
            host=""0.0.0.0"",
            port=8080
        )
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerRuntime
deleted,"    def list_flows() -> str:
        """"""List all available flows with their metadata.""""""
        flows_info = []
        for flow_id, graph in graphs.items():
            meta = metas.get(flow_id, {})
            flow_info = FlowInfo(
                id=flow_id,
                title=getattr(meta, 'title', flow_id),
                description=getattr(meta, 'description', None),
                inputs=None,  # Could be expanded to include input schema
                outputs=None  # Could be expanded to include output schema
            )
            flows_info.append(flow_info.model_dump())
        
        return json.dumps(flows_info, indent=2)
",src/backend/base/langflow/cli/mcp_server.py,
survived,"    def from_file(cls, file_path: Path) -> ""Registry"":
        """"""Load registry from a JSON file.

        Args:
            file_path: Path to the registry JSON file

        Returns:
            Registry: The loaded registry

        Raises:
            FileNotFoundError: If the file doesn't exist
            json.JSONDecodeError: If the file contains invalid JSON
            ValidationError: If the data doesn't match the expected schema
        """"""
        with open(file_path, ""r"") as f:
            data = json.load(f)
        return cls.from_json_list(data)
",terminal_bench/registry/client.py,Registry
survived,"def cost_per_token(model: str, usage: Usage) -> Tuple[float, float]:
    """"""
    Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Follows the same logic as other provider cost calculations.
    """"""
    return generic_cost_per_token(
        model=model, usage=usage, custom_llm_provider=""moonshot""
    )",litellm/llms/moonshot/cost_calculator.py,
survived,"    def test_csharp_local_functions_and_nested(self):
        patch = """"""
@@ -152,10 +152,6 @@ void OuterMethod()
{
    void InnerFunction()
    {
        // local function inside method
    }
}

@@ -165,15 +165,15 @@ static int Calculate(int x)

@@ -175,18 +175,18 @@ async Task<string> ProcessDataAsync()

@@ -185,20 +185,20 @@ T GenericLocalFunction<T>(T input)

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""OuterMethod"",
            ""InnerFunction"",
            ""Calculate"",
            ""ProcessDataAsync"",
            ""GenericLocalFunction"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase
survived,"    def test_csharp_generics_and_complex_types(self):
        patch = """"""
@@ -152,10 +152,6 @@ public List<T> GetItems<T>()

@@ -152,10 +152,6 @@ public Dictionary<string, int> GetDictionary()

@@ -152,10 +152,6 @@ public async Task<List<string>> GetStringsAsync()

@@ -152,10 +152,6 @@ public T[] GetArray<T>(int size)

@@ -152,10 +152,6 @@ public void ProcessItems(List<Dictionary<string, object>> items)

@@ -152,10 +152,6 @@ public Func<int, bool> GetPredicate()

@@ -152,10 +152,6 @@ public Action<string> GetAction()

@@ -152,10 +152,6 @@ public int? GetNullableInt()

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""GetItems"",
            ""GetDictionary"",
            ""GetStringsAsync"",
            ""GetArray"",
            ""ProcessItems"",
            ""GetPredicate"",
            ""GetAction"",
            ""GetNullableInt"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase
survived,"    def test_csharp_real_world_example(self):
        # Based on a typical C# class with various method types
        patch = """"""
@@ -73,9 +73,7 @@ public UserService(IUserRepository repository)

@@ -87,7 +87,8 @@ public async Task<User> GetUserAsync(int id)

@@ -95,6 +95,7 @@ public bool ValidateUser(User user)

@@ -103,4 +107,23 @@ private void LogUserAction(string action)

@@ -115,6 +118,13 @@ public static UserService CreateDefault()

@@ -125,7 +125,7 @@ protected virtual void OnUserChanged(UserEventArgs e)

@@ -135,8 +135,8 @@ public void Dispose()

@@ -145,10 +145,10 @@ ~UserService()

@@ -155,12 +155,12 @@ public string Name
{
    get { return _name; }
    set { _name = value; }
}

@@ -168,15 +168,15 @@ public int Count => _users.Count;

@@ -180,18 +180,18 @@ public User this[int index] => _users[index];

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""UserService"",
            ""GetUserAsync"",
            ""ValidateUser"",
            ""LogUserAction"",
            ""CreateDefault"",
            ""OnUserChanged"",
            ""Dispose"",
            ""get"",
            ""set"",
            ""Count"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase
survived,"async def train(model: art.TrainableModel[TauBenchPolicyConfig]):
    """"""Main training loop adapted from art-e example""""""
    config = model.config.run_config
    training_config = model.config.training_config
    
    if training_config is None:
        raise ValueError(""Training config is not set"")
    
    with LocalBackend() as backend:
        # Setup model with backend
        await model.register(backend)
        
        print(""Loading training tasks..."")
        # Get environment to access tasks
        env = get_env(
            config.env,
            user_strategy=config.user_strategy,
            user_model=config.user_model,
            user_provider=config.user_model_provider,
            task_split=config.task_split,
        )
        
        # Create list of task indices for training
        end_index = min(config.end_index, len(env.tasks)) if config.end_index != -1 else len(env.tasks)
        if config.task_ids:
            train_task_indices = config.task_ids
        else:
            train_task_indices = list(range(config.start_index, min(end_index, training_config.training_dataset_size)))
        
        # Validation task indices
        val_task_indices = list(range(len(train_task_indices), len(train_task_indices) + training_config.val_set_size))
        
        print(f""Training on {len(train_task_indices)} tasks"")
        print(f""Validation on {len(val_task_indices)} tasks"")
        
        # Training iterator
        train_iterator = iterate_dataset(
            train_task_indices,
            groups_per_step=training_config.groups_per_step,
            num_epochs=training_config.num_epochs,
            initial_step=await model.get_step(),
        )
        
        for batch, epoch, global_step, epoch_step in train_iterator:
            print(f""\n--- Training Step {global_step} (Epoch {epoch}, Step {epoch_step}) ---"")
            
            # Evaluation
            if global_step % training_config.eval_steps == 0:
                print(f""\n--- Evaluating at Step {global_step} ---"")
                await evaluate_model(model, config, num_eval_tasks=min(50, len(val_task_indices)))
                await model.delete_checkpoints()
            
            # Generate trajectory groups
            print(f""Generating trajectories for {len(batch)} tasks..."")
            groups = await art.gather_trajectory_groups(
                (
                    art.TrajectoryGroup(
                        (
                            rollout_tau_bench_task(model, task_index)
                            for _ in range(training_config.trajectories_per_group)
                        )
                    )
                    for task_index in batch
                )
            )
            
            # Training step
            print(f""Training on {len(groups)} trajectory groups..."")
            await model.train(
                groups,
                config=art.TrainConfig(
                    learning_rate=training_config.learning_rate
                ),
            )
            
            # Log progress
            total_reward = sum(
                sum(traj.reward for traj in group.trajectories) 
                for group in groups
            )
            num_trajectories = sum(len(group.trajectories) for group in groups)
            avg_reward = total_reward / num_trajectories if num_trajectories > 0 else 0
            print(f""Step {global_step}: Average training reward = {avg_reward}"")
        
        # Final evaluation
        print(""\n--- Final Evaluation ---"")
        final_reward = await evaluate_model(model, config, num_eval_tasks=len(val_task_indices))
        print(f""Final average reward: {final_reward}"")
        
        print(""Training completed!"")
",dev/tau-bench/run_rl.py,
survived,"async def evaluate_model(
    model: art.TrainableModel[TauBenchPolicyConfig],
    config: RunConfig,
    num_eval_tasks: int = 50
) -> float:
    """"""Evaluate the model on a subset of tasks""""""
    print(f""Evaluating model on {num_eval_tasks} tasks..."")
    
    # Get environment for evaluation
    env = get_env(
        config.env,
        user_strategy=config.user_strategy,
        user_model=config.user_model,
        user_provider=config.user_model_provider,
        task_split=""dev"",  # Use dev split for evaluation
    )
    
    total_reward = 0.0
    eval_tasks = min(num_eval_tasks, len(env.tasks))
    
    for i in range(eval_tasks):
        try:
            traj = await rollout_tau_bench_task(model, i)
            total_reward += traj.reward
            print(f""Eval task {i}: reward={traj.reward}"")
        except Exception as e:
            print(f""Error evaluating task {i}: {e}"")
    
    avg_reward = total_reward / eval_tasks
    print(f""Average evaluation reward: {avg_reward}"")
    return avg_reward
",dev/tau-bench/run_rl.py,
survived,"    def test_env_response_method(self, mock_singleturn_env):
        """"""Test the env_response method (which should never be called in practice).""""""
        messages = [{""role"": ""user"", ""content"": ""Hello""}]
        state = {}
        
        response, new_state = mock_singleturn_env.env_response(messages, state)
        
        # Should return minimal response
        assert response[""role""] == ""user""
        assert response[""content""] == """"
        assert new_state == state
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"    def test_env_group_rubric_type(self, mock_openai_client):
        """"""Test that EnvGroup creates EnvGroupRubric.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1])
        
        assert isinstance(env_group.rubric, EnvGroupRubric)
        assert env_group.rubric.env_map[""env_0""] == env1
",tests/test_env_group.py,TestEnvGroup
survived,"        def scalar_func(completion, **kwargs):
            return 0.5
",tests/test_rubric.py,TestRubric
survived,"    async def create_project(
        self,
        info: Info[Context, None],
        input: CreateProjectInput,
    ) -> ProjectMutationPayload:
        name = input.name
        description = input.description if input.description is not UNSET else None
        gradient_start_color = (
            input.gradient_start_color if input.gradient_start_color is not UNSET else None
        )
        gradient_end_color = (
            input.gradient_end_color if input.gradient_end_color is not UNSET else None
        )

        # Build the values dict, only including non-None values to use database defaults
        values = {""name"": name}
        if description is not None:
            values[""description""] = description
        if gradient_start_color is not None:
            values[""gradient_start_color""] = gradient_start_color
        if gradient_end_color is not None:
            values[""gradient_end_color""] = gradient_end_color

        async with info.context.db() as session:
            project = await session.scalar(
                insert(models.Project)
                .values(**values)
                .returning(models.Project)
            )
            assert project is not None
        
        info.context.event_queue.put(ProjectInsertEvent((project.id,)))
        return ProjectMutationPayload(project=to_gql_project(project), query=Query())
",src/phoenix/server/api/mutations/project_mutations.py,ProjectMutationMixin
survived,"    async def chat_completion_create(
        self,
        messages: list[
            tuple[ChatCompletionMessageRole, str, Optional[str], Optional[list[JSONScalarType]]]
        ],
        tools: list[JSONScalarType],
        **invocation_parameters: Any,
    ) -> AsyncIterator[ChatCompletionChunk]:
        # Transform max_tokens to the correct parameter name for Azure OpenAI
        transformed_parameters = invocation_parameters.copy()
        if ""max_tokens"" in transformed_parameters:
            correct_param_name = _get_azure_token_param_name(self.model_name)
            if correct_param_name != ""max_tokens"":
                transformed_parameters[correct_param_name] = transformed_parameters.pop(""max_tokens"")
        
        # Call the parent method with transformed parameters
        async for chunk in super().chat_completion_create(messages, tools, **transformed_parameters):
            yield chunk
",src/phoenix/server/api/helpers/playground_clients.py,AzureOpenAIStreamingClient
survived,"    def path(self):
        return reverse(""sentry-mcp-json"")
",tests/sentry/web/test_api.py,McpJsonTest
survived,"def mcp_json(request):
    if settings.SENTRY_MODE == SentryMode.SELF_HOSTED:
        return HttpResponse(status=404)

    return HttpResponse(json.dumps(MCP_CONFIG), content_type=""application/json"")
",src/sentry/web/api.py,
deleted,"    def _handle_gleaning(
        self, 
        response: Any, 
        output_schema: Dict[str, Any],
        output_mode: OutputMode,
        gleaning_config: Dict[str, Any],
        model: str,
        op_type: str,
        messages: List[Dict[str, str]],
        tools: Optional[str],
        scratchpad: Optional[str],
        litellm_completion_kwargs: Dict[str, Any],
        op_config: Dict[str, Any],
        verbose: bool
    ) -> tuple[Any, float, bool]:
        """"""Handle gleaning process.""""""
        additional_cost = 0.0
        num_gleaning_rounds = gleaning_config.get(""num_rounds"", 2)
        
        parsed_output = (
            self.parser.parse_response(response, output_schema, output_mode, json.loads(tools) if tools else None)[0]
            if isinstance(response, ModelResponse)
            else response
        )

        validator_messages = (
            [
                {
                    ""role"": ""system"",
                    ""content"": f""You are a helpful assistant, intelligently processing data. This is a {op_type} operation."",
                }
            ]
            + messages
            + [{""role"": ""assistant"", ""content"": json.dumps(parsed_output)}]
        )

        for rnd in range(num_gleaning_rounds):
            # Break early if gleaning condition is not met
            if not self.should_glean(gleaning_config, parsed_output):
                break
                
            # Prepare validator prompt
            validator_prompt = strict_render(
                gleaning_config[""validation_prompt""],
                {""output"": parsed_output},
            )
            
            self.runner.blocking_acquire(""llm_call"", weight=1)
            approx_num_tokens = approx_count_tokens(
                validator_messages + [{""role"": ""user"", ""content"": validator_prompt}]
            )
            self.runner.blocking_acquire(""llm_tokens"", weight=approx_num_tokens)

            # Build validator tool
            should_refine_params = {
                ""type"": ""object"",
                ""properties"": {
                    ""should_refine"": {""type"": ""boolean""},
                    ""improvements"": {""type"": ""string""},
                },
                ""required"": [""should_refine"", ""improvements""],
            }
            if ""gemini"" not in model:
                should_refine_params[""additionalProperties""] = False

            # Prepare extra kwargs
            extra_kwargs = {}
            if self.llm_handler.default_lm_api_base:
                extra_kwargs[""api_base""] = self.llm_handler.default_lm_api_base
            if is_snowflake(model):
                extra_kwargs[""allowed_openai_params""] = [""tools"", ""tool_choice""]

            validator_response = completion(
                model=gleaning_config.get(""model"", model),
                messages=truncate_messages(
                    validator_messages + [{""role"": ""user"", ""content"": validator_prompt}],
                    model,
                ),
                tools=[
                    {
                        ""type"": ""function"",
                        ""function"": {
                            ""name"": ""should_refine_answer"",
                            ""description"": ""Determine if the output should be refined based on the validation feedback"",
                            ""strict"": True,
                            ""parameters"": should_refine_params,
                            ""additionalProperties"": False,
                        },
                    }
                ],
                tool_choice=""required"",
                **litellm_completion_kwargs,
                **extra_kwargs,
            )
            additional_cost += completion_cost(validator_response)

            # Parse the validator response
            suggestion = json.loads(
                validator_response.choices[0].message.tool_calls[0].function.arguments
            )
            if not suggestion[""should_refine""]:
                break

            if verbose:
                self.console.log(
                    f""Validator improvements (gleaning round {rnd + 1}): {suggestion['improvements']}""
                )

            # Prompt for improvement
            improvement_prompt = f""""""Based on the validation feedback:

            ```
            {suggestion['improvements']}
            ```

            Please improve your previous response. Ensure that the output adheres to the required schema and addresses any issues raised in the validation.""""""
            messages.append({""role"": ""user"", ""content"": improvement_prompt})

            # Call LLM again
            response = self.llm_handler.make_completion_call(
                model, op_type, messages, output_mode, output_schema, tools, scratchpad, litellm_completion_kwargs, op_config
            )
            parsed_output = self.parser.parse_response(response, output_schema, output_mode, json.loads(tools) if tools else None)[0]
            validator_messages[-1] = {
                ""role"": ""assistant"",
                ""content"": json.dumps(parsed_output),
            }

            additional_cost += completion_cost(response)

        return response, additional_cost, True
",docetl/operations/utils/api.py,ValidationHandler
survived,"    async def test_rollout_with_sampling_args(self, mock_singleturn_env):
        """"""Test rollout with custom sampling arguments.""""""
        prompt = [{""role"": ""user"", ""content"": ""Hello""}]
        answer = ""Hi""
        sampling_args = {""temperature"": 0.8, ""max_tokens"": 100}
        
        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=answer,
            sampling_args=sampling_args
        )
        
        assert isinstance(completion, list)
        assert completion[0][""content""] == ""This is a test response""
        
        # Verify sampling args were passed
        call_args = mock_singleturn_env.client.chat.completions.create.call_args
        assert ""temperature"" in call_args.kwargs
        assert ""max_tokens"" in call_args.kwargs
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"    async def test_different_message_types_in_same_env(self, mock_openai_client, sample_dataset):
        """"""Test that environment respects its message_type setting.""""""
        # Chat environment
        chat_env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            message_type=""chat""
        )
        
        # Completion environment 
        completion_dataset = Dataset.from_dict({
            ""prompt"": [""Test prompt""],
            ""answer"": [""Test answer""]
        })
        completion_env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"", 
            dataset=completion_dataset,
            message_type=""completion""
        )
        
        # Test chat rollout
        chat_completion, _ = await chat_env.rollout(
            client=mock_openai_client,
            model=""test-model"",
            prompt=[{""role"": ""user"", ""content"": ""Hello""}],
            answer=""Hi""
        )
        assert isinstance(chat_completion, list)
        
        # Test completion rollout
        completion_result, _ = await completion_env.rollout(
            client=mock_openai_client,
            model=""test-model"", 
            prompt=""Complete this:"",
            answer=""Done""
        )
        assert isinstance(completion_result, str)",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"    def test_rubric_group_mixed_rubric_types(self):
        """"""Test RubricGroup with different types of rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        # Create rubrics with different configurations
        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.3], custom_attr=""test"")
        
        group = RubricGroup(rubrics=[rubric1, rubric2])
        
        # Should aggregate functions and weights correctly
        assert group.get_reward_func_names() == [""func1"", ""func2""]
        assert group.get_reward_weights() == [1.0, 0.3]
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_rubric_group_score_rollouts_with_kwargs(self):
        """"""Test scoring rollouts with additional kwargs.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def _create_mock_response(self, content):
        """"""Helper to create mock OpenAI response.""""""
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = content
        mock_response.choices[0].finish_reason = ""stop""
        return mock_response
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"def basic_parser():
    """"""Return a basic Parser instance.""""""
    return Parser()
",tests/conftest.py,
survived,"    def test_parse_with_custom_extractor_no_boxed(self, think_parser_with_extractor):
        """"""Test custom extractor when no boxed answer is found.""""""
        text = """"""<think>
        Thinking about the problem.
        </think>
        Just a plain answer.""""""
        
        result = think_parser_with_extractor.parse(text)
        assert result == ""Just a plain answer.""
",tests/test_think_parser.py,TestThinkParser
survived,"    def test_parse_answer_string_integration(self, think_parser):
        """"""Test parse_answer with string input.""""""
        text = ""<think>Calculating...</think>Result: 42""
        result = think_parser.parse_answer(text)
        assert result == ""Result: 42""",tests/test_think_parser.py,TestThinkParser
survived,"    def test_rubric_group_score_rollouts_basic(self):
        """"""Test basic scoring of rollouts with multiple rubrics.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        # It calls rubric.score_rollouts() which is async but doesn't await it
        pass
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def extract_boxed(text):
        """"""Simple boxed answer extractor for testing.""""""
        import re
        match = re.search(r'\\boxed\{([^}]+)\}', text)
        return match.group(1) if match else text
",tests/conftest.py,
survived,"    def test_add_multiple_reward_funcs(self):
        """"""Test adding multiple reward functions.""""""
        # Create fresh rubric to avoid test isolation issues
        rubric = Rubric(funcs=[], weights=[])
        
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        rubric.add_reward_func(func1, weight=1.0)
        rubric.add_reward_func(func2, weight=0.3)
        
        assert len(rubric.reward_funcs) == 2
        assert rubric.get_reward_func_names() == [""func1"", ""func2""]
        assert rubric.reward_weights == [1.0, 0.3]
",tests/test_rubric.py,TestRubric
survived,"    def test_rubric_initialization_with_kwargs(self):
        """"""Test Rubric initialization with additional kwargs.""""""
        rubric = Rubric(custom_param=""test_value"", another_param=42)
        
        assert rubric.custom_param == ""test_value""
        assert rubric.another_param == 42
",tests/test_rubric.py,TestRubric
survived,"    def test_add_reward_func_default_weight(self):
        """"""Test adding reward function with default weight.""""""
        rubric = Rubric(funcs=[], weights=[])
        
        def test_func(completion, **kwargs):
            return 1.0
        
        rubric.add_reward_func(test_func)
        
        assert rubric.reward_weights == [1.0]
",tests/test_rubric.py,TestRubric
survived,"    def test_parse_returns_text_as_is(self, basic_parser):
        """"""Test that parse method returns text unchanged.""""""
        text = ""This is a test string""
        result = basic_parser.parse(text)
        assert result == text
",tests/test_parser.py,TestParser
survived,"    async def test_non_list_prompt_assertion(self, mock_multiturn_env):
        """"""Test that non-list prompts raise AssertionError.""""""
        with pytest.raises(AssertionError):
            await mock_multiturn_env.rollout(
                client=mock_multiturn_env.client,
                model=""test-model"",
                prompt=""String prompt not allowed"",  # Should be list
                answer=""test_answer""
            )
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"            def is_completed(self, messages, state, **kwargs):
                # Complete if we have any assistant message
                return any(msg.get(""role"") == ""assistant"" for msg in messages)
",tests/test_multiturn_env.py,TestMultiTurnEnv.ImmediateCompletionEnv
survived,"    def __init__(self, completion_condition=""answer"", **kwargs):
        super().__init__(**kwargs)
        self.completion_condition = completion_condition  # ""answer"", ""max_turns"", ""error""
        self.env_response_count = 0
",tests/conftest.py,SimpleMultiTurnEnv
survived,"        def reward_func1(completion, **kwargs):
            return 1.0
",tests/test_rubric.py,TestRubric
survived,"    async def test_max_turns_limiting(self, mock_multiturn_env_max_turns):
        """"""Test that rollout stops at max_turns.""""""
        # Set up responses that would continue indefinitely
        mock_multiturn_env_max_turns.client.set_default_responses(
            chat_response=""Keep going""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Start conversation""}]
        completion, state = await mock_multiturn_env_max_turns.rollout(
            client=mock_multiturn_env_max_turns.client,
            model=""test-model"", 
            prompt=prompt,
            answer=""target_answer""
        )
        
        # Should stop at max_turns=2: assistant + user + assistant (3 messages)
        assert len(completion) == 3
        assert completion[0][""role""] == ""assistant""
        assert completion[1][""role""] == ""user""
        assert completion[2][""role""] == ""assistant""
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def test_parse_with_multiple_think_blocks(self, think_parser):
        """"""Test parsing with multiple think blocks (should use content after last one).""""""
        text = """"""<think>First thought</think>
        Some intermediate text.
        <think>Second thought</think>
        Final answer here.""""""
        
        result = think_parser.parse(text)
        assert result == ""Final answer here.""
",tests/test_think_parser.py,TestThinkParser
survived,"    def test_parse_answer_with_completion(self, basic_parser):
        """"""Test parse_answer with completion list.""""""
        completion = [
            {""role"": ""user"", ""content"": ""What is 2+2?""},
            {""role"": ""assistant"", ""content"": ""The answer is 4""}
        ]
        result = basic_parser.parse_answer(completion)
        assert result == ""The answer is 4""
",tests/test_parser.py,TestParser
survived,"def test_scenarios():
    """"""
    Collection of test scenarios for URL loading.
    Each scenario represents a different case we want to test.
    """"""
    scenarios = {
        ""new_urls"": {
            ""import_id"": ""github.com/certifi/python-certifi"",
            ""package_id"": UUID(""e0f18184-e743-40fb-8add-a5ccaac026a4""),
            # What transformer found
            ""transformer_urls"": [
                (""github.com/certifi/python-certifi"", [""homepage"", ""source""])
            ],
            # What's in DB (nothing, completely new)
            ""db_state"": {""urls"": {}, ""package_urls"": {}},
            ""expected_behavior"": {
                ""new_urls_created"": 2,  # homepage and source
                ""new_package_urls_created"": 2,
                ""urls_updated"": 0,
            },
        },
        ""existing_package_some_urls"": {
            ""import_id"": ""github.com/pyca/cryptography"",
            ""package_id"": UUID(""f0f18184-e743-40fb-8add-a5ccaac026a5""),
            # Transformer found homepage, source, and repository
            ""transformer_urls"": [
                (""github.com/pyca/cryptography"", [""homepage"", ""source"", ""repository""])
            ],
            # DB only has homepage
            ""db_state"": {
                ""urls"": {
                    (""github.com/pyca/cryptography"", ""homepage""): {
                        ""id"": UUID(""22222222-2222-2222-2222-222222222222""),
                        ""url"": ""github.com/pyca/cryptography"",
                    }
                },
                ""package_urls"": {
                    UUID(""f0f18184-e743-40fb-8add-a5ccaac026a5""): [
                        {
                            ""id"": UUID(""33333333-3333-3333-3333-333333333333""),
                            ""url_id"": UUID(""22222222-2222-2222-2222-222222222222""),
                        }
                    ]
                },
            },
            ""expected_behavior"": {
                ""new_urls_created"": 2,  # source and repository
                ""new_package_urls_created"": 2,
                ""urls_updated"": 1,  # homepage timestamp updated
            },
        },
        ""all_urls_exist"": {
            ""import_id"": ""github.com/requests/requests"",
            ""package_id"": UUID(""a0a18184-e743-40fb-8add-a5ccaac026a6""),
            # Transformer found these
            ""transformer_urls"": [
                (""github.com/requests/requests"", [""homepage"", ""source""])
            ],
            # DB has exact same ones
            ""db_state"": {
                ""urls"": {
                    (""github.com/requests/requests"", ""homepage""): {
                        ""id"": UUID(""44444444-4444-4444-4444-444444444444""),
                        ""url"": ""github.com/requests/requests"",
                    },
                    (""github.com/requests/requests"", ""source""): {
                        ""id"": UUID(""55555555-5555-5555-5555-555555555555""),
                        ""url"": ""github.com/requests/requests"",
                    },
                },
                ""package_urls"": {
                    UUID(""a0a18184-e743-40fb-8add-a5ccaac026a6""): [
                        {
                            ""id"": UUID(""66666666-6666-6666-6666-666666666666""),
                            ""url_id"": UUID(""44444444-4444-4444-4444-444444444444""),
                        },
                        {
                            ""id"": UUID(""77777777-7777-7777-7777-777777777777""),
                            ""url_id"": UUID(""55555555-5555-5555-5555-555555555555""),
                        },
                    ]
                },
            },
            ""expected_behavior"": {
                ""new_urls_created"": 0,
                ""new_package_urls_created"": 0,
                ""urls_updated"": 2,  # Both timestamps updated
            },
        },
        ""no_urls_in_db"": {
            ""import_id"": ""github.com/numpy/numpy"",
            ""package_id"": UUID(""b0b18184-e743-40fb-8add-a5ccaac026a7""),
            # Transformer found URLs but DB has no record of this package
            ""transformer_urls"": [
                (""github.com/numpy/numpy"", [""homepage"", ""repository"", ""documentation""])
            ],
            # DB is empty for this package
            ""db_state"": {""urls"": {}, ""package_urls"": {}},
            ""expected_behavior"": {
                ""new_urls_created"": 3,
                ""new_package_urls_created"": 3,
                ""urls_updated"": 0,
            },
        },
    }

    return scenarios
",tests/package_managers/pkgx/test_pkgx_load_urls.py,
survived,"    def create_formula(
        formula_name,
        dependencies=None,
        build_dependencies=None,
        test_dependencies=None,
        recommended_dependencies=None,
        optional_dependencies=None,
    ):
        return Actual(
            formula=formula_name,
            description=""Test formula"",
            license=""MIT"",
            homepage="""",
            source="""",
            repository="""",
            dependencies=dependencies or [],
            build_dependencies=build_dependencies or [],
            test_dependencies=test_dependencies or [],
            recommended_dependencies=recommended_dependencies or [],
            optional_dependencies=optional_dependencies or [],
        )
",tests/package_managers/homebrew/test_diff_dep.py,
survived,"    def test_handle_uploaders(self):
        """"""Test handling of uploaders with special characters and edge cases.""""""
        maintainer = """"""Package: example
Uploaders: ""Adam C. Powell, IV"" <hazelsct@debian.org>, Drew Parsons <dparsons@debian.org>""""""
        parser = DebianParser(maintainer)
        sources = list(parser.parse())
        assert len(sources) == 1
        source = sources[0]
        assert len(source.uploaders) == 2
        assert source.uploaders[0].name == ""Adam C. Powell, IV""
        assert source.uploaders[0].email == ""hazelsct@debian.org""
        assert source.uploaders[1].name == ""Drew Parsons""
        assert source.uploaders[1].email == ""dparsons@debian.org""

        maintainer = """"""Package: calamares-extensions
Binary: calamares-extensions, calamares-extensions-data
Version: 1.2.1-2
Maintainer: Debian KDE Extras Team <pkg-kde-extras@lists.alioth.debian.org>,""""""
        parser = DebianParser(maintainer)
        sources = list(parser.parse())
        assert len(sources) == 1
        source = sources[0]
        assert source.maintainer.name == ""Debian KDE Extras Team""
        assert source.maintainer.email == ""pkg-kde-extras@lists.alioth.debian.org""",tests/package_managers/debian/test_debian_parser.py,TestDebianParser
survived,"    def test_parse_package_data(self):
        """"""Test parsing a typical package entry from Packages file.""""""
        # Sample package data from a Packages file
        package_data = """"""Package: 0ad
Version: 0.0.26-1
Installed-Size: 19162
Maintainer: Debian Games Team <pkg-games-devel@lists.alioth.debian.org>
Architecture: amd64
Depends: 0ad-data (>= 0.0.26), 0ad-data-common (>= 0.0.26), libc6 (>= 2.29), libcurl4 (>= 7.16.2), libenet7 (>= 1.3.13), libgloox18, libjsoncpp25 (>= 1.9.5), libminiupnpc17 (>= 1.9.20140610), libnspr4 (>= 2:4.9.2), libnss3 (>= 2:3.22)
Recommends: fonts-freefont-ttf, fonts-texgyre
Suggests: 0ad-dbg
Description: Real-time strategy game of ancient warfare
Homepage: https://play0ad.com/
Section: games
Priority: optional
Filename: pool/main/0/0ad/0ad_0.0.26-1_amd64.deb
Size: 6050744
MD5sum: a777ddf01c18dbdef15c589f8325d7a3
SHA256: 9da19833c1a51e890aa8a11f82ec1e383c0e79410c3d2f6845fd2ec3e23249b8


""""""
        # Parse the package data
        parser = DebianParser(package_data)
        packages = list(parser.parse())

        # Validate we have one package
        assert len(packages) == 1
        package = packages[0]

        # Test basic fields
        assert package.package == ""0ad""
        assert package.version == ""0.0.26-1""
        assert package.installed_size == 19162
        assert package.architecture == ""amd64""

        # Test maintainer parsing
        assert package.maintainer.name == ""Debian Games Team""
        assert package.maintainer.email == ""pkg-games-devel@lists.alioth.debian.org""

        # Test dependency parsing
        assert len(package.depends) == 10
        assert package.depends[0].package == ""0ad-data""
        assert package.depends[0].semver == "">= 0.0.26""

        # Test recommends parsing
        assert len(package.recommends) == 2
        assert package.recommends[0].package == ""fonts-freefont-ttf""

        # Test suggests parsing
        assert len(package.suggests) == 1
        assert package.suggests[0].package == ""0ad-dbg""
",tests/package_managers/debian/test_debian_parser.py,TestDebianParser
survived,"def mock_db():
    """"""Fixture providing mock DedupeDB.""""""
    return MagicMock(spec=DedupeDB)
",tests/ranker/test_dedupe.py,
survived,"def map_config_with_drop_keys():
    return {
        ""name"": ""sentiment_analysis_with_drop"",
        ""type"": ""map"",
        ""prompt"": ""Analyze the sentiment of the following text: '{{ input.text }}'. Classify it as either positive, negative, or neutral."",
        ""output"": {""schema"": {""sentiment"": ""string""}},
        ""model"": ""gpt-4o-mini"",
        ""drop_keys"": [""to_be_dropped""],
    }
",tests/basic/test_basic_map.py,
survived,"def map_config_with_tools():
    return {
        ""type"": ""map"",
        ""name"": ""word_count"",
        ""prompt"": ""Count the number of words in the following text: '{{ input.text }}'"",
        ""output"": {""schema"": {""word_count"": ""integer""}},
        ""model"": ""gpt-4o-mini"",
        ""tools"": [
            {
                ""required"": True,
                ""code"": """"""
def count_words(text):
    return {""word_count"": len(text.split())}
                """""",
                ""function"": {
                    ""name"": ""count_words"",
                    ""description"": ""Count the number of words in a text string."",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""text"": {
                                ""type"": ""string"",
                            }
                        },
                        ""required"": [""text""],
                    },
                },
            }
        ],
        ""validate"": [""len(output['text']) > 0""],
        ""num_retries_on_validate_failure"": 3,
    }
",tests/basic/test_basic_map.py,
survived,"    def test_send_html_email_with_both_body_and_html(self, mock_smtp_class, smtp_provider):
        """"""Test that HTML takes precedence when both body and html are provided.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send email with both body and html
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            body=""Plain text content"",
            html=""<p>HTML content</p>"",
        )

        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        
        # Verify HTML content is used (not plain text)
        email_content = call_args[0][2]
        assert ""Content-Type: text/html"" in email_content
        assert ""<p>HTML content</p>"" in email_content
        assert ""Content-Type: text/plain"" not in email_content
        
        # Verify return value contains both
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test Subject"",
            ""body"": ""Plain text content"",
            ""html"": ""<p>HTML content</p>"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def smtp_config(self):
        """"""Create a test SMTP configuration.""""""
        return ProviderConfig(
            description=""Test SMTP Provider"",
            authentication={
                ""smtp_server"": ""smtp.example.com"",
                ""smtp_port"": 587,
                ""encryption"": ""TLS"",
                ""smtp_username"": ""test@example.com"",
                ""smtp_password"": ""testpassword"",
            },
        )
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def test_validate_scopes_failure(self, smtp_provider):
        """"""Test failed scope validation.""""""
        with patch.object(smtp_provider, ""generate_smtp_client"") as mock_generate:
            mock_generate.side_effect = Exception(""Connection failed"")
            
            result = smtp_provider.validate_scopes()
            
            assert result == {""send_email"": ""Connection failed""}",tests/test_smtp_provider.py,TestSmtpProvider
survived,"def workflow_share(
        workflow_share: schemas.WorkflowShare,
        _: schemas.TokenPayload = Depends(get_current_active_user)) -> Any:
    """"""
    分享工作流
    """"""
    if not workflow_share.id or not workflow_share.share_title or not workflow_share.share_user:
        return schemas.Response(success=False, message=""请填写工作流ID、分享标题和分享人"")
    
    state, errmsg = WorkflowHelper().workflow_share(workflow_id=workflow_share.id,
                                                    share_title=workflow_share.share_title or """",
                                                    share_comment=workflow_share.share_comment or """",
                                                    share_user=workflow_share.share_user or """")
    return schemas.Response(success=state, message=errmsg)
",app/api/endpoints/workflow.py,
survived,"def workflow_fork(
        workflow_share: schemas.WorkflowShare,
        current_user: schemas.User = Depends(get_current_active_user)) -> Any:
    """"""
    复用工作流
    """"""
    if not workflow_share.name:
        return schemas.Response(success=False, message=""工作流名称不能为空"")
    
    # 创建工作流
    workflow_dict = {
        ""name"": workflow_share.name,
        ""description"": workflow_share.description,
        ""timer"": workflow_share.timer,
        ""actions"": json.loads(workflow_share.actions or ""[]""),
        ""flows"": json.loads(workflow_share.flows or ""[]""),
        ""context"": json.loads(workflow_share.context or ""{}""),
        ""state"": ""P""  # 默认暂停状态
    }
    
    # 检查名称是否重复
    from app.db.workflow_oper import WorkflowOper
    if WorkflowOper().get_by_name(workflow_dict[""name""]):
        return schemas.Response(success=False, message=""已存在相同名称的工作流"")
    
    # 创建新工作流
    from app.db.models.workflow import Workflow as WorkflowModel
    from app.db import get_db
    db = next(get_db())
    workflow = WorkflowModel(**workflow_dict)
    workflow.create(db)
    
    # 更新复用次数
    if workflow_share.id:
        WorkflowHelper().workflow_fork(share_id=workflow_share.id)
    
    return schemas.Response(success=True, message=""复用成功"")
",app/api/endpoints/workflow.py,
survived,"    def supports_chain(self, chain) -> bool:
        return True
",python/src/plugins/opensea/goat_plugins/opensea/__init__.py,OpenSeaPlugin
survived,"    async def publish_cast(self, parameters: dict):
        url = f""{self.base_url}/cast""
        return await self._make_request(""POST"", url, json={
            ""signer_uuid"": parameters['signer_uuid'],
            ""text"": parameters['text'],
            ""parent"": parameters.get('parent'),
            ""channel_id"": parameters.get('channel_id'),
        })
",python/src/plugins/farcaster/goat_plugins/farcaster/service.py,FarcasterService
survived,"    def test_proxy_environment_variables_set(self):
        """"""Test that proxy configuration sets the correct environment variables""""""
        http_proxy_config = {
            ""proxy_url"": ""http://proxy.test.com:8080"",
            ""proxy_ca_certificate"": ""-----BEGIN CERTIFICATE-----\ntest\n-----END CERTIFICATE-----""
        }
        
        with patch.dict('os.environ', {}, clear=True), \
             patch('source_file.proxy._install_ca_certificate') as mock_install:
            
            mock_install.return_value = Path(""/tmp/test_cert.pem"")
            
            from source_file.proxy import configure_custom_http_proxy
            
            configure_custom_http_proxy(
                http_proxy_config=http_proxy_config,
                logger=logger
            )
            
            assert os.environ.get(""HTTP_PROXY"") == ""http://proxy.test.com:8080""
            assert os.environ.get(""HTTPS_PROXY"") == ""http://proxy.test.com:8080""
            assert ""NO_PROXY"" in os.environ
            mock_install.assert_called_once_with(""-----BEGIN CERTIFICATE-----\ntest\n-----END CERTIFICATE-----"")",airbyte-integrations/connectors/source-file/unit_tests/test_proxy_certificate_support.py,TestProxyCertificateSupport
survived,"    async def get_trending_tokens_24h(self, parameters: dict):
        """"""Get trending tokens in the last 24h from RugCheck""""""
        return await self._make_request(""/stats/trending"")
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService
survived,"    async def get_most_voted_tokens_24h(self, parameters: dict):
        """"""Get tokens with the most votes in the last 24h from RugCheck""""""
        return await self._make_request(""/stats/recent"")
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService
survived,"    def _get_master_connector_version(self, connector: Connector) -> semver.Version:
        """"""Get the version from the master branch.""""""
        metadata = self._get_master_metadata(connector)
        if not metadata:
            return semver.Version.parse(""0.0.0"")
        
        return semver.Version.parse(str(metadata[""data""][""dockerImageTag""]))
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck
deleted,"def connector():
    connector = MagicMock(spec=Connector)
    connector.technical_name = ""source-test""
    connector.metadata = {""dockerImageTag"": ""1.0.0""}
    connector.is_released = False
    return connector
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,
survived,"def list_columns(reasoning: str, csv_path: str) -> List[str]:
    """"""Returns a list of columns in the CSV file.

    The agent uses this to discover available columns and make informed decisions.
    This is typically the first tool called to understand the data structure.

    Args:
        reasoning: Explanation of why we're listing columns relative to user request
        csv_path: Path to the CSV file

    Returns:
        List of column names as strings

    Example:
        columns = list_columns(""Need to find age-related columns"", ""data.csv"")
        # Returns: ['user_id', 'age', 'name', ...]
    """"""
    try:
        df = pl.scan_csv(csv_path).collect()
        columns = df.columns
        console.log(f""[blue]List Columns Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Columns: {columns}[/dim]"")
        return columns
    except Exception as e:
        console.log(f""[red]Error listing columns: {str(e)}[/red]"")
        return []
",sfa_polars_csv_agent_openai_v2.py,
survived,"    def _tools_into(self, tools: List[common.Tool] | None) -> List[Dict[str, Any]] | None:
        if not tools:
            return None
        
        ollama_tools = []
        for tool in tools:
            ollama_tools.append({
                ""type"": ""function"",
                ""function"": {
                    ""name"": tool.get(""name"", """"),
                    ""description"": tool.get(""description"", """"),
                    ""parameters"": tool.get(""input_schema"", {})
                }
            })
        return ollama_tools
",agent/llm/ollama_client.py,OllamaLLM
survived,"def draw_axis(img, yaw, pitch, roll, tdx=None, tdy=None, size=100):
    pitch = pitch * np.pi / 180
    yaw = -(yaw * np.pi / 180)
    roll = roll * np.pi / 180

    if tdx != None and tdy != None:
        tdx = tdx
        tdy = tdy
    else:
        height, width = img.shape[:2]
        tdx = width / 2
        tdy = height / 2

    x1 = size * (cos(yaw) * cos(roll)) + tdx
    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy

    x2 = size * (-cos(yaw) * sin(roll)) + tdx
    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy

    x3 = size * (sin(yaw)) + tdx
    y3 = size * (-cos(yaw) * sin(pitch)) + tdy

    cv2.line(img, (int(tdx), int(tdy)), (int(x1), int(y1)), (0, 0, 255), 4)
    cv2.line(img, (int(tdx), int(tdy)), (int(x2), int(y2)), (0, 255, 0), 4)
    cv2.line(img, (int(tdx), int(tdy)), (int(x3), int(y3)), (255, 0, 0), 4)

    return img
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,
survived,"    def __call__(self, images):
        output = batch_detect(self.model, [images])[0]
        return output
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,RetinaFaceOnnx
survived,"    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)
",face_recognition/6d_repnet_360/convert_to_onnx.py,SixDRepNet360
survived,"def test_cohere_integration():
    """"""Integration test demonstrating all four Cohere call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Get API key with error handling
    api_key = os.getenv(""COHERE_API_KEY"")
    if not api_key:
        raise ValueError(""COHERE_API_KEY environment variable is required"")
    print(""\nInitializing test with Cohere API key..."")

    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()
    print(f""Started new session with ID: {session.session_id}"")

    # Initialize client and provider with error handling
    try:
        co = cohere.Client(api_key=api_key)
        aco = cohere.AsyncClient(api_key=api_key)
        from agentops.llms.providers.cohere import CohereProvider
        provider = CohereProvider(co)
        provider.client = session  # Pass session to provider before override
        provider.override()  # This will handle both sync and async clients
        
        # Set up async client with the same session
        aco.session = session
        # Ensure the async client's provider also has the session
        aco.provider = provider
        print(""Successfully initialized Cohere clients and provider"")
        print(f""Provider session ID: {provider.client.session_id}"")
    except Exception as e:
        print(f""Error initializing Cohere clients: {str(e)}"")
        raise

    def sync_no_stream():
        try:
            print(""\nExecuting sync_no_stream..."")
            response = co.chat(message=""Hello from sync no stream"", model=""command"", session=session)
            print(f""sync_no_stream completed successfully with response: {response.text}"")
        except Exception as e:
            print(f""Error in sync_no_stream: {str(e)}"")
            raise

    def sync_stream():
        try:
            print(""\nExecuting sync_stream..."")
            stream = co.chat_stream(message=""Hello from sync streaming"", model=""command"", session=session)
            completion = """"
            for chunk in stream:
                if hasattr(chunk, 'text'):
                    completion += chunk.text
                print(f""Received sync chunk: {chunk}"")
            print(f""sync_stream completed successfully with completion: {completion}"")
        except Exception as e:
            print(f""Error in sync_stream: {str(e)}"")
            raise

    async def async_no_stream():
        try:
            print(""\nExecuting async_no_stream..."")
            async with asyncio.timeout(30):
                response = await aco.chat(message=""Hello from async no stream"", model=""command"", session=session)
                print(f""async_no_stream completed successfully with response: {response.text}"")
        except asyncio.TimeoutError:
            print(""Warning: async_no_stream timed out"")
            raise
        except Exception as e: 
            print(f""Error in async_no_stream: {str(e)}"")
            raise

    async def async_stream(provider, session):
        try:
            print(""\nStarting async_stream call..."")
            async with asyncio.timeout(30):  # Add timeout to prevent hanging
                # Ensure provider has the current session
                provider.client = session
                # Create a new stream with the provider to ensure proper event tracking
                stream = await aco.chat_stream(
                    message=""Hello from async streaming"",
                    model=""command"",
                    session=session
                )
                print(""Stream created, starting iteration..."")
                async for chunk in stream:
                    print(f""Received async chunk: {chunk}"")
                print(""Stream completed successfully"")
        except asyncio.TimeoutError:
            print(""Warning: Async stream timed out"")
            raise
        except Exception as e:
            print(f""Error in async_stream: {str(e)}"")
            raise

    async def run_async_tests():
        print(""\nRunning async tests..."")
        print(""Starting async_no_stream..."")
        await async_no_stream()
        print(""Completed async_no_stream"")
        
        print(""\nStarting first async_stream..."")
        await async_stream(provider, session)
        print(""Completed first async_stream"")
        
        print(""\nStarting second async_stream..."")
        await async_stream(provider, session)  # Run twice to ensure we get all LLM calls
        print(""Completed second async_stream"")
        
        print(""\nStarting third async_stream..."")
        await async_stream(provider, session)  # Run thrice to ensure we get all LLM calls
        print(""Completed third async_stream"")
        
        print(""\nAll async tests completed successfully"")
        
        # End session and verify analytics after all tests
        session.end_session(""Success"")
        analytics = session.get_analytics()
        print(f""\nAnalytics: {analytics}"")
        assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during Cohere test: {str(e)}"")
        raise

    print(""\nTest completed successfully"")
",tests/core_manual_tests/providers/cohere_canary.py,
survived,"    async def run_async_tests():
        await async_no_stream()
        await async_stream()
",tests/core_manual_tests/providers/litellm_canary.py,
survived,"    async def run_async_tests():
        await async_no_stream()
        await async_stream()
",tests/core_manual_tests/providers/anthropic_canary.py,
survived,"    async def async_no_stream():
        # Mistral doesn't have async methods, use sync
        client.chat(
            model=""mistral-tiny"",
            messages=[ChatMessage(role=""user"", content=""Hello from async no stream"")]
        )
",tests/core_manual_tests/providers/mistral_canary.py,
survived,"def determine_if_file_is_relevant(prompt: str, file_path: str, client: Anthropic) -> Dict[str, Any]:
    """"""Determines if a single file is relevant to the prompt.
    
    Args:
        prompt: The user prompt
        file_path: Path to the file to check
        client: Anthropic client
        
    Returns:
        Dictionary with reasoning and is_relevant flag
    """"""
    result = {
        ""reasoning"": ""Error: Could not process file"",
        ""file_path"": file_path,
        ""is_relevant"": False
    }
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            file_content = f.read()
        
        # Truncate file content if it's too long
        if len(file_content) > 10000:
            file_content = file_content[:10000] + ""... [content truncated]""
        
        file_prompt = f""""""
        <purpose>
        You are a codebase context builder. Your task is to determine if a file is relevant to a user query.
        </purpose>
        
        <instructions>
        <instruction>Analyze the file content and determine if it's relevant to the user query.</instruction>
        <instruction>Provide clear reasoning for your decision.</instruction>
        <instruction>Return a structured output with your reasoning and a boolean indicating relevance.</instruction>
        </instructions>
        
        <user-query>
        {prompt}
        </user-query>
        
        <file-path>
        {file_path}
        </file-path>
        
        <file-content>
        {file_content}
        </file-content>
        """"""
        
        for attempt in range(MAX_RETRIES):
            try:
                response = client.messages.create(
                    model=""claude-3-7-sonnet-20250219"",
                    max_tokens=3000,  # Increased to be greater than thinking.budget_tokens
                    thinking={
                        ""type"": ""enabled"",
                        ""budget_tokens"": THINKING_BUDGET_TOKENS_PER_FILE
                    },
                    messages=[{""role"": ""user"", ""content"": file_prompt}],
                    system=""Determine if the file is relevant to the user query. Return a JSON object with 'reasoning' and 'is_relevant' fields.""
                )
                
                # Parse the response
                response_text = response.content[0].text
                result = json.loads(response_text)
                
                return {
                    ""reasoning"": result.get(""reasoning"", ""No reasoning provided""),
                    ""file_path"": file_path,
                    ""is_relevant"": result.get(""is_relevant"", False)
                }
            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    console.log(f""[yellow]Retry {attempt + 1}/{MAX_RETRIES} for {file_path}: {str(e)}[/yellow]"")
                    time.sleep(RETRY_WAIT)
                else:
                    console.log(f""[red]Failed to determine relevance for {file_path}: {str(e)}[/red]"")
                    return {
                        ""reasoning"": f""Error: {str(e)}"",
                        ""file_path"": file_path,
                        ""is_relevant"": False
                    }
    except Exception as e:
        console.log(f""[red]Error processing file {file_path}: {str(e)}[/red]"")
        return {
            ""reasoning"": f""Error: {str(e)}"",
            ""file_path"": file_path,
            ""is_relevant"": False
        }
",sfa_codebase_context_agent_v3.py,
survived,"def check_file_paths_line_length(reasoning: str, file_paths: List[str], file_line_limit: int = 500) -> Dict[str, int]:
    """"""Checks the line length of each file and returns a dictionary of file paths and their line counts.
    
    Args:
        reasoning: Explanation of why we're checking line lengths
        file_paths: List of file paths to check
        file_line_limit: Maximum number of lines per file
        
    Returns:
        Dictionary mapping file paths to their total line counts
    """"""
    try:
        console.log(f""[blue]Check File Paths Line Length Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Checking {len(file_paths)} files with line limit {file_line_limit}[/dim]"")
        
        result = {}
        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    if line_count <= file_line_limit:
                        result[file_path] = line_count
                    else:
                        console.log(f""[yellow]Skipping {file_path}: {line_count} lines exceed limit of {file_line_limit}[/yellow]"")
            except Exception as e:
                console.log(f""[red]Error reading file {file_path}: {str(e)}[/red]"")
        
        console.log(f""[dim]Found {len(result)} files within line limit[/dim]"")
        return result
    except Exception as e:
        console.log(f""[red]Error checking file paths: {str(e)}[/red]"")
        return {}
",sfa_codebase_context_agent_v3.py,
survived,"    def override(self):
        """"""Override Gemini's generate_content method to track LLM events.""""""
        if not hasattr(self.client, 'generate_content'):
            logger.warning(""Client does not have generate_content method. Skipping override."")
            return
            
        # Store original method
        self.original_generate = self.client.generate_content
        
        def patched_function(*args, **kwargs):
            init_timestamp = get_ISO_time()
            session = kwargs.pop(""session"", None) if ""session"" in kwargs else None
            
            # Handle positional content argument
            if args:
                kwargs[""contents""] = args[0]
                args = args[1:]  # Remove content from args
            
            # Ensure we have the original method
            if self.original_generate is None:
                logger.error(""Original generate_content method not found. Cannot proceed with override."")
                return None
            
            # Call original method and track event
            result = self.original_generate(*args, **kwargs)
            return self.handle_response(result, kwargs, init_timestamp, session=session)
        
        # Override the method
        self.client.generate_content = patched_function
",agentops/llms/providers/gemini.py,GeminiProvider
survived,"    def __init__(self, client):
        """"""Initialize the Gemini provider.
        
        Args:
            client: A configured google.generativeai client instance
        
        Raises:
            ValueError: If client is not properly configured
        """"""
        if not client:
            raise ValueError(""Client must be provided"")
        
        super().__init__(client)
        self._provider_name = ""Gemini""
        
        # Verify client has required methods
        if not hasattr(client, 'generate_content'):
            raise ValueError(""Client must have generate_content method"")
",agentops/llms/providers/gemini.py,GeminiProvider
survived,"    async def swap_tokens(self, wallet_client: SolanaWalletClient, parameters: dict):
        """"""Swap tokens using Jupiter DEX.""""""
        try:
            # First get the quote
            quote_response = await self.get_quote(wallet_client, parameters)
            
            # Prepare swap request
            swap_request = {
                ""userPublicKey"": wallet_client.get_address(),
                ""quoteResponse"": quote_response.dict(),
                ""dynamicComputeUnitLimit"": True,
                ""prioritizationFeeLamports"": ""auto""
            }
            
            # Get swap transaction
            async with aiohttp.ClientSession(**self._session_kwargs) as session:
                async with session.post(f""{self.base_url}/swap"", json={""swapRequest"": swap_request}) as response:
                    if response.status != 200:
                        error_data = await response.json()
                        raise Exception(f""Failed to create swap transaction: {error_data.get('error', 'Unknown error')}"")
                    
                    swap_response = await response.json()
                    swap_transaction = swap_response.get(""swapTransaction"")
                    
                    if not swap_transaction:
                        raise Exception(""No swap transaction returned"")
                    
                    # Deserialize the transaction
                    versioned_transaction = VersionedTransaction.from_bytes(base64.b64decode(swap_transaction))
                    
                    # Get instructions from the transaction
                    instructions = await wallet_client.decompile_versioned_transaction_to_instructions(versioned_transaction)
                    
                    # Send the transaction
                    result = await wallet_client.send_transaction({
                        ""instructions"": instructions,
                        ""address_lookup_table_addresses"": [
                            lookup.account_key.to_base58() 
                            for lookup in versioned_transaction.message.address_table_lookups
                        ]
                    })
                    
                    return {
                        ""hash"": result[""hash""]
                    }
                    
        except Exception as error:
            raise Exception(f""Failed to swap tokens: {error}"")",python/src/plugins/jupiter/goat_plugins/jupiter/service.py,JupiterService
survived,"def test_both_async_params_error(mock_isinstance):
    """"""Test that providing both _async and use_async raises an error.""""""
    mock_model = MagicMock()
    mock_model.generate_content = MagicMock()
    mock_model.generate_content_async = MagicMock()
    
    with pytest.raises(ConfigurationError, match=""Cannot provide both '_async' and 'use_async'. Use 'use_async' instead.""):
        client = from_vertexai(
            mock_model, 
            _async=True,
            use_async=True
        )",tests/llm/test_vertexai/test_deprecated_async.py,
survived,"def _get_stream_name(yaml_stream: dict) -> str | None:
    if ""name"" in yaml_stream:
        return yaml_stream[""name""]
    if ""$parameters"" in yaml_stream and ""name"" in yaml_stream[""$parameters""]:
        return yaml_stream[""$parameters""][""name""]
    return None
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,
survived,"def utils():
    """"""Utils API reference page.""""""
    with open(""docs/api-reference/utils.md"", encoding=""utf-8"") as f:
        content = f.read()
    return rx.markdown(content)",pcweb/pages/docs/api_reference/utils.py,
survived,"    def test_multiple_pause_resume_cycles(self):
        """"""Test multiple pause/resume cycles work correctly.""""""
        formatter = ConsoleFormatter()
        
        mock_live = MagicMock(spec=Live)
        formatter._live = mock_live
        formatter._live_paused = False
        
        formatter.pause_live_updates()
        assert formatter._live_paused
        mock_live.stop.assert_called_once()
        assert formatter._live is None  # Live session should be cleared
        
        formatter.resume_live_updates()
        assert not formatter._live_paused
        
        formatter.pause_live_updates()
        assert formatter._live_paused
        
        formatter.resume_live_updates()
        assert not formatter._live_paused
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume
survived,"        async def _(model_uuid: str) -> str:
            json_data = await quart.request.json

            await self.ap.embeddings_models_service.test_embeddings_model(model_uuid, json_data)

            return self.success()",pkg/api/http/controller/groups/provider/models.py,EmbeddingsModelsRouterGroup
deleted,"    async def remove_embeddings_model(self, model_uuid: str):
        """"""移除 Embeddings 模型""""""
        for model in self.embeddings_models:
            if model.model_entity.uuid == model_uuid:
                self.embeddings_models.remove(model)
                return
",pkg/provider/modelmgr/modelmgr.py,ModelManager
deleted,"    async def test_embeddings_model(self, model_uuid: str, model_data: dict) -> None:
        runtime_embeddings_model: model_requester.RuntimeEmbeddingsModel | None = None

        if model_uuid != '_':
            for model in self.ap.model_mgr.embeddings_models:
                if model.model_entity.uuid == model_uuid:
                    runtime_embeddings_model = model
                    break

            if runtime_embeddings_model is None:
                raise Exception('model not found')

        else:
            runtime_embeddings_model = await self.ap.model_mgr.init_runtime_embeddings_model(model_data)

        await runtime_embeddings_model.requester.invoke_embeddings(
            query=None,
            model=runtime_embeddings_model,
            input_text=""Hello, world!"",
            extra_args={},
        )",pkg/api/http/service/model.py,EmbeddingsModelsService
survived,"def test_write_skips_message_from_unknown_stream(client):
    stream = ""unknown_stream""
    data = {""field1"": ""test-value"", ""field2"": ""test-value""}
    pipeline = _init_mocks(client)
    input_messages = [_record(stream=stream, data=data), _state()]
    destination = DestinationGlassflow()
    for m in destination.write(config=config, configured_catalog=_configured_catalog(), input_messages=input_messages):
        assert m.type == Type.STATE
    pipeline.publish.assert_not_called()",airbyte-integrations/connectors/destination-glassflow/unit_tests/unit_test.py,
survived,"def test_unlimited_tool_usage():
    """"""Test that tools without usage limits work normally.""""""
    class UnlimitedTool(BaseTool):
        name: str = ""Unlimited Tool""
        description: str = ""A tool without usage limits""

        def _run(self, input_text: str) -> str:
            return f""Processed {input_text}""

    tool = UnlimitedTool()
    
    for i in range(5):
        result = tool.run(input_text=f""test{i}"")
        assert result == f""Processed test{i}""
        assert tool.current_usage_count == i + 1
",tests/tools/test_tool_usage_limit.py,
survived,"    def test_load_cache(self) -> None:
        """"""Test the load_cache method.""""""
        loader = MockPersistenceLoader(""test"", self.save_path)
        
        # Create and save a cache
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        loader.saved_caches[""Pure_hash1""] = cache
        
        # Create a placeholder file to trigger cache_hit
        cache_path = loader.build_path(""hash1"", ""Pure"")
        with open(cache_path, ""w"") as f:
            f.write(""placeholder"")
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")",tests/_save/loaders/test_loader.py,TestBasePersistenceLoader
survived,"    def test_cache_attempt_hit(self) -> None:
        """"""Test cache attempt with a hit.""""""
        loader = MockLoader(""test"")
        defs = {""var1""}
        stateful_refs: Set[str] = set()
        
        # Create and save a cache
        original_cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            stateful_refs,
            ""Pure"",
            True,
            {""version"": 1}
        )
        loader.saved_caches[""Pure_hash1""] = original_cache
        
        # Attempt to load the cache
        cache = loader.cache_attempt(defs, ""hash1"", stateful_refs, ""Pure"")
        
        assert cache.hash == ""hash1""
        assert cache.hit is True
        assert cache.cache_type == ""Pure""
        assert cache.defs[""var1""] == ""value1""
        assert cache.meta == {""version"": 1}
",tests/_save/loaders/test_loader.py,TestLoader
survived,"    def setup_method(self) -> None:
        """"""Set up a temporary directory for each test.""""""
        self.temp_dir = tempfile.TemporaryDirectory()
        self.save_path = self.temp_dir.name
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def __init__(self, name: str, config_value: str = ""default"") -> None:
        super().__init__(name)
        self.config_value = config_value
        self.saved_caches: Dict[str, Cache] = {}
",tests/_save/loaders/test_loader.py,MockLoader
survived,"    def cache_hit(self, hashed_context: str, cache_type: str) -> bool:
        key = f""{cache_type}_{hashed_context}""
        return key in self.saved_caches
",tests/_save/loaders/test_loader.py,MockLoader
survived,"    def test_init(self) -> None:
        """"""Test initialization.""""""
        loader = MockPersistenceLoader(""test"", self.save_path)
        assert loader.name == ""test""
        assert loader.suffix == ""mock""
        assert str(loader.save_path).endswith(""/test"")
        
        # Check that the directory was created
        assert (Path(self.save_path) / ""test"").exists()
",tests/_save/loaders/test_loader.py,TestBasePersistenceLoader
survived,"    def load_persistent_cache(
        self, hashed_context: str, cache_type: str
    ) -> Cache:
        key = f""{cache_type}_{hashed_context}""
        if key not in self.saved_caches:
            raise FileNotFoundError(f""No cache found for {key}"")
        return self.saved_caches[key]
",tests/_save/loaders/test_loader.py,MockPersistenceLoader
survived,"    def validate_schema(self):
        """"""Validate that the current database schema matches the expected schema.

        Raises:
            RuntimeError: If there is a mismatch between the current schema and expected schema,
                        with instructions to clear the cache.
        """"""
        expected_columns = [
            ""run_hash"",
            ""dataset_hash"",
            ""prompt_func"",
            ""model_name"",
            ""response_format"",
            ""batch_mode"",
            ""created_time"",
            ""last_edited_time"",
        ]
        current_info = self._get_current_schema()
        current_columns = [col[1] for col in current_info]  # col[1] = column name

        if set(current_columns) != set(expected_columns):
            msg = (
                ""Detected a mismatch between the local DB schema and the expected schema. ""
                ""Please clear your cache with `rm -rf ~/.cache/curator` or ""
                ""`rm -rf $CURATOR_CACHE_DIR` if set.""
            )
            raise RuntimeError(msg)
",src/bespokelabs/curator/db.py,MetadataDB
survived,"    def __init__(self, client: SolanaClient):
        """"""Initialize the Solana wallet client.

        Args:
            client: A Solana RPC client instance
        """"""
        super().__init__()
        self.client = client
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaWalletClient
survived,"    def sign_message(self, message: str) -> Signature:
        """"""Sign a message with the wallet's private key.""""""
        message_bytes = message.encode(""utf-8"")
        signed = nacl.signing.SigningKey(self.keypair.secret_key).sign(message_bytes)
        return {""signature"": signed.signature.hex()}
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaKeypairWalletClient
survived,"    def _is_css_element_empty(self, css_element):
        return css_element is None or css_element.strip() == """"
",crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py,SeleniumScrapingTool
survived,"    def _get_elements_content(self, driver, css_element, return_html):
        elements_content = []

        for element in driver.find_elements(By.CSS_SELECTOR, css_element):
            elements_content.append(
                element.get_attribute(""outerHTML"") if return_html else element.text
            )

        return elements_content
",crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py,SeleniumScrapingTool
survived,"    def _get_body_content(self, driver, return_html):
        body_element = driver.find_element(By.TAG_NAME, ""body"")

        return (
            body_element.get_attribute(""outerHTML"")
            if return_html
            else body_element.text
        )
",crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py,SeleniumScrapingTool
survived,"def test_scrape_with_css_selector(_mocked_chrome_driver):
    html_content = ""<html><body><div>test content</div><div class='test'>test content in a specific div</div></body></html>""
    mock_driver = mock_driver_with_html(html_content)
    tool = initialize_tool_with(mock_driver)

    result = tool._run(website_url=""https://example.com"", css_element=""div.test"")

    assert ""test content in a specific div"" in result
    mock_driver.get.assert_called_once_with(""https://example.com"")
    mock_driver.find_elements.assert_called_with(""css selector"", ""div.test"")
    mock_driver.close.assert_called_once()
",tests/tools/selenium_scraping_tool_test.py,
deleted,"def sanitize_collection_name(name: Optional[str]) -> str:
    """"""
    Sanitize a collection name to meet ChromaDB requirements:
    1. 3-63 characters long
    2. Starts and ends with alphanumeric character
    3. Contains only alphanumeric characters, underscores, or hyphens
    4. No consecutive periods
    5. Not a valid IPv4 address

    Args:
        name: The original collection name to sanitize

    Returns:
        A sanitized collection name that meets ChromaDB requirements
    """"""
    if not name:
        return ""default_collection""

    # Replace spaces and invalid characters with underscores
    sanitized = re.sub(r""[^a-zA-Z0-9_-]"", ""_"", name)

    # Ensure it starts with alphanumeric
    if not sanitized[0].isalnum():
        sanitized = ""a"" + sanitized

    # Ensure it ends with alphanumeric
    if not sanitized[-1].isalnum():
        sanitized = sanitized[:-1] + ""z""

    # Ensure length is between 3-63 characters
    if len(sanitized) < 3:
        # Add padding with alphanumeric character at the end
        sanitized = sanitized + ""x"" * (3 - len(sanitized))
    if len(sanitized) > 63:
        sanitized = sanitized[:63]
        # Ensure it still ends with alphanumeric after truncation
        if not sanitized[-1].isalnum():
            sanitized = sanitized[:-1] + ""z""

    return sanitized",src/crewai/utilities/string_utils.py,
survived,"async def send_data_to_slack(event_instance: DemoEvent):
    """"""Send demo form data to Slack webhook.
    
    Args:
        event_instance: An instance of DemoEvent with form data.
    """"""
    slack_payload = {
        ""lookingToBuild"": event_instance.internal_tools,
        ""businessName"": event_instance.company_email,
        ""howDidYouHear"": event_instance.referral_source,
        ""linkedinUrl"": event_instance.linkedin_url,
        ""jobTitle"": event_instance.job_title,
        ""numEmployees"": event_instance.num_employees,
        ""companyName"": event_instance.company_name,
        ""firstName"": event_instance.first_name,
        ""lastName"": event_instance.last_name
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                SLACK_DEMO_WEBHOOK_URL,
                json=slack_payload,
                headers={""Content-Type"": ""application/json""}
            )
            response.raise_for_status()
    except Exception:
        log(""Error sending data to Slack webhook"")",pcweb/telemetry/postog_metrics.py,
survived,"    def _start_subprocess(self):
        """"""Start the reflex app using subprocess instead of threads.""""""
        backend_port = reflex.utils.processes.handle_port(
            ""backend"", 8000, auto_increment=True
        )
        frontend_port = reflex.utils.processes.handle_port(
            ""frontend"", 3000, auto_increment=True
        )

        self.reflex_process = reflex.utils.processes.new_process(
            [
                sys.executable,
                ""-m"",
                ""reflex"",
                ""run"",
                ""--backend-port"",
                str(backend_port),
                ""--frontend-port"",
                str(frontend_port),
                ""--env"",
                ""dev"",
            ],
            cwd=self.app_path,
            env={""NO_COLOR"": ""1""},
        )

        self.backend_port = backend_port
        self.frontend_port = frontend_port

        self._wait_for_servers()
",reflex/testing.py,AppHarness
survived,"    def is_internal_url(self, url):
        """"""Check if URL is internal to our domain.""""""
        parsed = urlparse(url)
        return parsed.netloc == self.domain or parsed.netloc == ''
",scripts/check_dead_links.py,DeadLinkChecker
survived,"def test_openai_require() -> None:
    """"""Test that openai.require raises ModuleNotFoundError.""""""
    model = openai(""gpt-4"")
    messages = [ChatMessage(role=""user"", content=""Test prompt"")]
    config = ChatModelConfig()
    with pytest.raises(ModuleNotFoundError):
        model(messages, config)
",tests/_ai/llm/_impl.py,
survived,"    def test_require_api_key_missing(self, mock_get_context: MagicMock) -> None:
        """"""Test _require_api_key with missing key.""""""
        mock_context = MagicMock()
        mock_context.marimo_config = {""ai"": {""anthropic"": {""api_key"": """"}}}
        mock_get_context.return_value = mock_context

        model = anthropic(""claude-3-opus-20240229"")
        with pytest.raises(ValueError):
            _ = model._require_api_key",tests/_ai/llm/_impl.py,TestAnthropic
survived,"    def test_is_file_path_with_valid_file(self, tmp_path: Path) -> None:
        # Create a temporary file
        test_file = tmp_path / ""test_file.txt""
        test_file.write_text(""test content"")
        
        # Test with valid file path
        assert is_file_path(None, None, str(test_file)) == str(test_file)
",tests/_cli/test_cli_validators.py,TestIsFilePath
deleted,"    def test_is_sensitive_error(self) -> None:
        # These errors are not sensitive
        assert not is_sensitive_error(MarimoAncestorPreventedError(
            msg="""", raising_cell=""cell1"", blamed_cell=None
        ))
        assert not is_sensitive_error(MarimoAncestorStoppedError(
            msg="""", raising_cell=""cell1""
        ))
        assert not is_sensitive_error(MarimoInternalError(error_id=""""))

        # These errors are sensitive
        assert is_sensitive_error(MarimoExceptionRaisedError(
            msg="""", exception_type="""", raising_cell=None
        ))
        assert is_sensitive_error(MarimoSyntaxError(msg=""""))
        assert is_sensitive_error(UnknownError(msg=""""))",tests/_messaging/test_errors.py,TestErrorUtilityFunctions
survived,"    def test_write_console_output(self) -> None:
        # Test writing console output to stream
        stream = MockStream()
        _write_console_output(
            stream,
            CellChannel.STDOUT,
            ""cell1"",
            ""Hello"",
            ""text/plain"",
        )

        assert len(stream.messages) == 1
        assert stream.messages[0][0] == ""cell-op""  # op
        assert stream.messages[0][1][""cell_id""] == ""cell1""
        assert stream.messages[0][1][""console""][""channel""] == ""stdout""
        assert stream.messages[0][1][""console""][""mimetype""] == ""text/plain""
        assert stream.messages[0][1][""console""][""data""] == ""Hello""
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker
survived,"    def write(self, op: str, data: dict) -> None:
        self.messages.append((op, data))
",tests/_messaging/test_print_override.py,MockStream
deleted,"    def test_run_id_context_manager(self) -> None:
        # Test that run_id_context sets and unsets the run ID
        with run_id_context() as ctx:
            # Run ID should be set within the context
            run_id = RUN_ID_CTX.get()
            assert run_id is not None
            assert isinstance(run_id, str)

            # Should be a valid UUID
            uuid_obj = uuid.UUID(run_id)
            assert str(uuid_obj) == run_id

        # Run ID should be unset outside the context
        with pytest.raises(LookupError):
            RUN_ID_CTX.get()
",tests/_messaging/test_context.py,TestRunIDContext
survived,"    def test_buffered_writer_multiple_messages(self) -> None:
        # Test buffered writer with multiple messages
        stream = MockStream()
        msg_queue: deque[Optional[ConsoleMsg]] = deque()
        cv = threading.Condition()

        # Start the buffered writer in a separate thread
        thread = threading.Thread(
            target=buffered_writer, args=(msg_queue, stream, cv)
        )
        thread.daemon = True
        thread.start()

        try:
            # Add multiple messages to the queue
            with cv:
                msg_queue.append(
                    ConsoleMsg(
                        stream=CellChannel.STDOUT,
                        cell_id=""cell1"",
                        data=""Hello"",
                        mimetype=""text/plain"",
                    )
                )
                msg_queue.append(
                    ConsoleMsg(
                        stream=CellChannel.STDOUT,
                        cell_id=""cell1"",
                        data="" World"",
                        mimetype=""text/plain"",
                    )
                )
                msg_queue.append(
                    ConsoleMsg(
                        stream=CellChannel.STDERR,
                        cell_id=""cell1"",
                        data=""Error"",
                        mimetype=""text/plain"",
                    )
                )
                cv.notify()

            # Wait for the timeout to expire and the messages to be written
            time.sleep(TIMEOUT_S * 2)

            # Check that the messages were written to the stream
            assert len(stream.messages) == 2  # Merged stdout messages + stderr

            # First message should be the merged stdout messages
            assert stream.messages[0][1][""console""][""channel""] == ""stdout""
            assert stream.messages[0][1][""console""][""data""] == ""Hello World""

            # Second message should be the stderr message
            assert stream.messages[1][1][""console""][""channel""] == ""stderr""
            assert stream.messages[1][1][""console""][""data""] == ""Error""

        finally:
            # Signal the writer to terminate
            with cv:
                msg_queue.append(None)
                cv.notify()
            thread.join(timeout=1.0)",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker
survived,"    def test_print_override_with_multiple_args(self) -> None:
        # Test print_override with multiple arguments
        thread_id = threading.get_ident()
        THREADS.add(thread_id)

        try:
            stream = MockStream()

            # Create a mock context
            context = MagicMock(spec=RuntimeContext)
            context.stream = stream
            context.execution_context = MagicMock(spec=ExecutionContext)
            context.execution_context.cell_id = ""cell1""

            with patch(""marimo._messaging.print_override._original_print"") as mock_print:
                with patch(
                    ""marimo._messaging.print_override.get_context"",
                    return_value=context,
                ):
                    print_override(""Hello"", 123, True, None)

                    # Original print should not be called
                    mock_print.assert_not_called()

                    # Message should be sent to the stream with all args converted to strings
                    assert len(stream.messages) == 1
                    assert stream.messages[0][1][""console""][""data""] == ""Hello 123 True None\n""
        finally:
            # Clean up
            if thread_id in THREADS:
                THREADS.remove(thread_id)",tests/_messaging/test_print_override.py,TestPrintOverride
survived,"def test_colorized_url() -> None:
    """"""Test the _colorized_url function.""""""
    # Test with a simple URL
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        result = _colorized_url(""http://localhost:8000"")
        mock_bold.assert_called_once_with(""http://localhost:8000"")
        assert result == ""BOLD_URL""
    
    # Test with a URL with a path
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        result = _colorized_url(""http://localhost:8000/path"")
        mock_bold.assert_called_once_with(""http://localhost:8000/path"")
        assert result == ""BOLD_URL""
    
    # Test with a URL with a query string
    with patch(""marimo._server.print.bold"") as mock_bold:
        mock_bold.return_value = ""BOLD_URL""
        with patch(""marimo._server.print.muted"") as mock_muted:
            mock_muted.return_value = ""MUTED_QUERY""
            result = _colorized_url(""http://localhost:8000/path?query=value"")
            # The implementation separates the query part and applies muted() to it
            mock_bold.assert_called_once_with(""http://localhost:8000/pathMUTED_QUERY"")
            assert result == ""BOLD_URL""
",tests/_server/test_print.py,
survived,"def test_utf8() -> None:
    """"""Test the _utf8 function.""""""
    # Test with UTF8 supported
    with patch(""marimo._server.print.UTF8_SUPPORTED"", True):
        assert _utf8(""🌊🍃"") == ""🌊🍃""
    
    # Test with UTF8 not supported
    with patch(""marimo._server.print.UTF8_SUPPORTED"", False):
        assert _utf8(""🌊🍃"") == """"
",tests/_server/test_print.py,
survived,"    async def make_request(self, endpoint: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """"""Make a request to the Uniswap API.""""""
        url = f""{self.base_url}/{endpoint}""
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": self.api_key
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=parameters, headers=headers) as response:
                if not response.ok:
                    raise Exception(f""Failed to fetch {endpoint}: {await response.text()}"")
                return await response.json()
",python/src/plugins/uniswap/goat_plugins/uniswap/service.py,UniswapService
survived,"    async def get_quote(self, wallet_client: EVMWalletClient, parameters: dict):
        """"""Get a quote for token swap.""""""
        try:
            chain_id = wallet_client.get_chain()[""id""]
            return await self.make_request(""quote"", {
                **parameters,
                ""tokenInChainId"": chain_id,
                ""tokenOutChainId"": parameters.get(""tokenOutChainId"", chain_id),
                ""swapper"": wallet_client.get_address()
            })
        except Exception as error:
            raise Exception(f""Failed to get quote: {error}"")
",python/src/plugins/uniswap/goat_plugins/uniswap/service.py,UniswapService
survived,"    async def swap_tokens(self, wallet_client: EVMWalletClient, parameters: dict):
        """"""Execute a token swap on Uniswap.""""""
        try:
            quote = await self.get_quote(wallet_client, parameters)
            
            response = await self.make_request(""swap"", {
                ""quote"": quote[""quote""]
            })
            
            swap = response[""swap""]
            transaction = await wallet_client.send_transaction({
                ""to"": swap[""to""],
                ""value"": swap[""value""],
                ""data"": swap[""data""]
            })

            return {
                ""txHash"": transaction[""hash""]
            }
        except Exception as error:
            raise Exception(f""Failed to execute swap: {error}"")",python/src/plugins/uniswap/goat_plugins/uniswap/service.py,UniswapService
survived,"def test_url_encoding_email(custodial_api, test_email):
    """"""Test URL parameter encoding with email.""""""
    encoded = quote(test_email)
    with pytest.raises(Exception) as exc:
        custodial_api.get_wallet(f""email:{encoded}:solana-custodial-wallet"")
    # Should raise not found error, but URL should be properly encoded
    assert encoded in str(exc.value)
    assert "":"" not in str(exc.value).replace(""email:"", """").replace("":solana-custodial-wallet"", """")
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def test_phone():
    """"""Fixture providing test phone for wallet creation.""""""
    return ""+1234567890""
",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_smart_wallet_api_key(smart_api):
    """"""Test smart wallet API key configuration.""""""
    headers = smart_api._request(""/wallets"", method=""GET"").request.headers
    assert headers[""x-api-key""] == os.environ[""CROSSMINT_STAGING_API_KEY_SMART""]
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def test_smart_wallet_transaction(smart_api, test_wallet_options, test_evm_transaction, test_keypair):
    """"""Test transaction sending with smart wallet.""""""
    # Create wallet and client
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        test_wallet_options[""chain""],
        test_keypair,
        test_wallet_options[""provider""],
        test_wallet_options[""options""][""ensProvider""]
    )
    
    # Send transaction
    tx = client.send_transaction(test_evm_transaction)
    assert tx[""status""] in [""success"", ""pending""]
    if tx[""status""] == ""success"":
        assert tx[""hash""].startswith(""0x"")
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def compare_signature_responses(py_response: Dict[str, Any], ts_response: Dict[str, Any]) -> None:
    """"""Compare signature responses between implementations.
    
    Args:
        py_response: Response from Python implementation
        ts_response: Response from TypeScript implementation
        
    Raises:
        AssertionError: If responses don't match
    """"""
    assert py_response[""signature""] == ts_response[""signature""], ""Signatures don't match""
    
    # Compare optional fields if present
    if ""status"" in py_response or ""status"" in ts_response:
        assert py_response.get(""status"") == ts_response.get(""status""), ""Signature status doesn't match""
",python/src/wallets/crossmint/tests/utils/helpers.py,
survived,"    def get_json_schema(self):
        return get_generic_json_schema()
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractFolder
survived,"    def streams(self, config: Mapping[str, Any]) -> List[Stream]:
        """"""
        :param config: A Mapping of the user input configuration as defined in the connector spec.
        """"""
        box_client = get_box_ccg_client(config)
        box_folder_text_representation_stream = StreamTextRepresentationFolder(
            box_client, config[""box_folder_id""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_ask_ai_stream = StreamAIAskFolder(
            box_client, config[""box_folder_id""], config[""ask_ai_prompt""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_extract_ai_stream = StreamAIExtractFolder(
            box_client, config[""box_folder_id""], config[""extract_ai_prompt""], is_recursive=config.get(""is_recursive"", False)
        )

        box_folder_extract_structured_ai_stream = StreamAIExtractStructuredFolder(
            client=box_client,
            folder_id=config[""box_folder_id""],
            fields_json_str=config[""extract_structured_ai_fields""],
            is_recursive=config.get(""is_recursive"", False),
        )

        return [
            box_folder_text_representation_stream,
            box_folder_ask_ai_stream,
            box_folder_extract_ai_stream,
            box_folder_extract_structured_ai_stream,
        ]
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,SourceBoxDataExtract
survived,"    def read_records(
        self,
        sync_mode: SyncMode,
        cursor_field: Optional[List[str]] = None,
        stream_slice: Optional[Mapping[str, Any]] = None,
        stream_state: Optional[Mapping[str, Any]] = None,
    ) -> Iterable[StreamData]:
        logger.info(f""Asking AI {self.prompt} for all files in folder {self.folder_id} {'recursively' if self.is_recursive else ''}"")
        items = box_folder_ai_ask(self.client, self.folder_id, prompt=self.prompt, is_recursive=self.is_recursive)
        for item in items:
            airbyte_item: StreamData = item.file.to_dict()
            airbyte_item[""text_representation""] = item.text_representation
            logger.info(f""Reading file {item.file.id} - {item.file.name}"")
            yield airbyte_item
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIAskFolder
survived,"def main():
    """"""Main function to parse arguments and run the research blog system.""""""
    parser = argparse.ArgumentParser(description=""Research and Blog Agent System"")
    parser.add_argument(""--topic"", ""-t"", type=str, required=True, 
                        help=""The topic for the blog post"")
    parser.add_argument(""--output"", ""-o"", type=str, default=None,
                        help=""Optional file path to save the markdown blog post"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Create the blog post
        console.print(Panel(f""Creating a blog post about '{args.topic}'..."", title=""Status"", border_style=""blue""))
        blog_post = asyncio.run(create_research_blog(args.topic))
        
        # Display the blog post
        console.print(Panel(Markdown(blog_post), title=""Blog Post"", border_style=""green""))
        
        # Save to file if output path is provided
        if args.output:
            with open(args.output, ""w"") as f:
                f.write(blog_post)
            console.print(f""[green]Blog post saved to {args.output}[/green]"")
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/13_research_blog_system.py,
survived,"def create_account_agent() -> Agent:
    """"""
    Create an account management agent.
    
    Returns:
        An Agent instance specialized in account management.
    """"""
    instructions = """"""
    You are an account management specialist who can help customers with account-related issues.
    You can assist with questions about account creation, profile updates, security settings, and account recovery.
    Always prioritize account security and verify the customer's identity before making changes.
    Provide clear guidance on how customers can manage their account settings.
    """"""
    
    return Agent(
        name=""AccountManager"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for account management, profile updates, or security questions.""
    )
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"def test_create_specialist_agents():
    """"""Test that specialist agents are created with the correct configuration.""""""
    billing_agent = create_billing_agent()
    technical_agent = create_technical_agent()
    account_agent = create_account_agent()
    
    assert billing_agent.name == ""BillingSpecialist""
    assert technical_agent.name == ""TechnicalSupport""
    assert account_agent.name == ""AccountManager""
    
    assert ""billing specialist"" in billing_agent.instructions.lower()
    assert ""technical support"" in technical_agent.instructions.lower()
    assert ""account management"" in account_agent.instructions.lower()
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"def test_run_custom_tool_agent():
    """"""Test that the agent can use custom tools and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a test query that should use the currency conversion tool
    response = asyncio.run(run_custom_tool_agent(""Convert 50 USD to EUR""))
    
    # Verify we got a non-empty response that mentions the currencies
    assert response
    assert len(response) > 0
    assert ""USD"" in response
    assert ""EUR"" in response
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def test_run_function_tool_agent():
    """"""Test that the agent can use function tools and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a test query that should use the weather tool
    response = asyncio.run(run_function_tool_agent(""What's the weather in London?""))
    
    # Verify we got a non-empty response that mentions London
    assert response
    assert len(response) > 0
    assert ""London"" in response
",openai-agents-examples/05_agent_with_function_tools.py,
survived,"def main():
    """"""Main function to parse arguments and run the content creation system.""""""
    parser = argparse.ArgumentParser(description=""Agent Orchestration Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The content request to process"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the content creation system and get the final content
        console.print(Panel(""Starting content creation process..."", title=""Status"", border_style=""blue""))
        content = asyncio.run(orchestrate_content_creation(args.prompt))
        
        # Display the final content
        console.print(Panel(content, title=""Final Content"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/11_agent_orchestration.py,
survived,"def convert_currency(params: CurrencyConversionInput) -> str:
    """"""
    Convert an amount from one currency to another.
    
    Args:
        params: The currency conversion parameters
        
    Returns:
        A string containing the conversion result
    """"""
    # This is a mock implementation - in a real application, you would call a currency API
    exchange_rates = {
        ""USD"": {""EUR"": 0.92, ""GBP"": 0.79, ""JPY"": 149.50},
        ""EUR"": {""USD"": 1.09, ""GBP"": 0.86, ""JPY"": 162.50},
        ""GBP"": {""USD"": 1.27, ""EUR"": 1.16, ""JPY"": 189.20},
        ""JPY"": {""USD"": 0.0067, ""EUR"": 0.0062, ""GBP"": 0.0053},
    }
    
    from_curr = params.from_currency.upper()
    to_curr = params.to_currency.upper()
    
    # Check if currencies are supported
    if from_curr not in exchange_rates:
        return f""Sorry, {from_curr} is not a supported currency.""
    
    if to_curr not in exchange_rates[from_curr] and from_curr != to_curr:
        return f""Sorry, conversion from {from_curr} to {to_curr} is not supported.""
    
    # If same currency, return the amount
    if from_curr == to_curr:
        return f""{params.amount} {from_curr} is equal to {params.amount} {to_curr}.""
    
    # Calculate converted amount
    converted_amount = params.amount * exchange_rates[from_curr][to_curr]
    
    return f""{params.amount} {from_curr} is equal to {converted_amount:.2f} {to_curr}.""
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"async def run_custom_tool_agent(prompt: str) -> str:
    """"""
    Run the financial assistant agent with the given prompt.
    
    Args:
        prompt: The user's query or prompt
        
    Returns:
        The agent's response as a string
    """"""
    # Create the agent with custom tools
    agent = create_financial_assistant()
    
    # Run the agent with the prompt
    result = await Runner.run(agent, prompt)
    
    # Return the response
    return result.final_output
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def create_financial_assistant() -> Agent:
    """"""
    Create a financial assistant agent with custom tools.
    
    Returns:
        An Agent instance with custom tools for financial assistance
    """"""
    instructions = """"""
    You are a helpful financial assistant that can provide information about 
    currency conversions and stock prices.
    Use the tools available to you to provide accurate financial information when asked.
    If you don't have a tool for the specific request, acknowledge the limitations
    and provide the best information you can.
    """"""
    
    # Create custom tools
    currency_tool = Tool(
        name=""convert_currency"",
        description=""Convert an amount from one currency to another"",
        input_type=CurrencyConversionInput,
        function=convert_currency
    )
    
    stock_tool = Tool(
        name=""get_stock_price"",
        description=""Get the current price of a stock"",
        input_type=StockPriceInput,
        function=get_stock_price
    )
    
    # Create the agent with custom tools
    return Agent(
        name=""FinancialAssistant"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[currency_tool, stock_tool]
    )
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def create_billing_agent() -> Agent:
    """"""
    Create a billing specialist agent.
    
    Returns:
        An Agent instance specialized in billing issues.
    """"""
    instructions = """"""
    You are a billing specialist who can help customers with billing-related issues.
    You can assist with questions about invoices, payment methods, refunds, and subscription plans.
    Be helpful, clear, and concise in your responses.
    Always verify the customer's information before providing specific account details.
    """"""
    
    return Agent(
        name=""BillingSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for questions about billing, payments, invoices, or subscription issues.""
    )
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"def main():
    """"""Main function to parse arguments and run the customer support system.""""""
    parser = argparse.ArgumentParser(description=""Agent with Handoffs Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The customer inquiry to send to the support system"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the customer support system and get response
        response = asyncio.run(run_customer_support_system(args.prompt))
        
        # Display the response
        console.print(Panel(response, title=""Customer Support Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"    def filter(self, input_str: str) -> Optional[str]:
        """"""
        Filter the input string for potentially harmful content.
        
        Args:
            input_str: The input string to filter
            
        Returns:
            The filtered string if it passes, or None if it should be rejected
        """"""
        # Convert to lowercase for case-insensitive matching
        lower_input = input_str.lower()
        
        # Check for filtered terms
        for term in self.filtered_terms:
            if term in lower_input:
                return None  # Reject the input
        
        return input_str  # Accept the input
",openai-agents-examples/10_agent_with_guardrails.py,ContentModerationGuardrail
survived,"def test_create_protected_agent():
    """"""Test that the protected agent is created with the correct configuration.""""""
    agent = create_protected_agent()
    assert agent.name == ""ProtectedAssistant""
    assert ""helpful assistant"" in agent.instructions.lower()
    assert agent.model == ""gpt-4o-mini""
    assert len(agent.input_guardrails) == 2
",openai-agents-examples/10_agent_with_guardrails.py,
survived,"async def test_chat_clear_messages():
    def mock_model(
        messages: List[ChatMessage], config: ChatModelConfig
    ) -> str:
        del messages, config
        return ""Mock response""

    chat = ui.chat(mock_model)
    chat._chat_history = [
        ChatMessage(role=""user"", content=""Hello""),
        ChatMessage(role=""assistant"", content=""Hi there!""),
    ]

    # Simulate clearing messages
    chat._value = []
    assert chat.value == []
    assert chat._chat_history == []
",tests/_plugins/ui/_impl/chat/test_chat.py,
survived,"async def test_get_request_endpoint():
    request_id = ""test-request-id""
    subdomain = ""abcd1234""
    
    request_data = {
        ""_id"": request_id,
        ""type"": ""http"",
        ""raw"": ""SGVsbG8gV29ybGQ="",  # base64 encoded ""Hello World""
        ""uid"": subdomain,
        ""method"": ""GET"",
        ""path"": ""/test"",
        ""headers"": {""host"": ""test.com""},
        ""date"": int(datetime.datetime.now(datetime.timezone.utc).timestamp()),
    }
    
    mock_redis.get.return_value = json.dumps(request_data)
    
    response = client.get(f""/api/get_request?id={request_id}&subdomain={subdomain}"")
    
    assert response.status_code == 200
    assert response.json() == request_data
    
    mock_redis.get.assert_called_with(f""request:{subdomain}:{request_id}"")
",backend/tests/test_endpoints.py,
survived,"    def register_listener(
        self,
        event_type: typing.Type[platform_events.Event],
        func: typing.Callable[[platform_events.Event, msadapter.MessagePlatformAdapter], typing.Awaitable[None]],
    ):
        """"""注册事件监听器""""""
        pass
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"async def toggle_report_public_state(slug: str, api_key: str = Depends(verify_admin_api_key)) -> dict:
    try:
        from src.services.report_status import toggle_report_public_state
        is_public = toggle_report_public_state(slug)
        
        return {
            ""success"": True,
            ""is_public"": is_public
        }
    except ValueError as e:
        slogger.error(f""ValueError: {e}"", exc_info=True)
        raise HTTPException(status_code=404, detail=str(e)) from e
    except Exception as e:
        slogger.error(f""Exception: {e}"", exc_info=True)
        raise HTTPException(status_code=500, detail=""Internal server error"") from e",server/src/routers/admin_report.py,
survived,"    def hide_results(self):
        """"""Hide search results.""""""
        self.show_results = False
",pcweb/components/docpage/navbar/typesense.py,TypesenseSearchState
survived,"    def _run(self, api_method: str, instruction: str, **kwargs: Any) -> Any:
        """"""Execute a Stagehand command using the specified API method.
        
        Args:
            api_method: The Stagehand API to use ('act', 'extract', or 'observe')
            instruction: An atomic instruction for Stagehand to execute
            **kwargs: Additional keyword arguments passed to the Stagehand API
            
        Returns:
            The result from the Stagehand API call
            
        Raises:
            ValueError: If an invalid api_method is provided
            RuntimeError: If the Stagehand API call fails
        """"""
        try:
            # Initialize Stagehand with the OpenAI API key
            st = stagehand.Stagehand(api_key=self.api_key)
            
            # Call the appropriate Stagehand API based on the method
            if api_method == ""act"":
                return st.act(instruction)
            elif api_method == ""extract"":
                return st.extract(instruction)
            elif api_method == ""observe"":
                return st.observe(instruction)
            else:
                raise ValueError(f""Unknown api_method: {api_method}"")
                
        except Exception as e:
            raise RuntimeError(f""Stagehand API call failed: {str(e)}"")",crewai_tools/tools/stagehand_tool/stagehand_tool.py,StagehandTool
survived,"def test_create_directory_with_existing_directory():
    """"""Test that create_directory=False works when directory already exists.""""""
    from pathlib import Path
    
    output_path = ""existing_test_dir/output.txt""
    
    resolved_path = Path(output_path).expanduser().resolve()
    resolved_dir = resolved_path.parent
    resolved_dir.mkdir(parents=True, exist_ok=True)
    
    task = Task(
        description=""Test task"",
        expected_output=""Test output"",
        output_file=output_path,
        create_directory=False,
    )
    
    task._save_file(""test content"")
    assert resolved_path.exists()
    
    if resolved_path.exists():
        resolved_path.unlink()
    if resolved_dir.exists():
        import shutil
        shutil.rmtree(resolved_dir)
",tests/task_test.py,
survived,"def mark_notification_as_read(user_id: str, notification_id: str) -> bool:
    """"""
    Mark a notification as read.
    
    Args:
        user_id: The ID of the user
        notification_id: The ID of the notification
        
    Returns:
        True if the notification was marked as read, False otherwise
    """"""
    if user_id not in NOTIFICATION_STORE:
        return False
    
    for notification in NOTIFICATION_STORE[user_id]:
        if notification[""id""] == notification_id:
            notification[""is_read""] = True
            return True
    
    return False
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,
survived,"def login_user(username: str, password: str) -> Tuple[bool, Dict]:
    """"""
    Login a user and create an authentication token.
    
    Args:
        username: The username to authenticate
        password: The password to authenticate
        
    Returns:
        Tuple of (success, result) where result contains user data and token or error message
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""username"": username, ""password"": password},
        [""username"", ""password""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Authenticate the user
    user_data = authenticate(username, password)
    if not user_data:
        return False, {""error"": ""Invalid username or password""}
    
    # Create an authentication token
    token = create_token(user_data[""id""])
    
    return True, {
        ""user"": user_data,
        ""token"": token
    }
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"    def get_all(self, table_name):
        """"""Get all items from a table.""""""
        if table_name not in self.data:
            Logger.warning(self.logger, f""Table '{table_name}' not found"")
            return []
        
        items = list(self.data[table_name].values())
        Logger.debug(self.logger, f""Retrieved {len(items)} items from '{table_name}'"")
        return items
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase
survived,"def load_json_file(file_path):
    """"""Load data from a JSON file.""""""
    try:
        with open(file_path, 'r') as file:
            return json.load(file)
    except FileNotFoundError:
        raise ValueError(f""File not found: {file_path}"")
    except json.JSONDecodeError:
        raise ValueError(f""Invalid JSON format in file: {file_path}"")
",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def get_final_result(self):
        """"""
        Get the result from the final stage of the pipeline.
        
        Returns:
            dict: Result from the final stage
        """"""
        if not self.stages:
            return None
        
        final_stage_name = self.stages[-1][""name""]
        if final_stage_name in self.results:
            return self.results[final_stage_name]
        
        return None
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager
survived,"    def update_product(product_id, name=None, price=None, category_id=None, description=None, sku=None):
        """"""Update a product.""""""
        try:
            product = ProductService.update_product(product_id, name, price, category_id, description, sku)
            if not product:
                return {
                    ""success"": False,
                    ""message"": f""Product with ID {product_id} not found""
                }
            return {
                ""success"": True,
                ""message"": ""Product updated successfully"",
                ""data"": product
            }
        except ValueError as e:
            Logger.warning(app_logger, f""Validation error in update_product: {str(e)}"")
            return {
                ""success"": False,
                ""message"": str(e)
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in update_product: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while updating the product""
            }
",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI
survived,"def mark_all_notifications_as_read(user_id: str) -> int:
    """"""
    Mark all notifications for a user as read.
    
    Args:
        user_id: The ID of the user
        
    Returns:
        Number of notifications marked as read
    """"""
    if user_id not in NOTIFICATION_STORE:
        return 0
    
    count = 0
    for notification in NOTIFICATION_STORE[user_id]:
        if not notification[""is_read""]:
            notification[""is_read""] = True
            count += 1
    
    return count
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,
survived,"def validate_email(email: str) -> bool:
    """"""
    Validate an email address format.
    
    Args:
        email: The email address to validate
        
    Returns:
        True if the email is valid, False otherwise
    """"""
    # Simple regex for email validation
    # In a real application, consider using a more comprehensive validation
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, email))
",codebase-architectures/atomic-composable-architecture/modules/validation.py,
survived,"    def delete_user(user_id):
        """"""Delete a user.""""""
        return db.delete(""users"", user_id)",codebase-architectures/vertical-slice-architecture/features/users/service.py,UserService
survived,"    def get_logger(name):
        """"""Get a logger instance for the given name.""""""
        return logging.getLogger(name)
",codebase-architectures/layered-architecture/utils/logger.py,Logger
survived,"    def prepare(self, processing_result):
        """"""
        Prepare the output stage with data from the processing stage.
        
        Args:
            processing_result: Result from the processing stage
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        # Check if processing stage had errors
        if processing_result[""metadata""][""status""] in [""error"", ""skipped""]:
            self.metadata[""status""] = ""skipped""
            self.metadata[""errors""].append(""Processing stage had errors, output skipped"")
            return self._create_result()
        
        # Get data and metadata from processing stage
        self.data = processing_result[""data""]
        self.metadata[""input_metadata""] = processing_result[""metadata""][""input_metadata""]
        self.metadata[""processing_metadata""] = processing_result[""metadata""]
        
        # Get analysis if available
        if ""analysis"" in processing_result:
            self.analysis = processing_result[""analysis""]
        
        # Initialize output
        self.metadata[""status""] = ""preparing""
        self.metadata[""started_at""] = datetime.now().isoformat()
        
        return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/output_stage.py,OutputStage
survived,"def delete_notification(user_id: str, notification_id: str) -> bool:
    """"""
    Delete a notification.
    
    Args:
        user_id: The ID of the user
        notification_id: The ID of the notification
        
    Returns:
        True if the notification was deleted, False otherwise
    """"""
    if user_id not in NOTIFICATION_STORE:
        return False
    
    for i, notification in enumerate(NOTIFICATION_STORE[user_id]):
        if notification[""id""] == notification_id:
            del NOTIFICATION_STORE[user_id][i]
            return True
    
    return False
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,
survived,"def send_user_alert(user_id: str, message: str, level: str = ""info"", 
                   email: Optional[str] = None, phone: Optional[str] = None,
                   additional_data: Optional[Dict] = None) -> Tuple[bool, Dict]:
    """"""
    Send an alert to a user through multiple channels.
    
    Args:
        user_id: The ID of the user to alert
        message: The alert message
        level: Alert level (info, warning, error)
        email: Optional email address to send the alert to
        phone: Optional phone number to send the alert to
        additional_data: Additional data for the alert
        
    Returns:
        Tuple of (success, result) with notification details
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""user_id"": user_id, ""message"": message},
        [""user_id"", ""message""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Validate message length
    if not validate_string_length(message, min_length=1, max_length=500):
        return False, {""error"": ""Message must be between 1 and 500 characters""}
    
    # Validate level
    valid_levels = [""info"", ""warning"", ""error""]
    if level not in valid_levels:
        return False, {""error"": f""Level must be one of: {', '.join(valid_levels)}""}
    
    # Create the alert notification
    notification = create_alert(
        user_id=user_id,
        message=message,
        level=level,
        data=additional_data
    )
    
    # Send email if provided
    email_sent = False
    if email:
        if validate_email(email):
            subject = f""Alert: {level.capitalize()}""
            email_sent = send_email_notification(email, subject, message)
        else:
            return False, {""error"": ""Invalid email format""}
    
    # Send SMS if provided
    sms_sent = False
    if phone:
        sms_sent = send_sms_notification(phone, message)
    
    return True, {
        ""notification"": notification,
        ""channels"": {
            ""in_app"": True,
            ""email"": email_sent,
            ""sms"": sms_sent
        }
    }
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,
survived,"    def format_as_detailed_report(self):
        """"""
        Format the data as a detailed report.
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data to format"")
            return self._create_result()
        
        try:
            # Create detailed report
            report = {
                ""report_type"": ""detailed"",
                ""generated_at"": datetime.now().isoformat(),
                ""data_source"": self.metadata.get(""input_metadata"", {}).get(""source"", ""unknown""),
                ""record_count"": len(self.data) if isinstance(self.data, list) else 1,
                ""data"": self.data
            }
            
            # Add analysis if available
            if self.analysis:
                report[""analysis""] = self.analysis
            
            # Add processing information
            if ""processing_metadata"" in self.metadata:
                report[""processing_info""] = {
                    ""steps"": self.metadata[""processing_metadata""].get(""processing_steps"", []),
                    ""filters"": self.metadata[""processing_metadata""].get(""filters_applied"", []),
                    ""transformations"": self.metadata[""processing_metadata""].get(""transformations_applied"", []),
                    ""processing_time_seconds"": self.metadata[""processing_metadata""].get(""processing_time_seconds"")
                }
            
            # Store the detailed report
            self.detailed_report = report
            
            # Update metadata
            self.metadata[""output_formats""].append(""detailed_report"")
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Detailed report formatting error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/output_stage.py,OutputStage
survived,"    def update_user(user_id, user_data):
        """"""Update a user.""""""
        try:
            user = UserService.update_user(user_id, user_data)
            if not user:
                return {""error"": f""User with ID {user_id} not found""}
            return user
        except ValueError as e:
            return {""error"": str(e)}
",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI
survived,"    def configure_processing(self, config):
        """"""
        Configure the processing stage.
        
        Args:
            config: Dictionary with processing configuration
        """"""
        self.processing_config = config
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,DataProcessingPipeline
survived,"    def update(self, table_name, item_id, item):
        """"""Update an item in a table.""""""
        if table_name not in self.data or item_id not in self.data[table_name]:
            Logger.warning(self.logger, f""Cannot update: Item with ID {item_id} not found in '{table_name}'"")
            return None
        
        # Ensure ID remains the same
        item[""id""] = item_id
        self.data[table_name][item_id] = item
        Logger.info(self.logger, f""Updated item with ID {item_id} in '{table_name}'"")
        return item
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase
survived,"    def send_system_alert(token: str, user_id: str, notification_type: str, 
                         data: Dict, email: Optional[str] = None) -> Dict:
        """"""
        Send a system notification to a user (admin function).
        
        Args:
            token: Authentication token (must be admin)
            user_id: The ID of the user to notify
            notification_type: The type of notification
            data: Data for the notification template
            email: Optional email address to send the notification to
            
        Returns:
            Response with success status and notification details or error message
        """"""
        # Validate token (in a real app, would check if user is admin)
        success, admin_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Send system notification
        success, result = send_system_notification(
            user_id=user_id,
            notification_type=notification_type,
            data=data,
            email=email
        )
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""System notification sent successfully"",
                ""data"": result
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": result.get(""error"", ""Failed to send system notification""),
                ""data"": None
            }",codebase-architectures/atomic-composable-architecture/endpoints/alerts_api.py,AlertsAPI
survived,"def main():
    """"""Run the application.""""""
    display_header(""Vertical Slice Architecture Example"")
    
    # Create users
    display_header(""Creating Users"")
    user1 = UserAPI.create_user(""johndoe"", ""john@example.com"", ""John Doe"")
    display_result(user1)
    
    user2 = UserAPI.create_user(""janedoe"", ""jane@example.com"", ""Jane Doe"")
    display_result(user2)
    
    # Try to create a user with an existing username
    duplicate_user = UserAPI.create_user(""johndoe"", ""another@example.com"")
    display_result(duplicate_user)
    
    # Get all users
    display_header(""All Users"")
    all_users = UserAPI.get_all_users()
    for user in all_users:
        display_result(user)
    
    # Create tasks
    display_header(""Creating Tasks"")
    task1 = TaskAPI.create_task(""Complete project"", ""Finish the architecture example"", user1[""id""])
    display_result(task1)
    
    task2 = TaskAPI.create_task(""Review code"", ""Check for bugs and improvements"", user2[""id""])
    display_result(task2)
    
    task3 = TaskAPI.create_task(""Write documentation"", ""Document the architecture"", user1[""id""])
    display_result(task3)
    
    # Get user tasks
    display_header(f""Tasks for {user1['name']}"")
    user1_tasks = TaskAPI.get_user_tasks(user1[""id""])
    for task in user1_tasks:
        display_result(task)
    
    # Update a task
    display_header(""Updating a Task"")
    updated_task = TaskAPI.update_task(task1[""id""], {""status"": ""completed""})
    display_result(updated_task)
    
    # Delete a task
    display_header(""Deleting a Task"")
    delete_result = TaskAPI.delete_task(task2[""id""])
    display_result(delete_result)
    
    # Get all remaining tasks
    display_header(""All Remaining Tasks"")
    all_tasks = TaskAPI.get_all_tasks()
    for task in all_tasks:
        display_result(task)
",codebase-architectures/vertical-slice-architecture/main.py,
survived,"    def warning(logger, message):
        """"""Log a warning message.""""""
        logger.warning(message)
",codebase-architectures/layered-architecture/utils/logger.py,Logger
survived,"    def mark_all_as_read(token: str) -> Dict:
        """"""
        Mark all alerts as read.
        
        Args:
            token: Authentication token
            
        Returns:
            Response with success status and count of alerts marked as read
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Mark all as read
        count = mark_all_alerts_as_read(user_data[""id""])
        
        return {
            ""status"": ""success"",
            ""message"": f""Marked {count} alerts as read"",
            ""data"": {""count"": count}
        }
",codebase-architectures/atomic-composable-architecture/endpoints/alerts_api.py,AlertsAPI
survived,"    def get_all_users():
        """"""Get all users.""""""
        return db.get_all(""users"")
",codebase-architectures/vertical-slice-architecture/features/users/service.py,UserService
survived,"    def insert(self, collection_name, id, item):
        """"""Insert an item into a collection.""""""
        if collection_name not in self.data:
            self.create_collection(collection_name)
        self.data[collection_name][id] = item
        return id
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"def get_user_alerts(user_id: str, unread_only: bool = False, 
                   level: Optional[str] = None) -> List[Dict]:
    """"""
    Get alerts for a user with optional filtering.
    
    Args:
        user_id: The ID of the user
        unread_only: Whether to return only unread alerts
        level: Optional filter by alert level
        
    Returns:
        List of alert notifications
    """"""
    # Get all notifications for the user
    notifications = get_user_notifications(user_id, unread_only)
    
    # Filter to only alert type notifications
    alerts = [n for n in notifications if n[""type""] == ""alert""]
    
    # Filter by level if specified
    if level:
        alerts = [a for a in alerts if a[""data""].get(""level"") == level]
    
    return alerts
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,
survived,"    def update_product(product_id, name=None, price=None, category_id=None, description=None, sku=None):
        """"""Update a product.""""""
        try:
            # Get existing product
            product_data = db.get(""products"", product_id)
            if not product_data:
                Logger.warning(app_logger, f""Cannot update: Product not found: {product_id}"")
                return None
            
            # Validate price if provided
            if price is not None:
                try:
                    price = float(price)
                    if price < 0:
                        raise ValueError()
                except (ValueError, TypeError):
                    raise ValueError(""Price must be a positive number"")
            
            # Validate category if provided
            if category_id:
                category = db.get(""categories"", category_id)
                if not category:
                    raise ValueError(f""Category with ID {category_id} not found"")
            
            # Validate SKU if provided
            if sku and sku != product_data[""sku""]:
                existing_products = db.query(""products"", lambda p: p[""sku""] == sku and p[""id""] != product_id)
                if existing_products:
                    raise ValueError(f""Product with SKU '{sku}' already exists"")
            
            # Update fields
            if name:
                product_data[""name""] = name
            if price is not None:
                product_data[""price""] = price
            if category_id is not None:
                product_data[""category_id""] = category_id
            if description is not None:
                product_data[""description""] = description
            if sku is not None:
                product_data[""sku""] = sku
            
            # Update timestamp
            product_data[""updated_at""] = datetime.now().isoformat()
            
            # Save to database
            updated_product = db.update(""products"", product_id, product_data)
            Logger.info(app_logger, f""Updated product: {product_id}"")
            return updated_product
        except Exception as e:
            Logger.error(app_logger, f""Error updating product: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/product_service.py,ProductService
survived,"    def get_user_tasks(user_id):
        """"""Get all tasks for a specific user.""""""
        return TaskService.get_user_tasks(user_id)
",codebase-architectures/vertical-slice-architecture/features/tasks/api.py,TaskAPI
survived,"def validate_data(data: Dict[str, Any], schema: Dict[str, Dict[str, Any]]) -> Dict[str, List[str]]:
    """"""
    Validate data against a schema.
    
    Args:
        data: The data to validate
        schema: Validation schema defining field types and constraints
        
    Returns:
        Dictionary mapping field names to lists of validation error messages
    """"""
    errors: Dict[str, List[str]] = {}
    
    for field_name, field_schema in schema.items():
        field_type = field_schema.get(""type"")
        required = field_schema.get(""required"", False)
        
        # Check if required field is missing
        if required and (field_name not in data or data[field_name] is None):
            errors.setdefault(field_name, []).append(""Field is required"")
            continue
        
        # Skip validation for optional fields that are not present
        if field_name not in data or data[field_name] is None:
            continue
        
        value = data[field_name]
        
        # Type validation
        if field_type == ""string"" and not isinstance(value, str):
            errors.setdefault(field_name, []).append(""Must be a string"")
        elif field_type == ""number"" and not isinstance(value, (int, float)):
            errors.setdefault(field_name, []).append(""Must be a number"")
        elif field_type == ""integer"" and not isinstance(value, int):
            errors.setdefault(field_name, []).append(""Must be an integer"")
        elif field_type == ""boolean"" and not isinstance(value, bool):
            errors.setdefault(field_name, []).append(""Must be a boolean"")
        elif field_type == ""array"" and not isinstance(value, list):
            errors.setdefault(field_name, []).append(""Must be an array"")
        elif field_type == ""object"" and not isinstance(value, dict):
            errors.setdefault(field_name, []).append(""Must be an object"")
        
        # String-specific validations
        if field_type == ""string"" and isinstance(value, str):
            min_length = field_schema.get(""min_length"")
            max_length = field_schema.get(""max_length"")
            pattern = field_schema.get(""pattern"")
            
            if min_length is not None and len(value) < min_length:
                errors.setdefault(field_name, []).append(f""Must be at least {min_length} characters"")
            
            if max_length is not None and len(value) > max_length:
                errors.setdefault(field_name, []).append(f""Must be at most {max_length} characters"")
            
            if pattern is not None and not re.match(pattern, value):
                errors.setdefault(field_name, []).append(""Does not match required pattern"")
        
        # Number-specific validations
        if field_type in [""number"", ""integer""] and isinstance(value, (int, float)):
            minimum = field_schema.get(""minimum"")
            maximum = field_schema.get(""maximum"")
            
            if minimum is not None and value < minimum:
                errors.setdefault(field_name, []).append(f""Must be at least {minimum}"")
            
            if maximum is not None and value > maximum:
                errors.setdefault(field_name, []).append(f""Must be at most {maximum}"")
    
    return errors",codebase-architectures/atomic-composable-architecture/modules/validation.py,
survived,"    def error(logger, message, exc_info=None):
        """"""Log an error message.""""""
        logger.error(message, exc_info=exc_info)
",codebase-architectures/layered-architecture/utils/logger.py,Logger
survived,"    def add_stage(self, name: str, stage: PipelineStage) -> None:
        """"""
        Add a stage to the pipeline.
        
        Args:
            name: The name of the stage
            stage: The stage to add
        """"""
        self.stages[name] = stage
        self.stage_order.append(name)
        console.log(f""[pipeline] Added stage: {name}"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/pipeline_manager.py,Pipeline
survived,"def normalize_path(path: str) -> str:
    """"""
    Normalize file paths to handle various formats (absolute, relative, Windows paths, etc.)

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """"""
    if not path:
        return path

    # Handle Windows backslash paths if provided
    path = path.replace(""\\"", os.sep)

    is_windows_path = False
    if os.name == ""nt"" and len(path) > 1 and path[1] == "":"":
        is_windows_path = True

    # Handle /repo/ paths from Claude (tool use convention)
    if path.startswith(""/repo/""):
        path = os.path.join(os.getcwd(), path[6:])
        return path

    if path.startswith(""/""):
        # Handle case when Claude provides paths with leading slash
        if path == ""/"" or path == ""/."":
            # Special case for root directory
            path = os.getcwd()
        else:
            # Replace leading slash with current working directory
            path = os.path.join(os.getcwd(), path[1:])
    elif path.startswith(""./""):
        # Handle relative paths starting with ./
        path = os.path.join(os.getcwd(), path[2:])
    elif not os.path.isabs(path) and not is_windows_path:
        # For non-absolute paths that aren't Windows paths either
        path = os.path.join(os.getcwd(), path)

    return path
",example-agent-codebase-arch/vertical-slice-architecture/shared/utils.py,
survived,"def normalize_path(path: str) -> str:
    """"""
    Normalize file paths to handle various formats (absolute, relative, Windows paths, etc.)

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """"""
    if not path:
        return path

    # Handle Windows backslash paths if provided
    path = path.replace(""\\"", os.sep)

    is_windows_path = False
    if os.name == ""nt"" and len(path) > 1 and path[1] == "":"":
        is_windows_path = True

    # Handle /repo/ paths from Claude (tool use convention)
    if path.startswith(""/repo/""):
        path = os.path.join(os.getcwd(), path[6:])
        return path

    if path.startswith(""/""):
        # Handle case when Claude provides paths with leading slash
        if path == ""/"" or path == ""/."":
            # Special case for root directory
            path = os.getcwd()
        else:
            # Replace leading slash with current working directory
            path = os.path.join(os.getcwd(), path[1:])
    elif path.startswith(""./""):
        # Handle relative paths starting with ./
        path = os.path.join(os.getcwd(), path[2:])
    elif not os.path.isabs(path) and not is_windows_path:
        # For non-absolute paths that aren't Windows paths either
        path = os.path.join(os.getcwd(), path)

    return path
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,
survived,"    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """"""
        Process data through the pipeline.
        
        Args:
            data: The input data to process
            
        Returns:
            The processed data after passing through all stages
        """"""
        console.log(f""[pipeline] Starting pipeline: {self.name}"")
        
        current_data = data
        
        for stage_name in self.stage_order:
            stage = self.stages[stage_name]
            console.log(f""[pipeline] Processing stage: {stage_name}"")
            
            try:
                current_data = stage.process(current_data)
                
                # Check if there was an error in the stage
                if ""error"" in current_data:
                    console.log(f""[pipeline] Error in stage {stage_name}: {current_data['error']}"")
                    # Continue to the next stage, which may handle the error
                
            except Exception as e:
                console.log(f""[pipeline] Exception in stage {stage_name}: {str(e)}"")
                current_data = {""error"": f""Exception in stage {stage_name}: {str(e)}""}
        
        console.log(f""[pipeline] Completed pipeline: {self.name}"")
        return current_data",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/pipeline_manager.py,Pipeline
survived,"def temp_dir():
    temp_path = tempfile.mkdtemp()
    yield temp_path
    shutil.rmtree(temp_path)
",tests/cli/test_create_crew.py,
survived,"    def _restore_state(self, stored_state: Dict[str, Any]) -> None:
        """"""Restore flow state from persistence.
        
        Args:
            stored_state: Previously stored state to restore
            
        Raises:
            ValueError: If validation fails for structured state
            TypeError: If state is neither BaseModel nor dictionary
        """"""
        self._initialize_state(stored_state)
",src/crewai/flow/flow.py,Flow
survived,"    def load_state(self, flow_uuid: str) -> Optional[Dict[str, Any]]:
        """"""Load the most recent state for a given flow UUID.
        
        Args:
            flow_uuid: Unique identifier for the flow instance
            
        Returns:
            The most recent state as a dictionary, or None if no state exists
        """"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(""""""
            SELECT state_json
            FROM flow_states
            WHERE flow_uuid = ?
            ORDER BY id DESC
            LIMIT 1
            """""", (flow_uuid,))
            row = cursor.fetchone()
            
        if row:
            return json.loads(row[0])
        return None",src/crewai/flow/persistence/sqlite.py,SQLiteFlowPersistence
survived,"    def get_total_flow_rate(self, wallet_client: EVMWalletClient, parameters: dict):
        result = wallet_client.read(
            {
                ""address"": parameters[""poolAddress""],
                ""abi"": POOL_ABI,
                ""functionName"": ""getTotalFlowRate"",
                ""args"": [],
            }
        )
        return result[""value""]",python/src/plugins/superfluid/goat_plugins/superfluid/service.py,SuperfluidService
survived,"def test_xai_raw_response_with_validator_sync(model, mode):
    """"""Test that _raw_response works with validated models in sync mode""""""
    client = instructor.from_provider(f""xai/{model}"", mode=mode)
    
    user = client.chat.completions.create(
        response_model=UserValidated,
        max_retries=2,
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""You are a helpful assistant that extracts information."",
            },
            {
                ""role"": ""user"",
                ""content"": ""Extract: Jason is 25 years old."",
            },
        ],
    )
    
    assert isinstance(user, UserValidated)
    assert user.name == ""JASON""
    assert user.age == 25
    assert hasattr(user, ""_raw_response""), (
        ""The raw response should be available from XAI""
    )
    assert user._raw_response is not None
",tests/llm/test_xai/test_raw_response.py,
survived,"    def __init__(self, jwt_token: str):
        self.jwt_token = jwt_token
        self.calls = []
        self.stop = []
",tests/custom_llm_test.py,JWTAuthLLM
survived,"    def get_context_window_size(self) -> int:
        """"""Get the context window size of the LLM.
        
        Returns:
            The context window size as an integer.
        """"""
        pass
",src/crewai/llm.py,BaseLLM
survived,"    def call(
        self,
        messages: Union[str, List[Dict[str, str]]],
        tools: Optional[List[dict]] = None,
        callbacks: Optional[List[Any]] = None,
        available_functions: Optional[Dict[str, Any]] = None,
    ) -> Union[str, Any]:
        """"""Call the LLM with the given messages.
        
        Args:
            messages: Input messages for the LLM.
                     Can be a string or list of message dictionaries.
                     If string, it will be converted to a single user message.
                     If list, each dict must have 'role' and 'content' keys.
            tools: Optional list of tool schemas for function calling.
                  Each tool should define its name, description, and parameters.
            callbacks: Optional list of callback functions to be executed
                      during and after the LLM call.
            available_functions: Optional dict mapping function names to callables
                               that can be invoked by the LLM.
            
        Returns:
            Either a text response from the LLM (str) or
            the result of a tool function call (Any).
        """"""
        pass
",src/crewai/llm.py,BaseLLM
survived,"def test_custom_llm_implementation():
    """"""Test that a custom LLM implementation works with create_llm.""""""
    custom_llm = CustomLLM(response=""The answer is 42"")
    
    # Test that create_llm returns the custom LLM instance directly
    result_llm = create_llm(custom_llm)
    
    assert result_llm is custom_llm
    
    # Test calling the custom LLM
    response = result_llm.call(""What is the answer to life, the universe, and everything?"")
    
    # Verify that the custom LLM was called
    assert len(custom_llm.calls) > 0
    # Verify that the response from the custom LLM was used
    assert response == ""The answer is 42""
",tests/custom_llm_test.py,
survived,"def test_custom_llm_with_jwt_auth():
    """"""Test a custom LLM implementation with JWT authentication.""""""
    jwt_llm = JWTAuthLLM(jwt_token=""example.jwt.token"")
    
    # Test that create_llm returns the JWT-authenticated LLM instance directly
    result_llm = create_llm(jwt_llm)
    
    assert result_llm is jwt_llm
    
    # Test calling the JWT-authenticated LLM
    response = result_llm.call(""Test message"")
    
    # Verify that the JWT-authenticated LLM was called
    assert len(jwt_llm.calls) > 0
    # Verify that the response from the JWT-authenticated LLM was used
    assert response == ""Response from JWT-authenticated LLM""",tests/custom_llm_test.py,
