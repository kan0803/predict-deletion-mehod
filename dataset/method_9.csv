status,method,filepath,class_name
survived,"def is_github_actions() -> bool:
    return os.environ.get(""GITHUB_ACTIONS"") == ""true""
",dev/check_function_signatures.py,
survived,"def test_multiple_optional_became_required():
    old_code = ""def func(a, b=1, c=2): pass""
    new_code = ""def func(a, b, c): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 2
    assert errors[0].message == ""Optional positional param 'b' became required.""
    assert errors[1].message == ""Optional positional param 'c' became required.""
",tests/dev/test_check_function_signatures.py,
survived,"    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        self.stack.append(node)
        self.generic_visit(node)
        self.stack.pop()
",dev/check_function_signatures.py,FunctionSignatureExtractor
survived,"def update_data():
    update_global_data()
    return jsonify({""status"": ""Data updated successfully""})
",triton_viz/visualizer/interface.py,
survived,"    def test_rolling_simple(self, move_func, static_func, window):
        """"""Test rolling functions with simple data.""""""
        data = np.array([[1, 2, 3, 4, 5, 6], [2, 4, 6, 8, 10, 12]], dtype=np.float64)
        result = move_func(data, window=window, min_count=2)

        # Shape should be (n_obs, n_vars, n_vars)
        assert result.shape == (6, 2, 2)

        # Check symmetry for each time point
        for t in range(6):
            assert_allclose(result[t], result[t].T, equal_nan=True)

        # For perfect linear relationship, correlation should be 1
        if move_func == move_nancorrmatrix:
            for i in range(1, 6):  # From second window onwards (min_count=2)
                assert_allclose(result[i], [[1.0, 1.0], [1.0, 1.0]], rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"def test_move_matrix_pandas_min_count_simple(func_name, window, min_count):
    """"""Test matrix functions against pandas with different min_count values.""""""
    # Create test array directly
    rs = np.random.RandomState(0)
    array = rs.rand(4, 15)

    # Get the function
    func = (
        move_nancorrmatrix if func_name == ""move_nancorrmatrix"" else move_nancovmatrix
    )

    # Get comparisons
    c = COMPARISONS[func]

    # Get numbagg result
    result = c[""numbagg""](array, window=window, min_count=min_count)()

    # Get pandas result and convert to our format
    pandas_callable = c[""pandas""](array, window=window, min_count=min_count)
    pandas_result = pandas_callable()

    # Convert pandas MultiIndex DataFrame to 3D array
    n_obs = array.shape[-1]
    n_vars = array.shape[-2]
    expected_pandas = np.full((n_obs, n_vars, n_vars), np.nan)

    for t in range(n_obs):
        if t in pandas_result.index.get_level_values(0):
            expected_pandas[t] = pandas_result.loc[t].values

    assert_allclose(result, expected_pandas)
",numbagg/test/test_moving.py,
survived,"def pandas_rolling_covmatrix(a, window=20, min_count=None):
    """"""Compute rolling covariance matrix using pandas.

    Note: Returns pandas MultiIndex DataFrame, not numbagg's 3D array format.
    For benchmark purposes, we compare the raw computation without format conversion.
    """"""
    rolling = pandas_rolling_matrix_setup(a, window, min_count)
    return lambda: rolling.cov()
",numbagg/test/conftest.py,
survived,"def pandas_rolling_matrix_setup(a, window=20, min_count=None):
    """"""Setup rolling window for matrix operations (corr/cov).""""""
    df = pandas_matrix_setup(a)
    rolling = df.rolling(window, min_periods=min_count)
    return rolling
",numbagg/test/conftest.py,
survived,"def pandas_static_covmatrix(a):
    """"""Compute covariance matrix using pandas.""""""
    df = pandas_matrix_setup(a)
    return lambda: df.cov()
",numbagg/test/conftest.py,
survived,"    def test_perfect_correlation_consistency(self):
        """"""Test with perfectly correlated data.""""""
        np.random.seed(222)

        # Create perfectly correlated data
        n_obs = 15
        a1 = np.random.randn(n_obs)
        a2 = 2 * a1 + 1  # Perfect linear relationship

        alpha = 0.6

        # Compute using non-matrix function
        corr_nonmatrix = move_exp_nancorr(a1, a2, alpha=alpha)

        # Compute using matrix function
        data_matrix = np.array([a1, a2])
        corr_matrix_result = move_exp_nancorrmatrix(data_matrix, alpha=alpha)
        corr_from_matrix = corr_matrix_result[:, 0, 1]

        # They should match and approach 1.0
        assert_allclose(corr_nonmatrix, corr_from_matrix, rtol=1e-10)

        # For later time points with perfect correlation, should be close to 1.0
        final_corr = corr_from_matrix[-1]
        assert abs(final_corr - 1.0) < 1e-10
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency
survived,"    def __call__(
        self,
        a: np.ndarray,
        alpha: float | np.ndarray,
        min_weight: float = 0,
        axis: int = -1,
        **kwargs,
    ):
        a = np.asarray(a)

        if a.ndim < 2:
            raise ValueError(f""{self.func.__name__} requires at least a 2D array."")

        # Move the axis to the last position for the gufunc
        a = np.moveaxis(a, axis, -1)

        # Handle alpha parameter similar to ndmoveexp
        if not isinstance(alpha, np.ndarray):
            alpha = np.broadcast_to(alpha, a.shape[-1])  # type: ignore[assignment,unused-ignore]

        gufunc = self.gufunc(target=self.target)
        with np.errstate(invalid=""ignore"", divide=""ignore""):
            return gufunc(a, alpha, min_weight, **kwargs)
",numbagg/decorators.py,ndmoveexpmatrix
survived,"    def test_regime_change_data(self):
        """"""Test with data that has structural breaks.""""""
        np.random.seed(789)

        # First regime: positive correlation
        regime1 = np.random.randn(2, 25)
        regime1[1] = 0.8 * regime1[0] + 0.6 * np.random.randn(25)

        # Second regime: negative correlation
        regime2 = np.random.randn(2, 25)
        regime2[1] = -0.8 * regime2[0] + 0.6 * np.random.randn(25)

        data = np.concatenate([regime1, regime2], axis=1)

        # Test with high alpha (should adapt quickly to regime change)
        result_high = move_exp_nancorrmatrix(data, alpha=0.9)

        # Test with low alpha (should be more stable across regime change)
        result_low = move_exp_nancorrmatrix(data, alpha=0.1)

        # The key insight: high alpha should reach the new regime correlation faster
        # Compare how close each gets to the true second regime correlation

        # Compute what the second regime correlation should be approximately
        regime2_corr = np.corrcoef(regime2)[0, 1]

        # High alpha should be closer to the true second regime correlation
        final_high = result_high[-1, 0, 1]
        final_low = result_low[-1, 0, 1]

        error_high = abs(final_high - regime2_corr)
        error_low = abs(final_low - regime2_corr)

        # High alpha should adapt faster, so should be closer to true regime 2 correlation
        assert (
            error_high < error_low or abs(error_high - error_low) < 0.1
        )  # Allow some tolerance
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"    def gufunc(self, *, target):
        vectorize = numba.guvectorize(
            *self.signature,
            nopython=True,
            target=target,
            cache=self.cache,
            fastmath=_FASTMATH,
        )
        return vectorize(self.func)
",numbagg/decorators.py,ndmoveexpmatrix
survived,"    def _load_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """"""Load information about a specific model""""""
        from .scanner import ModelScanner
        
        scanner = ModelScanner(self.base_path)
        search_results = scanner.search_by_model_name(model_name)
        
        if not search_results['matches']:
            return None
        
        model_info = {
            'name': model_name,
            'files': [],
            'config': None,
            'metadata': {},
            'size': 0
        }
        
        # Collect all files from matches
        for category, files in search_results['matches'].items():
            for file_info in files:
                model_info['files'].append(file_info)
                model_info['size'] += file_info.get('size', 0)
                
                # Try to load config
                file_path = self.base_path / file_info['path']
                if file_path.name in ['config.json', 'model_config.json']:
                    try:
                        with open(file_path, 'r') as f:
                            model_info['config'] = json.load(f)
                    except:
                        pass
        
        return model_info
",src/haconiwa/scan/comparator.py,ModelComparator
survived,"    def test_generate_for_pattern_fix(self):
        """"""Test pattern fix YAML generation""""""
        pattern = ""deprecated_api\\(\\)""
        fix_description = ""replace with new_api()""
        files = [
            'src/main.py',
            'src/utils.py',
            'src/api.py'
        ]
        
        config = self.generator.generate_for_pattern_fix(
            pattern,
            fix_description,
            files
        )
        
        assert config['provider'] == 'claude'
        assert config['metadata']['pattern'] == pattern
        assert config['metadata']['fix'] == fix_description
        assert len(config['tasks']) == 3
        
        # Check pattern fix prompts
        for task in config['tasks']:
            assert f""Find all occurrences of pattern '{pattern}'"" in task['prompt']
            assert fix_description in task['prompt']
        
        assert config['options']['permission_mode'] == 'acceptEdits'  # Auto-accept
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _format_yaml(self, data: Any) -> str:
        """"""Format as YAML""""""
        return yaml.dump(data, default_flow_style=False, allow_unicode=True)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def test_scan_content_with_context(self, runner, temp_model_dir):
        """"""Test content search with context lines""""""
        result = runner.invoke(
            scan_app,
            [""content"", ""Hello"", ""--path"", str(temp_model_dir), ""--context"", ""5"", ""--format"", ""json""]
        )
        
        assert result.exit_code == 0
        output = json.loads(result.stdout)
        assert 'matches' in output
        if output['matches']:
            assert 'context' in output['matches'][0]
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_get_action_prompt(self):
        """"""Test action prompt retrieval""""""
        # Known action
        prompt = self.generator._get_action_prompt('add_type_hints')
        assert 'type hints' in prompt
        
        # Unknown action
        prompt = self.generator._get_action_prompt('unknown_action')
        assert prompt == 'Perform unknown_action on this file following best practices'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def generate(self, model_name: str, guide_type: str = 'development') -> str:
        """"""Generate a guide for the specified model""""""
        # Load model information
        model_info = self._load_model_info(model_name)
        
        if not model_info:
            return f""# Error: Model '{model_name}' not found\n\nPlease check the model name and try again.""
        
        # Generate guide using appropriate template
        generator = self.templates.get(guide_type, self._generate_development_guide)
        return generator(model_info)
",src/haconiwa/scan/guide_generator.py,GuideGenerator
survived,"def guide(
    model_name: str = typer.Argument(..., help=""Model name to generate guide for""),
    type: str = typer.Option(""development"", ""--type"", ""-t"", 
                            help=""Guide type (development/usage/integration/quickstart)""),
    path: Optional[Path] = typer.Option(None, ""--path"", ""-p"", help=""Base path to search in""),
    output: Optional[Path] = typer.Option(None, ""--output"", ""-o"", help=""Output file path"")
):
    """"""Generate development guide for specific AI model""""""
    generator = GuideGenerator(base_path=path or Path.cwd())
    
    guide_text = generator.generate(model_name, guide_type=type)
    
    if output:
        output.write_text(guide_text)
        typer.echo(f""Guide saved to: {output}"")
    else:
        typer.echo(guide_text)
",src/haconiwa/scan/cli.py,
survived,"    def test_generate_project_wide(self):
        """"""Test project-wide YAML generation""""""
        # Create test files in temp directory
        src_dir = self.temp_dir / 'src'
        src_dir.mkdir()
        models_dir = src_dir / 'models'
        models_dir.mkdir()
        tests_dir = self.temp_dir / 'tests'
        tests_dir.mkdir()
        
        # Create test files
        (src_dir / 'main.py').write_text('def main(): pass')
        (src_dir / 'utils.py').write_text('def helper(): pass')
        (models_dir / 'user.py').write_text('class User: pass')
        (tests_dir / 'test_main.py').write_text('def test_main(): pass')
        
        config = self.generator.generate_project_wide(
            action='add_type_hints',
            file_pattern='*.py',
            exclude_patterns=['tests/']
        )
        
        assert config['provider'] == 'claude'
        assert config['metadata']['action'] == 'add_type_hints'
        assert config['metadata']['file_pattern'] == '*.py'
        
        # Should exclude test files
        file_names = [task['file'] for task in config['tasks']]
        assert 'tests/test_main.py' not in file_names
        for task in config['tasks']:
            assert 'tests/' not in task['file']
            assert task['prompt'] == 'Add comprehensive type hints to all functions and methods'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"def help():
    """"""Show detailed help for scan command""""""
    help_text = """"""
üîç Haconiwa Scan Command - AI Model Search & Analysis

The scan command provides comprehensive search and analysis capabilities for AI model
directories, supporting model name searching, file content searching, and various 
output formats.

COMMANDS:
  model         Search by model name (with automatic prefix stripping)
  content       Search file contents with regex
  list          List all available AI models
  analyze       Analyze directory structure and categorization
  compare       Compare multiple AI models
  guide         Generate development guide for specific model
  generate-parallel-config  Generate parallel development configuration YAML

EXAMPLES:
  # Search for a model
  haconiwa scan model gpt-4
  
  # Search with prefix
  haconiwa scan model claude-3-opus --no-strip-prefix
  
  # Search content
  haconiwa scan content ""model.forward"" --type .py --context 5
  
  # List models by provider
  haconiwa scan list --provider openai --format json
  
  # Analyze directory
  haconiwa scan analyze --show-structure
  
  # Compare models
  haconiwa scan compare gpt-4 claude-3-opus
  
  # Generate guide
  haconiwa scan guide gpt-4 --type quickstart --output guide.md
  
  # Generate parallel development configuration YAML
  haconiwa scan generate-parallel-config --source model:gpt-4 --action add_tests
  haconiwa scan generate-parallel-config --example
  haconiwa scan generate-parallel-config --migration gpt-3.5:gpt-4 --max-files 20
  haconiwa scan generate-parallel-config --project-wide ""*.py"" --action add_type_hints

For more information on a specific command, use:
  haconiwa scan <command> --help
    """"""
    typer.echo(help_text)",src/haconiwa/scan/cli.py,
survived,"    def test_list_models_with_filters(self, temp_model_dir):
        """"""Test listing models with category and provider filters""""""
        scanner = ModelScanner(temp_model_dir)
        
        # Filter by provider
        openai_models = scanner.list_all_models(provider=""openai"")
        assert all(m['provider'] == 'openai' for m in openai_models)
        
        # Filter by category
        llm_models = scanner.list_all_models(category=""llm"")
        assert all('llm' in m['category'] for m in llm_models)
",tests/test_scan/test_scanner.py,TestModelScanner
survived,"    def _is_model_file(self, path: Path) -> bool:
        """"""Check if file is a model file""""""
        return path.suffix in self.model_extensions or path.name in self.config_files
",src/haconiwa/scan/analyzer.py,ModelAnalyzer
survived,"    def temp_model_dir(self):
        """"""Create a temporary directory with test models""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            base_path = Path(tmpdir)
            
            # Create test model structure
            model_dir = base_path / ""models"" / ""test"" / ""o1-mini""
            model_dir.mkdir(parents=True)
            
            # Create config
            config = {
                ""model_name"": ""o1-mini"",
                ""model_type"": ""language"",
                ""parameters"": ""3B""
            }
            with open(model_dir / ""config.json"", ""w"") as f:
                json.dump(config, f)
            
            # Create model file
            (model_dir / ""model.pt"").touch()
            
            # Create example
            with open(model_dir / ""example.py"", ""w"") as f:
                f.write(""# Example for o1-mini\nprint('Hello from o1-mini')"")
            
            yield base_path
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_search_by_model_name_no_strip(self, temp_model_dir):
        """"""Test searching without prefix stripping""""""
        scanner = ModelScanner(temp_model_dir, strip_prefix=False)
        
        results = scanner.search_by_model_name(""claude-3-opus"")
        assert results['normalized_name'] == ""claude-3-opus""
",tests/test_scan/test_scanner.py,TestModelScanner
survived,"    def test_scan_generate_parallel_config_from_model(self, runner, temp_model_dir):
        """"""Test generate-parallel-config from model search""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / ""parallel-dev.yaml""
            
            # Change to temp_model_dir before running command
            import os
            original_cwd = os.getcwd()
            try:
                os.chdir(temp_model_dir)
                result = runner.invoke(
                    scan_app,
                    [""generate-parallel-config"", 
                     ""--source"", ""model:o1-mini"", 
                     ""--action"", ""add_tests"",
                     ""--output"", str(output_path)]
                )
            finally:
                os.chdir(original_cwd)
            
            assert result.exit_code == 0
            assert ""Generated parallel-dev YAML"" in result.stdout
            assert output_path.exists()
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def list_all_models(self, 
                       category: Optional[str] = None,
                       provider: Optional[str] = None) -> List[Dict[str, Any]]:
        """"""List all available models in the directory structure""""""
        models = []
        model_dirs = defaultdict(lambda: {'files': [], 'categories': set()})
        
        for root, dirs, files in os.walk(self.base_path):
            root_path = Path(root)
            
            # Filter directories
            dirs[:] = [d for d in dirs if not self._should_ignore(root_path / d)]
            
            # Look for model-related directories
            if self._is_model_directory(root_path):
                model_name = self._extract_model_name(root_path)
                model_provider = self._extract_provider(root_path)
                model_category = self._determine_category(root_path)
                
                if category and model_category != category:
                    continue
                if provider and model_provider != provider:
                    continue
                
                for file in files:
                    file_path = root_path / file
                    if not self._should_ignore(file_path):
                        model_dirs[model_name]['files'].append(str(file_path))
                        model_dirs[model_name]['categories'].add(model_category)
                        model_dirs[model_name]['provider'] = model_provider
        
        # Convert to list format
        for model_name, info in model_dirs.items():
            models.append({
                'name': model_name,
                'provider': info.get('provider', 'Unknown'),
                'category': ', '.join(info['categories']),
                'file_count': len(info['files']),
                'files': info['files'][:5]  # First 5 files as sample
            })
        
        return sorted(models, key=lambda x: (x['provider'], x['name']))
",src/haconiwa/scan/scanner.py,ModelScanner
survived,"    def test_scan_content_command(self, runner, temp_model_dir):
        """"""Test the scan content command""""""
        result = runner.invoke(
            scan_app,
            [""content"", ""Hello"", ""--path"", str(temp_model_dir), ""--type"", "".py""]
        )
        
        assert result.exit_code == 0
        assert ""Hello from o1-mini"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_extract_files_from_matches(self):
        """"""Test file extraction from match results""""""
        matches = {
            'model': [
                {'path': 'file1.py'},
                {'path': 'file2.py'}
            ],
            'api': [
                {'path': 'file3.py'},
                {'path': 'file4.py'}
            ]
        }
        
        files = self.generator._extract_files_from_matches(matches, max_files=3)
        
        assert len(files) == 3
        assert 'file1.py' in files
        assert 'file2.py' in files
        assert 'file3.py' in files
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def test_different_alphas(self, func):
        """"""Test behavior with different alpha values.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25]], dtype=np.float64)

        # High alpha (fast decay) vs low alpha (slow decay)
        result_high = func(data, alpha=0.9)
        result_low = func(data, alpha=0.1)

        # Both should have same shape
        assert result_high.shape == result_low.shape == (5, 2, 2)

        # Results should be different
        assert not np.allclose(result_high[-1], result_low[-1], rtol=1e-3)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def test_fixed_dimensional_conventions(self):
        """"""Test fixed dimensional conventions: (..., obs, vars) -> (..., obs, vars, vars).""""""
        np.random.seed(42)

        # Basic test: (obs, vars) -> (obs, vars, vars)
        data = np.random.randn(20, 3)  # (obs, vars)
        corr_result = move_exp_nancorrmatrix(data, alpha=0.4)
        cov_result = move_exp_nancovmatrix(data, alpha=0.4)

        assert corr_result.shape == (20, 3, 3)
        assert cov_result.shape == (20, 3, 3)

        # Broadcasting test: (batch, obs, vars) -> (batch, obs, vars, vars)
        data_3d = np.random.randn(2, 20, 3)  # (batch, obs, vars)
        corr_3d = move_exp_nancorrmatrix(data_3d, alpha=0.4)

        assert corr_3d.shape == (2, 20, 3, 3)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    async def test_windows_process_termination(
        self, mock_subprocess_run, mock_platform
    ):
        """"""Test Windows-specific process termination.""""""
        from ocode_python.tools.bash_tool import _process_manager

        # Mock process that needs termination
        mock_process = AsyncMock()
        mock_process.pid = 1234
        mock_process.returncode = None  # Process still running
        mock_process.terminate.return_value = None
        mock_process.wait.side_effect = asyncio.TimeoutError()
        mock_process.kill.return_value = None

        # Test that taskkill is called on Windows
        await _process_manager._terminate_process(mock_process)

        mock_subprocess_run.assert_called_with(
            [""taskkill"", ""/F"", ""/T"", ""/PID"", ""1234""], check=False, capture_output=True
        )
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility
survived,"    def test_git_detection(self, mock_which):
        """"""Test Git executable detection.""""""
        # Test when Git is available
        mock_which.return_value = ""/usr/bin/git""
        assert shutil.which(""git"") is not None

        # Test when Git is not available
        mock_which.return_value = None
        assert shutil.which(""git"") is None
",tests/unit/test_windows_compatibility.py,TestCrossPlatformDetection
survived,"    def __enter__(self):
        """"""Context manager entry.""""""
        return self
",ocode_python/core/context_manager.py,ContextManager
survived,"def deploy_staticfiles_windows(release: Release) -> bool:
    """"""Deploy static files to CDN for Windows.""""""
    print(""Deploying static files to cdn (Windows)"")
    cc = f""public, max-age={int(datetime.timedelta(days=365).total_seconds())}""

    if not release.static_key:
        print(""No static files to deploy"")
        return True

    with tempfile.NamedTemporaryFile(suffix=os.path.basename(release.static_key)) as f:
        download_release_fileobj(release.static_key, f)
        f.flush()
        with DeploymentJob(
            f.name, ""ce-cdn.net"", version=release.version, cache_control=cc, bucket_path=""windows""
        ) as job:
            return job.run()
",bin/lib/builds_core.py,
survived,"    def test_run_with_uv_error_handling(self, mock_run):
        """"""Test run_with_uv error handling.""""""
        mock_run.side_effect = subprocess.CalledProcessError(1, [""uv"", ""run""])

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(""server.py"")

        assert exc_info.value.code == 1
",tests/cli/test_run_with_uv.py,TestRunWithUv
survived,"    def load_checkpoint(self, filename: str, model_class: type[T]) -> Optional[List[T]]:
        """"""Load data from a checkpoint file if it exists.
        
        Args:
            filename: Name of the checkpoint file
            model_class: Pydantic model class for deserializing the data
            
        Returns:
            List of model instances if checkpoint exists, None otherwise
        """"""
        if not self.enabled:
            return None
            
        checkpoint_path = self.get_checkpoint_path(filename)
        if os.path.exists(checkpoint_path):
            logger.info(f""Loading checkpoint from {checkpoint_path} for {model_class.__name__}"")
            with open(checkpoint_path, ""r"") as f:
                return [model_class.model_validate_json(line) for line in f]
        return None
",kura/v1/kura.py,CheckpointManager
survived,"def visualise_clusters_enhanced(
    clusters: Optional[List[Cluster]] = None,
    *,
    checkpoint_path: Optional[Union[str, Path]] = None
) -> None:
    """"""Print an enhanced hierarchical visualization of clusters with colors and statistics.
    
    This function provides a more detailed visualization than visualise_clusters(),
    including conversation counts, percentages, progress bars, and descriptions.
    
    Args:
        clusters: List of clusters to visualize. If None, loads from checkpoint_path
        checkpoint_path: Path to checkpoint file to load clusters from
        
    Raises:
        ValueError: If neither clusters nor checkpoint_path is provided
        FileNotFoundError: If checkpoint file doesn't exist
    """"""
    # Load clusters
    if clusters is None:
        if checkpoint_path is None:
            raise ValueError(""Either clusters or checkpoint_path must be provided"")
        clusters = _load_clusters_from_checkpoint(checkpoint_path)
    
    logger.info(f""Enhanced visualization of {len(clusters)} clusters"")
    
    print(""\n"" + ""=""*80)
    print(""üéØ ENHANCED CLUSTER VISUALIZATION"")
    print(""=""*80)
    
    # Build tree structure
    node_id_to_cluster = _build_cluster_tree(clusters)
    
    # Calculate total conversations from root clusters only
    root_clusters = [cluster for cluster in clusters if not cluster.parent_id]
    total_conversations = sum(len(cluster.chat_ids) for cluster in root_clusters)

    # Find root nodes
    root_nodes = [
        node_id_to_cluster[cluster.id] for cluster in root_clusters
    ]

    fake_root = ClusterTreeNode(
        id=""root"",
        name=f""üìö All Clusters ({total_conversations:,} total conversations)"",
        description=""Hierarchical conversation clustering results"",
        count=total_conversations,
        children=[node.id for node in root_nodes],
    )

    tree_output = _build_enhanced_tree_structure(
        fake_root, node_id_to_cluster, 0, False, """", total_conversations
    )

    print(tree_output)
    
    # Add summary statistics
    print(""=""*80)
    print(""üìà CLUSTER STATISTICS"")
    print(""=""*80)
    print(f""üìä Total Clusters: {len(clusters)}"")
    print(f""üå≥ Root Clusters: {len(root_nodes)}"")
    print(f""üí¨ Total Conversations: {total_conversations:,}"")
    print(f""üìè Average Conversations per Root Cluster: {total_conversations/len(root_nodes):.1f}"")
    print(""=""*80 + ""\n"")
",kura/v1/visualization.py,
survived,"    def test_duplicate_step_names_error(self):
        """"""Test that duplicate step names raise ValueError.""""""
        mock_learner = MockLearner()
        with pytest.raises(ValueError, match=""Step names must be unique""):
            LearnerPipeline(
                steps=[(""dup"", StandardScaler()), (""dup"", StandardScaler())],
                learner=mock_learner
            )
",tests/test_learner_pipeline.py,TestLearnerPipelineInit
survived,"    def test_pre_fitted_transformers(self):
        """"""Test pre-fitted transformers work correctly.""""""
        scaler = StandardScaler()
        X_train = np.array([[1, 2], [3, 4], [5, 6]])
        scaler.fit(X_train)  # Pre-fit

        mock_learner = MockLearner()
        pipeline = LearnerPipeline(steps=[(""scale"", scaler)], learner=mock_learner)

        X = np.array([[2, 3]])
        y = np.array([1])

        pipeline.partial_fit(X, y)

        # Should use pre-fitted scaler
        received_X, _, _ = mock_learner.partial_fit_calls[0]
        expected_X = scaler.transform(X)
        np.testing.assert_array_almost_equal(received_X, expected_X)
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers
survived,"    def test_validate_empty_steps(self):
        """"""Test validation with empty steps.""""""
        with pytest.raises(ValueError, match=""Pipeline steps cannot be empty""):
            _validate_steps([])
",tests/test_agent_pipeline.py,TestValidateSteps
survived,"def _transform_data(X: Any, steps: List[Tuple[str, Any]]) -> Any:
    """"""Apply all transformers to input data.

    Transformers must be either stateless or pre-fitted.
    No fitting occurs during transformation.
    """"""
    result = X

    for name, transformer in steps:
        try:
            result = transformer.transform(result)
        except Exception as e:
            # Provide helpful error for common case
            if hasattr(e, ""args"") and ""not fitted"" in str(e).lower():
                raise RuntimeError(
                    f""Transformer '{name}' is not fitted. In online learning, ""
                    f""all transformers must be either stateless or pre-fitted ""
                    f""before use. Common stateless transformers include ""
                    f""FunctionTransformer, FeatureHasher, and HashingVectorizer. ""
                    f""Stateful transformers like StandardScaler must be fit on ""
                    f""historical data before creating the pipeline.""
                ) from e
            raise

    return result
",bayesianbandits/pipelines/_agent.py,
survived,"    def test_multiple_transformers(self):
        """"""Test multiple transformers work correctly.""""""

        def add_one(X):
            return X + 1

        def multiply_two(X):
            return X * 2

        mock_learner = MockLearner()
        pipeline = LearnerPipeline(steps=[(""add"", FunctionTransformer(add_one)), (""multiply"", FunctionTransformer(multiply_two))], learner=mock_learner)

        X = np.array([[1, 2]])
        y = np.array([1])

        pipeline.partial_fit(X, y)

        # Should apply transformations in sequence: (X + 1) * 2
        received_X, _, _ = mock_learner.partial_fit_calls[0]
        expected_X = (X + 1) * 2
        np.testing.assert_array_equal(received_X, expected_X)
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers
survived,"    def test_sample_method(self):
        """"""Test sample method delegates correctly.""""""
        X = np.array([[1, 2], [3, 4]])

        # Train the pipeline first
        self.pipeline.partial_fit(X, np.array([1, 2]))

        # Now sample
        self.pipeline.sample(X, size=5)

        assert len(self.mock_learner.sample_calls) == 1
        received_X, size = self.mock_learner.sample_calls[0]
        assert size == 5
        # X should be transformed
        assert received_X.shape == X.shape
",tests/test_learner_pipeline.py,TestLearnerPipelineInterface
survived,"    def test_validate_valid_steps(self):
        """"""Test validation with valid steps.""""""
        steps = [
            (""step1"", FunctionTransformer()),
            (""step2"", StandardScaler()),
        ]
        # Should not raise
        _validate_steps(steps)
",tests/test_agent_pipeline.py,TestValidateSteps
survived,"    def test_noncontextual_pipeline_policy_setter(self):
        """"""Test policy setter on non-contextual pipeline.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        pipeline = NonContextualAgentPipeline([], agent)

        new_policy = EpsilonGreedy(epsilon=0.2)
        pipeline.policy = new_policy
        assert pipeline.policy is new_policy
",tests/test_agent_pipeline.py,TestCoverage
survived,"    def __init__(self):
        self.partial_fit_calls = []
        self.sample_calls = []
        self.predict_calls = []
        self.decay_calls = []
        self.random_state = None
",tests/test_learner_pipeline.py,MockLearner
survived,"    def decay(self, X, *, decay_rate=None):
        self.decay_calls.append((X, decay_rate))
",tests/test_learner_pipeline.py,MockLearner
survived,"    def test_transformer_not_fitted_error(self):
        """"""Test helpful error message when transformer is not fitted.""""""
        from sklearn.preprocessing import StandardScaler
        
        mock_learner = MockLearner()
        # Create pipeline with unfitted transformer
        pipeline = LearnerPipeline(steps=[(""unfitted_scaler"", StandardScaler())], learner=mock_learner)

        X = np.array([[1, 2], [3, 4]])
        y = np.array([1, 2])

        # Should provide helpful error message
        with pytest.raises(RuntimeError) as exc_info:
            pipeline.partial_fit(X, y)

        error_msg = str(exc_info.value)
        assert ""unfitted_scaler"" in error_msg
        assert ""not fitted"" in error_msg
        assert ""stateless or pre-fitted"" in error_msg
        assert ""FunctionTransformer"" in error_msg
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers
survived,"    def __init__(
        self,
        steps: List[Tuple[str, Any]],
        final_agent: ContextualAgent[ContextType, TokenType],
    ) -> None:
        _validate_steps(steps)
        self.steps = steps
        self._agent = final_agent
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def remove_arm(self, token: TokenType) -> None:
        """"""Remove an arm from the wrapped agent.""""""
        self._agent.remove_arm(token)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"def handle_reask_kwargs(
    kwargs: dict[str, Any],
    mode: Mode,
    response: Any,
    exception: Exception,
) -> dict[str, Any]:
    """"""Handle validation errors by reformatting the request for retry (reask).

    When a response fails validation (e.g., missing required fields, wrong types),
    this function prepares a new request that includes information about the error.
    This allows the LLM to understand what went wrong and correct its response.

    The reask logic is provider-specific because each provider has different ways
    of handling function/tool calls and different message formats.

    Args:
        kwargs (dict[str, Any]): The original request parameters that resulted in
            a validation error. Includes messages, tools, temperature, etc.
        mode (Mode): The provider/format mode that determines which reask handler
            to use. Each mode has a specific strategy for formatting error feedback.
        response (Any): The raw response from the LLM that failed validation.
            Type varies by provider:
            - OpenAI: ChatCompletion with tool_calls
            - Anthropic: Message with tool_use blocks
            - Google: GenerateContentResponse with function calls
        exception (Exception): The validation error that occurred. Usually a
            Pydantic ValidationError with details about which fields failed.

    Returns:
        dict[str, Any]: Modified kwargs for the retry request, typically including:
            - Updated messages with error context
            - Same tool/function definitions
            - Preserved generation parameters
            - Provider-specific formatting

    Reask Strategies by Provider:
        Each provider has a specific strategy for handling retries:

        **JSON Modes:**
        - Adds assistant message with failed attempt
        - Adds user message with error details

        **Tool Calls:**
        - Preserves tool definitions
        - Formats the errors as tool calls responses

    Note:
        This function is typically called internally by the retry logic when
        max_retries > 1. It ensures that each retry attempt includes context
        about previous failures, helping the LLM learn from its mistakes.
    """"""
    # Create a shallow copy of kwargs to avoid modifying the original
    kwargs_copy = kwargs.copy()

    # Organized by provider (matching process_response.py structure)
    REASK_HANDLERS = {
        # OpenAI modes
        Mode.FUNCTIONS: reask_default,
        Mode.TOOLS_STRICT: reask_tools,
        Mode.TOOLS: reask_tools,
        Mode.JSON_O1: reask_default,
        Mode.JSON: reask_md_json,
        Mode.MD_JSON: reask_md_json,
        Mode.JSON_SCHEMA: reask_md_json,
        Mode.PARALLEL_TOOLS: reask_tools,
        Mode.RESPONSES_TOOLS: reask_responses_tools,
        Mode.RESPONSES_TOOLS_WITH_INBUILT_TOOLS: reask_responses_tools,
        # Mistral modes
        Mode.MISTRAL_TOOLS: reask_mistral_tools,
        Mode.MISTRAL_STRUCTURED_OUTPUTS: reask_mistral_structured_outputs,
        # Anthropic modes
        Mode.ANTHROPIC_TOOLS: reask_anthropic_tools,
        Mode.ANTHROPIC_REASONING_TOOLS: reask_anthropic_tools,
        Mode.ANTHROPIC_JSON: reask_anthropic_json,
        Mode.ANTHROPIC_PARALLEL_TOOLS: reask_anthropic_tools,
        # Cohere modes
        Mode.COHERE_TOOLS: reask_cohere_tools,
        Mode.COHERE_JSON_SCHEMA: reask_cohere_tools,
        # Gemini/Google modes
        Mode.GEMINI_TOOLS: reask_gemini_tools,
        Mode.GEMINI_JSON: reask_gemini_json,
        Mode.GENAI_TOOLS: reask_genai_tools,
        Mode.GENAI_STRUCTURED_OUTPUTS: reask_genai_structured_outputs,
        # VertexAI modes
        Mode.VERTEXAI_TOOLS: reask_vertexai_tools,
        Mode.VERTEXAI_JSON: reask_vertexai_json,
        Mode.VERTEXAI_PARALLEL_TOOLS: reask_vertexai_tools,
        # Cerebras modes
        Mode.CEREBRAS_TOOLS: reask_cerebras_tools,
        Mode.CEREBRAS_JSON: reask_default,
        # Fireworks modes
        Mode.FIREWORKS_TOOLS: reask_fireworks_tools,
        Mode.FIREWORKS_JSON: reask_fireworks_json,
        # Writer modes
        Mode.WRITER_TOOLS: reask_writer_tools,
        Mode.WRITER_JSON: reask_writer_json,
        # Bedrock modes
        Mode.BEDROCK_TOOLS: reask_bedrock_tools,
        Mode.BEDROCK_JSON: reask_bedrock_json,
        # Perplexity modes
        Mode.PERPLEXITY_JSON: reask_perplexity_json,
        # OpenRouter modes
        Mode.OPENROUTER_STRUCTURED_OUTPUTS: reask_default,
        # XAI modes
        Mode.XAI_JSON: reask_xai_json,
        Mode.XAI_TOOLS: reask_xai_tools,
    }

    if mode in REASK_HANDLERS:
        return REASK_HANDLERS[mode](kwargs_copy, response, exception)
    else:
        return reask_default(kwargs_copy, response, exception)",instructor/process_response.py,
survived,"    def analyze_codebase_patterns(self, project_path: str, file_patterns: List[str] = None) -> Dict[str, Any]:
        """"""
        Analyze codebase to extract patterns, conventions, and architectural insights.
        
        Args:
            project_path (str): Path to the project directory to analyze
            file_patterns (List[str]): Optional list of file patterns to focus on (e.g., ['*.py', '*.js'])
            
        Returns:
            Dict[str, Any]: Comprehensive analysis of codebase patterns
        """"""
        if not os.path.exists(project_path):
            return {""error"": f""Project path does not exist: {project_path}""}
        
        if file_patterns is None:
            file_patterns = ['*.py', '*.js', '*.ts', '*.java', '*.cpp', '*.c', '*.rb', '*.go']
        
        analysis = {
            ""project_structure"": self._analyze_project_structure(project_path),
            ""code_patterns"": self._extract_code_patterns(project_path, file_patterns),
            ""naming_conventions"": self._analyze_naming_conventions(project_path, file_patterns),
            ""import_patterns"": self._analyze_import_patterns(project_path, file_patterns),
            ""architecture_insights"": self._analyze_architecture(project_path),
            ""documentation_style"": self._analyze_documentation_style(project_path)
        }
        
        return analysis
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def test_basic_functionality():
    """"""Test basic functionality of ContextAgent methods.""""""
    print(""\nüß™ Testing Basic Functionality..."")
    
    try:
        from praisonaiagents import create_context_agent
        
        context_agent = create_context_agent()
        
        # Test codebase analysis with current directory
        test_path = str(Path(__file__).parent)
        analysis = context_agent.analyze_codebase_patterns(test_path)
        assert isinstance(analysis, dict), ""analyze_codebase_patterns should return dict""
        print(""‚úÖ analyze_codebase_patterns works correctly"")
        
        # Test context document generation
        context_doc = context_agent.generate_context_document(
            project_path=test_path,
            requirements=""Test feature implementation""
        )
        assert isinstance(context_doc, str), ""generate_context_document should return string""
        assert len(context_doc) > 100, ""Context document should be substantial""
        print(""‚úÖ generate_context_document works correctly"")
        
        # Test validation loop creation
        validation = context_agent.create_validation_loop(
            implementation_requirements=""Test implementation"",
            success_criteria=[""Test passes"", ""Code works""]
        )
        assert isinstance(validation, dict), ""create_validation_loop should return dict""
        assert 'validation_steps' in validation, ""Should have validation_steps""
        print(""‚úÖ create_validation_loop works correctly"")
        
        # Test prompt enhancement
        enhanced = context_agent.enhance_prompt_with_context(
            base_prompt=""Test prompt"",
            context_data={""test"": ""data""}
        )
        assert isinstance(enhanced, str), ""enhance_prompt_with_context should return string""
        assert len(enhanced) > len(""Test prompt""), ""Enhanced prompt should be longer""
        print(""‚úÖ enhance_prompt_with_context works correctly"")
        
        # Test PRP generation
        prp = context_agent.generate_prp(
            feature_request=""Test feature"",
            context_analysis=analysis
        )
        assert isinstance(prp, str), ""generate_prp should return string""
        assert ""PRP"" in prp, ""Should contain PRP reference""
        print(""‚úÖ generate_prp works correctly"")
        
        return True
        
    except Exception as e:
        print(f""‚ùå Functionality test failed: {e}"")
        return False
",test_context_agent.py,
survived,"    def _format_context_data(self, context_data: Dict[str, Any]) -> str:
        """"""Format context data for prompt enhancement.""""""
        return json.dumps(context_data, indent=2)
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _generate_executable_tests(self, requirements: str, criteria: List[str]) -> List[Dict[str, str]]:
        """"""Generate executable test specifications.""""""
        return [{""type"": ""unit_test"", ""description"": f""Test {criterion}""} for criterion in criteria]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def create_context_agent(llm: Optional[Union[str, Any]] = None, **kwargs) -> ContextAgent:
    """"""
    Factory function to create a ContextAgent with sensible defaults.
    
    Args:
        llm: Language model to use (e.g., ""gpt-4o-mini"", ""claude-3-haiku"")
        **kwargs: Additional arguments to pass to ContextAgent constructor
        
    Returns:
        ContextAgent: Configured ContextAgent instance
    """"""
    if llm is None:
        llm = ""gpt-4o-mini""  # Default to a capable model for context generation
    
    return ContextAgent(llm=llm, **kwargs)",src/praisonai-agents/praisonaiagents/agent/context_agent.py,
survived,"    def _message(self) -> str:
        return f""Missing parameters in docstring: {self.params}""",dev/clint/src/clint/rules/missing_docstring_param.py,MissingDocstringParam
survived,"    def _message(self) -> str:
        return ""This function looks like a test, but its name does not start with 'test_'.""",dev/clint/src/clint/rules/test_name_typo.py,TestNameTypo
survived,"    def _message(self) -> str:
        return ""Do not use RST style. Use Google style instead.""",dev/clint/src/clint/rules/no_rst.py,NoRst
survived,"    def _message(self) -> str:
        return (
            ""Invalid usage of `@experimental` decorator. It must be used with a `version` ""
            ""argument that is a valid semantic version string.""
        )
",dev/clint/src/clint/rules/invalid_experimental_decorator.py,InvalidExperimentalDecorator
survived,"    def _message(self) -> str:
        return (
            f""`{self.full_name}` is not allowed to use. Only {self.allowlist} are allowed. ""
            ""You can extend `tool.clint.typing-extensions-allowlist` in `pyproject.toml` if needed ""
            ""but make sure that the version requirement for `typing-extensions` is compatible with ""
            ""the added types.""
        )",dev/clint/src/clint/rules/typing_extensions.py,TypingExtensions
survived,"    def _message(self) -> str:
        return ""Builtin modules must be imported at the top level.""",dev/clint/src/clint/rules/lazy_builtin_import.py,LazyBuiltinImport
survived,"    def _message(self) -> str:
        return ""Do not delete `os.environ` in test directly. Use `monkeypatch.delenv` (https://docs.pytest.org/en/stable/reference/reference.html#pytest.MonkeyPatch.delenv).""
",dev/clint/src/clint/rules/os_environ_delete_in_test.py,OsEnvironDeleteInTest
survived,"    def __init__(self, function_name: str) -> None:
        self.function_name = function_name
",dev/clint/src/clint/rules/unknown_mlflow_function.py,UnknownMlflowFunction
survived,"    def _message(self) -> str:
        return (
            ""Abstract method should only contain a single statement/expression, ""
            ""and it must be `pass`, `...`, or a docstring.""
        )
",dev/clint/src/clint/rules/invalid_abstract_method.py,InvalidAbstractMethod
survived,"def test_webhook_test_failed_endpoint(mlflow_client: MlflowClient, app_client: AppClient) -> None:
    # Create webhook pointing to non-existent endpoint
    webhook = mlflow_client.create_webhook(
        name=""failed_webhook"",
        url=app_client.get_url(""/nonexistent-endpoint""),
        events=[WebhookEvent.REGISTERED_MODEL_CREATED],
    )

    # Test the webhook
    result = mlflow_client.test_webhook(webhook.webhook_id)

    # Check that the test failed
    assert result.success is False
    assert result.response_status == 404
    assert result.error_message is None  # No error message for HTTP errors
    assert result.response_body is not None  # Should contain error response
",tests/webhooks/test_e2e.py,
survived,"    def test_label(self, mock_label):
        """"""Test the implementation of the label method""""""
        # Set up the mock to return the value we want
        mock_label.return_value = ""nucleus""
        
        # Test label retrieval
        label = self.oi.label(""GO:0005634"")
        self.assertEqual(label, ""nucleus"")
        
        # Verify the mock was called correctly
        mock_label.assert_called_with(""GO:0005634"")
",tests/test_implementations/test_ols.py,TestOlsImplementation
survived,"    def test_dtype_preservation(self):
        # Test float32
        data32 = np.random.randn(5, 20).astype(np.float32)
        result32 = nancovmatrix(data32)
        assert result32.dtype == np.float32

        # Test float64
        data64 = np.random.randn(5, 20).astype(np.float64)
        result64 = nancovmatrix(data64)
        assert result64.dtype == np.float64
",numbagg/test/test_nancovmatrix.py,TestNanCovMatrix
survived,"    def test_axis_parameter(self):
        # Test with different axes
        data = np.random.randn(3, 4, 5)

        # Default should correlate along last axis
        result_default = nancorrmatrix(data)
        assert result_default.shape == (3, 4, 4)

        # Test with axis=0
        result_0 = nancorrmatrix(data, axis=0)
        assert result_0.shape == (4, 5, 5)

        # Test with axis=1
        result_1 = nancorrmatrix(data, axis=1)
        assert result_1.shape == (3, 5, 5)
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix
survived,"    def nested_mcp_server(self, nested_middleware: RecordingMiddleware):
        mcp = FastMCP(name=""Nested MCP"")

        @mcp.tool
        def add(a: int, b: int) -> int:
            return a + b

        @mcp.resource(""resource://test"")
        def test_resource() -> str:
            return ""test resource""

        @mcp.resource(""resource://test-template/{x}"")
        def test_resource_with_path(x: int) -> str:
            return f""test resource with {x}""

        @mcp.prompt
        def test_prompt(x: str) -> str:
            return f""test prompt with {x}""

        @mcp.tool
        async def progress_tool(context: Context) -> None:
            await context.report_progress(progress=1, total=10, message=""test"")

        @mcp.tool
        async def log_tool(context: Context) -> None:
            await context.info(message=""test log"")

        @mcp.tool
        async def sample_tool(context: Context) -> None:
            await context.sample(""hello"")

        mcp.add_middleware(nested_middleware)

        return mcp
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    def add_middleware(self, middleware: MCPMiddleware) -> None:
        self.middleware.append(middleware)
",src/fastmcp/server/server.py,FastMCP
survived,"    async def test_list_resources_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.list_resources()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_resources"", times=1)

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(method=""resources/list"", times=3)
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(hook=""on_list_resources"", times=1)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    async def test_read_resource_on_parent_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.read_resource(""resource://test"")

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/read"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_read_resource"", times=1)

        assert nested_middleware.assert_called(times=0)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
deleted,"    async def _middleware_get_prompt(
        self,
        name: str,
        arguments: dict[str, Any] | None = None,
    ) -> GetPromptResult:
        """"""
        Get a prompt with middleware.
        """"""

        async def _handler(
            context: MiddlewareContext[mcp.types.GetPromptRequestParams],
        ) -> GetPromptResult:
            return await self._get_prompt(
                name=context.message.name,
                arguments=context.message.arguments,
            )

        mw_context = MiddlewareContext(
            message=mcp.types.GetPromptRequestParams(name=name, arguments=arguments),
            source=""client"",
            type=""request"",
            method=""prompts/get"",
            fastmcp_context=fastmcp.server.dependencies.get_context(),
        )
        return await self._apply_middleware(mw_context, _handler)
",src/fastmcp/server/server.py,FastMCP
survived,"def _load_native_library():
    """"""
    Load the native wvlet library for the current platform.
    
    Returns:
        ctypes.CDLL: The loaded native library, or None if not available.
    """"""
    system = platform.system()
    machine = platform.machine()
    
    # Map platform to library path
    lib_map = {
        ('Linux', 'x86_64'): 'linux_x86_64/libwvlet.so',
        ('Linux', 'aarch64'): 'linux_aarch64/libwvlet.so',
        ('Darwin', 'arm64'): 'darwin_arm64/libwvlet.dylib',
    }
    
    key = (system, machine)
    if key not in lib_map:
        return None
    
    # Get the library path relative to this file
    lib_dir = os.path.dirname(os.path.abspath(__file__))
    lib_path = os.path.join(lib_dir, 'libs', lib_map[key])
    
    if not os.path.exists(lib_path):
        return None
    
    try:
        lib = ctypes.CDLL(lib_path)
        # Set the return type for wvlet_compile_query
        lib.wvlet_compile_query.restype = ctypes.c_char_p
        lib.wvlet_compile_query.argtypes = [ctypes.c_char_p]
        return lib
    except Exception:
        return None
",sdks/python/wvlet/compiler.py,
deleted,"def get_git_version():
    """"""Get the current version from Git tags or fallback to 'latest'.""""""
    try:
        # Check if current HEAD matches any tag (exact release)
        result = subprocess.run(
            [""git"", ""tag"", ""--points-at"", ""HEAD""],
            capture_output=True,
            text=True,
            cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        )
        if result.returncode == 0 and result.stdout.strip():
            tags = result.stdout.strip().split('\n')
            # Prefer semantic version tags
            for tag in tags:
                if tag.startswith('v') and len(tag.split('.')) >= 3:
                    return tag[1:]  # Remove 'v' prefix
            # Fallback to first tag
            return tags[0][1:] if tags[0].startswith('v') else tags[0]
    except (subprocess.SubprocessError, FileNotFoundError):
        pass

    try:
        # Get the latest semantic version tag for development builds
        result = subprocess.run(
            [""git"", ""tag"", ""--list""],
            capture_output=True,
            text=True,
            cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        )
        if result.returncode == 0 and result.stdout.strip():
            # Filter for semantic version tags and get the latest
            tags = result.stdout.strip().split('\n')
            version_tags = []
            for tag in tags:
                if tag.startswith('v') and len(tag.split('.')) >= 3:
                    try:
                        # Check if it's a proper semantic version
                        parts = tag[1:].split('.')
                        if len(parts) >= 3 and all(part.isdigit() for part in parts[:3]):
                            version_tags.append(tag)
                    except (ValueError, IndexError):
                        continue
            
            if version_tags:
                # Sort by version number and get the latest
                version_tags.sort(key=lambda x: [int(part) for part in x[1:].split('.')[:3]])
                latest_tag = version_tags[-1]
                
                # Check if we're on the exact tag or ahead of it
                try:
                    result = subprocess.run(
                        [""git"", ""describe"", ""--tags"", ""--exact-match"", ""HEAD""],
                        capture_output=True,
                        text=True,
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                    )
                    if result.returncode == 0:
                        # We're on an exact tag
                        return latest_tag[1:]
                except (subprocess.SubprocessError, FileNotFoundError):
                    pass
                
                # We're ahead of the latest tag, add dev suffix
                return f""{latest_tag[1:]}.dev""
    except (subprocess.SubprocessError, FileNotFoundError):
        pass
    
    # Default fallback
    return ""latest""
",docs/source/conf.py,
survived,"def test_inferred_parameters_in_actual_measurement():
    """"""
    Test the full measurement flow to ensure inferred parameters are saved correctly.
    """"""
    
    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = Path(temp_dir) / ""test.db""
        initialise_or_create_database_at(db_path)
        
        # Create experiment  
        exp = new_experiment(""test_exp"", sample_name=""test_sample"")
        
        # Create mock instruments
        dac = DummyInstrument(""dac"", gates=[""ch1""])
        
        # Create delegate parameter
        del_param = DelegateParameter(""del_param_1"", label=""del param 1"", source=dac.ch1)
        
        # Create measurement
        meas = Measurement(name=""test_measurement"", exp=exp)
        
        # Register parameters  
        meas.register_parameter(dac.ch1)
        meas.register_parameter(del_param, basis=(dac.ch1,))
        
        # Run measurement
        with meas.run() as datasaver:
            # Set values and add results
            dac.ch1.set(0.5)
            del_param.set(0.5) 
            
            datasaver.add_result(
                (dac.ch1, dac.ch1()),
                (del_param, del_param()),
            )
            
        # Retrieve the dataset
        dataset = datasaver.dataset
        
        # Get parameter data - both parameters should be present
        param_data = dataset.get_parameter_data()
        
        # Both parameters should be in the dataset
        assert ""dac_ch1"" in param_data, ""dac_ch1 should be in parameter data""
        assert ""del_param_1"" in param_data, ""del_param_1 should be in parameter data""
        
        # Check that the data is correct
        assert len(param_data[""dac_ch1""][""dac_ch1""]) == 1
        assert len(param_data[""del_param_1""][""del_param_1""]) == 1",tests/dataset/measurement/test_inferred_parameters_fix.py,
survived,"async def test_mcp_client_autogen_pagination(tmp_path: Path) -> None:
    script = tmp_path / ""app.py""
    script.write_text(
        textwrap.dedent(
            """"""
            from sqlalchemy import ForeignKey
            from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
            from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship

            from enrichmcp import EnrichMCP
            from enrichmcp.sqlalchemy import (
                EnrichSQLAlchemyMixin,
                include_sqlalchemy_models,
                sqlalchemy_lifespan,
            )

            class Base(DeclarativeBase, EnrichSQLAlchemyMixin):
                pass

            class User(Base):
                __tablename__ = ""users""
                id: Mapped[int] = mapped_column(primary_key=True, info={""description"": ""ID""})
                name: Mapped[str] = mapped_column(info={""description"": ""Name""})
                orders: Mapped[list[""Order""]] = relationship(
                    back_populates=""user"", info={""description"": ""Orders""}
                )

            class Order(Base):
                __tablename__ = ""orders""
                id: Mapped[int] = mapped_column(primary_key=True, info={""description"": ""ID""})
                user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))
                user: Mapped[User] = relationship(
                    back_populates=""orders"", info={""description"": ""User""}
                )

            async def seed(session: AsyncSession) -> None:
                user = User(id=1, name=""Alice"")
                orders = [Order(id=i, user=user) for i in range(1, 4)]
                session.add_all([user, *orders])

            engine = create_async_engine(""sqlite+aiosqlite:///:memory:"")
            lifespan = sqlalchemy_lifespan(Base, engine, seed=seed)
            app = EnrichMCP(""Test"", ""Desc"", lifespan=lifespan)
            include_sqlalchemy_models(app, Base)

            if __name__ == ""__main__"":
                app.run()
            """"""
        )
    )

    config = {""mcpServers"": {""app"": {""command"": sys.executable, ""args"": [str(script)]}}}
    client = MCPClient(config=config)
    session = await client.create_session(""app"")

    result = await session.connector.call_tool(
        ""get_userenrichmodel_orders"",
        {""page"": 1, ""page_size"": 2, ""kwargs"": {""user_id"": 1}},
    )
    data = json.loads(result.content[0].text)
    assert len(data[""items""]) == 2
    assert data[""has_next""]

    result2 = await session.connector.call_tool(
        ""get_userenrichmodel_orders"",
        {""page"": 2, ""page_size"": 2, ""kwargs"": {""user_id"": 1}},
    )
    data2 = json.loads(result2.content[0].text)
    assert len(data2[""items""]) == 1
    assert not data2[""has_next""]

    await client.close_all_sessions()",tests/test_sqlalchemy_mcp_use.py,
survived,"def test_register_mesh_retries(monkeypatch: pytest.MonkeyPatch) -> None:
    client = StubClient()
    fake_adk = types.SimpleNamespace(Client=lambda: client)
    monkeypatch.setattr(biotech_agent, ""adk"", fake_adk)
    monkeypatch.setattr(asyncio, ""sleep"", no_sleep)

    agent = biotech_agent.BiotechAgent()
    asyncio.run(agent._register_mesh())
    assert client.calls == 3
",tests/test_register_mesh_backoff.py,
survived,"async def test_policy_checker_deny():
    config = GuardrailConfig(rules=[GuardrailRule(name=""block"", pattern=""bad"")])
    checker = PolicyChecker(config)

    await checker.run(""good text"")  # should pass

    with pytest.raises(ValueError):
        await checker.run(""bad text"")
",tests/test_policy_checker.py,
survived,"                    async def _check(text: str) -> str:
                        if p.search(text):
                            raise ValueError(f""Policy violation: {r.name}"")
                        return text
",src/meta_agent/policy.py,PolicyChecker
survived,"    def add_from_config(self, config: GuardrailConfig) -> None:
        """"""Add checks from a :class:`GuardrailConfig`.""""""

        for rule in config.rules:
            pattern = re.compile(rule.pattern)

            def _make_check(
                p: re.Pattern[str], r: GuardrailRule
            ) -> Callable[[str], Awaitable[str]]:
                if r.action is GuardrailAction.REDACT:

                    async def _check(text: str) -> str:
                        return p.sub(""[REDACTED]"", text)

                else:  # DENY or FLAG -> raise error on match

                    async def _check(text: str) -> str:
                        if p.search(text):
                            raise ValueError(f""Policy violation: {r.name}"")
                        return text

                return _check

            self.checks.append(_make_check(pattern, rule))
",src/meta_agent/policy.py,PolicyChecker
survived,"    def __init__(
        self,
        bus: object,
        ledger: object,
        repo: str | Path,
        patch_file: str | Path,
        *,
        metric_file: str = ""metric.txt"",
        log_file: str = ""improver_log.json"",
        allowed: Sequence[str] | None = None,
        backend: str = ""gpt-4o"",
        island: str = ""default"",
    ) -> None:
        super().__init__(""self_improver"", bus, ledger, backend=backend, island=island)
        self.repo = Path(repo)
        self.patch_file = Path(patch_file)
        self.metric_file = metric_file
        self.log_file = log_file
        self.allowed = list(allowed or [""**""])
",src/agents/self_improver_agent.py,SelfImproverAgent
survived,"    def __init__(self, settings: config.Settings) -> None:
        super().__init__(settings)
        self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_safety_agent.py,CaptureBus
survived,"    def get_model(self, model_name: str) -> str:
        """"""Get actual model name from alias""""""
        if model_name.lower() in self.AVAILABLE_MODELS:
            return self.AVAILABLE_MODELS[model_name.lower()]
        return model_name
",webscout/Provider/TTI/aiarta.py,AIArtaImager
survived,"    def generate(
        self,
        prompt: str,
        max_retries: int = 3,
        retry_delay: int = 5,
    ) -> List[bytes]:
        """"""Generate some fire images from your prompt! üé®

        Args:
            prompt (str): Your image description
            max_retries (int): Max retry attempts if something fails (default: 3)
            retry_delay (int): Seconds to wait between retries (default: 5)

        Returns:
            List[bytes]: Your generated images as bytes

        Raises:
            ValueError: If the inputs ain't valid
            RequestException: If the API calls fail after retries
        """"""
        # Input validation
        if not prompt:
            raise ValueError(""Yo fam, the prompt can't be empty! ü§î"")

        self.prompt = prompt
        response = []
        
        # Get request ID
        data = {""prompt"": prompt}
        resp = self.session.post(self.request_id_endpoint, json=data, timeout=self.timeout)
        resp.raise_for_status()
        request_id = resp.json()[""requestId""]

        # Poll for results
        for attempt in range(max_retries):
            try:
                # Get image URLs
                resp = self.session.get(
                    f""{self.image_response_endpoint}?requestId={request_id}"",
                    timeout=self.timeout
                )
                resp.raise_for_status()
                image_data = resp.json()

                if ""results"" in image_data and len(image_data[""results""]) >= 2:
                    # Get provider names
                    provider_resp = self.session.post(
                        self.image_provider_endpoint,
                        json={""requestId"": request_id, ""preference"": 0},
                        timeout=self.timeout
                    )
                    provider_resp.raise_for_status()
                    provider_data = provider_resp.json()

                    # Download images
                    for i, url in enumerate(image_data[""results""][:2]):
                        img_resp = self.session.get(url, timeout=self.timeout)
                        img_resp.raise_for_status()
                        response.append(img_resp.content)
                    
                    break
                else:
                    if attempt == max_retries - 1:
                        raise RequestException(""Failed to get image results after max retries"")
                    time.sleep(retry_delay)

            except RequestException as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(retry_delay)

        return response
",webscout/Provider/TTI/imgsys.py,ImgSys
survived,"    def generate(
        self,
        prompt: str,
        amount: int = 1,
        model: str = ""flux_1_schnell"",
        size: str = ""1_1"",
        is_public: bool = False,
        max_retries: int = 3,
        retry_delay: int = 5
    ) -> List[bytes]:
        """"""Generate some fire images from your prompt! üé®

        Examples:
            >>> provider = FastFluxImager()
            >>> # Basic usage
            >>> images = provider.generate(""Cool art"")
            >>> # Advanced usage
            >>> images = provider.generate(
            ...     prompt=""Epic dragon"",
            ...     amount=2,
            ...     model=""flux_1_dev"",
            ...     size=""16_9""
            ... )

        Args:
            prompt (str): Your image description
            amount (int): How many images you want (default: 1)
            model (str): Model to use - check AVAILABLE_MODELS (default: ""flux_1_schnell"")
            size (str): Image size ratio (default: ""1_1"")
            is_public (bool): Whether to make the image public (default: False)
            max_retries (int): Max retry attempts if something fails (default: 3)
            retry_delay (int): Seconds to wait between retries (default: 5)

        Returns:
            List[bytes]: Your generated images

        Raises:
            ValueError: If the inputs ain't valid
            RequestException: If the API calls fail after retries
        """"""
        if not prompt:
            raise ValueError(""Yo fam, the prompt can't be empty! ü§î"")
        if not isinstance(amount, int) or amount < 1:
            raise ValueError(""Amount needs to be a positive number! üìà"")
        if model not in self.AVAILABLE_MODELS:
            raise ValueError(f""Model must be one of {self.AVAILABLE_MODELS}! üéØ"")
        if size not in self.AVAILABLE_SIZES:
            raise ValueError(f""Size must be one of {self.AVAILABLE_SIZES}! üìè"")

        self.prompt = prompt
        response = []

        # Prepare payload
        payload = {
            ""prompt"": prompt,
            ""model"": model,
            ""size"": size,
            ""isPublic"": is_public
        }

        for i in range(amount):
            for attempt in range(max_retries):
                try:
                    if self.logging:
                        print(f""Generating image {i+1}/{amount}... üé®"")
                    
                    resp = self.session.post(
                        self.api_endpoint,
                        json=payload,
                        timeout=self.timeout
                    )
                    resp.raise_for_status()
                    result = resp.json()
                    
                    if result and 'result' in result:
                        # Get base64 data and remove header
                        image_data = result['result']
                        base64_data = image_data.split(',')[1]
                        
                        # Decode base64 data
                        image_bytes = base64.b64decode(base64_data)
                        response.append(image_bytes)

                        break
                    else:
                        raise RequestException(""Invalid response format"")
                        
                except RequestException as e:
                    if attempt == max_retries - 1:
                        raise RequestException(f""Failed to generate image after {max_retries} attempts: {e}"")

                    time.sleep(retry_delay)

        return response
",webscout/Provider/TTI/fastflux.py,FastFluxImager
survived,"    def _create_payload(self, prompt: str, model: str, style: str, aspect_ratio: str) -> dict:
        """"""Create the API request payload üì¶

        Args:
            prompt (str): The image generation prompt
            model (str): Model to use
            style (str): Style to apply
            aspect_ratio (str): Aspect ratio

        Returns:
            dict: API request payload
        """"""
        return {
            ""prompt"": prompt,
            ""model"": model,
            ""style"": style,
            ""aspect_ratio"": aspect_ratio
        }
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager
survived,"def grad_of_fn(klong, fn, x):
    """"""Return gradient of Klong or Python function ``fn`` at ``x``.""""""
    def call_fn(v):
        if isinstance(fn, (KGSym, KGLambda)):
            return klong.call(KGCall(fn, [v], 1))
        elif isinstance(fn, KGCall):
            return klong.call(KGCall(fn.a, [v], fn.arity))
        elif isinstance(fn, KGFn):
            return klong.call(KGCall(fn.a, [v], fn.arity))
        else:
            return fn(v)
    return numeric_grad(call_fn, x)",klongpy/autograd.py,
survived,"    def test_matrix_grad_torch(self):
        klong = KlongInterpreter()
        klong('A::Àô[2 2]:^!4')
        klong('B::[2 2]:^!4')
        r = klong('(A ‚àá {+/(+/ (A*B)) })')

        A = torch.arange(4, dtype=torch.float64, requires_grad=True).reshape(2,2)
        B = torch.arange(4, dtype=torch.float64).reshape(2,2)
        loss = (A * B).sum()
        loss.backward()
        self.assertTrue(np.allclose(r, A.grad.numpy(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"def eval_monad_track(a):
    """"""

        Àôa                                                    [Track]

        Identity operator used when marking values for gradient tracking.

    """"""
    return a
",klongpy/monads.py,
survived,"    def test_scalar_grad_torch(self):
        klong = KlongInterpreter()
        klong['sin'] = lambda x: np.sin(x)
        klong['cos'] = lambda x: np.cos(x)
        klong('g::‚àá{sin(x)+x*x}')
        r = klong('g(3.14)')
        x = torch.tensor(3.14, dtype=torch.float64, requires_grad=True)
        f = torch.sin(x) + x * x
        f.backward()
        self.assertTrue(np.isclose(r, x.grad.item(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"def test_with_retry_async(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    calls = {""n"": 0}

    async def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 2:
            raise ValueError(""fail"")
        return ""ok""

    wrapped = retry.with_retry(func, max_tries=2)
    result = asyncio.run(wrapped())
    assert result == ""ok""
    assert calls[""n""] == 2
",tests/test_retry_wrapper.py,
survived,"async def test_send_http_error():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        with pytest.raises(ValueError):
            await client.send(""trace"", {""d"": 1})
        await client.close()
",tests/unit/test_telemetry_client.py,
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        await self._session.close()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    def __init__(
        self,
        endpoints: Dict[str, EndpointConfig],
        *,
        rate_limit: int = 5,
        timeout: int = 10,
    ) -> None:
        if not endpoints:
            raise ValueError(""At least one endpoint must be configured"")
        self.endpoints = endpoints
        self.timeout = timeout
        self._sem = asyncio.Semaphore(rate_limit)
        self._session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=None)
        )
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    def __init__(
        self,
        endpoints: Dict[str, EndpointConfig],
        *,
        rate_limit: int = 5,
        timeout: int = 10,
    ) -> None:
        if not endpoints:
            raise ValueError(""At least one endpoint must be configured"")
        self.endpoints = endpoints
        self.timeout = timeout
        self._sem = asyncio.Semaphore(rate_limit)
        self._session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=None)
        )
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"        def __init__(self, *_, **__):
            pass
",src/meta_agent/services/telemetry_client.py,TCPConnector
survived,"def test_form(monkeypatch):
    inputs = iter([""foo"", ""bar""])
    monkeypatch.setattr(""builtins.input"", lambda _: next(inputs))
    inter = Interactive()
    result = inter.form([""first"", ""second""])
    assert result == {""first"": ""foo"", ""second"": ""bar""}",tests/ux/test_interactive.py,
survived,"    def test_stub_producer(self):
        class Stub:
            def __init__(self, bootstrap_servers=None, value_serializer=None, linger_ms=None):
                self.args = (bootstrap_servers, value_serializer, linger_ms)
        os.environ[""ALPHA_KAFKA_BROKER""] = ""a:1,b:2 ,""
        orig = base_mod.KafkaProducer
        base_mod.KafkaProducer = Stub
        prod = base_mod._kafka_producer()
        self.assertIsInstance(prod, Stub)
        self.assertEqual(prod.args[0], [""a:1"", ""b:2""])
        base_mod.KafkaProducer = orig
        os.environ.pop(""ALPHA_KAFKA_BROKER"", None)
",tests/test_base_helpers.py,TestKafkaProducer
survived,"    def setUp(self):
        self.agent = SupplyChainAgent()
",tests/test_supply_chain_agent.py,TestSupplyChainAgent
survived,"    def test_build_network(self):
        g = self.agent._build_network()
        self.assertEqual(len(g.nodes), 4)
        self.assertEqual(len(g.edges), 3)
",tests/test_supply_chain_agent.py,TestSupplyChainAgent
survived,"    def test_energy_calc(self):
        ops = [
            {""machine"": ""m1"", ""start"": 0, ""end"": 5},
            {""machine"": ""m1"", ""start"": 5, ""end"": 15},
        ]
        rate = {""m1"": 2.0}
        payload = self.agent._energy_calc(ops, rate)
        self.assertAlmostEqual(payload[""kwh""], 30.0)
        expected_co2 = 30.0 * self.agent.cfg.energy_rate_co2
        self.assertAlmostEqual(payload[""co2_kg""], expected_co2)
",tests/test_manufacturing_agent.py,TestManufacturingAgent
survived,"    def test_build_async_returns_ops(self):
        jobs = [[{""machine"": ""m1"", ""proc"": 2}, {""machine"": ""m2"", ""proc"": 3}]]
        req = {""jobs"": jobs, ""horizon"": 10}
        result = asyncio.run(self.agent._build_async(req))
        payload = json.loads(result)[""payload""]
        self.assertIn(""ops"", payload)
        self.assertIsInstance(payload[""ops""], list)
        self.assertGreaterEqual(payload[""horizon""], 5)
",tests/test_manufacturing_agent.py,TestManufacturingAgent
survived,"def _make_request(ip: str) -> Request:
    scope = {
        ""type"": ""http"",
        ""method"": ""GET"",
        ""path"": ""/"",
        ""headers"": [],
        ""client"": (ip, 0),
    }
    return Request(scope)  # type: ignore[arg-type]
",tests/test_rate_limiter_eviction.py,
survived,"async def get_user_orders(user_id: int, ctx: EnrichContext) -> list[""OrderEnrichModel""]:
    """"""Get all orders for a specific user.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        result = await session.execute(
            select(Order).where(Order.user_id == user_id).order_by(Order.created_at.desc())
        )
        orders = result.scalars().all()

        return [
            OrderEnrichModel(
                id=order.id,
                order_number=order.order_number,
                user_id=order.user_id,
                status=order.status,
                total_amount=order.total_amount,
                created_at=order.created_at,
                updated_at=order.updated_at,
                shipping_address=order.shipping_address,
                notes=order.notes,
            )
            for order in orders
        ]
",examples/sqlalchemy_shop/app.py,
survived,"async def get_order_user(order_id: int, ctx: EnrichContext) -> UserEnrichModel | None:
    """"""Get the user who placed a specific order.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        order = await session.get(Order, order_id)
        if not order:
            return None

        # Load the user (SQLAlchemy will handle the join)
        await session.refresh(order, [""user""])
        user = order.user

        return UserEnrichModel(
            id=user.id,
            username=user.username,
            email=user.email,
            full_name=user.full_name,
            is_active=user.is_active,
            created_at=user.created_at,
        )
",examples/sqlalchemy_shop/app.py,
survived,"    def test_model_documentation(self):
        """"""Test that model docstring is preserved.""""""

        class Base(DeclarativeBase):
            pass

        class Order(Base, EnrichSQLAlchemyMixin):
            """"""Order represents a customer purchase.""""""

            __tablename__ = ""orders""

            id: Mapped[int] = mapped_column(primary_key=True)
            total: Mapped[float] = mapped_column()

        OrderEnrichModel = Order.__enrich_model__()
        assert OrderEnrichModel.__doc__ == ""Order represents a customer purchase.""
",tests/test_sqlalchemy_integration.py,TestBasicModel
survived,"    def test_sqlalchemy_model_reference_stored(self):
        """"""Test that reference to original SQLAlchemy model is stored.""""""

        class Base(DeclarativeBase):
            pass

        class Order(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""orders""
            id: Mapped[int] = mapped_column(primary_key=True)

        OrderEnrichModel = Order.__enrich_model__()
        assert hasattr(OrderEnrichModel, ""_sqlalchemy_model"")
        assert OrderEnrichModel._sqlalchemy_model is Order
",tests/test_sqlalchemy_integration.py,TestEdgeCases
survived,"    def __init__(self, ledger_path: str | pathlib.Path):
        self.path = pathlib.Path(ledger_path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,LineageTracer
survived,"def _bell(value: float, ideal: float, sigma: float = 0.15) -> float:
    """"""Gaussian-shaped around ``ideal`` (15% std default).""""""
    return math.exp(-0.5 * ((value - ideal) / (sigma * ideal)) ** 2)
",alpha_factory_v1/demos/era_of_experience/reward_backends/fitness_reward.py,
survived,"    def __init__(self):
        self.gen=POETGenerator()
        self.envs=[self.gen.propose() for _ in range(CFG.env_batch)]
        self.learners=[Learner(e) for e in self.envs]
        self.stop=False
        A2ABus.subscribe(""orch"",self._on_cmd)
        LOG.info(""Orchestrator online with %d envs"", CFG.env_batch)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Orchestrator
survived,"    def loop(self):
        obs=[e.reset() for e in self.envs]
        for t in range(CFG.max_steps):
            if self.stop: break
            for i,(env,learner) in enumerate(zip(self.envs,self.learners)):
                a=learner.act(obs[i])
                nxt,r,done,_=env.step(a)
                learner.remember(obs[i],r)
                loss=learner.train_once()
                obs[i]=env.reset() if done else nxt
                if t%CFG.ui_tick==0 and i==0:
                    A2ABus.publish(""ui"",{""t"":t,""r"":r,""loss"":loss})
        LOG.info(""Orchestrator loop exit at t=%d"", t)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Orchestrator
survived,"    def handle(self,msg):
        if ""loss"" in msg and (np.isnan(msg[""loss""]) or msg[""loss""]>1e3):
            LOG.warning(""[SAFETY] triggered ‚Äì halting learner"")
            self.emit(""orch"",{""cmd"":""stop""})
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,BasicSafetyAgent
survived,"    def run(self, code: str, func_name: str, *args, **kw):
        loc: Dict[str,Any] = {}
        with self:
            exec(code, {}, loc)
        if func_name not in loc:
            raise AttributeError(f""{func_name} not found"")
        return loc[func_name](*args, **kw)
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,SafeExec
survived,"    def __init__(self, tps: float = 3.0):
        self._tps = float(tps)
        self._allow = self._tps
        self._last = time.perf_counter()
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,RateLimiter
survived,"def emit_notebook(fp:Path=Path(""alpha_asi_world_model_demo.ipynb"")):
    import nbformat as nbf
    nb=nbf.v4.new_notebook()
    nb.cells=[nbf.v4.new_markdown_cell(""# Œ±‚ÄëASI demo ‚Äì quickstart""), nbf.v4.new_code_cell(""!python -m alpha_asi_world_model_demo --demo &"")]
    nbf.write(nb,fp); print(""Notebook ‚Üí"",fp)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def _parse(self, ep: str):
        if "":"" not in ep:
            raise ValueError(""Endpoint must be <backend>:<model>"")
        return ep.split("":"",1)
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,LMClient
survived,"    def score(self, metrics: Dict[str,float]) -> float:
        return (
            self.latency * (1/ (1+metrics.get(""latency"",0))) +
            self.cost    * (1/ (1+metrics.get(""cost"",0))) +
            self.carbon  * (1/ (1+metrics.get(""carbon"",0))) +
            self.risk    * (1- metrics.get(""risk"",0))
        )
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,ObjectiveWeights
survived,"    def __init__(self, cpu_sec:int=2, mem_mb:int=128):
        self.cpu_sec = cpu_sec
        self.mem_mb = mem_mb
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,SafeExec
survived,"    def _obs(self):
        vec = np.zeros(self.size*self.size, dtype=np.float32)
        vec[self.agent[0]*self.size+self.agent[1]] = 1.0
        return vec
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,MiniWorld
survived,"    def __init__(self, cpu_sec:int=2, mem_mb:int=128):
        self.cpu_sec = cpu_sec
        self.mem_mb = mem_mb
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,SafeExec
survived,"    def __call__(self, prompt:str, **kw):
        return self.run(prompt, **kw)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,Agent
deleted,"def _poly_eval(coeffs: Iterable[int], x: int, mod: int) -> int:
    """"""Evaluates polynomial defined by `coeffs` at x modulo mod.""""""
    result = 0
    for c in reversed(list(coeffs)):
        result = (result * x + int(c)) % mod
    return result
",src/zklora/polynomial_commit.py,
survived,"    def forward(self, input_ids, labels=None):
        out = types.SimpleNamespace(loss=torch.tensor(0.0))
        return out
",tests/test_multi_contributor.py,DummyModel
survived,"def test_main_subprocess() -> None:
    """"""Running the demo via ``python -m`` should output the ŒîG message.""""""
    env = os.environ.copy()
    env[""OPENAI_API_KEY""] = ""dummy""
    result = subprocess.run(
        [
            sys.executable,
            ""-m"",
            ""alpha_factory_v1.demos.alpha_agi_business_3_v1"",
            ""--cycles"",
            ""1"",
        ],
        capture_output=True,
        text=True,
        env=env,
    )
    assert result.returncode == 0, result.stderr
    assert ""ŒîG=0.03"" in (result.stdout + result.stderr)
",tests/test_alpha_agi_business_3_v1.py,
survived,"def temp_db_path():
    fd, path = tempfile.mkstemp()
    os.close(fd)
    yield Path(path)
    if os.path.exists(path):
        os.unlink(path)
",src/mcp_server_pocket_pick/tests/functionality/test_list_ids.py,
survived,"def test_load_vocab(tmp_path):
    vocab_file = tmp_path / ""vocab.csv""
    with open(vocab_file, ""w"", encoding=""utf-8"", newline="""") as f:
        writer = csv.writer(f)
        writer.writerow([0, "" hello""])
        writer.writerow([1, "" world ""])

    vocab = sampler.load_vocab(str(vocab_file))

    assert vocab == [""hello"", ""world""]
",tests/test_sampler_io.py,
survived,"    async def restart(self, bus: object, ledger: object) -> None:
        if self.task:
            self.task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self.task
        try:
            close = getattr(self.agent, ""close"")
        except AttributeError:
            pass
        else:
            close()
        self.agent = self.cls(bus, ledger)
        self.error_count = 0
        self.restarts += 1
        self.restart_streak += 1
        self.start(bus, ledger)
",alpha_factory_v1/backend/orchestrator_utils.py,AgentRunner
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_codegen_safety.py,DummyLedger
survived,"def test_run_macro_demo_offline_download(tmp_path: Path) -> None:
    """"""Missing offline CSVs should trigger downloads.""""""
    offline_dir = RUN_SCRIPT.parent / ""offline_samples""
    backup = tmp_path / ""offline_backup""
    offline_dir.rename(backup)
    offline_dir.mkdir()
    try:
        _, curl_log = _run_script(tmp_path, env={""OPENAI_API_KEY"": ""dummy-key""})
    finally:
        shutil.rmtree(offline_dir)
        backup.rename(offline_dir)

    assert ""fed_speeches.csv"" in curl_log
    assert ""yield_curve.csv"" in curl_log
    assert ""stable_flows.csv"" in curl_log
    assert ""cme_settles.csv"" in curl_log",tests/test_macro_launcher.py,
survived,"    def test_venv_python_posix(self):
        with mock.patch.object(os, 'name', 'posix'):
            self.assertEqual(
                quickstart._venv_python(Path('/tmp/venv')),
                Path('/tmp/venv/bin/python')
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest
survived,"    def policy(self, obs):
        return mcts_policy(self.net, self.env, obs)
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,MiniMu
survived,"    def expanded(self) -> bool:
        return self.children is not None
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,Node
survived,"    def _simulate_worker(env_cls, archive, js: str):
        g = Genome.from_json(js)
        env = env_cls()
        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.n
        net = EvoNet(obs_dim, act_dim, g).to(Device)
        obs, _ = env.reset()
        total, bc = 0.0, []
        for _ in range(env.genome.max_steps):
            with torch.no_grad():
                a = net(torch.tensor(obs, dtype=torch.float32, device=Device)).argmax().item()
            obs, rew, done, truncated, _ = env.step(a)
            total += rew; bc.append(obs)
            if done or truncated:
                break
        bc_vec = np.mean(bc, axis=0)
        if g.novelty_weight and archive:
            novelty = float(np.mean([np.linalg.norm(bc_vec - a) for a in archive]))
            total += g.novelty_weight * novelty
        return total, bc_vec
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"def submit_job(path: str | Path, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT) -> None:
    """"""Convenience wrapper to submit a job from a JSON file.""""""
    job = load_job(path)
    MarketplaceClient(host, port).queue_job(job)
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,
survived,"    def test_missing_agent(self):
        client = MarketplaceClient()
        with self.assertRaises(ValueError):
            client.queue_job({})
",alpha_factory_v1/tests/test_marketplace_client.py,MarketplaceClientTest
survived,"    def tearDown(self):
        if hasattr(self, ""server""):
            self.server.shutdown()
            self.thread.join()
            self.server.server_close()
",alpha_factory_v1/tests/test_marketplace_client.py,MarketplaceClientTest
survived,"            def observe(self, *a, **k):
                self.calls.append(""observe"")
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest.DummyMetric
survived,"    def test_build_env_removes_secrets(self):
        env = {
            'TOKEN': 'x',
            'my_secret': 'y',
            'PASSWORD': 'z',
            'KEY': 'k',
            'OTHER': 'ok',
        }
        with mock.patch.dict(os.environ, env, clear=True):
            cleaned = local_pytest._build_env()
            self.assertNotIn('TOKEN', cleaned)
            self.assertNotIn('my_secret', cleaned)
            self.assertNotIn('PASSWORD', cleaned)
            self.assertNotIn('KEY', cleaned)
            self.assertEqual(cleaned['OTHER'], 'ok')
",alpha_factory_v1/tests/test_local_pytest.py,LocalPytestUtilsTest
survived,"    def test_check_python_version(self):
        with mock.patch.object(sys, 'version_info', (3, 9)):
            self.assertTrue(preflight.check_python())
        with mock.patch.object(sys, 'version_info', (3, 8)):
            self.assertFalse(preflight.check_python())
",alpha_factory_v1/tests/test_preflight.py,PreflightTest
survived,"    def test_vector_ram_mode(self):
        self.assertEqual(self.fabric.vector._mode, ""ram"")
        self.fabric.add_memory(""X"", ""data"")
        self.assertEqual(self.fabric.search(""data""), [])
",alpha_factory_v1/tests/test_memory_provider.py,MemoryFabricFallbackTest
survived,"def example5(b1, b2):
    if b1:
        None
    else:
        if b2:
            None
        else:
            None",tests/rosetta/transpiler/Python/conditional-structures-5.py,
survived,"def timeStr(sec):
    wks = sec // 604800
    sec = sec % 604800
    ds = sec // 86400
    sec = sec % 86400
    hrs = sec // 3600
    sec = sec % 3600
    mins = sec // 60
    sec = sec % 60
    res = """"
    comma = False
    if wks != 0:
        res = res + str(wks) + "" wk""
        comma = True
    if ds != 0:
        if comma:
            res = res + "", ""
        res = res + str(ds) + "" d""
        comma = True
    if hrs != 0:
        if comma:
            res = res + "", ""
        res = res + str(hrs) + "" hr""
        comma = True
    if mins != 0:
        if comma:
            res = res + "", ""
        res = res + str(mins) + "" min""
        comma = True
    if sec != 0:
        if comma:
            res = res + "", ""
        res = res + str(sec) + "" sec""
    return res
",tests/rosetta/transpiler/Python/convert-seconds-to-compound-duration.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/concurrent-computing-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/constrained-random-points-on-a-circle-1.py,
survived,"    def eat(self):
        print(""mm, that "" + self.value + "" was good!"")
",tests/rosetta/transpiler/Python/constrained-genericity-3.py,PeelFirst
survived,"    def __str__(self) -> str:
        type_repr = self.type
        if self.mutable and ""mutable"" not in type_repr:
            type_repr = f""{type_repr}, mutable""
        return f""- **{self.name}** ({type_repr}): {self.description}""
",src/enrichmcp/datamodel.py,FieldDescription
survived,"def test_evonet_no_relu_layers() -> None:
    g = me.Genome(layers=(4,), activation=""tanh"")
    net = me.EvoNet(2, 1, g)
    assert all(not isinstance(m, torch.nn.ReLU) for m in net.model)
",tests/test_evo_net_activation.py,
survived,"    def test_main_offline_skips_network(self) -> None:
        with mock.patch.multiple(
            preflight,
            check_python=lambda: True,
            check_cmd=lambda cmd: True,
            check_docker_daemon=lambda: True,
            check_docker_compose=lambda: True,
            check_pkg=lambda pkg: True,
            ensure_dir=lambda p: None,
            banner=lambda *a, **k: None,
        ):
            with mock.patch.object(preflight, ""check_network"") as cn:
                preflight.main([""--offline""])
                cn.assert_not_called()
",alpha_factory_v1/tests/test_preflight.py,PreflightTest
survived,"    async def setup(self):
        self.iter = 0
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv
survived,"def test_available_scenarios() -> None:
    names = set(replay.available_scenarios())
    assert EXPECTED.issubset(names)
",tests/test_replay_scenarios.py,
survived,"def test_entrypoint_offline(monkeypatch):
    monkeypatch.setitem(
        sys.modules,
        ""gradio"",
        types.SimpleNamespace(Blocks=DummyBlocks, Markdown=DummyMarkdown, Button=DummyButton),
    )

    monkeypatch.setattr(llm_client, ""call_local_model"", lambda msgs: ""local"")

    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)
    sys.modules.pop(""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint"", None)
    entrypoint = importlib.import_module(
        ""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint""
    )

    assert entrypoint.LLM(""hi"") == ""local""",tests/test_selfheal_entrypoint_offline.py,
survived,"    def Tool(*_a, **_kw):  # type: ignore
        def _decorator(func):
            return func

        return _decorator
",alpha_factory_v1/demos/self_healing_repo/agent_selfheal_entrypoint.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/save_jsonl_stdout.py,Person
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/load_yaml.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/right_join.py,Customer
survived,"        def save(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyDocxDocument
survived,"        def add_page(self):
            pass
",tests/conftest.py,DummyFPDF
survived,"def _save_result(result: ResultsResponse) -> None:
    path = _results_dir / f""{result.id}.json""
    path.write_text(result.json())
",src/interface/api_server.py,
survived,"    def convert_expr(self, node: ast.expr) -> str:
        if isinstance(node, ast.Constant):
            if isinstance(node.value, str):
                return json.dumps(node.value)
            return str(node.value)
        if isinstance(node, ast.Name):
            return self.name_map.get(node.id, node.id)
        if isinstance(node, ast.Attribute):
            if (
                node.attr == ""lower""
                and isinstance(node.value, ast.Call)
                and isinstance(node.value.func, ast.Name)
                and node.value.func.id == ""str""
                and len(node.value.args) == 1
            ):
                return self.convert_expr(node.value.args[0])
            if isinstance(node.value, ast.Name) and node.value.id == ""self"":
                return node.attr
            return f""{self.convert_expr(node.value)}.{node.attr}""
        if isinstance(node, ast.Subscript):
            target = self.convert_expr(node.value)
            sl = node.slice
            if isinstance(sl, ast.Slice):
                start = self.convert_expr(sl.lower) if sl.lower else """"
                stop = self.convert_expr(sl.upper) if sl.upper else """"
                return f""{target}[{start}:{stop}]""
            return f""{target}[{self.convert_expr(sl)}]""
        if isinstance(node, ast.BinOp):
            op_map = {ast.Add: ""+"", ast.Sub: ""-"", ast.Mult: ""*"", ast.Div: ""/""}
            op = op_map.get(type(node.op), ""?"")
            return (
                f""{self.convert_expr(node.left)} {op} {self.convert_expr(node.right)}""
            )
        if isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.USub):
            return ""-"" + self.convert_expr(node.operand)
        if isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.Not):
            return ""!"" + self.convert_expr(node.operand)
        if isinstance(node, ast.BoolOp):
            if isinstance(node.op, ast.And):
                op = "" and ""
            else:
                op = "" or ""
            return op.join(self.convert_expr(v) for v in node.values)
        if isinstance(node, ast.IfExp):
            test = self.convert_expr(node.test)
            body = self.convert_expr(node.body)
            orelse = self.convert_expr(node.orelse)
            return f""if {test} then {body} else {orelse}""
        if isinstance(node, ast.Compare):
            op_map = {
                ast.Gt: "">"",
                ast.Lt: ""<"",
                ast.GtE: "">="",
                ast.LtE: ""<="",
                ast.Eq: ""=="",
                ast.NotEq: ""!="",
                ast.In: ""?"",
                ast.NotIn: ""!?"",
            }
            left = self.convert_expr(node.left)
            op = op_map.get(type(node.ops[0]), ""?"")
            right = self.convert_expr(node.comparators[0])
            return f""{left} {op} {right}""
        if isinstance(node, ast.Call):
            if isinstance(node.func, ast.Name):
                if (
                    node.func.id == ""_get""
                    and len(node.args) == 2
                    and isinstance(node.args[1], ast.Constant)
                    and isinstance(node.args[1].value, str)
                ):
                    obj = self.convert_expr(node.args[0])
                    return f""{obj}.{node.args[1].value}""
                if node.func.id == ""_fetch"" and len(node.args) >= 1:
                    url = self.convert_expr(node.args[0])
                    if len(node.args) > 1 and not (
                        isinstance(node.args[1], ast.Constant) and node.args[1].value is None
                    ):
                        opts = self.convert_expr(node.args[1])
                        return f""fetch {url} with {opts}""
                    return f""fetch {url}""
                if node.func.id == ""_load"" and len(node.args) >= 1:
                    path = self.convert_expr(node.args[0])
                    base = ""load"" if path == ""None"" else f""load {path}""
                    if len(node.args) > 1 and not (
                        isinstance(node.args[1], ast.Constant) and node.args[1].value is None
                    ):
                        opts = self.convert_expr(node.args[1])
                        return f""{base} with {opts}""
                    return base
                if node.func.id == ""_save"" and len(node.args) >= 1:
                    target = self.convert_expr(node.args[0])
                    base = f""save {target}""
                    opts_arg = None
                    if len(node.args) > 2:
                        opts_arg = node.args[2]
                    elif len(node.args) > 1 and not (
                        isinstance(node.args[1], ast.Constant) and node.args[1].value is None
                    ):
                        opts_arg = node.args[1]
                    if opts_arg is not None and not (
                        isinstance(opts_arg, ast.Constant) and opts_arg.value is None
                    ):
                        opts = self.convert_expr(opts_arg)
                        return f""{base} with {opts}""
                    return base
            func = self.convert_expr(node.func)
            if func in self.dataclasses:
                if (
                    not node.args
                    and len(node.keywords) == 1
                    and node.keywords[0].arg is None
                ):
                    kw = node.keywords[0].value
                    if (
                        isinstance(kw, ast.Call)
                        and isinstance(kw.func, ast.Name)
                        and kw.func.id == ""_fetch""
                    ):
                        url = self.convert_expr(kw.args[0])
                        if len(kw.args) > 1 and not (
                            isinstance(kw.args[1], ast.Constant) and kw.args[1].value is None
                        ):
                            opts = self.convert_expr(kw.args[1])
                            return f""fetch {url} with {opts} as {func}""
                        return f""fetch {url} as {func}""
                    if isinstance(kw, ast.Name):
                        return f""{func} {{ {kw.id} }}""
                fields = [
                    f""{k.arg}: {self.convert_expr(k.value)}"" for k in node.keywords if k.arg
                ]
                return f""{func} {{ "" + "", "".join(fields) + "" }""
            args = [self.convert_expr(a) for a in node.args]
            args += [
                f""{k.arg}: {self.convert_expr(k.value)}"" for k in node.keywords if k.arg
            ]
            if func == ""dict"" and len(node.args) == 1 and not node.keywords and isinstance(node.args[0], ast.Dict):
                return self.convert_expr(node.args[0])
            args += [self.convert_expr(k.value) for k in node.keywords if k.arg is None]
            return f""{func}("" + "", "".join(args) + "")""
        if isinstance(node, ast.Dict):
            items = []
            for k, v in zip(node.keys, node.values):
                key = self.convert_expr(k)
                if isinstance(k, ast.Constant) and isinstance(k.value, str):
                    key = k.value
                items.append(f""{key}: {self.convert_expr(v)}"")
            return ""{"" + "", "".join(items) + ""}""
        if isinstance(node, ast.DictComp):
            parts = [f""{self.convert_expr(node.key)}: {self.convert_expr(node.value)}""]
            for gen in node.generators:
                target = self.convert_expr(gen.target)
                iter_ = self.convert_expr(gen.iter)
                parts.append(f""for {target} in {iter_}"")
                for if_ in gen.ifs:
                    parts.append(f""if {self.convert_expr(if_)}"")
            return ""{"" + "" "".join(parts) + ""}""
        if isinstance(node, ast.Tuple):
            return ""("" + "", "".join(self.convert_expr(e) for e in node.elts) + "")""
        if isinstance(node, ast.Starred):
            return self.convert_expr(node.value)
        if isinstance(node, ast.List):
            return ""["" + "", "".join(self.convert_expr(e) for e in node.elts) + ""]""
        if isinstance(node, ast.ListComp):
            return self.convert_list_comp(node)
        if isinstance(node, ast.GeneratorExp):
            fake = ast.ListComp(node.elt, node.generators)
            return self.convert_list_comp(fake)
        if isinstance(node, ast.Lambda):
            return self.convert_lambda(node)
        line = self.src_lines[getattr(node, ""lineno"", 1) - 1]
        raise ConversionError(""unhandled expression"", getattr(node, ""lineno"", 0), line)
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def parse_callable(self, node: ast.expr | None) -> tuple[list[str], str] | None:
        if not isinstance(node, ast.Subscript):
            return None
        if not (
            isinstance(node.value, ast.Attribute) and node.value.attr == ""Callable""
        ):
            return None
        if not (isinstance(node.slice, ast.Tuple) and len(node.slice.elts) == 2):
            return None
        args_node, ret_node = node.slice.elts
        if isinstance(args_node, ast.List):
            arg_types = [self.convert_type(e) for e in args_node.elts]
        else:
            arg_types = [self.convert_type(args_node)]
        ret_type = self.convert_type(ret_node)
        return arg_types, ret_type
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def __init__(self, src: str):
        self.lines: list[str] = []
        self.indent = 0
        self.src_lines = src.splitlines()
        self.dataclasses: set[str] = set()
        self.seen_assigns: set[str] = set()
        self.assign_values: dict[str, str] = {}
        self.current_callable: tuple[list[str], str] | None = None
        self.structs: dict[str, tuple[list[tuple[str, str]], list[ast.FunctionDef]]] = {}
        self.unions: dict[str, list[tuple[str, list[tuple[str, str]]]]] = {}
        self.name_map = {""_next"": ""next""}
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def _get_tool_stats(self) -> ResponseToolStats:
        return ResponseToolStats(stats=get_tool_stats())
",src/serena/dashboard.py,SerenaDashboardAPI
survived,"async def test_combine_lifespans_merges_and_overrides():
    call_order = []

    @asynccontextmanager
    async def first(app: EnrichMCP):
        call_order.append(""first"")
        yield {""a"": 1}

    @asynccontextmanager
    async def second(app: EnrichMCP):
        call_order.append(""second"")
        yield {""b"": 2, ""a"": 0}

    combined = combine_lifespans(first, second)
    app = EnrichMCP(""Test"", ""Desc"")
    async with combined(app) as ctx:
        assert ctx == {""a"": 0, ""b"": 2}
    assert call_order == [""first"", ""second""]
",tests/test_lifespan.py,
survived,"        def __init__(self, *a, **kw):
            agent_args.update(kw)
",tests/test_selfheal_env.py,FakeAgent
survived,"    def __exit__(self, exc_type, exc, tb):
        pass
",tests/test_selfheal_env.py,DummyBlocks
survived,"def _allow_local_code() -> bool:
    """"""Check both new and legacy opts for enabling local PythonTool.""""""
    return (
        os.getenv(ALLOW_LOCAL_CODE_ENV)
        or os.getenv(LEGACY_ALLOW_LOCAL_CODE_ENV)
    ) == ""1""
",alpha_factory_v1/backend/agent_factory.py,
deleted,"  async def test_dispense_custom_flow_rate(self):
    op = SingleChannelDispense(
      resource=self.plate.get_item(""A1""),
      offset=Coordinate.zero(),
      tip=self.tr.get_tip(""A1""),
      volume=100,
      flow_rate=200,
      liquid_height=10,
      blow_out_air_volume=0,
      liquids=[(None, 100)],
    )
    await self.evo.dispense([op], use_channels=[0])
    self.evo.send_command.assert_any_call(
      module=""C5"",
      command=""SEP"",
      params=[2400, None, None, None, None, None, None, None],
    )
",pylabrobot/liquid_handling/backends/tecan/EVO_tests.py,EVOTests
survived,"    def test_insight_bridge_compiles(self):
        """"""Ensure the Œ±‚ÄëAGI Insight demo bridge compiles.""""""
        path = Path('alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py')
        py_compile.compile(path, doraise=True)
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"    def __init__(self, num_levels, vocab, parent=None, level=0,
                 random_state=None):

        self.node_id = NCRPNode.last_node_id
        NCRPNode.last_node_id += 1

        self.customers = 0
        self.parent = parent
        self.children = []
        self.level = level
        self.total_words = 0
        self.num_levels = num_levels

        self.vocab = np.array(vocab)
        self.word_counts = np.zeros(len(vocab))

        if random_state is None:
            self.random_state = RandomState()
        else:
            self.random_state = random_state
",src/hlda/sampler.py,NCRPNode
survived,"    def sample_topics(self, d):

        doc = self.corpus[d]

        # initialise level counts
        doc_levels = self.levels[d]
        level_counts = np.zeros(self.num_levels, dtype=int)
        for c in doc_levels:
            level_counts[c] += 1

        # get the leaf node and populate the path
        path = np.zeros(self.num_levels, dtype=object)
        node = self.document_leaves[d]
        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4
            path[level] = node
            node = node.parent

        # sample a new level for each word
        level_weights = np.zeros(self.num_levels)
        for n in range(len(doc)):

            w = doc[n]
            word_level = doc_levels[n]

            # remove from model
            level_counts[word_level] -= 1
            node = path[word_level]
            node.word_counts[w] -= 1
            node.total_words -= 1

            # pick new level
            for level in range(self.num_levels):
                level_weights[level] = (self.alpha + level_counts[level]) *                     \
                    (self.eta + path[level].word_counts[w]) /                                   \
                    (self.eta_sum + path[level].total_words)
            level_weights = level_weights / np.sum(level_weights)
            level = self.random_state.multinomial(1, level_weights).argmax()

            # put the word back into the model
            doc_levels[n] = level
            level_counts[level] += 1
            node = path[level]
            node.word_counts[w] += 1
            node.total_words += 1
",src/hlda/sampler.py,HierarchicalLDA
survived,"def _write_executable(path: Path, content: str) -> None:
    path.write_text(content)
    path.chmod(0o755)
",tests/test_cross_industry_patch.py,
survived,"    def __init__(self) -> None:
        self.loaded = None
",tests/test_orchestrator_rest.py,DummyAgent
survived,"    def load_weights(self, path: str) -> None:
        self.loaded = path
",tests/test_orchestrator_rest.py,DummyAgent
survived,"    def test_infer_fastmcp_v1_server(self):
        """"""FastMCP 1.0 server instances should infer to FastMCPTransport.""""""
        from mcp.server.fastmcp import FastMCP as FastMCP1

        server = FastMCP1()
        transport = infer_transport(server)
        assert isinstance(transport, FastMCPTransport)",tests/client/test_client.py,TestInferTransport
survived,"def test_compare_df_mixed_types_equal():
    df1 = pd.DataFrame({
        'int_col': [1, 2],
        'float_col': [1.5, 2.5],
        'date_col': pd.to_datetime(['2023-01-01', '2023-01-02']),
        'str_col': ['x', 'y'],
    })
    df2 = df1.copy()
    assert compare_df(df1, df2, question=""mixed"") is True
",backend/tests/test_utils_sql_compare_df.py,
survived,"            def __init__(self, url: str) -> None:
                captured[""url""] = url
",tests/test_merkle_broadcast.py,TestMerkleBroadcast.DummyClient
survived,"            async def close(self) -> None:  # pragma: no cover - dummy
                pass
",tests/test_merkle_broadcast.py,TestMerkleBroadcast.DummyClient
survived,"    def path_retro_route():
        args = request.args
        data = rs.path_retro(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
        )
        return Response(data, mimetype=""application/json"")
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"def test_mutate_rejects_traversal(server: str) -> None:
    """"""Tarball members must not escape the extraction directory.""""""
    import io
    import tarfile

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""../evil.txt"")
        data = b""bad""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""bad.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 400",tests/test_evolution_worker.py,
survived,"        def _tool(*_a: object, **_k: object) -> Callable[[object], object]:
            def dec(f: object) -> object:
                return f

            return dec
",tests/test_alpha_opportunity_stub.py,TestAlphaOpportunityStub
survived,"def test_results_checksum(tmp_path: Path, cfg: dict[str, int]) -> None:
    os.environ[""SIM_RESULTS_DIR""] = str(tmp_path)
    os.environ.setdefault(""API_TOKEN"", ""test-token"")
    from src.interface import api_server

    api = importlib.reload(api_server)
    req = api.SimRequest(**cfg)
    asyncio.run(api._background_run(""chk"", req))

    checksum = _dir_checksum(tmp_path)
    golden = Path(__file__).with_name(""golden_checksum.txt"").read_text().strip()

    diff_bits = _hamming_dist(bytes.fromhex(checksum), bytes.fromhex(golden))
    max_bits = max(len(checksum), len(golden)) * 4
    diff_ratio = diff_bits / max_bits
    assert diff_ratio <= 0.001, (
        f""Checksum differs by {diff_ratio*100:.3f}% (threshold 0.1%)""
    )",tests/test_checksum.py,
survived,"def test_refinement_no_bottleneck(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    logs = tmp_path / ""logs""
    logs.mkdir()

    reg = StakeRegistry()
    reg.set_stake(""meta"", 1.0)

    agent = MetaRefinementAgent(repo, logs, reg)
    with (
        patch.object(MetaRefinementAgent, ""_load_logs"", return_value=[]),
        patch.object(harness, ""vote_and_merge"") as vote,
    ):
        merged = agent.refine()

    assert not merged
    vote.assert_not_called()
",tests/test_meta_refinement_agent.py,
survived,"def test_broadcast_merkle_root_handles_corrupt_db(tmp_path: Path) -> None:
    ledger_path = tmp_path / ""ledger.db""
    ledger = Ledger(str(ledger_path), rpc_url=""http://rpc.test"", broadcast=True)
    ledger.log(messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0))
    ledger.compute_merkle_root()
    # truncate database file to simulate missing pages
    data = ledger_path.read_bytes()
    ledger_path.write_bytes(data[: len(data) // 2])
    with mock.patch.object(insight_logging, ""_log"") as log:
        asyncio.run(ledger.broadcast_merkle_root())
        log.warning.assert_called()",tests/test_ledger_corruption.py,
survived,"def test_creator_register(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    creator = TemplateCreator(reg)
    path = creator.create(_meta(), ""hi {{name}}"", version=""0.1.0"")
    assert path
    assert reg.load_template(""demo"") == ""hi {{name}}""",tests/test_template_creator.py,
survived,"def main() -> None:
    if len(sys.argv) != 2:
        print(f""Usage: {Path(sys.argv[0]).name} <wheel>"", file=sys.stderr)
        raise SystemExit(1)
    wheel = Path(sys.argv[1])
    if not wheel.is_file():
        print(f""Wheel not found: {wheel}"", file=sys.stderr)
        raise SystemExit(1)
    if verify(wheel):
        print(f""OK: {wheel}"")
        raise SystemExit(0)
    print(f""FAILED: {wheel}"", file=sys.stderr)
    raise SystemExit(2)
",alpha_factory_v1/scripts/verify_wheel_sig.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q2.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q22.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Part
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q10.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto2
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q15.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto11
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto4
survived,"def test_Q13_finds_earliest_German_movie_info():
    assert result == Auto1(
        release_date=""1997-05-10"", rating=""6.0"", german_movie=""Alpha""
    )
",tests/dataset/job/compiler/py/q13.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto8
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q23.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q26.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto2
survived,"def test_Q33_finds_linked_TV_series_with_low_rated_sequel():
    assert result == [
        Auto1(
            first_company=""US Studio"",
            second_company=""GB Studio"",
            first_rating=""7.0"",
            second_rating=""2.5"",
            first_movie=""Series A"",
            second_movie=""Series B"",
        )
    ]
",tests/dataset/job/compiler/py/q33.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto11
survived,"def test_Q16_finds_series_named_after_a_character_between_episodes_50_and_99():
    assert result == [
        Auto1(cool_actor_pseudonym=""Alpha"", series_named_after_char=""Hero Bob"")
    ]
",tests/dataset/job/compiler/py/q16.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto8
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q19.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,WebReturn
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q3.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q9.py,Reason
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Inventory
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,CatalogSale
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q19.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q80.py,StoreSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q93.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q23.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto2
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,CustomerAddres
survived,"def _q2():
    _groups = {}
    _order = []
    for ctr in customer_total_return:
        _k = ctr.ctr_state
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ctr)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto4(
            state=g.key,
            avg_return=(
                sum([x.ctr_total_return for x in g])
                / len([x.ctr_total_return for x in g])
                if [x.ctr_total_return for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q30.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Reason
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,StoreSale
survived,"def _count(v):
    if isinstance(v, list):
        return len(v)
    if hasattr(v, ""Items""):
        return len(v.Items)
    raise Exception(""count() expects list or group"")
",tests/dataset/tpc-ds/compiler/py/q54.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q46.py,
survived,"def test_TPCDS_Q62_simplified():
    assert result == 62
",tests/dataset/tpc-ds/compiler/py/q62.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q29.py,_Group
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q77.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q83.py,SrItem
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q93.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q9.py,StoreSale
survived,"def test_TPCDS_Q74_simplified():
    assert result == [
        Auto1(customer_id=1, customer_first_name=""Alice"", customer_last_name=""Smith"")
    ]
",tests/dataset/tpc-ds/compiler/py/q74.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Auto1
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q76.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q55.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Store
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": customer_demographics,
                ""on"": lambda ss, cd: ss.ss_cdemo_sk == cd.cd_demo_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda ss, cd, d: ss.ss_sold_date_sk == d.d_date_sk,
            },
            {""items"": store, ""on"": lambda ss, cd, d, s: ss.ss_store_sk == s.s_store_sk},
            {""items"": item, ""on"": lambda ss, cd, d, s, i: ss.ss_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda ss, cd, d, s, i: (ss, cd, d, s, i),
            ""where"": lambda ss, cd, d, s, i: (
                (
                    (cd.cd_gender == ""F"" and cd.cd_marital_status == ""M"")
                    and cd.cd_education_status == ""College""
                )
                and d.d_year == 2000
            )
            and s.s_state in [""CA""],
        },
    )
    _groups = _group_by(
        _rows, lambda ss, cd, d, s, i: Auto2(item_id=i.i_item_id, state=s.s_state)
    )
    _items1 = _groups
    _items1 = sorted(
        _items1, key=lambda g: _sort_key([g.key[""item_id""], g.key[""state""]])
    )
    return [
        Auto1(
            i_item_id=g.key[""item_id""],
            s_state=g.key[""state""],
            agg1=(
                sum([x[0].ss_quantity for x in g]) / len([x[0].ss_quantity for x in g])
                if [x[0].ss_quantity for x in g]
                else 0
            ),
            agg2=(
                sum([x[0].ss_list_price for x in g])
                / len([x[0].ss_list_price for x in g])
                if [x[0].ss_list_price for x in g]
                else 0
            ),
            agg3=(
                sum([x[0].ss_coupon_amt for x in g])
                / len([x[0].ss_coupon_amt for x in g])
                if [x[0].ss_coupon_amt for x in g]
                else 0
            ),
            agg4=(
                sum([x[0].ss_sales_price for x in g])
                / len([x[0].ss_sales_price for x in g])
                if [x[0].ss_sales_price for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q27.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q63.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q38.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,WebSite
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,CustomerAddres
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q12.py,_Group
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q97.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q34.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,Item
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q8.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q52.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,WebSale
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q12.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Auto2
survived,"def _q0():
    _src = date_dim
    _rows = _query(
        _src,
        [
            {
                ""items"": store_sales,
                ""on"": lambda dt, ss: dt.d_date_sk == ss.ss_sold_date_sk,
            },
            {""items"": item, ""on"": lambda dt, ss, i: ss.ss_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda dt, ss, i: (dt, ss, i),
            ""where"": lambda dt, ss, i: i.i_manufact_id == 100 and dt.d_moy == 12,
        },
    )
    _groups = _group_by(
        _rows,
        lambda dt, ss, i: Auto2(
            d_year=dt.d_year, brand_id=i.i_brand_id, brand=i.i_brand
        ),
    )
    _items1 = _groups
    _items1 = sorted(
        _items1,
        key=lambda g: _sort_key(
            [
                g.key[""d_year""],
                -_sum([x[1].ss_ext_sales_price for x in g]),
                g.key[""brand_id""],
            ]
        ),
    )
    return [
        Auto1(
            d_year=g.key[""d_year""],
            brand_id=g.key[""brand_id""],
            brand=g.key[""brand""],
            sum_agg=_sum([x[1].ss_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q3.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q90.py,HouseholdDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CatalogSale
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q32.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Auto2
survived,"def test_TPCDS_Q21_inventory_ratio():
    assert result == [
        Auto1(
            w_warehouse_name=""Backup"", i_item_id=""ITEM2"", inv_before=20, inv_after=20
        ),
        Auto1(w_warehouse_name=""Main"", i_item_id=""ITEM1"", inv_before=30, inv_after=40),
    ]
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,Auto2
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q19.py,
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q6.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Auto2
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q51.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Item
survived,"def test_TPCDS_Q25_aggregated_profit():
    assert result == [
        Auto1(
            i_item_id=""ITEM1"",
            i_item_desc=""Desc1"",
            s_store_id=""S1"",
            s_store_name=""Store1"",
            store_sales_profit=50.0,
            store_returns_loss=10.0,
            catalog_sales_profit=30.0,
        ),
        Auto1(
            i_item_id=""ITEM2"",
            i_item_desc=""Desc2"",
            s_store_id=""S1"",
            s_store_name=""Store1"",
            store_sales_profit=20.0,
            store_returns_loss=5.0,
            catalog_sales_profit=15.0,
        ),
    ]
",tests/dataset/tpc-ds/compiler/py/q25.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q45.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,StoreSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q81.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,CustomerDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Store
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q7.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Item
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q94.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,CatalogReturn
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q99.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,Auto1
survived,"def test_TPCDS_Q88_sample():
    assert result == 88
",tests/dataset/tpc-ds/compiler/py/q88.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q36.py,
survived,"def _q0():
    _src = sales_detail
    _rows = _query(
        _src,
        [
            {
                ""items"": item,
                ""on"": lambda sd, i: i.i_item_sk
                == (
                    sd.get(""i_item_sk"")
                    if isinstance(sd, dict)
                    else getattr(sd, ""i_item_sk"")
                ),
            }
        ],
        {
            ""select"": lambda sd, i: (sd, i),
            ""where"": lambda sd, i: i.i_category == ""Electronics"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda sd, i: Auto4(
            year=sd.get(""d_year"") if isinstance(sd, dict) else getattr(sd, ""d_year""),
            brand_id=i.i_brand_id,
            class_id=i.i_class_id,
            category_id=i.i_category_id,
            manuf_id=i.i_manufact_id,
        ),
    )
    _items1 = _groups
    return [
        Auto3(
            d_year=g.key[""year""],
            i_brand_id=g.key[""brand_id""],
            i_class_id=g.key[""class_id""],
            i_category_id=g.key[""category_id""],
            i_manufact_id=g.key[""manuf_id""],
            sales_cnt=_sum(
                [
                    (
                        x[0].get(""quantity"")
                        if isinstance(x[0], dict)
                        else getattr(x[0], ""quantity"")
                    )
                    for x in g
                ]
            ),
            sales_amt=_sum(
                [
                    (
                        x[0].get(""amount"")
                        if isinstance(x[0], dict)
                        else getattr(x[0], ""amount"")
                    )
                    for x in g
                ]
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Auto2
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q13.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,CustomerAddres
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q42.py,
survived,"def average(xs):
    if len(xs) == 0:
        return 0.0
    sum = 0.0
    for x in xs:
        sum = sum + x
    return sum / float(len(xs))
",tests/dataset/tpc-ds/compiler/py/q65.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q33.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Reason
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,CatalogSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Customer
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,TimeDim
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q33.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Auto2
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q30.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,DateDim
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q24.py,
survived,"def _q0():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = ss.ss_item_sk
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto2(
            item_sk=g.key,
            avg_profit=(
                sum([x.ss_net_profit for x in g]) / len([x.ss_net_profit for x in g])
                if [x.ss_net_profit for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q44.py,
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {""items"": date_dim, ""on"": lambda ss, d: ss.ss_sold_date_sk == d.d_date_sk},
            {""items"": item, ""on"": lambda ss, d, i: ss.ss_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda ss, d, i: (ss, d, i),
            ""where"": lambda ss, d, i: d.d_year == 2000,
        },
    )
    _groups = _group_by(
        _rows, lambda ss, d, i: Auto1(item_sk=i.i_item_sk, date_sk=d.d_date_sk)
    )
    _items1 = _groups
    _items1 = [g for g in _items1 if len(g) > 4]
    return [g.key[""item_sk""] for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q23.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Inventory
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Warehouse
survived,"def _q2():
    _groups = {}
    _order = []
    for s in web_sales:
        _k = s.item
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(s)
    _items1 = [_groups[k] for k in _order]
    return [Auto2(item=g.key, total=sum([x.price for x in g])) for g in _items1]
",tests/dataset/tpc-ds/compiler/py/q56.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,Inventory
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Auto3
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q56.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,StoreSale
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q25.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q5.py,Result
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q36.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q41.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,DateDim
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": store_returns,
                ""on"": lambda ss, sr: ss.ss_ticket_number == sr.sr_ticket_number
                and ss.ss_item_sk == sr.sr_item_sk,
            },
            {""items"": store, ""on"": lambda ss, sr, s: ss.ss_store_sk == s.s_store_sk},
            {""items"": item, ""on"": lambda ss, sr, s, i: ss.ss_item_sk == i.i_item_sk},
            {
                ""items"": customer,
                ""on"": lambda ss, sr, s, i, c: ss.ss_customer_sk == c.c_customer_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda ss, sr, s, i, c, ca: c.c_current_addr_sk
                == ca.ca_address_sk,
            },
        ],
        {
            ""select"": lambda ss, sr, s, i, c, ca: (ss, sr, s, i, c, ca),
            ""where"": lambda ss, sr, s, i, c, ca: (
                c.c_birth_country != ca.ca_country.upper() and s.s_zip == ca.ca_zip
            )
            and s.s_market_id == 5,
        },
    )
    _groups = _group_by(
        _rows,
        lambda ss, sr, s, i, c, ca: Auto3(
            last=c.c_last_name,
            first=c.c_first_name,
            store_name=s.s_store_name,
            color=i.i_color,
        ),
    )
    _items1 = _groups
    return [
        Auto2(
            c_last_name=g.key[""last""],
            c_first_name=g.key[""first""],
            s_store_name=g.key[""store_name""],
            color=g.key[""color""],
            netpaid=sum([x[0].ss_net_paid for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q24.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto3
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q91.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,CustomerAddres
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q33.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q28.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Auto2
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q52.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,CatalogSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,StoreSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q2.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto2
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,CustomerAddres
survived,"def test_TPCDS_Q59_simplified():
    assert result == [Auto1(s_store_id1=1, ratio=1.5)]
",tests/dataset/tpc-ds/compiler/py/q59.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto4
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q1.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q24.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,DateDim
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q21.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Store
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q9.py,StoreSale
survived,"def test_workers_in_sandbox_iframes() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(""document.querySelectorAll('iframe[sandbox]').length >= 2"")
        frames = page.query_selector_all(""iframe[sandbox]"")
        assert len(frames) >= 2
        for f in frames:
            assert f.get_attribute(""sandbox"") == ""allow-scripts""
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"    def eval_fn(genome: list[float]) -> tuple[float, float, float]:
        x, y = genome
        return x**2, y**2, (x + y) ** 2
",src/interface/api_server.py,
survived,"    async def get_population(sim_id: str, _: None = Depends(verify_token)) -> PopulationResponse:
        """"""Return the final population for ``sim_id`` if available.""""""
        result = _simulations.get(sim_id)
        if result is None:
            raise HTTPException(status_code=404)
        return PopulationResponse(id=sim_id, population=result.population)
",src/interface/api_server.py,
survived,"    async def async_step_init(self, user_input: dict | None = None) -> FlowResult:
        if user_input is not None:
            return self.async_create_entry(title="""", data=user_input)

        options = {**self.config_entry.options}
        schema = vol.Schema(
            {
                vol.Optional(CONF_TARGET_TEMP_STEP, default=options.get(CONF_TARGET_TEMP_STEP, DEFAULT_TARGET_TEMP_STEP)): vol.Coerce(float),
                vol.Optional(CONF_TEMP_SENSOR, default=options.get(CONF_TEMP_SENSOR)): str,
                vol.Optional(CONF_LIGHTS, default=options.get(CONF_LIGHTS)): str,
                vol.Optional(CONF_XFAN, default=options.get(CONF_XFAN)): str,
                vol.Optional(CONF_HEALTH, default=options.get(CONF_HEALTH)): str,
                vol.Optional(CONF_POWERSAVE, default=options.get(CONF_POWERSAVE)): str,
                vol.Optional(CONF_SLEEP, default=options.get(CONF_SLEEP)): str,
                vol.Optional(CONF_EIGHTDEGHEAT, default=options.get(CONF_EIGHTDEGHEAT)): str,
                vol.Optional(CONF_AIR, default=options.get(CONF_AIR)): str,
                vol.Optional(CONF_TARGET_TEMP, default=options.get(CONF_TARGET_TEMP)): str,
                vol.Optional(CONF_AUTO_XFAN, default=options.get(CONF_AUTO_XFAN)): str,
                vol.Optional(CONF_AUTO_LIGHT, default=options.get(CONF_AUTO_LIGHT)): str,
                vol.Optional(CONF_HORIZONTAL_SWING, default=options.get(CONF_HORIZONTAL_SWING, False)): bool,
                vol.Optional(CONF_ANTI_DIRECT_BLOW, default=options.get(CONF_ANTI_DIRECT_BLOW)): str,
                vol.Optional(CONF_DISABLE_AVAILABLE_CHECK, default=options.get(CONF_DISABLE_AVAILABLE_CHECK, False)): bool,
                vol.Optional(CONF_MAX_ONLINE_ATTEMPTS, default=options.get(CONF_MAX_ONLINE_ATTEMPTS, 3)): int,
                vol.Optional(CONF_LIGHT_SENSOR, default=options.get(CONF_LIGHT_SENSOR)): str,
                vol.Optional(CONF_TEMP_SENSOR_OFFSET, default=options.get(CONF_TEMP_SENSOR_OFFSET)): bool,
                vol.Optional(CONF_LANGUAGE, default=options.get(CONF_LANGUAGE)): str,
            }
        )
        return self.async_show_form(step_id=""init"", data_schema=schema)",custom_components/gree/config_flow.py,OptionsFlowHandler
survived,"    async def async_step_import(self, import_data: dict) -> FlowResult:
        """"""Handle configuration via YAML import.""""""
        return await self.async_step_user(import_data)
",custom_components/gree/config_flow.py,ConfigFlow
survived,"    def __init__(self, *args, **kwargs):
        self.chat = _Chat()
",openai/__init__.py,OpenAI
survived,"    def __init__(self, message: str = """", response=None, body=None):
        super().__init__(message)
        self.response = response
        self.body = body
",openai/__init__.py,AuthenticationError
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        if not errors:
            self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""tests"" / ""test_main.py"").write_text(
        ""def test_fail():\n    assert False""
    )
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def test_import_csv_utf8_encoding(base_task: Task, tmp_path):
    """"""Ensure UTF-8 encoded files are read correctly.""""""

    row_data = [
        {
            ""input"": ""Espa√±ol entrada ‰Ω†Â•Ωüëã"",
            ""output"": ""salida √°√©√≠ ‰Ω†Â•Ωüëã"",
            ""tags"": """",
        },
    ]

    file_path = dicts_to_file_as_csv(row_data, ""utf8.csv"", tmp_path)

    with patch(""kiln_ai.utils.dataset_import.open"", wraps=open) as mock_open:
        importer = DatasetFileImporter(
            base_task,
            ImportConfig(
                dataset_type=DatasetImportFormat.CSV,
                dataset_path=file_path,
                dataset_name=""utf8.csv"",
            ),
        )

        importer.create_runs_from_file()

        mock_open.assert_called_once_with(
            file_path,
            ""r"",
            newline="""",
            encoding=""utf-8"",
        )

    assert len(base_task.runs()) == 1
    run = base_task.runs()[0]
    assert run.input == row_data[0][""input""]
    assert run.output.output == row_data[0][""output""]
",libs/core/kiln_ai/utils/test_dataset_import.py,
deleted,"    def next_turn(
        self, previous_output: str | None = None
    ) -> Optional[List[ChatMessage]]:
        """"""Advance the conversation and return the next messages if any.""""""
        raise NotImplementedError
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,ChatFormatter
survived,"def get(url: str, **kwargs):
    with _request.urlopen(url) as resp:
        data = resp.read().decode()
        return Response(resp.getcode(), data)",alpha_factory_v1/requests.py,
survived,"    async def request(
        self,
        url: str,
        method: Literal[""head"", ""get"", ""post"", ""patch"", ""put"", ""delete""],
        headers: Optional[dict[str, str]] = None,
        **kwargs: Any,
    ) -> ClientResponse:
        loop = asyncio.get_running_loop()
        ctx = contextvars.copy_context()
        func_call = functools.partial(
            ctx.run, self._do_request, url=url, method=method, headers=headers, **kwargs
        )
        return await loop.run_in_executor(None, func_call)  # type: ignore
",src/tests/http/clients/webob.py,WebobHttpClient
survived,"    def get_root_value(self, request: Request) -> Query:
        super().get_root_value(request)  # for coverage
        return Query()
",src/tests/http/clients/webob.py,GraphQLView
survived,"def test_mongodb_connection_uri_generation():
    db = MongoDBDatabase(
        host='localhost',
        user='user name',
        password='p@ssword',
        database='mydb',
        port=1234,
        replicaSet='rs0'
    )
    mock_client = MagicMock()
    with patch('pymongo.MongoClient', return_value=mock_client) as mock_mc:
        db.connect()
        mock_mc.assert_called_once()
        uri = mock_mc.call_args.args[0]

    assert uri == 'mongodb://user+name:p%40ssword@localhost:1234/mydb?replicaSet=rs0'
    assert db.connection is mock_client
    assert db.db == mock_client.__getitem__.return_value",peepdb/tests/test_mongodb_uri.py,
survived,"def find_deps(code):
    deps = []
    for imp in re.findall(r""import[^'\""]*['\""](.*?)['\""]"", code):
        if imp.startswith(ALIAS_PREFIX) or imp.startswith("".""):
            deps.append(imp)
    return deps
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"def _import_detectors():
    try:
        from alpha_factory_v1.demos.era_of_experience.alpha_detection import (
            detect_yield_curve_alpha,
            detect_supply_chain_alpha,
        )
    except ModuleNotFoundError:  # pragma: no cover - running as stand-alone script
        import pathlib
        import sys

        sys.path.append(str(pathlib.Path(__file__).resolve().parents[3]))
        from alpha_factory_v1.demos.era_of_experience.alpha_detection import (
            detect_yield_curve_alpha,
            detect_supply_chain_alpha,
        )
    return detect_yield_curve_alpha, detect_supply_chain_alpha
",alpha_factory_v1/demos/era_of_experience/alpha_report.py,
survived,"    def diversity_histogram(self) -> dict[tuple[str, str], int]:
        cur = self.conn.execute(
            ""SELECT sector, approach, COUNT(*) FROM solutions GROUP BY sector, approach""
        )
        rows = cur.fetchall()
        return {(r[0], r[1]): int(r[2]) for r in rows}
",src/archive/solution_archive.py,SolutionArchive
survived,"    def add(self, sector: str, approach: str, score: float, data: Mapping[str, Any]) -> None:
        band = self._band(score)
        self.conn.execute(
            ""INSERT INTO solutions(sector, approach, score, band, data, ts) VALUES (?,?,?,?,?,?)"",
            (sector, approach, score, band, json.dumps(dict(data)), time.time()),
        )
        if isinstance(self.conn, sqlite3.Connection):  # pragma: no cover - sqlite
            self.conn.commit()
",src/archive/solution_archive.py,SolutionArchive
survived,"def _make_repo(tmp_path: Path) -> Path:
    repo = tmp_path / ""repo""
    repo.mkdir()
    (repo / ""metric.txt"").write_text(""1\n"", encoding=""utf-8"")
    (repo / ""test_dummy.py"").write_text(""def test_ok():\n    assert True\n"", encoding=""utf-8"")
    return repo
",tests/test_meta_refinement_agent.py,
survived,"def test_mutate_rejects_traversal(server: str) -> None:
    import io
    import tarfile

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""../evil.txt"")
        data = b""bad""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""bad.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 400",tests/test_evolution_worker_safe_extract.py,
survived,"def test_evolution_panel_persists_after_reload() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri() + ""#s=1&p=3&g=3""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""window.gen >= 3"")
            page.reload()
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""document.querySelectorAll('#evolution-panel table tr').length > 1"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",tests/test_evolution_panel_reload.py,
survived,"    def run(self) -> None:
        pass
",tests/resources/openai_agents.py,AgentRuntime
survived,"def main():
    root = 'pages/docs'
    for dirpath, dirnames, filenames in os.walk(root):
        for name in filenames:
            if name.endswith('.mdx'):
                fix_file(os.path.join(dirpath, name))
",scripts/fix_titlecase.py,
survived,"def makeAdder(n):
    def adder(x):
        return x + n
    return adder
",tests/human/python/closure.py,
survived,"        def get_proxied_curl_session(impersonate=""chrome120"", **kw):
            if CurlSession:
                return CurlSession(proxies=proxies, impersonate=impersonate, **kw)
            raise ImportError(""curl_cffi is not installed"")
",webscout/Provider/TTI/base.py,ProxyAutoMeta
survived,"def is_linux():
    return platform.system() == 'Linux'
",build.py,
survived,"def test_thermodynamic_trigger_edges() -> None:
    sec = sector.Sector(""x"", energy=1.0, entropy=2.0)
    assert not forecast.thermodynamic_trigger(sec, 0.5)
    assert forecast.thermodynamic_trigger(sec, 0.50001)
    sec2 = sector.Sector(""y"", energy=0.0, entropy=1.0)
    assert not forecast.thermodynamic_trigger(sec2, 0.0)
    assert forecast.thermodynamic_trigger(sec2, 0.1)
",tests/test_forecast.py,
survived,"    def test_log_dir_created_lazily(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            with mock.patch(""tempfile.gettempdir"", return_value=tmp):
                sys.modules.pop(""alpha_factory_v1.backend"", None)
                backend = importlib.import_module(""alpha_factory_v1.backend"")
                log_dir = Path(tmp) / ""alphafactory""
                self.assertFalse(log_dir.exists())
                backend._read_logs()
                self.assertTrue(log_dir.exists())
            sys.modules.pop(""alpha_factory_v1.backend"", None)",tests/test_log_dir_lazy.py,TestLogDirLazy
survived,"def _local_available_space(path: str) -> int:
    """"""Return available bytes on the local filesystem.""""""
    statvfs = os.statvfs(path)
    return statvfs.f_frsize * statvfs.f_bavail
",pioreactor/actions/leader/backup_database.py,
survived,"def test_maybe_await_async():
    result = asyncio.run(maybe_await(_async_fn, 5))
    assert result == 10
",tests/test_agent_runner_utils.py,
survived,"def test_maybe_await_sync():
    result = asyncio.run(maybe_await(_sync_fn, 5))
    assert result == 6
",tests/test_agent_runner_utils.py,
survived,"async def _async_fn(x):
    await asyncio.sleep(0)
    return x * 2
",tests/test_agent_runner_utils.py,
survived,"async def trigger_execution() -> str:
    resp = requests.post(f""{HOST}/agent/alpha_execution/trigger"", timeout=5)
    resp.raise_for_status()
    return ""alpha_execution queued""
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def _require_openai_agents() -> None:
    """"""Ensure the ``openai_agents`` package is available.

    Attempts an automatic install via :mod:`check_env` when the package is
    missing so the bridge remains usable in fresh environments or Colab
    runtimes. Any installation errors are surfaced to the user.
    """"""

    try:  # soft dependency
        import openai_agents  # type: ignore
    except ModuleNotFoundError:  # pragma: no cover - optional dep
        try:
            import check_env

            print(""‚ÑπÔ∏è  openai_agents missing ‚Äì attempting auto-install‚Ä¶"")
            check_env.main([""--auto-install""])
        except Exception as exc:  # pragma: no cover - install failed
            sys.stderr.write(
                f""\n‚ùå  openai_agents not installed and auto-install failed: {exc}\n""
            )
            sys.stderr.write(""   Install manually with 'pip install openai-agents'\n"")
            sys.exit(1)
        try:
            import openai_agents  # type: ignore  # noqa: F401
        except ModuleNotFoundError:
            sys.stderr.write(
                ""\n‚ùå  openai_agents still missing after auto-install.\n""
            )
            sys.stderr.write(""   Install manually with 'pip install openai-agents'\n"")
            sys.exit(1)
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def main() -> None:
    try:
        check_env.main([""--auto-install""])
    except Exception as exc:  # pragma: no cover - optional network failure
        print(f""‚ö†Ô∏è  Environment check failed: {exc}"")

    env = os.environ.copy()
    port = env.get(""PORT"", ""8000"")
    cmd = [sys.executable, ""run_business_v1_local.py"", ""--bridge""]
    proc = subprocess.Popen(cmd, cwd=SCRIPT_DIR, env=env)

    url = f""http://localhost:{port}/docs""
    for _ in range(20):
        if proc.poll() is not None:
            break
        try:
            import requests

            if requests.get(f""http://localhost:{port}/healthz"", timeout=1).status_code == 200:
                break
        except Exception:
            time.sleep(0.5)
    try:
        webbrowser.open(url, new=1)
    except Exception:
        print(f""Open {url} to access the dashboard"")
    try:
        proc.wait()
    except KeyboardInterrupt:
        proc.terminate()
        proc.wait()
",alpha_factory_v1/demos/alpha_agi_business_v1/start_alpha_business.py,
survived,"def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Launch the alpha_agi_business_v1 demo"")
    parser.add_argument(
        ""--no-browser"",
        action=""store_true"",
        help=""Do not open the REST docs in a web browser"",
    )
    return parser.parse_args(argv)
",alpha_factory_v1/demos/alpha_agi_business_v1/start_alpha_business.py,
survived,"def test_group_tag_data_by_resource_type():
    grouped = rgta._group_tag_data_by_resource_type(
        copy.deepcopy(test_data.GET_RESOURCES_RESPONSE),
        rgta.TAG_RESOURCE_TYPE_MAPPINGS,
    )
    assert len(grouped[""ec2:instance""]) == 1
    assert len(grouped[""s3""]) == 1
",tests/unit/cartography/intel/aws/test_resourcegroupstaggingapi.py,
survived,"    def _rewrite(match: re.Match[str]) -> str:
        url, anchor = match.group(1), match.group(2) or """"
        if url.startswith((""http://"", ""https://"", ""#"", ""mailto:"")):
            return match.group(0)
        target = (demo / url).resolve()
        try:
            rel = target.relative_to(REPO_ROOT)
        except ValueError:
            return match.group(0)
        return f""({github_base}{rel.as_posix()}{anchor})""
",scripts/generate_demo_docs.py,
survived,"def test_dslice_oob_read_and_write():
    Seq = hax.Axis(""seq"", 5)
    from haliax import ds

    arr = hax.arange((Seq,), dtype=int)
    out = arr[{""seq"": ds(3, 4)}]
    ref = jnp.take(arr.array, jnp.arange(3, 7), mode=""fill"", fill_value=0)
    assert jnp.array_equal(out.array, ref)

    upd = hax.arange((Seq.resize(4),), dtype=int)
    updated = arr.at[{""seq"": ds(3, 4)}].set(upd)
    ref_upd = arr.array.at[jnp.arange(3, 7)].set(upd.array, mode=""drop"")
    assert jnp.array_equal(updated.array, ref_upd)
",tests/core_test.py,
survived,"    def __init__(self, bus: orchestrator.messaging.A2ABus, ledger: orchestrator.Ledger) -> None:
        super().__init__(""fail"", bus, ledger)
",tests/test_orchestrator.py,FailingAgent
survived,"    async def handle(self, _env: orchestrator.messaging.Envelope) -> None:  # pragma: no cover - test helper
        pass
",tests/test_orchestrator.py,FailingAgent
survived,"    def boom(*_a, **_kw):
        raise FileNotFoundError(""docker"")
",tests/test_start_aiga_demo.py,
survived,"    def rotate_ip(self) -> str:
        """"""Rotate through the IP pool and return the next IP.""""""
        if not self.ip_pool:
            self.ip_pool = self._generate_ip_pool(20)
            self._ip_index = 0

        ip = self.ip_pool[self._ip_index]
        self._ip_index = (self._ip_index + 1) % len(self.ip_pool)
        return ip
",webscout/litagent/agent.py,LitAgent
survived,"def test_strategy_agent_logs_exception(monkeypatch):
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = strategy_agent.StrategyAgent(bus, led)

    monkeypatch.setattr(local_llm, ""chat"", lambda *_: (_ for _ in ()).throw(RuntimeError(""boom"")))
    env = messaging.Envelope(""research"", ""strategy"", {""research"": ""foo""}, 0.0)
    with mock.patch.object(strategy_agent.log, ""warning"") as warn:
        asyncio.run(agent.handle(env))
        warn.assert_called_once()
",tests/test_agent_logging.py,
survived,"def reset_env(monkeypatch):
    monkeypatch.delenv(""SELF_IMPROVE_PROVIDER"", raising=False)
",tests/test_self_edit_prompting.py,
survived,"        def chat(self, prompt: str, system_prompt: str | None = None) -> str:
            calls[""prompt""] = prompt
            calls[""system""] = system_prompt
            return ""patch-prov""
",tests/test_self_edit_prompting.py,Dummy
survived,"def test_simulate_export_csv() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--pop-size"",
                    ""1"",
                    ""--generations"",
                    ""1"",
                    ""--export"",
                    ""csv"",
                ],
            )
    assert res.exit_code == 0
    assert ""year,capability,affected"" in res.output",tests/test_cli.py,
survived,"def test_accepts_normal_patch() -> None:
    diff = """"""--- a/src/foo.py
+++ b/src/foo.py
@@
-a
+b
""""""
    assert is_patch_valid(diff)",tests/test_patch_guard.py,
survived,"def plot_histogram(counts: Iterable[int], out_file: str | Path = DEFAULT_OUT) -> None:
    """"""Save histogram of ``counts`` to ``out_file``.""""""
    df = pd.DataFrame({""backtracks"": list(counts)})
    fig = px.histogram(df, x=""backtracks"")
    path = Path(out_file)
    path.parent.mkdir(parents=True, exist_ok=True)
    fig.write_image(str(path))
",src/tools/analyse_backtrack.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"def pytest_pyfunc_call(pyfuncitem):
    test_func = pyfuncitem.obj
    if inspect.iscoroutinefunction(test_func):
        asyncio.run(test_func(**pyfuncitem.funcargs))
        return True
    return None",pytest_asyncio.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,
survived,"    def emit(self, message: str, level: LogLevel):
        raise NotImplementedError
",webscout/litlogger/handlers.py,Handler
survived,"    def _format(self, level: LogLevel, message: str) -> str:
        now = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
        return self.format.format(time=now, level=level.name, name=self.name, message=message)
",webscout/litlogger/logger.py,Logger
survived,"    def __enter__(self):
        return self
",webscout/litlogger/logger.py,Logger
survived,"    def warning(self, message: str):
        self.log(LogLevel.WARNING, message)
",webscout/litlogger/logger.py,Logger
survived,"    async def fetch_and_replace() -> None:
        try:
            models = await asyncio.to_thread(load_from_url, url)
            built_in_models[:] = models
        except Exception:
            pass
",libs/core/kiln_ai/adapters/remote_config.py,
survived,"        async def __call__(self, _text: str) -> str:  # pragma: no cover - stub
            return ""ok""
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,OpenAIAgent
survived,"def _pkg_installed(pkg: str) -> bool:
    """"""Return ``True`` when ``pkg`` is installed.

    The check uses ``python -m pip show`` for reliability when namespace
    packages are involved.
    """"""

    result = subprocess.run(
        [sys.executable, ""-m"", ""pip"", ""show"", pkg],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    return result.returncode == 0
",scripts/check_python_deps.py,
survived,"def test_extract_relative_link():
    scraper = AutoScraper()
    url = ""https://example.com/index.html""
    result = scraper.build(url=url, html=HTML_COMPLEX, wanted_list=[""https://example.com/apple""])
    assert ""https://example.com/apple"" in result
    similar = scraper.get_result_similar(
        url=url, html=HTML_COMPLEX, contain_sibling_leaves=True, unique=True
    )
    assert set(similar) == {
        ""https://example.com/banana"",
        ""https://example.com/apple"",
        ""https://example.com/orange"",
    }
    exact = scraper.get_result_exact(url=url, html=HTML_COMPLEX)
    assert exact == [""https://example.com/apple""]
",tests/integration/test_complex_features.py,
survived,"def test_keep_blank_returns_empty():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX, wanted_list=[""/shop""])
    html_blank = HTML_COMPLEX.replace('href=""/shop""', 'href=""""')
    result = scraper.get_result_exact(html=html_blank, keep_blank=True)
    assert result == [""""]
",tests/integration/test_complex_features.py,
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    req_txt = repo_root / ""alpha_factory_v1"" / ""requirements-colab.txt""
    lock_file = repo_root / ""alpha_factory_v1"" / ""requirements-colab.lock""

    with tempfile.TemporaryDirectory() as tmpdir:
        out_path = Path(tmpdir) / ""requirements.lock""
        pip_compile = shutil.which(""pip-compile"")
        if pip_compile:
            cmd = [pip_compile]
        else:
            cmd = [sys.executable, ""-m"", ""piptools"", ""compile""]
        cmd += [""--quiet"", ""--generate-hashes"", str(req_txt), ""-o"", str(out_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        sys.stdout.write(result.stdout)
        sys.stderr.write(result.stderr)
        if result.returncode != 0:
            return result.returncode
        if out_path.read_bytes() != lock_file.read_bytes():
            sys.stderr.write(
                ""alpha_factory_v1/requirements-colab.lock is outdated. ""
                ""Run 'pip-compile --quiet --generate-hashes alpha_factory_v1/requirements-colab.txt'\n""
            )
            return 1
    return 0
",scripts/verify_alpha_colab_requirements_lock.py,
survived,"    def start_merkle_task(self, interval: int = 3600) -> None:
        if self._task is None:
            self._task = asyncio.create_task(self._loop(interval))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"    def tail(self, count: int = 10) -> List[dict[str, object]]:
        """"""Return the last ``count`` ledger entries.""""""

        cur = self.conn.execute(
            ""SELECT ts, sender, recipient, payload FROM messages ORDER BY id DESC LIMIT ?"",
            (count,),
        )
        rows = cur.fetchall()
        result: List[dict[str, object]] = []
        for ts, sender, recipient, payload in reversed(rows):
            try:
                data = json.loads(payload)
            except Exception:
                data = payload
            result.append({""ts"": ts, ""sender"": sender, ""recipient"": recipient, ""payload"": data})
        return result
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"def test_show_results_table(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 1.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            res = CliRunner().invoke(cli.main, [""show-results""])
            assert ""sender"" in res.output
            assert ""a"" in res.output",tests/test_demo_cli.py,
survived,"    def append(self, new_child: Union['Tag', NavigableString, str]) -> None:
        """"""Append a new child to this tag.""""""
        if isinstance(new_child, str):
            new_child = NavigableString(new_child)
        new_child.parent = self
        self.contents.append(new_child)
",webscout/scout/element.py,Tag
survived,"def test_schema_checker_labels_valid():
    usage_scenario_name_dict = 'schema_checker_valid_labels_as_dict.yml'
    usage_scenario_path_dict = os.path.join(CURRENT_DIR, '../data/usage_scenarios/schema_checker/', usage_scenario_name_dict)
    with open(usage_scenario_path_dict, encoding='utf8') as file:
        usage_scenario_dict = yaml.safe_load(file)
    schema_checker_dict = SchemaChecker(validate_compose_flag=True)
    schema_checker_dict.check_usage_scenario(usage_scenario_dict)

    usage_scenario_name_list = 'schema_checker_valid_labels_as_list.yml'
    usage_scenario_path_list = os.path.join(CURRENT_DIR, '../data/usage_scenarios/schema_checker/', usage_scenario_name_list)
    with open(usage_scenario_path_list, encoding='utf8') as file:
        usage_scenario_list = yaml.safe_load(file)
    schema_checker_list = SchemaChecker(validate_compose_flag=True)
    schema_checker_list.check_usage_scenario(usage_scenario_list)
",tests/lib/test_schema_checker.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q10.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q7.py,
survived,"def test_run_inference_raises_for_empty_glob(tmp_path):
    config = InferenceConfig(
        input_path=str(tmp_path),
        output_path=str(tmp_path / ""out""),
        model_name=""dummy"",
        model_type=""fasttext"",
        attribute_name=""test"",
    )

    with pytest.raises(FileNotFoundError):
        ray.get(run_inference.remote(config))",tests/test_classification_inference_empty_glob.py,
survived,"                def _loop(self) -> None:
                    while True:
                        try:
                            res = step_fn()
                            if asyncio.iscoroutine(res):
                                asyncio.run(res)
                        except Exception as exc:  # pragma: no cover
                            LOG.debug(""[Adapter:%s] step error: %s"", name, exc)
                        time.sleep(max(1, interval))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,StepAdapter
survived,"    def Histogram(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Histogram, name, desc, labels)
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def Gauge(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Gauge, name, desc, labels)
",alpha_factory_v1/backend/agents/registry.py,
survived,"def _register(meta: AgentMetadata, *, overwrite: bool = False) -> None:
    if not _should_register(meta):
        return
    with _REGISTRY_LOCK:
        if meta.name in AGENT_REGISTRY and not overwrite:
            existing = AGENT_REGISTRY[meta.name]
            try:
                if _parse_version(meta.version) > _parse_version(existing.version):
                    logger.info(
                        ""Overriding agent %s with newer version %s > %s"",
                        meta.name,
                        meta.version,
                        existing.version,
                    )
                    overwrite = True
                else:
                    logger.error(""Duplicate agent name '%s' ignored"", meta.name)
                    return
            except Exception:  # pragma: no cover - version parse failed
                logger.error(""Duplicate agent name '%s' ignored"", meta.name)
                return

        AGENT_REGISTRY[meta.name] = meta
        for cap in meta.capabilities:
            CAPABILITY_GRAPH.add(cap, meta.name)

    logger.info(""\u2713 agent %-18s caps=%s"", meta.name, "","".join(meta.capabilities))
    _emit_kafka(""agent.manifest"", meta.to_json())
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def _to_example(row):
        ids = row[""input_ids""].tolist()
        src_len = int(row[""sources_len""])
        if len(ids) > Pos.size:
            ids = ids[: Pos.size]
        else:
            ids = ids + [pad_id] * (Pos.size - len(ids))
        tokens = hax.named(np.array(ids, dtype=np.int32), Pos)
        return LmExample.from_prompt_and_completion(Pos, tokens, prompt_length=src_len)
",src/levanter/main/eval_sliding_lm.py,
survived,"    def test_restart_unresponsive_agent(self) -> None:
        tmp = tempfile.TemporaryDirectory()
        settings = config.Settings(bus_port=0, ledger_path=os.path.join(tmp.name, ""ledger.db""), offline=True)

        def _agents(self: orchestrator.Orchestrator) -> list[BaseAgent]:
            return [FreezeAgent(self.bus, self.ledger)]

        with mock.patch.object(orchestrator.Orchestrator, ""_init_agents"", _agents):
            orch = orchestrator.Orchestrator(settings)

        runner = orch.runners[""freeze""]

        async def run() -> bool:
            await orch.bus.start()
            orch.ledger.start_merkle_task(3600)
            runner.start(orch.bus, orch.ledger)
            monitor = asyncio.create_task(orch._monitor())
            await asyncio.sleep(3)
            active = runner.task is not None and not runner.task.done()
            monitor.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await monitor
            if runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
            await orch.bus.stop()
            await orch.ledger.stop_merkle_task()
            orch.ledger.close()
            return active

        with mock.patch.object(_log, ""warning"") as warn:
            active = asyncio.run(run())
            warn.assert_any_call(""%s unresponsive ‚Äì restarting"", ""freeze"")
        self.assertTrue(active)
        tmp.cleanup()
",tests/test_insight_orchestrator_restart.py,TestInsightOrchestratorRestart
survived,"    def test_symbol(self):
        r = self.klong(',:foo')
        self.assertTrue(r.dtype == object)
        self.assertEqual(len(r), 1)
        self.assertEqual(r[0], KGSym('foo'))
",tests/test_eval_monad_list.py,TestEvalMonadList
survived,"def _session() -> requests.Session:
    """"""Return a session with basic retry logic.""""""
    retry = Retry(total=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    s = requests.Session()
    s.mount(""https://"", adapter)
    s.mount(""http://"", adapter)
    return s
",scripts/download_hf_gpt2.py,
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - interface
        pass
",tests/test_safety_guardian_fuzz.py,DummyLedger
survived,"def load_sectors(path: str | os.PathLike[str]) -> list[Sector]:
    """"""Load sector definitions from a JSON file.

    The file may contain a list of strings representing sector names or a list
    of objects with ``name`` and optional ``energy``, ``entropy`` and ``growth``
    fields.
    """"""
    with open(path, ""r"", encoding=""utf-8"") as f:
        data = json.load(f)

    sectors: list[Sector] = []
    for entry in data:
        if isinstance(entry, str):
            sectors.append(Sector(entry))
        elif isinstance(entry, dict):
            sectors.append(
                Sector(
                    entry.get(""name"", """"),
                    float(entry.get(""energy"", 1.0)),
                    float(entry.get(""entropy"", 1.0)),
                    float(entry.get(""growth"", 0.05)),
                    bool(entry.get(""disrupted"", False)),
                )
            )
        else:
            raise ValueError(f""Invalid sector entry: {entry!r}"")
    return sectors",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/sector.py,
survived,"    async def status(_: None = Depends(verify_token)) -> StatusResponse:
        """"""Return orchestrator agent stats.""""""

        orch = cast(Any, app_f.state.orchestrator)
        if orch is None:
            raise HTTPException(status_code=503, detail=""Orchestrator not running"")
        items = [
            AgentStatus(name=r.agent.name, last_beat=r.last_beat, restarts=r.restarts) for r in orch.runners.values()
        ]
        return StatusResponse(agents=items)
",src/interface/api_server.py,
survived,"    async def no_sleep(_: float) -> None:
        return None
",tests/test_retry_property.py,
survived,"    def test_latest_fed_speech_uses_feedparser(self) -> None:
        async def run_once() -> str | None:
            data_feeds._CACHE_SPEECH.clear()
            with (
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds._session"", new_callable=AsyncMock),
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds.feedparser.parse"") as parse_mock,
            ):
                parse_mock.return_value = type(""F"", (), {""entries"": [type(""E"", (), {""title"": ""Hello""})()]})()
                result = await data_feeds._latest_fed_speech()
                parse_mock.assert_called_once_with(data_feeds.RSS_URL)
                return result

        title = asyncio.run(run_once())
        self.assertEqual(title, ""Hello"")

        async def run_again() -> str | None:
            with (
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds._session"", new_callable=AsyncMock),
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds.feedparser.parse"") as parse_mock,
            ):
                parse_mock.return_value = type(""F"", (), {""entries"": [type(""E"", (), {""title"": ""Hello""})()]})()
                return await data_feeds._latest_fed_speech()

        title2 = asyncio.run(run_again())
        self.assertIsNone(title2)
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"        async def run_again() -> str | None:
            with (
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds._session"", new_callable=AsyncMock),
                patch(""alpha_factory_v1.demos.macro_sentinel.data_feeds.feedparser.parse"") as parse_mock,
            ):
                parse_mock.return_value = type(""F"", (), {""entries"": [type(""E"", (), {""title"": ""Hello""})()]})()
                return await data_feeds._latest_fed_speech()
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"def test_docs_bundle_integrity() -> None:
    bundle = DOCS_DIR / ""insight.bundle.js""
    if not bundle.is_file():
        pytest.skip(""insight.bundle.js missing"")
    html = (DOCS_DIR / ""index.html"").read_text()
    match = re.search(r""<script[^>]*src=['\""]insight.bundle.js['\""][^>]*>"", html)
    assert match, ""insight.bundle.js script tag missing""
    tag = match.group(0)
    integrity = re.search(r""integrity=['\""]([^'\""]+)['\""]"", tag)
    assert integrity, ""integrity attribute missing""
    expected = _sha384(bundle)
    assert integrity.group(1) == expected",tests/test_docs_bundle_hash.py,
survived,"def _reload_module(monkeypatch=None):
    if MODULE in sys.modules:
        del sys.modules[MODULE]
    if monkeypatch:
        import types
        fake = types.ModuleType(""torch"")
        fake.__path__ = []  # mark as package
        fake.manual_seed = lambda *_a, **_k: None
        fake.cuda = types.SimpleNamespace(is_available=lambda: False)
        fake.tensor = lambda *a, **k: None
        fake.float32 = ""float32""
        fake.no_grad = contextlib.nullcontext
        fake.tanh = lambda x: x
        fake.cat = lambda xs, dim=None: None
        nn_mod = types.ModuleType(""torch.nn"")
        nn_mod.Module = object
        nn_mod.Linear = lambda *a, **k: object()
        f_mod = types.ModuleType(""torch.nn.functional"")
        f_mod.one_hot = lambda x, num_classes: x
        f_mod.mse_loss = lambda a, b: 0.0
        f_mod.log_softmax = lambda x, dim=-1: x
        nn_mod.functional = f_mod
        optim_mod = types.ModuleType(""torch.optim"")
        optim_mod.Adam = lambda params, lr: object()
        fake.nn = nn_mod
        fake.optim = optim_mod
        monkeypatch.setitem(sys.modules, ""torch"", fake)
        monkeypatch.setitem(sys.modules, ""torch.nn"", nn_mod)
        monkeypatch.setitem(sys.modules, ""torch.nn.functional"", f_mod)
        monkeypatch.setitem(sys.modules, ""torch.optim"", optim_mod)
    return importlib.import_module(MODULE)
",tests/test_world_model_safety.py,
survived,"def prune_expired_tokens(buffer: dict[str, float]) -> None:
    """"""Remove tokens older than ``TOKEN_TTL`` from *buffer*.""""""
    now = time.time()
    expired = [t for t, ts in buffer.items() if now - ts > TOKEN_TTL]
    for t in expired:
        buffer.pop(t, None)
",alpha_factory_v1/backend/trace_ws.py,
survived,"def test_cli_execution() -> None:
    result = subprocess.run(
        [
            sys.executable,
            ""-m"",
            ""alpha_factory_v1.demos.alpha_agi_business_3_v1.alpha_agi_business_3_v1"",
            ""--cycles"",
            ""1"",
            ""--loglevel"",
            ""warning"",
        ],
        capture_output=True,
        text=True,
    )
    assert result.returncode == 0, result.stderr
",tests/test_alpha_agi_business_3_v1.py,
survived,"def _demo_url(demo: str) -> str:
    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"")
    repo_path = repo_path.removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    return f""https://{org}.github.io/{repo}/{demo}/""
",scripts/open_demo.py,
survived,"def test_regression_guard(monkeypatch) -> None:
    alerts: list[str] = []
    monkeypatch.setattr(orchestrator.alerts, ""send_alert"", lambda m: alerts.append(m))
    runner = DummyRunner()
    runners = {""aiga_evolver"": runner}

    async def drive() -> float:
        guard = asyncio.create_task(orchestrator._regression_guard(runners))
        start = time.time()
        for v in [1.0, 0.7, 0.5, 0.3]:
            metrics.dgm_best_score.set(v)
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.5)
        duration = time.time() - start
        guard.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await guard
        return duration

    dur = asyncio.run(drive())
    assert runner.task.cancelled
    assert dur < 10
    assert alerts
",tests/test_governance.py,
survived,"async def _regression_guard(runners: Dict[str, AgentRunner]) -> None:
    history: deque[float] = deque(maxlen=3)
    while True:
        await asyncio.sleep(1)
        try:
            sample = metrics.dgm_best_score.collect()[0].samples[0]
            score = float(sample.value)
        except Exception:  # pragma: no cover - metrics optional
            continue
        history.append(score)
        if (
            len(history) == 3
            and history[1] <= history[0] * 0.8
            and history[2] <= history[1] * 0.8
        ):
            runner = runners.get(""aiga_evolver"")
            if runner and runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
            alerts.send_alert(""Evolution paused due to metric regression"")
            history.clear()
",alpha_factory_v1/backend/orchestrator.py,
survived,"    def critique(self, text: str) -> float:
        """"""Return a score in [0,1] based on word overlap with :data:`_WORDS`.""""""
        tokens = re.findall(r""[a-zA-Z']+"", text.lower())
        if not tokens:
            return 0.0
        good = sum(1 for t in tokens if t in self._WORDS)
        return good / len(tokens)",src/agents/reviewer_agent.py,ReviewerAgent
survived,"    async def get_proof(agent_id: str, _: None = Depends(verify_token)) -> ProofResponse | JSONResponse:
        """"""Return stored proof CID for ``agent_id`` if present.""""""

        start = time.perf_counter()
        status = ""200""
        try:
            path = Path(os.getenv(""ARCHIVE_PATH"", ""archive.db""))
            db = ArchiveDB(path)
            cid = db.get_proof_cid(agent_id)
            if cid is None:
                raise HTTPException(status_code=404)
            proof = db.get_state(f""proof:{agent_id}"")
            return ProofResponse(cid=cid, proof=proof)
        except HTTPException as exc:
            status = str(exc.status_code)
            return problem_response(exc)
        finally:
            REQ_COUNT.labels(""GET"", ""/proof/{agent_id}"", status).inc()
            REQ_LAT.labels(""GET"", ""/proof/{agent_id}"").observe(time.perf_counter() - start)
",src/interface/api_server.py,
survived,"def _hash_scores(scores: Sequence[float]) -> str:
    data = "","".join(f""{s:.8f}"" for s in scores).encode()
    return sha256(data).hexdigest()
",src/snark/proof.py,
survived,"    def fake_run(*_a, **_k):
        return subprocess.CompletedProcess([], 1, """", """")
",tests/test_preflight_sandbox.py,
survived,"def download_model(dest: Path, model: str = ""124M"") -> None:
    """"""Download GPT-2 weights using the official helper script.""""""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        subprocess.run([""git"", ""clone"", ""--depth"", ""1"", OPENAI_REPO, str(tmp_path)], check=True)
        script = tmp_path / ""download_model.py""
        subprocess.run([sys.executable, str(script), model], cwd=tmp_path, check=True)
        target = dest / model
        target.mkdir(parents=True, exist_ok=True)
        shutil.copytree(tmp_path / ""models"" / model, target, dirs_exist_ok=True)
",scripts/download_gpt2_small.py,
survived,"async def _client(ctx: EnrichContext) -> httpx.AsyncClient:
    """"""Helper to get the shared HTTP client.""""""
    return ctx.request_context.lifespan_context[""client""]
",examples/shop_api_gateway/app.py,
survived,"async def get_order_user(user_id: int, ctx: EnrichContext) -> User:
    return await get_user(user_id=user_id, ctx=ctx)
",examples/shop_api_gateway/app.py,
survived,"def main() -> int:
    env_vars = parse_env_sample(ENV_SAMPLE)
    md_vars = parse_agents_table(AGENTS_MD)

    missing_in_md = sorted(env_vars - md_vars)
    missing_in_env = sorted(md_vars - env_vars)

    if missing_in_md or missing_in_env:
        if missing_in_md:
            print(""Missing from AGENTS.md:"", "", "".join(missing_in_md))
        if missing_in_env:
            print(""Missing from .env.sample:"", "", "".join(missing_in_env))
        return 1

    print(""Environment variable table is up-to-date."")
    return 0
",tools/check_env_table.py,
survived,"def get_image_analysis_google(api_key, model_name, prompt, base64_image):
    client = google_genai.Client(api_key=api_key)
    from google.genai import types as google_types

    blob = google_types.Blob(data=base64.b64decode(base64_image), mime_type=""image/jpeg"")
    content = [
        google_types.Content(role=""user"", parts=[
            google_types.Part(text=prompt),
            google_types.Part(inlineData=blob),
        ])
    ]

    config = google_types.GenerateContentConfig()
    response = client.models.generate_content(model=model_name, contents=content, config=config)

    return {""choices"": [{""message"": {""content"": response.text}}]}
",threat_model.py,
survived,"def format2(f):
    s = str(f)
    idx = indexOf(s, ""."")
    if idx < 0:
        s = s + "".00""
    else:
        need = idx + 3
        if len(s) > need:
            s = s[0:need]
        else:
            while len(s) < need:
                s = s + ""0""
    return s
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def indexOfStr(h, n):
    hlen = len(h)
    nlen = len(n)
    if nlen == 0:
        return 0
    i = 0
    while i <= hlen - nlen:
        if h[i:i + nlen] == n:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/boyer-moore-string-search.py,
survived,"def parseBool(s):
    l = s.lower()
    if l == ""1"" or l == ""t"" or l == True or l == ""yes"" or l == ""y"":
        return True
    return False
",tests/rosetta/transpiler/Python/boolean-values.py,
survived,"def main():
    fn = lambda r: ("""" if r == "" "" else r)
    mapString(""Spaces removed"", fn)
    mapString(""Test"", lambda r: r.lower())
    mapString(""shift"", lambda r: r)
",tests/rosetta/transpiler/Python/call-a-function-8.py,
survived,"def strdup(s):
    return s + """"
",tests/rosetta/transpiler/Python/call-a-foreign-language-function.py,
survived,"def primesUpTo(n):
    sieve = []
    i = 0
    while i <= n:
        sieve = sieve + [True]
        i = i + 1
    p = 2
    while p * p <= n:
        if sieve[p]:
            m = p * p
            while m <= n:
                sieve[m] = False
                m = m + p
        p = p + 1
    res = []
    x = 2
    while x <= n:
        if sieve[x]:
            res = res + [x]
        x = x + 1
    return res
",tests/rosetta/transpiler/Python/brilliant-numbers.py,
survived,"def main():
    pt = ""The five boxing wizards jump quickly""
    print(""Plaintext: "" + pt)
    for key in [0, 1, 7, 25, 26]:
        if key < 1 or key > 25:
            print(""Key "" + str(key) + "" invalid"")
            continue
        ct = encipher(pt, key)
        print(""Key "" + str(key))
        print(""  Enciphered: "" + ct)
        print(""  Deciphered: "" + decipher(ct, key))
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"def _lead_time(truth: list[bool], pred: list[bool]) -> int:
    def first_true(seq: list[bool]) -> int:
        for i, val in enumerate(seq):
            if val:
                return i
        return len(seq)

    return first_true(pred) - first_true(truth)
",src/eval/foresight.py,
survived,"def main():
    global seed
    print(eqIndices([-7, 1, 5, 2, -4, 3, 0]))
    verylong = []
    i = 0
    while i < 10000:
        seed = (seed * 1664525 + 1013904223) % 2147483647
        verylong = verylong + [seed % 1001 - 500]
        i = i + 1
    print(eqIndices(verylong))
",tests/rosetta/transpiler/Python/equilibrium-index.py,
survived,"def search(xs, target):
    low = 0
    high = len(xs)
    while low < high:
        mid = (low + high) // 2
        if xs[mid] < target:
            low = mid + 1
        else:
            high = mid
    sys.exit(low)
",tests/rosetta/transpiler/Python/equal-prime-and-composite-sums.py,
survived,"def main():
    limit = 45000
    compMap = primeSieve(limit)
    compSums = []
    primeSums = []
    csum = 0
    psum = 0
    i = 2
    while i <= limit:
        if compMap[i]:
            csum = csum + i
            compSums = compSums + [csum]
        else:
            psum = psum + i
            primeSums = primeSums + [psum]
        i = i + 1
    print(""Sum        | Prime Index | Composite Index"")
    print(""------------------------------------------"")
    idx = 0
    while idx < len(primeSums):
        s = primeSums[idx]
        j = search(compSums, s)
        if j < len(compSums) and compSums[j] == s:
            sumStr = commatize(s).rjust(10, "" "")
            piStr = commatize(idx + 1).rjust(11, "" "")
            ciStr = commatize(j + 1).rjust(15, "" "")
            print(sumStr + "" | "" + piStr + "" | "" + ciStr)
        idx = idx + 1
",tests/rosetta/transpiler/Python/equal-prime-and-composite-sums.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/extend-your-language.py,
survived,"def copyInts(xs):
    out = []
    for v in xs:
        out = out + [v]
    return out
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,
survived,"def expF(b, p):
    neg = False
    if p < 0:
        neg = True
        p = -p
    r = 1.0
    pow = b
    while p > 0:
        if p % 2 == 1:
            r = r * pow
        pow = pow * pow
        p = p // 2
    if neg:
        r = 1.0 / r
    return r
",tests/rosetta/transpiler/Python/exponentiation-operator.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/extreme-floating-point-values.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-sequence-2.py,
survived,"def floorf(x):
    y = int(x)
    sys.exit(float(y))
",tests/rosetta/transpiler/Python/fibonacci-word.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""Hello World!"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/execute-snusp.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/even-or-odd.py,
survived,"def run(code):
    acc = 0
    i = 0
    while i < len(code):
        ch = code[i:i + 1]
        if ch == ""H"":
            print(""Hello, World!"")
        else:
            if ch == ""Q"":
                print(code)
            else:
                if ch == ""9"":
                    sing99()
                else:
                    if ch == ""+"":
                        acc = acc + 1
        i = i + 1
",tests/rosetta/transpiler/Python/execute-hq9+.py,
survived,"def else1(i, f):
    if i.cond1 and (i.cond2 == False):
        f()
    return i
",tests/rosetta/transpiler/Python/extend-your-language.py,
survived,"def dfs(n, m, i):
    global esths
    if i >= n and i <= m:
        esths = esths + [i]
    if i == 0 or i > m:
        sys.exit()
    d = i % 10
    i1 = i * 10 + d - 1
    i2 = i1 + 2
    if d == 0:
        dfs(n, m, i2)
    else:
        if d == 9:
            dfs(n, m, i1)
        else:
            dfs(n, m, i1)
            dfs(n, m, i2)
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def listEsths(n, n2, m, m2, perLine, showAll):
    global esths
    esths = []
    i = 0
    while i < 10:
        dfs(n2, m2, i)
        i = i + 1
    le = len(esths)
    print(""Base 10: "" + commatize(le) + "" esthetic numbers between "" + commatize(n) + "" and "" + commatize(m) + "":"")
    if showAll:
        c = 0
        line = """"
        for v in esths:
            if len(line) > 0:
                line = line + "" ""
            line = line + str(v)
            c = c + 1
            if c % perLine == 0:
                print(line)
                line = """"
        if len(line) > 0:
            print(line)
    else:
        line = """"
        idx = 0
        while idx < perLine:
            if len(line) > 0:
                line = line + "" ""
            line = line + str(esths[idx])
            idx = idx + 1
        print(line)
        print(""............"")
        line = """"
        idx = le - perLine
        while idx < le:
            if len(line) > 0:
                line = line + "" ""
            line = line + str(esths[idx])
            idx = idx + 1
        print(line)
    print("""")
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/eulers-identity.py,
survived,"def commatize(n):
    s = str(n)
    res = """"
    i = 0
    while i < len(s):
        if i > 0 and (len(s) - i) % 3 == 0:
            res = res + "",""
        res = res + """".join(s[i:i + 1])
        i = i + 1
    sys.exit(res)
",tests/rosetta/transpiler/Python/file-size-distribution.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            sys.exit(i)
        i = i + 1
    sys.exit(-1)
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def ln(x):
    k = 0.0
    v = x
    while v >= 2.0:
        v = v / 2.0
        k = k + 1.0
    while v < 1.0:
        v = v * 2.0
        k = k - 1.0
    z = (v - 1.0) / (v + 1.0)
    zpow = z
    sum = z
    i = 3
    while i <= 9:
        zpow = zpow * z * z
        sum = sum + zpow / (float(i))
        i = i + 2
    ln2 = 0.6931471805599453
    sys.exit((k * ln2) + 2.0 * sum)
",tests/rosetta/transpiler/Python/eulers-constant-0.5772....py,
survived,"def eulerStep(f, x, y, h):
    sys.exit(y + h * f(x, y))
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""main: start"")
    err = foo()
    if len(err) > 0:
        print(""main: unhandled "" + err)
    else:
        print(""main: success"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/exceptions-catch-an-exception-thrown-in-a-nested-call.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    primes = generatePrimes(1000)
    es = []
    for _ in range(0, 12):
        es = es + [[]]
    print(""First 200 primes:\n"")
    idx = 0
    while idx < 200:
        p = primes[idx]
        c = cat(p, primes)
        es[c - 1] = es[c - 1] + [p]
        idx = idx + 1
    c = 1
    while c <= 6:
        if len(es[c - 1]) > 0:
            print(""Category "" + str(c) + "":"")
            print(str(es[c - 1]))
            print("""")
        c = c + 1
    print(""First thousand primes:\n"")
    while idx < 1000:
        p = primes[idx]
        cv = cat(p, primes)
        es[cv - 1] = es[cv - 1] + [p]
        idx = idx + 1
    c = 1
    while c <= 12:
        e = es[c - 1]
        if len(e) > 0:
            line = ""Category "" + padLeft(c, 2) + "": First = "" + padLeft(e[0], 7) + ""  Last = "" + padLeft(e[len(e) - 1], 8) + ""  Count = "" + padLeft(len(e), 6)
            print(line)
        c = c + 1
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/erd-s-selfridge-categorization-of-primes.py,
survived,"def entropy(s):
    counts = {}
    i = 0
    while i < len(s):
        ch = s[i:i + 1]
        if ch in counts:
            counts[ch] = counts[ch] + 1
        else:
            counts[ch] = 1
        i = i + 1
    hm = 0.0
    for k in list(counts.keys()):
        c = float(counts[k])
        hm = hm + c * (math.log(c) / math.log(2.0))
    l = float(len(s))
    sys.exit((math.log(l) / math.log(2.0)) - hm // l)
",tests/rosetta/transpiler/Python/fibonacci-word.py,
survived,"async def test_relationship_resolver_validation():
    app, lifespan = create_app()
    async with lifespan(app) as ctx:
        session_factory = ctx[""session_factory""]
        mock_ctx = Mock(spec=EnrichContext)
        mock_ctx.request_context = Mock()
        mock_ctx.request_context.lifespan_context = {""session_factory"": session_factory}

        get_orders = app.resources[""get_userenrichmodel_orders""]

        with pytest.raises(ValueError):
            await get_orders(user_id=1, page=0, page_size=1, ctx=mock_ctx)

        empty = await get_orders(page=1, page_size=1, ctx=mock_ctx)
        assert empty.items == []
        assert not empty.has_next
",tests/test_sqlalchemy_autogen_extra.py,
survived,"    def _tool(*_a, **_k):
        def _decorator(func):
            return func

        return _decorator
",tests/test_agents_fallback.py,
survived,"def test_selector_frequency_monte_carlo() -> None:
    pop = [
        Candidate(1.0, 0),
        Candidate(0.2, 3),
        Candidate(1.5, 1),
    ]
    beta, gamma = 0.7, 0.3
    logits = np.asarray([beta * p.score + gamma * p.edit_children_count for p in pop])
    expected = softmax(logits)
    observed = sample_distribution(pop, beta, gamma, runs=50000)
    assert np.allclose(observed, expected, atol=0.02)",tests/test_selector_v2.py,
survived,"def test_run_cycle_posts_job(monkeypatch) -> None:
    """"""`post_alpha_job` should be called once when ŒîG < 0.""""""
    mod = importlib.import_module(MODULE)

    monkeypatch.setattr(mod, ""_A2A"", None)

    orchestrator = mod.Orchestrator()
    fin = mod.AgentFin()
    res = mod.AgentRes()
    ene = mod.AgentEne()
    gdl = mod.AgentGdl()
    model = mod.Model()

    monkeypatch.setattr(orchestrator, ""collect_signals"", lambda: {})
    monkeypatch.setattr(fin, ""latent_work"", lambda _b: 0.0)
    monkeypatch.setattr(res, ""entropy"", lambda _b: 1.0)
    monkeypatch.setattr(ene, ""market_temperature"", lambda _b: 1.0)

    calls: list[tuple[str, float]] = []

    def _post(bundle_id: str, delta_g: float) -> None:
        calls.append((bundle_id, delta_g))

    monkeypatch.setattr(orchestrator, ""post_alpha_job"", _post)

    async def _llm(_: float) -> str:
        return ""ok""

    monkeypatch.setattr(mod, ""_llm_comment"", _llm)

    asyncio.run(
        mod.run_cycle_async(orchestrator, fin, res, ene, gdl, model)
    )

    assert len(calls) == 1
",tests/test_alpha_agi_business_3_v1.py,
survived,"def test_get_context_propagates_errors():
    app = EnrichMCP(""Test API"", description=""desc"")

    with (
        patch.object(app.mcp, ""get_context"", side_effect=RuntimeError(""boom"")),
        pytest.raises(RuntimeError),
    ):
        app.get_context()",tests/test_core.py,
survived,"def _turn_to_pyobj(turn: Turn) -> dict:
    """"""Convert a :class:`Turn` into a Python mapping understood by Arrow.""""""

    return {
        ""message"": turn.message,
        ""role"": turn.role,
        ""logprobs"": list(turn.logprobs) if turn.logprobs is not None else None,
        ""reward"": turn.reward,
        ""inference_metadata_json"": json.dumps(turn.inference_metadata, separators=("","", "":"")),
    }
",marin/rl/parquet_store.py,
survived,"    async def run(self) -> None:
        iteration = 0
        while not await self._should_stop():
            if self._max_iters is not None and iteration >= self._max_iters:
                break

            # ------------------------------------------------------------------
            # Sample a problem (reuse the dataset when exhausted).
            # ------------------------------------------------------------------
            if self._example_idx >= len(self._examples):
                self._rng.shuffle(self._examples)
                self._example_idx = 0
            example = self._examples[self._example_idx]
            self._example_idx += 1

            user_prompt: str = example[""prompt""]
            gt_answer: str = example[""answer""]

            # ------------------------------------------------------------------
            # Call inference server.
            # ------------------------------------------------------------------
            completion = self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {""role"": ""user"", ""content"": user_prompt},
                ],
            )

            assistant_msg: str = completion.choices[0].message.content

            # ------------------------------------------------------------------
            # Reward calculation.
            # ------------------------------------------------------------------
            parsed = validate_format(assistant_msg + "">"")  # util expects trailing >
            is_valid = parsed[""is_valid""]
            extracted_answer = parsed[""answer""]
            is_correct = grade_answer(extracted_answer, gt_answer) if is_valid else False
            reward = float(is_correct)

            # ------------------------------------------------------------------
            # Build rollout & emit.
            # ------------------------------------------------------------------
            turns = [
                Turn(
                    message=user_prompt,
                    role=""user"",
                    logprobs=None,
                    reward=None,
                    inference_metadata={},
                ),
                Turn(
                    message=assistant_msg,
                    role=""assistant"",
                    logprobs=None,  # logprobs not available via OpenAI client
                    reward=reward,
                    inference_metadata={
                        ""model"": self._model,
                        ""finish_reason"": completion.choices[0].finish_reason,
                    },
                ),
            ]
            rollout = Rollout(turns=turns, metadata={""problem"": user_prompt})
            group = RolloutGroup(
                id=f""math-{iteration}"",
                source=""math_env"",
                created=time.time(),
                rollouts=[rollout],
                metadata={""valid_format"": is_valid, ""correct"": is_correct},
            )

            self._rollout_sink([group])

            iteration += 1
            await asyncio.sleep(0)  # yield control to Ray scheduler
",marin/rl/envs/math_env.py,MathEnv
survived,"    def sink(groups: list[RolloutGroup]):
        collected.extend(groups)
",tests/rl/test_openai_env.py,
survived,"async def trigger_planning() -> str:
    resp = requests.post(f""{HOST}/agent/planning/trigger"", timeout=5)
    resp.raise_for_status()
    return ""planning queued""
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def main(argv: list[str] | None = None) -> None:
    args = _parse_args(argv)
    payload: dict[str, object] = {""action"": args.action}
    if args.job:
        payload[""job""] = json.loads(Path(args.job).read_text(encoding=""utf-8""))
    headers = {}
    api_key = os.getenv(""OPENAI_API_KEY"")
    if api_key:
        headers[""Authorization""] = f""Bearer {api_key}""
    url = f""{args.host}/v1/agents/business_helper/invoke""
    resp = requests.post(url, json=payload, headers=headers, timeout=10)
    try:
        print(json.dumps(resp.json(), indent=2))
    except Exception:
        print(resp.text)
",alpha_factory_v1/demos/alpha_agi_business_v1/examples/openai_agent_client.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bernoulli-numbers.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bioinformatics-base-count.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitcoin-address-validation.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/convert-decimal-number-to-rational.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/constrained-genericity-2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-6.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/copy-a-string-1.py,
survived,"def main(argv: list[str] | None = None) -> None:  # pragma: no cover - entry point
    """"""Launch the lineage dashboard.""""""
    if st is None:
        print(""Streamlit not installed"")
        return

    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        ""--db"",
        default=os.getenv(""ARCHIVE_PATH"", ""archive.db""),
        help=""Path to archive database"",
    )
    parser.add_argument(
        ""--refresh"",
        type=int,
        default=int(os.getenv(""DASH_REFRESH"", ""10"")),
        help=""Auto-refresh interval in seconds"",
    )
    args = parser.parse_args(argv)

    st.set_page_config(page_title=""Lineage Dashboard"", layout=""wide"")
    if st_autorefresh is not None:
        st_autorefresh(interval=args.refresh * 1000, key=""refresh"")

    df = load_df(Path(args.db))
    if df.empty:
        st.info(""Archive empty"")
        return

    fig = build_tree(df)
    st.plotly_chart(fig, use_container_width=True)
",src/interface/lineage_dashboard.py,
survived,"    def is_available(cls) -> bool:
        try:
            import importlib

            importlib.import_module(""mcp"")
            return True
        except Exception:
            return False
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mcp_adapter.py,MCPAdapter
survived,"def slugify(name: str) -> str:
    """"""Return a filesystem-friendly slug for ``name``.""""""
    slug = re.sub(r""[^A-Za-z0-9]+"", ""-"", name)
    return slug.strip(""-"").lower()
",generate_pdf_pages.py,
survived,"def add(a, b):
    return a + b
",tests/transpiler/x/py/partial_application.py,
survived,"  async def stop(self):
    """"""Stop the temperature controller and close the backend connection.""""""
    await self.deactivate()
    await super().stop()
",pylabrobot/temperature_controlling/temperature_controller.py,TemperatureController
survived,"def test_comparative_advantage():
    client = get_client()
    resp = client.post(
        ""/comparative-advantage/execute"",
        json={""skills"": {""a"": 1}, ""tasks"": {""t1"": [""a""]}},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""advantage_map""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"def create_app() -> FastAPI:
    app = FastAPI()
    for tool_cls in load_tools():
        router = tool_cls.get_router()
        app.include_router(router, prefix=tool_cls.endpoint_path, tags=[tool_cls.slug])
    return app",servers/server_clear_thought/app.py,
survived,"def create_app() -> FastAPI:
    app = FastAPI()
    for tool_cls in load_tools():
        router = tool_cls.get_router()
        app.include_router(router, prefix=tool_cls.endpoint_path, tags=[tool_cls.slug])
    return app",servers/server_clear_thought/app.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        k = payload.get(""k"") or 3
        analogies = [
            {""domain"": d, ""analogy"": f""{payload['problem']} ~ {d}""}
            for d in (payload.get(""seed_domains"") or [""math"", ""biology"", ""art""])[:k]
        ]
        prompts = [f""How would {a['domain']} approach it?"" for a in analogies]
        return {
            ""analogies"": analogies,
            ""suggested_prompts"": prompts,
        }",servers/server_clear_thought/tools/analogical_mapper.py,AnalogicalMapper
survived,"def test_safe_struggle_designer():
    client = get_client()
    resp = client.post(
        ""/safe-struggle-designer/execute"",
        json={""skill"": ""x"", ""current_level"": 1, ""target_level"": 2},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""scaffold_steps"", ""safety_measures"", ""review_intervals""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        result = []
        skills = payload[""skills""]
        for task, _ in payload[""tasks""].items():
            best = max(skills, key=lambda k: skills[k])
            result.append({""task"": task, ""assignee"": best})
        return {""advantage_map"": result}",servers/server_clear_thought/tools/comparative_advantage.py,ComparativeAdvantage
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        result = []
        skills = payload[""skills""]
        for task, _ in payload[""tasks""].items():
            best = max(skills, key=lambda k: skills[k])
            result.append({""task"": task, ""assignee"": best})
        return {""advantage_map"": result}",servers/server_clear_thought/tools/comparative_advantage.py,ComparativeAdvantage
survived,"    def get_router(cls) -> APIRouter:
        router = APIRouter()
        OutputModel = cls.OutputSchema

        @router.post(""/execute"", response_model=OutputModel)
        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)

        return router
",servers/server_clear_thought/core/base_tool.py,BaseTool
survived,"def test_drag_point_audit():
    client = get_client()
    resp = client.post(
        ""/drag-point-audit/execute"",
        json={""log"": ""...""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""drag_points"", ""summary_score""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"def test_comparative_advantage():
    client = get_client()
    resp = client.post(
        ""/comparative-advantage/execute"",
        json={""skills"": {""a"": 1}, ""tasks"": {""t1"": [""a""]}},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""advantage_map""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        result = []
        skills = payload[""skills""]
        for task, _ in payload[""tasks""].items():
            best = max(skills, key=lambda k: skills[k])
            result.append({""task"": task, ""assignee"": best})
        return {""advantage_map"": result}",servers/server_clear_thought/tools/comparative_advantage.py,ComparativeAdvantage
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        tools = payload.get(""downstream_tools"") or [f""tool_{i}"" for i in range(7)]
        results = [{""tool"": t, ""result"": f""{payload['query']} -> {t}""} for t in tools]
        resonance = {t: 1.0 for t in tools}
        synthesis = ""; "".join(r[""result""] for r in results)
        return {
            ""seeker_results"": results,
            ""resonance_map"": resonance,
            ""synthesis"": synthesis,
        }",servers/server_clear_thought/tools/seven_seekers_orchestrator.py,SevenSeekersOrchestrator
survived,"def test_performance_drop() -> None:
    rng1 = random.Random(1)
    op_good = SelfRewriteOperator(steps=1, rng=rng1, templates=[""meme""], reuse_rate=1.0)
    score_good = len(op_good(""improve quick test""))

    rng2 = random.Random(1)
    op_bad = SelfRewriteOperator(steps=1, rng=rng2, templates=[""meme""], reuse_rate=0.0)
    score_bad = len(op_bad(""improve quick test""))
    assert score_good > score_bad",tests/test_meme_reuse.py,
survived,"def test_gallery_html(tmp_path, monkeypatch):
    repo = tmp_path
    demos = repo / ""alpha_factory_v1"" / ""demos"" / ""demo_b""
    demos.mkdir(parents=True)
    (demos / ""README.md"").write_text(""# Demo B\nText"", encoding=""utf-8"")
    assets = repo / ""docs"" / ""demo_b"" / ""assets""
    assets.mkdir(parents=True)
    (assets / ""preview.png"").write_text(""data"", encoding=""utf-8"")
    docs_demos = repo / ""docs"" / ""demos""

    # generate docs
    monkeypatch.setattr(gdd, ""REPO_ROOT"", repo)
    monkeypatch.setattr(gdd, ""DEMOS_DIR"", repo / ""alpha_factory_v1"" / ""demos"")
    monkeypatch.setattr(gdd, ""DOCS_DIR"", docs_demos)
    gdd.generate_docs()

    # build gallery
    monkeypatch.setattr(ggh, ""REPO_ROOT"", repo)
    monkeypatch.setattr(ggh, ""DEMOS_DIR"", docs_demos)
    gallery = repo / ""docs"" / ""gallery.html""
    monkeypatch.setattr(ggh, ""GALLERY_FILE"", gallery)

    html_text = ggh.build_html(ggh.collect_entries())
    gallery.write_text(html_text, encoding=""utf-8"")

    out = gallery.read_text(encoding=""utf-8"")
    assert ""Demo B"" in out
    assert ""demo_b/assets/preview.png"" in out
    assert ""href=\""demos/demo_b/\"""" in out",tests/test_generate_gallery_html.py,
survived,"def test_orchestrator_command_runs() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio"") as aio:
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(cli.main, [""orchestrator""])
            assert res.exit_code == 0
        aio.run.assert_called_once()",tests/test_demo_cli.py,
survived,"def main() -> int:
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            context = browser.new_context()
            page = context.new_page()
            page.goto(URL)
            page.wait_for_function(""navigator.serviceWorker.ready"")
            page.wait_for_selector(""body"")
            context.set_offline(True)
            page.reload()
            page.wait_for_selector(""body"")
            browser.close()
        return 0
    except PlaywrightError as exc:
        print(f""Playwright error: {exc}"", file=sys.stderr)
        return 1
    except Exception as exc:  # noqa: BLE001
        print(f""Offline check failed: {exc}"", file=sys.stderr)
        return 1
",scripts/verify_insight_offline.py,
survived,"    def test_concurrent_writes_with_filelock(self) -> None:
        try:
            import filelock
        except Exception:  # pragma: no cover - optional dependency
            self.skipTest(""filelock not installed"")

        from alpha_factory_v1.demos.cross_industry_alpha_factory import (
            cross_alpha_discovery_stub as stub,
        )

        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""thread_lock_log.json""

            def worker(seed: int) -> None:
                stub.discover_alpha(num=1, seed=seed, ledger=ledger, model=""gpt-4o-mini"")

            threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]
            with patch.object(stub, ""FileLock"", filelock.FileLock):
                for t in threads:
                    t.start()
                for t in threads:
                    t.join()

            data = json.loads(ledger.read_text())
            self.assertIsInstance(data, list)
            self.assertEqual(len(data), 5)
            self.assertEqual(len(data), len({json.dumps(i, sort_keys=True) for i in data}))
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    async def close(self) -> None:
        pass
",src/aiohttp/__init__.py,ClientSession
survived,"    def __init__(self, *_, **__):
        pass
",src/aiohttp/__init__.py,TCPConnector
survived,"    def __init__(self, *args, **kwargs):
        pass
",src/aiohttp/__init__.py,TCPConnector
survived,"def test_cost_cap_threshold_events(caplog):
    t = TelemetryCollector(cost_cap=0.02)
    with caplog.at_level(logging.INFO):
        t.add_usage(1000, 0, model=""o3"")
        assert len(t.events) == 0
        t.add_usage(500, 0, model=""o3"")
        assert len(t.events) == 1
        assert t.events[0].severity == TelemetryCollector.Severity.WARNING
        t.add_usage(300, 0, model=""o3"")
        assert len(t.events) == 2
        assert t.events[1].severity == TelemetryCollector.Severity.ERROR
        with pytest.raises(RuntimeError):
            t.add_usage(200, 0, model=""o3"")
        assert len(t.events) == 3
        assert t.events[-1].severity == TelemetryCollector.Severity.CRITICAL
",tests/unit/test_telemetry_collector.py,
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """"""
            CREATE TABLE IF NOT EXISTS telemetry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                tokens INTEGER,
                cost REAL,
                latency REAL,
                guardrail_hits INTEGER
            )
            """"""
        )
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def verify(self) -> bool:
        cur = self.conn.cursor()
        res = cur.execute(""PRAGMA integrity_check"").fetchone()
        return res[0] == ""ok""
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def archive(self, path: Optional[str] = None) -> str:
        """"""Export all telemetry records to a gzipped JSON file.""""""
        data = self.fetch_all()
        if path is None:
            name = datetime.utcnow().isoformat().replace("":"", """").replace(""."", """")
            path = f""telemetry_{name}.json.gz""
        with gzip.open(path, ""wt"", encoding=""utf-8"") as f:
            json.dump(data, f)
        return path
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def test_with_retry_async(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    orig_sleep = asyncio.sleep
    monkeypatch.setattr(retry.asyncio, ""sleep"", lambda *_: orig_sleep(0))
    calls = {""n"": 0}

    async def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 2:
            raise ValueError(""boom"")
        return ""ok""

    wrapped = retry.with_retry(func, max_tries=2)
    result = asyncio.run(wrapped())
    assert result == ""ok""
    assert calls[""n""] == 2",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_retry.py,
survived,"    def _urlopen(req: object, timeout: float = 1.0) -> _Resp:
        called.append(getattr(req, ""full_url"", """"))
        return _Resp()
",tests/test_check_env_network.py,
survived,"def format_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]) -> str:
    parts: list[str] = []
    for message in prompt_messages:
        role = message[""role""]
        content = message[""content""]
        text = """"
        image_count = 0

        if isinstance(content, list):
            for item in content:
                if item[""type""] == ""text"":
                    text += item[""text""] + "" ""
                elif item[""type""] == ""image_url"":
                    image_count += 1
        else:
            text = str(content)

        text = text.strip()
        if len(text) > 40:
            text = text[:40] + ""...""

        img_part = f"" + [{image_count} images]"" if image_count else """"
        parts.append(f""{role}: {text}{img_part}"")

    return "" / "".join(parts)
",backend/utils.py,
survived,"def test_cli_report_flag(capsys):
    folder = os.path.dirname(__file__)
    source_path = os.path.join(folder, ""samples_in"", ""all_named.py"")

    return_code = run_flynt_cli([""--dry-run"", ""--report"", source_path])
    assert return_code == 0

    out, err = capsys.readouterr()
    assert ""Flynt run has finished. Stats:"" in out
    assert err == """"",test/integration/test_cli.py,
survived,"async def test_orchestrator_lifecycle(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Start the orchestrator, verify health, then shut down cleanly.""""""

    # Allocate random ports
    with socket.socket() as s:
        s.bind(("""", 0))
        rest_port = s.getsockname()[1]
    with socket.socket() as s:
        s.bind(("""", 0))
        grpc_port = s.getsockname()[1]

    monkeypatch.setenv(""DEV_MODE"", ""true"")
    monkeypatch.setenv(""API_TOKEN"", ""t"")
    monkeypatch.setenv(""NEO4J_PASSWORD"", ""x"")
    monkeypatch.setenv(""PORT"", str(rest_port))
    monkeypatch.setenv(""A2A_PORT"", str(grpc_port))

    # Prepare stub packages before importing orchestrator
    agents_stub = types.ModuleType(""backend.agents"")
    setattr(agents_stub, ""list_agents"", lambda _detail=False: [""dummy""])
    setattr(agents_stub, ""get_agent"", lambda name: DummyAgent())
    setattr(agents_stub, ""start_background_tasks"", lambda: None)

    fin_stub = types.ModuleType(""alpha_factory_v1.backend.agents.finance_agent"")
    setattr(fin_stub, ""metrics_asgi_app"", lambda: None)

    mem_stub = types.ModuleType(""backend.memory_fabric"")
    setattr(
        mem_stub,
        ""mem"",
        types.SimpleNamespace(
            vector=types.SimpleNamespace(
                recent=lambda *a, **k: [],
                search=lambda *a, **k: [],
            )
        ),
    )

    monkeypatch.setitem(sys.modules, ""backend.agents"", agents_stub)
    monkeypatch.setitem(sys.modules, ""alpha_factory_v1.backend.agents"", agents_stub)
    monkeypatch.setitem(sys.modules, ""backend.memory_fabric"", mem_stub)
    monkeypatch.setitem(sys.modules, ""alpha_factory_v1.backend.agents.finance_agent"", fin_stub)
    monkeypatch.setitem(sys.modules, ""backend.finance_agent"", fin_stub)

    orch_mod = importlib.import_module(""alpha_factory_v1.backend.orchestrator"")

    # Provide minimal protobuf stubs so gRPC server starts
    pb2 = types.SimpleNamespace(
        StreamReply=object,
        Ack=object,
        AgentStat=object,
        StatusReply=object,
    )
    pb2_grpc = types.SimpleNamespace(
        PeerServiceServicer=object,
        add_PeerServiceServicer_to_server=lambda serv, server: None,
    )
    proto_pkg = types.ModuleType(""backend.proto"")
    setattr(proto_pkg, ""a2a_pb2"", pb2)
    setattr(proto_pkg, ""a2a_pb2_grpc"", pb2_grpc)
    monkeypatch.setitem(sys.modules, ""backend.proto"", proto_pkg)
    monkeypatch.setitem(sys.modules, ""backend.proto.a2a_pb2"", pb2)
    monkeypatch.setitem(sys.modules, ""backend.proto.a2a_pb2_grpc"", pb2_grpc)

    stop = asyncio.Event()
    orch = orch_mod.Orchestrator()

    run_task = asyncio.create_task(orch.run(stop))
    await asyncio.sleep(0.2)  # allow servers to start

    assert orch._rest_task is not None
    assert orch._grpc_server is not None

    import httpx

    async with httpx.AsyncClient() as client:
        res = await client.get(
            f""http://localhost:{rest_port}/healthz"",
            headers={""Authorization"": ""Bearer t""},
        )
    assert res.status_code == 200 and res.text == ""ok""

    stop.set()
    await run_task

    assert orch._rest_task.done()
    for r in orch.manager.runners.values():
        assert r.task is None or r.task.done()",tests/test_orchestrator_lifecycle.py,
survived,"    def add_child(self, parent: Node, child: Node) -> None:
        child.parent = parent
        parent.children.append(child)
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/tree.py,Tree
survived,"    async def broadcast_merkle_root(self) -> None:
        try:
            root = self.compute_merkle_root()
        except Exception as exc:  # pragma: no cover - corruption
            _log.warning(""Failed to compute Merkle root: %s"", exc)
            return
        if AsyncClient is None or not self.broadcast:
            _log.info(""Merkle root %s"", root)
            return
        try:
            client = AsyncClient(self.rpc_url or ""https://api.testnet.solana.com"")
            memo_prog = PublicKey(""MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr"")
            tx = Transaction().add(TransactionInstruction(program_id=memo_prog, data=root.encode(), keys=[]))
            signer = None
            if self.wallet:
                try:  # pragma: no cover - optional dependency
                    from solana.keypair import Keypair

                    signer = Keypair.from_secret_key(bytes.fromhex(self.wallet))
                except Exception as exc:  # noqa: BLE001 - invalid key
                    _log.warning(""Invalid wallet key: %s"", exc)
            if signer:
                await client.send_transaction(tx, signer)
            else:
                await client.send_transaction(tx)
            _log.info(""Broadcasted Merkle root %s"", root)
        except Exception as exc:  # pragma: no cover - network errors
            _log.warning(""Failed to broadcast Merkle root: %s"", exc)
        finally:
            try:
                await client.close()
            except Exception:  # pragma: no cover - ignore close errors
                pass
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"    async def _fail_handshake(self, peer: str, context: Any) -> bytes:
        """"""Record a handshake failure and abort if the limit is exceeded.""""""
        count = self._handshake_failures.get(peer, 0) + 1
        self._handshake_failures[peer] = count
        if count >= self.settings.bus_fail_limit:
            if grpc:
                await context.abort(grpc.StatusCode.PERMISSION_DENIED, ""too many handshake failures"")
            return b""denied""
        if grpc:
            await context.abort(grpc.StatusCode.FAILED_PRECONDITION, ""handshake required"")
        return b""handshake required""
",alpha_factory_v1/common/utils/messaging.py,A2ABus
survived,"def with_retry(func: Callable[P, Any], *, max_tries: int = 3) -> Callable[P, Any]:
    """"""Wrap *func* with exponential backoff and logging.""""""

    def _log_retry(details: dict[str, Any]) -> None:
        _log.warning(
            ""Retry %d/%d for %s due to %s"",
            details[""tries""],
            max_tries,
            getattr(details.get(""target""), ""__name__"", ""call""),
            details.get(""exception""),
        )

    is_async = inspect.iscoroutinefunction(func)

    if backoff is not None:
        wrapped = backoff.on_exception(
            backoff.expo,
            Exception,
            max_tries=max_tries,
            jitter=backoff.full_jitter,
            on_backoff=_log_retry,
        )(func)
        if is_async:
            return cast(Callable[P, Awaitable[T]], wrapped)
        return cast(Callable[P, T], wrapped)

    if is_async:

        async def wrapper_async(*args: P.args, **kwargs: P.kwargs) -> Any:
            for attempt in range(max_tries):
                try:
                    return await cast(Callable[P, Awaitable[T]], func)(*args, **kwargs)
                except Exception as exc:  # pragma: no cover - runtime errors
                    if attempt + 1 >= max_tries:
                        raise
                    _log_retry(
                        {
                            ""tries"": attempt + 1,
                            ""exception"": exc,
                            ""target"": func,
                        }
                    )
                    await asyncio.sleep(2**attempt * 0.1)
            raise AssertionError(""unreachable"")

        return cast(Callable[P, Any], wrapper_async)

    def wrapper_sync(*args: P.args, **kwargs: P.kwargs) -> Any:
        for attempt in range(max_tries):
            try:
                return cast(Callable[P, T], func)(*args, **kwargs)
            except Exception as exc:  # pragma: no cover - runtime errors
                if attempt + 1 >= max_tries:
                    raise
                _log_retry(
                    {
                        ""tries"": attempt + 1,
                        ""exception"": exc,
                        ""target"": func,
                    }
                )
                time.sleep(2**attempt * 0.1)
        raise AssertionError(""unreachable"")

    return cast(Callable[P, Any], wrapper_sync)",alpha_factory_v1/common/utils/retry.py,
survived,"def _load_model(cfg: Settings | None = None) -> None:
    """"""Load a local model if available, otherwise use an echo stub.""""""
    global _MODEL, _CALL
    cfg = cfg or config.CFG
    model_path = os.getenv(""LLAMA_MODEL_PATH"", cfg.model_name)
    n_ctx = int(os.getenv(""LLAMA_N_CTX"", str(cfg.context_window)))

    def _wrap(fn: Callable[[str, Settings], str]) -> Callable[[str, Settings], str]:
        return fn

    if Llama is not None:
        try:
            _MODEL = Llama(model_path=model_path, n_ctx=n_ctx)

            def call_llama(prompt: str, s: Settings) -> str:
                out = cast(Any, _MODEL)(prompt, temperature=s.temperature)
                return cast(str, out[""choices""][0][""text""]).strip()

            _CALL = _wrap(call_llama)
            return
        except Exception as exc:  # pragma: no cover - model load failure
            _log.warning(""Failed to load Llama model: %s"", exc)
            _MODEL = None
    if AutoModelForCausalLM is not None:
        try:
            _MODEL = AutoModelForCausalLM.from_pretrained(model_path, model_type=""llama"")

            def call_ctrans(prompt: str, s: Settings) -> str:
                return cast(str, cast(Any, _MODEL)(prompt, temperature=s.temperature))

            _CALL = _wrap(call_ctrans)
            return
        except Exception as exc:  # pragma: no cover - model load failure
            _log.warning(""Failed to load ctransformers model: %s"", exc)
            _MODEL = None

    def call_stub(prompt: str, s: Settings) -> str:
        return f""[offline] {prompt}""

    _CALL = _wrap(call_stub)
",alpha_factory_v1/common/utils/local_llm.py,
survived,"        async def sleeper():
            await asyncio.sleep(0)
",tests/test_api_server_service.py,
survived,"    async def stop(self) -> None:
        if self._rest_task:
            self._rest_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._rest_task
        if self._grpc_server:
            self._grpc_server.stop(0)",alpha_factory_v1/backend/services/api_server_service.py,APIServer
survived,"    async def fake_start_servers(*a, **k):
        events.append(""start"")

        async def sleeper():
            await asyncio.sleep(0)
        task = asyncio.create_task(sleeper())
        server = SimpleNamespace(stop=lambda code=0: events.append(""stop""))
        return task, server
",tests/test_api_server_service.py,
survived,"def fetch_latest(owner_repo: str) -> tuple[str, str] | None:
    """"""Return the newest tag name and commit sha for a GitHub action.""""""
    url = f""https://api.github.com/repos/{owner_repo}/tags""
    try:
        resp = requests.get(url, timeout=60)
        resp.raise_for_status()
    except requests.RequestException as exc:
        sys.stderr.write(f""Failed to fetch {url}: {exc}\n"")
        return None
    tags = resp.json()
    if not tags:
        return None
    tag = tags[0]
    return tag[""name""], tag[""commit""][""sha""]
",tools/update_actions.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q21.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q16.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q12.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q15.py,Auto2
survived,"def test_alpha_factory_import():
    mod = importlib.import_module('alpha_factory_v1')
    assert hasattr(mod, '__version__')
",tests/test_imports.py,
survived,"    def tearDown(self):
        AGENT_REGISTRY.clear()
        AGENT_REGISTRY.update(self._registry_backup)
",tests/test_agents_registry.py,TestHealthQuarantine
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestAgentRegistryFunctions.DAgent
survived,"    def inc(self) -> None:
        self.count += 1
",tests/test_agent_base.py,_Counter
survived,"    async def subscribe(self, topic):
        if False:
            yield
",tests/test_ping_agent.py,DummyOrch
survived,"    def test_backend_alias(self):
        a = importlib.import_module(""alpha_factory_v1.backend.agents"")
        b = importlib.import_module(""backend.agents"")
        self.assertIs(a, b)
        self.assertGreater(len(a.AGENT_REGISTRY), 1)
",tests/test_agents_alias.py,TestAgentsAlias
survived,"def test_show_results_export_csv(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 1.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            res = CliRunner().invoke(cli.main, [""show-results"", ""--export"", ""csv""])
            assert ""ts,sender,recipient,payload"" in res.output
",tests/test_cli.py,
survived,"def test_run_evolution_different_seeds() -> None:
    def fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    pop1 = mats.run_evolution(fn, 2, population_size=3, generations=1, seed=1)
    pop2 = mats.run_evolution(fn, 2, population_size=3, generations=1, seed=2)

    assert [ind.genome for ind in pop1] != [ind.genome for ind in pop2]",tests/test_mats.py,
survived,"        def log(self, env: messaging.Envelope) -> None:
            events.append((""log"", env.sender))
",tests/test_agent_runner.py,Ledger
survived,"  def is_in_subtree_of(self, other: Resource) -> bool:
    """"""Return ``True`` if ``self`` is in the subtree rooted at ``other``.""""""

    current: Optional[Resource] = self
    while current is not None:
      if current is other:
        return True
      current = current.parent
    return False
",pylabrobot/resources/resource.py,Resource
survived,"def test_run_with_research(monkeypatch):
    calls = []

    class DummyResearch:
        def research(self, name: str, purpose: str):
            calls.append((name, purpose))
            return [""info""]

    agent = ToolDesignerAgent(research_manager=DummyResearch(), enable_research=True)
    result = await agent.run(VALID_DICT_SPEC)
    assert result['status'] == 'success'
    assert calls == [(VALID_DICT_SPEC['name'], VALID_DICT_SPEC['purpose'])]
",tests/agents/test_tool_designer_agent.py,
survived,"    def __init__(self):
        self.calls = []
",tests/unit/test_tool_research_manager.py,DummyTool
survived,"        def __call__(self, *_, **__):  # noqa: D401
            return ""Hosted tool unavailable in this environment.""
",src/meta_agent/research_manager.py,_StubWebSearchTool
survived,"    def test_openai_failure_falls_back(self) -> None:
        os.environ[""OPENAI_API_KEY""] = ""sk-test""
        llm._OPENAI_KEY = ""sk-test""
        llm._sync_embed.cache_clear()
        with patch.object(llm.openai.Embedding, ""create"", side_effect=llm.openai.OpenAIError(""boom"")) as mock_create:

            class _Vec(list):
                def tolist(self):
                    return list(self)

            fake_mod = SimpleNamespace(
                SentenceTransformer=lambda *_: SimpleNamespace(
                    encode=lambda text, normalize_embeddings=True: _Vec([0.1, 0.2])
                )
            )
            with patch.dict(sys.modules, {""sentence_transformers"": fake_mod}):
                vec = llm._sync_embed(""hi"")
        mock_create.assert_called_once()
        self.assertEqual(vec, [0.1, 0.2])
",tests/test_embedder_fallback.py,TestEmbedderFallback
survived,"def _enc(val: str | None) -> str:
    return base64.b64encode(str(val or """").encode()).decode()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"  def __init__(self, dbc_name: str):
    if isinstance(dbc_name, bytes):
      dbc_name = dbc_name.decode(""utf-8"")
    self.dbc_name = dbc_name
    self.dbc = _get_dbc(dbc_name)
    if not self.dbc:
      raise RuntimeError(f""Can't find DBC: '{dbc_name}'"")

    dv = defaultdict(dict)
    for val in self.dbc.vals:
      sgname = val.name
      address = val.address
      msg = self.dbc.addr_to_msg.get(address)
      if msg is None:
        raise KeyError(address)
      msgname = msg.name
      parts = val.def_val.split()
      values = [int(v) for v in parts[::2]]
      defs = parts[1::2]
      dv[address][sgname] = dict(zip(values, defs, strict=True))
      dv[msgname][sgname] = dv[address][sgname]

    self.dv = dict(dv)
",opendbc/can/parser.py,CANDefine
survived,"def test_setup_config_creates_file(tmp_path: Path) -> None:
    sample = tmp_path / ""config.env.sample""
    sample.write_text(""SECRET=1\n"")
    path, created = setup_config.ensure_config(tmp_path)
    assert created is True
    assert path == tmp_path / ""config.env""
    assert path.read_text() == ""SECRET=1\n""",tests/test_setup_config.py,
survived,"def test_plain_ndarray_selector():
    B, V = Axis(""batch"", 3), Axis(""vocab"", 5)
    x = hax.arange((B, V))
    idx = jnp.array([0, 2, 4], dtype=jnp.int32)
    out = x[""vocab"", idx]
    assert out.axes == (B,)
    assert jnp.array_equal(out.array, x.array[jnp.arange(3), idx])
",tests/test_scatter_gather.py,
survived,"    def __post_init__(self):
        if self.seq_id < 0:
            self.seq_id = next(self._counter)
        self.token_ids = list(self.prompt_token_ids)
",src/levanter/inference/sequence.py,Sequence
survived,"    def generate(
        self, prompts: List[str] | List[List[int]], sampling_params: SamplingParams | List[SamplingParams]
    ) -> List[dict]:
        if not isinstance(sampling_params, list):
            sampling_params = [sampling_params] * len(prompts)

        prompt_ids_list: List[List[int]] = [
            self.tokenizer.encode(p, add_special_tokens=False) if isinstance(p, str) else list(p)
            for p in prompts
        ]

        if len(prompt_ids_list) > MAX_SEQS:
            raise ValueError(f""Too many prompts: got {len(prompt_ids_list)}, max {MAX_SEQS}"")

        max_prompt = max(len(p) for p in prompt_ids_list)
        max_tokens = max(sp.max_tokens for sp in sampling_params)
        page_size = _round_preferred(max_prompt + max_tokens)
        page_table = PageTable.init(
            max_pages=MAX_SEQS,
            max_seqs=MAX_SEQS,
            page_size=page_size,
            max_pages_per_seq=1,
        )
        seq_ids = []
        for _ in prompt_ids_list:
            page_table, seq_id = page_table.assign_seq_id_to_seq()
            seq_ids.append(seq_id)
        cache = self.model.initial_cache(page_table, dtype=self.trainer_cfg.mp.compute_dtype)

        scheduler = Scheduler(self.eos)
        seq_objs = []
        for p, sp, seq_id in zip(prompt_ids_list, sampling_params, seq_ids):
            seq = Sequence(p, sp, seq_id=seq_id)
            seq_objs.append(seq)
            scheduler.add(seq)

        outputs = {}
        while not scheduler.is_finished():
            seqs, is_prefill = scheduler.schedule()
            token_ids = []
            for seq in seqs:
                if is_prefill and seq.status is SequenceStatus.WAITING:
                    tok, cache, page_table = self._prefill(seq, cache, page_table)
                    seq.status = SequenceStatus.RUNNING
                else:
                    tok, cache, page_table = self._decode(seq, cache, page_table, len(seq))
                token_ids.append(tok)
            scheduler.postprocess(seqs, token_ids)
            for seq in seqs:
                if seq.is_finished:
                    outputs[seq.seq_id] = seq.token_ids
        return [
            {""text"": self.tokenizer.decode(out, skip_special_tokens=True), ""token_ids"": out}
            for _, out in sorted(outputs.items())
        ]
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"    def __init__(self, max_seqs: int, max_len: int, eos: int):
        self.max_seqs = max_seqs
        self.max_len = max_len
        self.eos = eos
",src/levanter/inference/scheduler.py,JittedScheduler
survived,"def path_as_0_moves(path):
    """"""
    Takes the path which is a list of Position
    objects and outputs it as a string of rlud
    directions to match output desired by
    Rosetta Code task.
    """"""
    strpath = """"
    if len(path) < 1:
        return """"
    prev_pos = path[0]
    p_row, p_col = find_zero(prev_pos.tiles)
    for i in range(1,len(path)):
        curr_pos = path[i]
        c_row, c_col = find_zero(curr_pos.tiles)
        if c_row > p_row:
            strpath += 'd'
        elif c_row < p_row:
            strpath += 'u'
        elif c_col > p_col:
            strpath += 'r'
        elif c_col < p_col:
            strpath += 'l'
        # reset for next loop
        prev_pos = curr_pos
        p_row = c_row
        p_col = c_col
    return strpath",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,
survived,"def find_zero(tiles):
    """""" file the 0 tile """"""
    for row in range(4):
        for col in range(4):
            if tiles[row][col] == 0:
                return (row, col)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,
survived,"def __dir__():
    return sorted(__all__ + [
        ""__all__"", ""__builtins__"", ""__cached__"", ""__doc__"", ""__file__"",
        ""__loader__"", ""__name__"", ""__package__"", ""__path__"", ""__spec__"",
    ])",third_party/tree-sitter-racket/bindings/python/tree_sitter_racket/__init__.py,
survived,"    def _TryToFindUnixSocket(self) -> Optional[str]:

        # This is required to find the socket.
        if self.MoonrakerConfigFilePath is None:
            self.Logger.error(""_TryToFindUnixSocket - No moonraker config file path provided - Is this a companion plugin?"")
            return None

        # First, try to parse the moonraker config to find the klipper socket path, since the moonraker socket should be similar.
        try:
            # Open and read the config.
            # allow_no_value allows keys with no values - strict allows duplicate sections, because sometimes that happens for unknown reasons.
            # Since this is edited by the user, we allow non-strict stuff, since they can make mistakes like multiple sections.
            moonrakerConfig = configparser.ConfigParser(allow_no_value=True, strict=False)
            moonrakerConfig.read(self.MoonrakerConfigFilePath)
            if ""server"" not in moonrakerConfig:
                self.Logger.info(""_TryToFindUnixSocket - No server block found in moonraker config."")
            else:
                if ""klippy_uds_address"" not in moonrakerConfig[""server""]:
                    self.Logger.info(""_TryToFindUnixSocket - klippy_uds_address not found in moonraker config."")
                else:
                    # In most installs, this will be something like `~/printer_data/comms/klippy.sock`
                    klippySocketFilePath = moonrakerConfig[""server""][""klippy_uds_address""]
                    self.Logger.info(""Moonraker klippy unix socket path found in config: ""+klippySocketFilePath)
                    possibleComFolderPath = self._GetParentDirectory(klippySocketFilePath)
                    possibleMoonrakerSocketFilePath = os.path.join(possibleComFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
                    if os.path.exists(possibleMoonrakerSocketFilePath):
                        self.Logger.info(""Moonraker socket path found from moonraker config klippy socket path. :""+possibleMoonrakerSocketFilePath)
                        return possibleMoonrakerSocketFilePath
        except configparser.ParsingError as e:
            if ""Source contains parsing errors"" in str(e):
                self.Logger.error(""_TryToFindUnixSocket failed to handle moonraker config. ""+str(e))
        except Exception as e:
            Sentry.OnException(""_TryToFindUnixSocket failed to handle moonraker config."", e)

        # If that failed, try to find the path by stepping back from the moonraker config a few times.
        moonrakerConfigFolderPath = self._GetParentDirectory(self.MoonrakerConfigFilePath)

        # Test the config folder for the file and file + comms folder
        # This isn't likely, but we might as well try.
        testPath = os.path.join(moonrakerConfigFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker config path. :""+testPath)
            return testPath
        testPath = os.path.join(moonrakerConfigFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileNameWithCommsFolder)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker config path. :""+testPath)
            return testPath

        # Move a folder up and try again. This is where we expect the comms folder to be located, next to the config folder
        moonrakerPrinterFolderPath = self._GetParentDirectory(moonrakerConfigFolderPath)
        testPath = os.path.join(moonrakerPrinterFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker printer folder path. :""+testPath)
            return testPath
        testPath = os.path.join(moonrakerPrinterFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileNameWithCommsFolder)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker printer folder path. :""+testPath)
            return testPath
        return None
",moonraker_octoeverywhere/moonrakercredentialmanager.py,MoonrakerCredentialManager
survived,"    def Get() -> ""MoonrakerCredentialManager"":
        return MoonrakerCredentialManager._Instance
",moonraker_octoeverywhere/moonrakercredentialmanager.py,MoonrakerCredentialManager
survived,"def _pspec_parts(spec_part) -> str:
    if spec_part is None:
        return ""unsharded""
    elif isinstance(spec_part, (tuple, list)):
        return ""+"".join(str(p) for p in spec_part)
    else:
        return str(spec_part)
",src/haliax/debug.py,
survived,"    def _show(x):
        if isinstance(x, NamedArray):
            arr = x.array
            axes = x.axes
        else:
            arr = x
            axes = None

        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass

        jax.debug.inspect_array_sharding(arr, callback=cb)
        return x
",src/haliax/debug.py,
survived,"        def Tool(*_a, **_k):
            def dec(f):
                return f

            return dec
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry
survived,"def heuristic_policy(obs: List[float]) -> dict[str, Any]:
    """"""Return a suggested action based on observation heuristics.""""""
    power_ok, traffic_ok, _ = obs
    if power_ok < traffic_ok:
        # Prioritise power grid repairs
        return {""action"": {""id"": 0}}
    return {""action"": {""id"": 1}}
",alpha_factory_v1/demos/omni_factory_demo/plugins/example_agent_plugin.py,
survived,"async def list_agents() -> list[str]:
    resp = requests.get(""http://localhost:7860/agents"", timeout=5)
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_asi_world_model/openai_agents_bridge.py,
survived,"    def Tool(*_, **__):  # type: ignore[misc]
        def wrapper(func):
            return func

        return wrapper
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,
survived,"def main() -> None:
    """"""Entry-point for Meta-Agentic AGI v3 demo.""""""
    pkg_dir = Path(__file__).resolve().parents[1]
    sys.path.insert(0, str(pkg_dir))
    from meta_agentic_agi_demo_v3 import main as demo_main
    demo_main()
",alpha_factory_v1/demos/meta_agentic_agi_v3/src/main.py,
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    runtime.register(BusinessAgent())
    print(""Registered BusinessAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    runtime.register(EvolverAgent())
    print(""Registered EvolverAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,
survived,"async def evolve(generations: int = 1) -> str:
    EVOLVER.run_generations(generations)
    return EVOLVER.latest_log()
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,
survived,"def _rest_pnl() -> Any:
    """"""Return P&L via the REST fallback.""""""
    return requests.get(f""{BASE}/api/finance/pnl"", timeout=3).json()
",alpha_factory_v1/demos/finance_alpha/agent_control.py,
survived,"    async def reset(self) -> None:
        async with self._lock:
            self.evolver.reset()
",alpha_factory_v1/demos/aiga_meta_evolution/agent_aiga_entrypoint.py,AIGAMetaService
survived,"def test_experience_launcher_api_key(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    script = Path(""alpha_factory_v1/demos/era_of_experience/run_experience_demo.sh"")
    config = script.parent / ""config.env""
    docker_log = tmp_path / ""docker.log""
    curl_log = tmp_path / ""curl.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()

    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""$@"" >> ""$DOCKER_LOG""\n'
        'if [ ""$1"" = ""info"" ]; then echo ""{}""; fi\n'
        'if [ ""$1"" = ""version"" ]; then echo ""24.0.0""; fi\n'
        ""exit 0\n""
    )
    docker_stub.chmod(0o755)

    curl_stub = bin_dir / ""curl""
    curl_stub.write_text(
        ""#!/usr/bin/env bash\n""
        'echo ""$@"" >> ""$CURL_LOG""\n'
        'out=""""\n'
        ""for ((i=1;i<=$#;i++)); do\n""
        '  if [ ""${!i}"" = ""-o"" ]; then\n'
        ""    j=$((i+1))\n""
        ""    out=${!j}\n""
        ""  fi\n""
        ""done\n""
        'if [ -n ""$out"" ]; then echo sample > ""$out""; fi\n'
        'echo ""OK""\n'
    )
    curl_stub.chmod(0o755)

    env = os.environ.copy()
    env.update(
        {
            ""PATH"": f""{bin_dir}:{env['PATH']}"",
            ""SKIP_ENV_CHECK"": ""1"",
            ""SAMPLE_DATA_DIR"": str(tmp_path / ""samples""),
            ""DOCKER_LOG"": str(docker_log),
            ""CURL_LOG"": str(curl_log),
            ""OPENAI_API_KEY"": ""dummy"",
        }
    )

    if config.exists():
        config.unlink()
    try:
        result = subprocess.run([f""./{script.name}""], cwd=script.parent, env=env, capture_output=True, text=True)
        created = config.exists()
    finally:
        if config.exists():
            config.unlink()

    assert result.returncode == 0, result.stderr
    assert docker_log.exists()
    log = docker_log.read_text()
    assert ""--profile offline"" not in log
    assert created
",tests/test_experience_launcher.py,
survived,"def inc(c: Counter):
    c.n = c.n + 1
",tests/machine/x/python/record_assign.py,
survived,"def sum_tree(t: Tree) -> int:
    if isinstance(t, Leaf):
        return 0
    elif isinstance(t, Node):
        return sum_tree(t.left) + t.value + sum_tree(t.right)
    else:
        raise TypeError(""Unknown node"")
",tests/machine/x/python/tree_sum.py,
survived,"def inc(x: int) -> int:
    return x + k
",tests/machine/x/python/pure_global_fold.py,
survived,"def test_grpc_bus_tls_message_exchange(tmp_path: Path) -> None:
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils import config, messaging

    port = _free_port()
    cert, key, ca = _make_cert(tmp_path)
    cfg = config.Settings(bus_port=port, bus_cert=cert, bus_key=key, bus_token=""tok"")
    bus = messaging.A2ABus(cfg)
    received: list[messaging.Envelope] = []

    async def run() -> None:
        bus.subscribe(""b"", lambda e: received.append(e))
        await bus.start()
        try:
            creds = grpc.ssl_channel_credentials(root_certificates=ca)
            async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                stub = ch.unary_unary(""/bus.Bus/Send"")
                payload = {
                    ""sender"": ""a"",
                    ""recipient"": ""b"",
                    ""payload"": {""v"": 1},
                    ""ts"": 0.0,
                    ""token"": ""tok"",
                }
                await stub(json.dumps(payload).encode())
            await asyncio.sleep(0.05)
        finally:
            await bus.stop()

    asyncio.run(run())
    assert received and received[0].payload[""v""] == 1
",tests/test_agents.py,
survived,"    def traceback(self, seq1: str, seq2: str, matrix) -> tuple[str, str]:
        """"""Reconstruct the best local alignment from a score matrix.""""""
        import numpy as _np  # local import for type check

        H = _np.array(matrix)
        i, j = _np.unravel_index(H.argmax(), H.shape)
        aligned1: list[str] = []
        aligned2: list[str] = []
        while i > 0 and j > 0 and H[i][j] > 0:
            score = H[i][j]
            diag = H[i - 1][j - 1]
            up = H[i - 1][j]
            left = H[i][j - 1]
            match_score = self.match if seq1[i - 1] == seq2[j - 1] else self.mismatch
            if score == diag + match_score:
                aligned1.append(seq1[i - 1])
                aligned2.append(seq2[j - 1])
                i -= 1
                j -= 1
            elif score == up + self.gap:
                aligned1.append(seq1[i - 1])
                aligned2.append(""-"")
                i -= 1
            elif score == left + self.gap:
                aligned1.append(""-"")
                aligned2.append(seq2[j - 1])
                j -= 1
            else:
                break
        return """".join(reversed(aligned1)), """".join(reversed(aligned2))
",src/python/gpu_smith_waterman.py,SmithWatermanGPU
survived,"def main():
    parser = argparse.ArgumentParser(
        description=(""Run hierarchical LDA on the BBC tech dataset""),
    )
    parser.add_argument(
        ""--data-dir"",
        default=os.path.join(
            os.path.dirname(__file__),
            "".."",
            ""data"",
            ""bbc"",
            ""tech"",
        ),
        help=""Directory containing BBC .txt files"",
    )
    parser.add_argument(
        ""--iterations"",
        type=int,
        default=100,
        help=""Number of Gibbs samples"",
    )
    parser.add_argument(
        ""--display-topics"",
        type=int,
        default=50,
        help=""Report topics every N iterations"",
    )
    parser.add_argument(
        ""--n-words"",
        type=int,
        default=5,
        help=""Number of words to display per topic"",
    )
    parser.add_argument(
        ""--num-levels"",
        type=int,
        default=3,
        help=""Depth of the topic hierarchy"",
    )
    parser.add_argument(
        ""--alpha"",
        type=float,
        default=10.0,
        help=""Alpha hyperparameter"",
    )
    parser.add_argument(
        ""--gamma"",
        type=float,
        default=1.0,
        help=""Gamma hyperparameter"",
    )
    parser.add_argument(
        ""--eta"",
        type=float,
        default=0.1,
        help=""Eta hyperparameter"",
    )
    parser.add_argument(
        ""--seed"",
        type=int,
        default=0,
        help=""Random seed"",
    )

    args = parser.parse_args()
    run_hlda(args)
",examples/bbc_demo.py,
survived,"def test_transform_acm_certificates():
    result = acm.transform_acm_certificates(
        [test_data.DESCRIBE_CERTIFICATE[""Certificate""]],
        ""us-east-1"",
    )
    assert len(result) == 1
    cert = result[0]
    assert cert[""Arn""] == ""arn:aws:acm:us-east-1:000000000000:certificate/test-cert""
    assert cert[""DomainName""] == ""example.com""
    assert cert[""Status""] == ""ISSUED""
    assert cert[""InUseBy""] == [
        ""arn:aws:elasticloadbalancing:us-east-1:000000000000:listener/app/test-lb/abcd/efgh""
    ]
    assert cert[""ELBV2ListenerArns""] == [
        ""arn:aws:elasticloadbalancing:us-east-1:000000000000:listener/app/test-lb/abcd/efgh""
    ]",tests/unit/cartography/intel/aws/test_acm.py,
survived,"    def inc(self, *a, **kw):
        pass
",tests/test_eventbus.py,_M
survived,"    async def stop_consumer(self) -> None:
        if self._consumer_task is None:
            return
        self._consumer_task.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await self._consumer_task
        self._consumer_task = None
",alpha_factory_v1/backend/agent_runner.py,EventBus
survived,"    def allocate_for_seqs(
        self,
        updated_seqs: ht.i32[NamedArray, "" seq""],  # type: ignore[name-defined]
        new_counts: ht.i32[NamedArray, "" seq""],  # type: ignore[name-defined]
        tokens: ht.i32[NamedArray, ""position""],  # type: ignore[name-defined]
    ) -> tuple[""PageTable"", ""PageBatchInfo""]:
        """"""Allocate pages for new sequences and update ``seq_lens``.""""""

        page_indices = self.page_indices
        page_owners = self.page_owners
        seq_lens = self.seq_lens

        padded_updated_seqs = hax.where(updated_seqs < 0, self.max_seqs, updated_seqs)
        current_lens = hax.where(seq_lens < 0, 0, seq_lens)
        new_lens_tmp = current_lens.at[""seq"", padded_updated_seqs].add(new_counts, mode=""drop"")
        new_lens = hax.where(seq_lens < 0, hax.where(new_lens_tmp > 0, new_lens_tmp, -1), new_lens_tmp)

        new_num_pages_needed = (new_lens + self.page_size - 1) // self.page_size
        old_num_pages_needed = (seq_lens + self.page_size - 1) // self.page_size

        def _alloc_pages_for_seq(seq_id, carry):
            page_indices, page_owners = carry
            num_needed = new_num_pages_needed[""seq"", seq_id].scalar()
            old_needed = old_num_pages_needed[""seq"", seq_id].scalar()

            def body(page_idx, state):
                page_indices, page_owners = state
                free_page_idx = hax.argmin(page_owners, ""page"")
                page_owners = page_owners.at[""page"", free_page_idx].set(seq_id)
                page_indices = page_indices.at[""seq"", seq_id, ""page"", page_idx].set(free_page_idx)
                return page_indices, page_owners

            new_page_indices, new_page_owners = jax.lax.fori_loop(
                old_needed, num_needed, body, (page_indices, page_owners)
            )
            return new_page_indices, new_page_owners

        page_indices, page_owners = jax.lax.fori_loop(
            0, self.max_seqs, _alloc_pages_for_seq, (page_indices, page_owners)
        )

        new_table = dataclasses.replace(
            self,
            page_indices=page_indices,
            page_owners=page_owners,
            seq_lens=new_lens,
        )

        batch_info = self._slice_batch_info(updated_seqs, self.seq_lens, new_table, new_counts, tokens)

        return new_table, batch_info
",src/levanter/layers/page_table.py,PageTable
survived,"        def token_body(i, carry):
            token_dests, seq_cursors = carry
            seq_id = tokens[""position"", i].scalar()

            def assign(carry):
                token_dests, seq_cursors = carry
                page_idx = seq_cursors[seq_id] // self.page_size
                page_offset = seq_cursors[seq_id] % self.page_size
                page = new_table.page_indices[""seq"", seq_id, ""page"", page_idx]
                dest = hax.where(page < 0, -1, page * self.page_size + page_offset)
                token_dests = token_dests.at[""position"", i].set(dest)
                seq_cursors = seq_cursors.at[seq_id].add(1)
                return token_dests, seq_cursors

            token_dests, seq_cursors = jax.lax.cond(seq_id >= 0, assign, lambda c: c, (token_dests, seq_cursors))
            return token_dests, seq_cursors
",src/levanter/layers/page_table.py,PageTable
survived,"    def open_logs():
        try:
            open_logs_folder()
            return {""message"": ""opened""}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))",app/desktop/studio_server/settings_api.py,
survived,"    def test_stop_grad_torch(self):
        self._check_stop_grad(""torch"")
",tests/test_autograd.py,TestAutograd
survived,"    def test_scalar_grad_torch(self):
        klong = KlongInterpreter()
        klong['sin'] = lambda x: np.sin(x)
        klong['cos'] = lambda x: np.cos(x)
        klong('g::‚àá{sin(x)+x*x}')
        r = klong('g(3.14)')
        x = torch.tensor(3.14, dtype=torch.float64, requires_grad=True)
        f = torch.sin(x) + x * x
        f.backward()
        self.assertTrue(np.isclose(r, x.grad.item(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"    def _check_vector_elemwise_grad(self, name: str):
        """"""Verify gradient of ‚àë(x+1)(x+2) = 2x+3 via the chain rule.""""""
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x):
            return b.sum(b.mul(b.add(x, 1), b.add(x, 2)))

        g = b.grad(f)
        x = b.array([0.0, 1.0, 2.0], requires_grad=True)
        grad = to_numpy(g(x))
        expected = 2 * np.array([0.0, 1.0, 2.0]) + 3
        np.testing.assert_allclose(np.array(grad), expected)
",tests/test_autograd.py,TestAutograd
survived,"def vectorElemwiseGrad(x, backend_name=""numpy""):
    backend.set_backend(backend_name)
    b = backend.current()

    def f(t):
        return b.sum(b.mul(b.add(t, 1), b.add(t, 2)))

    g = b.grad(f)
    out = g(b.array(x, requires_grad=True))
    out = to_numpy(out)
    return out.tolist() if isinstance(out, np.ndarray) else out
",tests/kgtests/autograd/helpers.py,
survived,"def update_pyodide(version: str) -> None:
    base_url = f""https://cdn.jsdelivr.net/pyodide/v{version}/full""
    root = Path(__file__).resolve().parent
    fetch_assets = root / ""fetch_assets.py""
    text = fetch_assets.read_text()

    text = re.sub(r""DEFAULT_PYODIDE_BASE_URL = \""[^\""]+\"""", f'DEFAULT_PYODIDE_BASE_URL = ""{base_url}""', text)
    text = re.sub(r""# Updated to Pyodide [^\n]+"", f""# Updated to Pyodide {version}"", text)

    files = [""pyodide.js"", ""pyodide.asm.wasm""]
    checksums: Dict[str, str] = {}
    for name in files:
        data = fetch(f""{base_url}/{name}"")
        checksums[name] = f""sha384-{sha384_b64(data)}""

    for name, checksum in checksums.items():
        pattern = rf'""{name}"":\s*""[^""]+""'
        text = re.sub(pattern, f'""{name}"": ""{checksum}""', text)

    fetch_assets.write_text(text)

    subprocess.run([sys.executable, str(root / ""generate_build_manifest.py"")], check=True)
",scripts/update_pyodide.py,
survived,"    def test_jsonl_gz_load(dest_uri):
        """"""When the source URI is a gzipped JSONL file, the data should be ingested.""""""
        with (
            patch(target_fs) as target_fs_mock,
            patch(""ingestr.src.filesystem.glob_files"", wraps=glob_files_override),
        ):
            target_fs_mock.return_value = test_fs
            schema_rand_prefix = f""testschema_fs_{get_random_string(5)}""
            dest_table = f""{schema_rand_prefix}.fs_{get_random_string(5)}""
            result = invoke_ingest_command(
                f""{protocol}://bucket?{auth}"",
                ""/data.jsonl.gz"",
                dest_uri,
                dest_table,
            )
            assert result.exit_code == 0
            assert_rows(dest_uri, dest_table, 5)
",ingestr/main_test.py,
survived,"        def __enter__(self) -> ""_Sock"":
            return self
",tests/test_check_env_network.py,_Sock
survived,"    async def run_cycle(self) -> None:
        pass
",tests/test_agent_runner.py,DummyBaseAgent
survived,"    def parse_scalar(value: str) -> Any:
        if value.lower() in {""true"", ""false""}:
            return value.lower() == ""true""
        if value == ""null"" or value == ""~"":
            return None
        try:
            if ""."" in value:
                return float(value)
            return int(value)
        except ValueError:
            return value
",src/yaml/__init__.py,
survived,"        async def runner() -> None:
            self.assertIsInstance(self.agent.forecast_demand(), str)
            self.assertIsInstance(self.agent.optimise_dispatch(), str)
            self.assertIsInstance(self.agent.hedge_strategy(), str)
",tests/test_energy_agent.py,TestEnergyAgentSyncRun
survived,"async def _evaluate(g):
    await asyncio.sleep(0)
    return g, 0.01
",tests/test_backtrack_boost.py,
survived,"def _results(dataset: str, rate: float, count: int = 10):
    passed = int(rate * count)
    items = []
    for i in range(count):
        items.append({""task_id"": f""{dataset}/task_{i:03d}"", ""pass"": i < passed, ""time_ms"": 1})
    return items
",tests/test_curriculum_switcher.py,
survived,"def run_loop(
    *,
    cost_budget: float | None = None,
    wallclock: float | None = None,
    cost_per_cycle: float = 1.0,
    state_file: str = ""loop_state.json"",
) -> Result:
    """"""Run the FSM until budgets are exhausted.

    Args:
        cost_budget: Optional cost limit.
        wallclock: Optional wall-clock limit in seconds.
        cost_per_cycle: Cost incurred per complete cycle.
        state_file: Path used when persisting state on ``KeyboardInterrupt``.

    Returns:
        :class:`Result` with final state, completed cycles and cost spent.
    """"""

    state = State.SELECT
    cycles = 0
    cost_spent = 0.0
    start = time.time()

    try:
        while True:
            if state is State.SELECT:
                state = State.SELF_MOD
                continue
            if state is State.SELF_MOD:
                state = State.BENCHMARK
                continue
            if state is State.BENCHMARK:
                cost_spent += cost_per_cycle
                state = State.ARCHIVE
                continue
            if state is State.ARCHIVE:
                cycles += 1
                state = State.SELECT
                if cost_budget is not None and cost_spent >= cost_budget:
                    break
                if wallclock is not None and time.time() - start >= wallclock:
                    break
    except KeyboardInterrupt:  # pragma: no cover - interactive
        Path(state_file).write_text(
            json.dumps({""state"": state.name, ""cycles"": cycles, ""cost"": cost_spent})
        )
        return Result(state=state, cycles=cycles, cost=cost_spent)

    return Result(state=state, cycles=cycles, cost=cost_spent)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/loop.py,
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.index_path = self.registry.templates_dir / self.INDEX_FILE_NAME
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def test_high_delta_promotes_cooperation(self):
        coop = run_sim(agents=20, rounds=100, delta=0.8, stake=2.5, seed=0)
        self.assertGreaterEqual(coop, 0.8)
",alpha_factory_v1/tests/test_governance_sim.py,GovernanceSimTest
survived,"    def __init__(self, obs_dim: int, act_dim: int, g: Genome):
        super().__init__()
        last, modules = obs_dim, []
        for h in g.layers:
            modules.append(nn.Linear(last, h))
            modules.append(nn.ReLU())  # placeholder
            last = h
        modules.append(nn.Linear(last, act_dim))
        self.model = nn.Sequential(*modules)
        self.genome = g
        if g.hebbian:
            self.hFast = torch.zeros_like(next(self.model.parameters()))
        self._init()
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,EvoNet
survived,"    def forward(self, x: torch.Tensor):
        act_fn = _ACT[self.genome.activation]
        h = x
        for layer in self.model[:-1]:
            if isinstance(layer, nn.Linear):
                h = act_fn(layer(h))
                if self.genome.hebbian:
                    with torch.no_grad():
                        dw = 0.03 * torch.bmm(h.unsqueeze(2), x.unsqueeze(1))
                        self.hFast = (self.hFast + dw.mean(0)).clamp(-0.02, 0.02)
                        layer.weight.data += self.hFast
            else:
                h = layer(h)
        return self.model[-1](h)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,EvoNet
survived,"    def _close_producer() -> None:  # graceful flush on exit
        try:
            _producer.flush()
            _producer.close()
        except Exception:  # noqa: BLE001
            log.exception(""Kafka producer close failed"")
",alpha_factory_v1/backend/orchestrator.py,
survived,"def _require_node_20() -> None:
    """"""Exit when Node.js is missing or too old.""""""
    if not shutil.which(""node""):
        sys.exit(
            ""Node.js 20+ is required. Install Node.js and ensure 'node' is in your PATH.""
        )
    try:
        out = subprocess.check_output([""node"", ""--version""], text=True).strip()
    except subprocess.CalledProcessError:
        sys.exit(""Failed to execute 'node --version'. Is Node.js installed correctly?"")
    version = out.lstrip(""v"")
    major = int(version.split(""."")[0])
    if major < 20:
        sys.exit(f""Node.js 20+ is required. Current version: {version}"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"def swap(a, b):
    return [b, a]
",tests/rosetta/transpiler/Python/generic-swap.py,
survived,"def test_tiff(h, f):
    """"""Verify if the image is a TIFF (can be in Motorola or Intel byte order).""""""
    if h[:2] in (b'MM', b'II'):
        return 'tiff'
",metaflow/_vendor/imghdr/__init__.py,
survived,"def test_spanish_labels() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(locale=""es-ES"")
        page = context.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        label_text = page.locator(""#controls label"").first.inner_text()
        assert ""Semilla"" in label_text
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_spanish_locale.py,
survived,"def test_resolve_url_fallback(monkeypatch: pytest.MonkeyPatch, requests_mock: ""requests_mock.Mocker"") -> None:
    import scripts.download_wasm_gpt2 as dw

    urls = [
        ""https://example.com/wasm-gpt2.tar"",
        ""https://another.com/wasm-gpt2.tar"",
        dw._DEFAULT_URLS[0],
    ]

    with monkeypatch.context() as m:
        m.delenv(""WASM_GPT2_URL"", raising=False)
        m.setattr(dw, ""_DEFAULT_URLS"", urls)
        requests_mock.head(urls[0], status_code=404)
        requests_mock.head(urls[1], exc=requests.exceptions.RequestException)
        requests_mock.head(urls[2], status_code=200)
        assert dw._resolve_url() == urls[2]",tests/test_download_openai_gpt2.py,
survived,"def _diversity(pop: list[mats.Individual]) -> float:
    if len(pop) < 2:
        return 0.0
    dists = []
    for i in range(len(pop)):
        for j in range(i + 1, len(pop)):
            a = pop[i].genome
            b = pop[j].genome
            d = sum((x - y) ** 2 for x, y in zip(a, b)) ** 0.5
            dists.append(d)
    return sum(dists) / len(dists)
",tests/test_mats_ops.py,
survived,"def resolve_relative_path(target: str, base_path: str) -> str:
    """"""Resolve only the path component for a target.""""""
    path, _ = resolve_module(target, base_path)
    return path",jac/jaclang/utils/module_resolver.py,
survived,"    def fake_post(url: str, json=None, timeout=None):
        called[""url""] = url
        called[""json""] = json
        return DummyResp()
",tests/test_selfheal_entrypoint_offline.py,
survived,"    def test_entries_feed_includes_subscribe_note(self):
        EntryFactory()
        response = self.client.get(""/atom/entries/"")
        self.assertIn(
            ""You are only seeing the entries from my blog. Subscribe to"",
            response.content.decode(),
        )
",blog/tests.py,BlogTests
survived,"def test_macro_launcher_health_check(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Health gate should hit the expected endpoint.""""""
    curl_calls: list[list[str]] = []

    def fake_run(cmd: list[str], *a, **k) -> subprocess.CompletedProcess[str]:
        if cmd[0] == ""curl"":
            curl_calls.append(cmd)
        return subprocess.CompletedProcess(cmd, 0, """", """")

    monkeypatch.setattr(subprocess, ""run"", fake_run)
    monkeypatch.setenv(""OPENAI_API_KEY"", ""dummy-key"")

    mod = __import__(
        ""alpha_factory_v1.demos.macro_sentinel.macro_launcher"", fromlist=[""main""]
    )
    mod.main([])

    urls = "" "".join("" "".join(c) for c in curl_calls)
    assert ""http://localhost:7864/healthz"" in urls",tests/test_macro_launcher.py,
survived,"    def tearDown(self) -> None:
        agents_mod._WHEEL_PUBKEY = self.orig_pub
        self.tmpdir.cleanup()
",tests/test_verify_wheel_sig.py,VerifyWheelSigTests
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = int(s.getsockname()[1])
    s.close()
    return port
",tests/test_message_bus.py,
survived,"def react_ui():
    """"""Serve the minimal React-based interface.""""""
    csrf_token = generate_csrf_token()
    return render_template('react.html', csrf_token=csrf_token)
",routes.py,
survived,"        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"def cytomat_rack_60mm_8(name: str):
  return _cytomat_rack(name=name, site_height=60, num_sites=8, model=""cytomat_rack_60mm_8"")
",pylabrobot/storage/cytomat/racks.py,
survived,"def cytomat_rack_43mm_11(name: str):
  return _cytomat_rack(name=name, site_height=43, num_sites=11, model=""cytomat_rack_43mm_11"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def set_temperature(self, temperature: float):
    print(f""Setting temperature to {temperature}"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend
survived,"  async def stop_shaking(self):
    pass",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def setup(self):
    print(""Setting up incubator backend"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend
survived,"  async def start_shaking(self, frequency: float, shakers: Optional[List[int]] = None):
    if self.model == CytomatType.C5C:
      raise NotImplementedError(""Shaking is not supported on this model"")
    await self.set_shaking_frequency(frequency=int(frequency), shakers=shakers)
    return hex_to_binary(await self.send_command(""ll"", ""va"", """"))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def send_action(
    self, command_type: str, command: str, params: str, timeout: Optional[int] = 60
  ) -> OverviewRegisterState:
    """"""Calls send_command, but has a timeout handler and returns the overview register state.
    Args:
      timeout: The maximum time to wait for the command to complete. If None, the command will not
        wait for completion.
    """"""
    await self.send_command(command_type, command, params)
    if timeout is not None:
      overview_register = await self.wait_for_task_completion(timeout=timeout)
    return overview_register
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"def cytomat_rack_9mm_51(name: str):
  return _cytomat_rack(name=name, site_height=9, num_sites=51, model=""cytomat_rack_9mm_51"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def action_transfer_to_storage(  # used by insert_plate
    self, site: PlateHolder
  ) -> OverviewRegisterState:
    """"""Open lift door, retrieve from transfer, close door, place at storage""""""
    return await self.send_action(""mv"", ""ts"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def action_read_barcode(
    self,
    site_number_a: str,
    site_number_b: str,
  ) -> OverviewRegisterState:
    # Read barcode of storage locations
    validate_storage_location_number(site_number_a)
    validate_storage_location_number(site_number_b)
    resp = await self.send_command(""mv"", ""sn"", f""{site_number_a} {site_number_b}"")
    return OverviewRegisterState.from_resp(resp)
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  def find_smallest_site_for_plate(self, plate: Plate) -> PlateHolder:
    return self._find_available_sites_sorted(plate)[0]
",pylabrobot/storage/incubator.py,Incubator
survived,"  def __init__(self, port: str):
    super().__init__()
    self.io = Serial(
      port=port,
      baudrate=self.default_baud,
      bytesize=serial.EIGHTBITS,
      parity=serial.PARITY_EVEN,
      stopbits=serial.STOPBITS_ONE,
      write_timeout=1,
      timeout=1,
      rtscts=True,
    )
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  def _site_to_firmware_string(self, site: PlateHolder) -> str:
    rack = cast(PlateCarrier, site.parent)
    rack_idx = [rack.name for rack in self._racks].index(
      rack.name
    )  # autoreload resistant, should work
    site_idx = next(idx for idx, s in rack.sites.items() if s == site)

    if self.model in [CytomatType.C2C_425]:
      return f""{str(rack_idx).zfill(2)} {str(site_idx).zfill(2)}""

    # TODO: configure all cytomats to use `rack site` format
    if self.model in [
      CytomatType.C6000,
      CytomatType.C6002,
      CytomatType.C2C_450_SHAKE,
      CytomatType.C5C,
    ]:
      slots_to_skip = sum(r.capacity for r in self._racks[:rack_idx])
      absolute_slot = slots_to_skip + site_idx + 1  # 1-indexed

      return f""{absolute_slot:03}""

    raise ValueError(f""Unsupported Cytomat model: {self.model}"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  def get_num_free_sites(self) -> int:
    return sum(len(rack.get_free_sites()) for rack in self._racks)
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def fetch_plate_to_loading_tray(self, plate: Plate):
    pass
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def stop(self):
    await self.io.stop()
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  def __init__(self, *args, **kwargs):
    raise RuntimeError(""`Cytomat` is deprecated. Please use `CytomatBackend` instead. "")",pylabrobot/storage/cytomat/cytomat.py,Cytomat
survived,"def cytomat_rack_38mm_13(name: str):
  return _cytomat_rack(name=name, site_height=38, num_sites=13, model=""cytomat_rack_38mm_13"")
",pylabrobot/storage/cytomat/racks.py,
survived,"    def _save_ratings(self, data: Dict[str, List[int]]) -> None:
        with open(self.ratings_path, ""w"", encoding=""utf-8"") as f:
            json.dump(data, f, indent=2)
",src/meta_agent/template_sharing.py,TemplateSharingManager
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.ratings_path = self.registry.templates_dir / ""ratings.json""
        if not self.ratings_path.exists():
            self.ratings_path.write_text(""{}"", encoding=""utf-8"")
",src/meta_agent/template_sharing.py,TemplateSharingManager
survived,"        def __init__(self, app: FastAPI, limit: int = 60, window: int = 60) -> None:
            super().__init__(app)
            self.limit = int(os.getenv(""API_RATE_LIMIT"", str(limit)))
            self.window = window
            self.counters: dict[str, tuple[int, float]] = {}
            self.lock = asyncio.Lock()
",src/interface/api_server.py,SimpleRateLimiter
survived,"def test_entrypoint_import_with_stubs(monkeypatch):
    monkeypatch.setitem(
        sys.modules,
        ""gradio"",
        types.SimpleNamespace(Blocks=DummyBlocks, Markdown=DummyMarkdown, Button=DummyButton),
    )
    stub = types.SimpleNamespace(
        Agent=lambda *a, **k: object(),
        OpenAIAgent=object,
        Tool=lambda *a, **k: (lambda f: f),
    )
    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    sys.modules.pop(
        ""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint"",
        None,
    )
    mod = importlib.import_module(""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint"")
    assert mod.apply_patch_and_retst is mod.apply_and_test",tests/test_selfheal_import_stubs.py,
survived,"def format_values(node, values):
    return '{}({})'.format(node.__class__.__name__, ',\n    '.join(values))
",test/integration/samples_in/issue192.py,
survived,"def open_logs_folder() -> None:
    log_dir = os.path.dirname(get_log_file_path(""dummy.log""))
    if sys.platform.startswith(""darwin""):
        subprocess.run([""open"", log_dir], check=True)
    elif sys.platform.startswith(""win""):
        os.startfile(log_dir)  # type: ignore[attr-defined]
    else:
        subprocess.run([""xdg-open"", log_dir], check=True)
",app/desktop/studio_server/settings_api.py,
survived,"  def test_full_range(self):
    self.assertEqual(getbits(0b11010110, 0, 7), 0b11010110)
",test/unit/test_helpers.py,TestGetBits
survived,"        async def run_live() -> None:
            os.environ[""LIVE_FEED""] = ""1""
            orig = data_feeds.aiohttp  # type: ignore[attr-defined]
            data_feeds.aiohttp = None  # type: ignore[attr-defined]
            try:
                it = data_feeds.stream_macro_events(live=True)
                await anext(it)
            finally:
                data_feeds.aiohttp = orig  # type: ignore[attr-defined]
                os.environ.pop(""LIVE_FEED"", None)
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"def isAlphaNumDot(ch):
    return (ch >= ""A"" and ch <= ""Z"") or (ch >= ""a"" and ch <= ""z"") or (ch >= ""0"" and ch <= ""9"") or ch == ""_"" or ch == "".""
",tests/rosetta/transpiler/Python/function-frequency.py,
survived,"def newFps(fn):
    return Fps(coeffs=[], compute=fn)
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def xor(a, b):
    return (a and (not b)) or ((not a) and b)
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,
survived,"def newBoard():
    b = []
    r = 0
    while r < rows:
        row = []
        c = 0
        while c < cols:
            if _now() % 2 == 0:
                row = row + [""T""]
            else:
                row = row + ["" ""]
            c = c + 1
        b = b + [row]
        r = r + 1
    return b
",tests/rosetta/transpiler/Python/forest-fire.py,
survived,"def repeat(ch, n):
    s = """"
    i = 0
    while i < n:
        s = s + ch
        i = i + 1
    return s
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,
survived,"def chance(prob):
    threshold = int(prob * 1000.0)
    return _now() % 1000 < threshold
",tests/rosetta/transpiler/Python/forest-fire.py,
survived,"def formatGre(d, m, y):
    return str(d) + "" "" + gregorianStr[m - 1] + "" "" + str(y)
",tests/rosetta/transpiler/Python/french-republican-calendar.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/floyds-triangle.py,
survived,"def bresenham(x0, y0, x1, y1, g):
    dx = x1 - x0
    if dx < 0:
        dx = -dx
    dy = y1 - y0
    if dy < 0:
        dy = -dy
    sx = -1
    if x0 < x1:
        sx = 1
    sy = -1
    if y0 < y1:
        sy = 1
    err = dx - dy
    while True:
        drawPoint(g, x0, y0)
        if x0 == x1 and y0 == y1:
            break
        e2 = 2 * err
        if e2 > (-dy):
            err = err - dy
            x0 = x0 + sx
        if e2 < dx:
            err = err + dx
            y0 = y0 + sy
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def path(u, v, next):
    if next[u][v] < 0:
        return []
    p = [u]
    x = u
    while x != v:
        x = next[x][v]
        p = p + [x]
    return p
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/gui-maximum-window-dimensions.py,
survived,"def setCoverage(n, value):
    n[""coverage""] = value
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,
survived,"    def fake_eval(self, env, policy, episodes):
        return calls.pop(0)
",tests/test_world_model_open_endedness.py,
survived,"def test_percent_dict_fmt_extra_aggressive(state: State):
    s_in = """"""a = '%(?)ld world' % {'?': var}""""""
    s_expected = """"""a = f'{var} world'""""""
    state.aggressive = 2
    assert code_editor.fstringify_code_by_line(s_in, state)[0] == s_expected
",test/test_edits.py,
survived,"        def eval_fn(genome: list[float]) -> tuple[float, float, float]:
            x, y = genome
            return x**2, y**2, (x + y) ** 2
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def get_latest(_: None = Depends(verify_token)) -> ResultsResponse:
        if _latest_id is None:
            raise HTTPException(status_code=404)
        result = _simulations.get(_latest_id)
        if result is None:
            raise HTTPException(status_code=404)
        return result
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"        def _fake_import(name: str, *args: object, **kwargs: object) -> object:
            if name == ""agents"":
                raise ModuleNotFoundError
            return orig_import_module(name, *args, **kwargs)
",tests/test_agent_factory.py,TestAgentFactory
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/if_else.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_index.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_by_join.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/list_index.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/go_auto.py,
survived,"    def test_bridge_market_data(self) -> None:
        import tempfile

        with tempfile.NamedTemporaryFile(""w"", delete=False) as fh:
            fh.write(""6,6,6"")
            feed_path = fh.name

        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge"",
                ""--episodes"",
                ""1"",
                ""--market-data"",
                feed_path,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"        def list_entries(self) -> list[tuple[int, str, str, int]]:
            return [(1, ""foo.tar"", ""deadbeef"", 1)]
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,DummyArchive
survived,"    def File(*_a, **_kw):
        ...
",alpha_factory_v1/backend/orchestrator.py,
survived,"def twoSum(nums, target):
    n = len(nums)
    for i in range(n):
        for j in range(i + 1, n):
            if nums[i] + nums[j] == target:
                return [i, j]
    return [-1, -1]
",tests/human/x/python/two-sum.py,
survived,"def outer(x: int) -> int:
    def inner(y: int) -> int:
        return x + y
    return inner(5)
",tests/human/x/python/nested_function.py,
survived,"    def inner(y: int) -> int:
        return x + y
",tests/human/x/python/nested_function.py,
survived,"def sum_rec(n: int, acc: int) -> int:
    if n == 0:
        return acc
    return sum_rec(n - 1, acc + n)
",tests/human/x/python/tail_recursion.py,
survived,"    def boom(*args, **kwargs):
        raise RuntimeError(""boom"")
",tests/test_fetch_assets.py,
survived,"    def test_exception_logged(self):
        saved = wm_mod._kafka
        wm_mod._kafka = DummyKafka()
        with mock.patch.object(wm_mod._LOG, ""exception"") as log_exc:
            wm_mod._kafka_send(""test.topic"", {""x"": 1})
            log_exc.assert_called_once()
            msg, topic_arg = log_exc.call_args.args
            self.assertIn(""Kafka emit failed"", msg)
            self.assertEqual(topic_arg, ""test.topic"")
        wm_mod._kafka = saved
",tests/test_world_model_kafka.py,TestKafkaSend
survived,"    def run(
        self,
        input_data: Input,
        *,
        credentials: GoogleCredentials,
        graph_exec_id: str,
        **kwargs,
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        message = self._reply(
            service,
            input_data,
            graph_exec_id,
        )
        yield ""messageId"", message[""id""]
        yield ""threadId"", message.get(""threadId"", input_data.threadId)
        yield ""message"", message
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailReplyBlock
survived,"    async def step(self) -> None:
        return None
",tests/test_skill_test_route.py,SimpleAgent
survived,"def test_seed_iteration():
    cfg = make_cfg()
    es = EnvStateManager(cfg, mode='train')
    es.reset()
    first_seed = es.envs[0]['status'].seed
    es.reset()
    second_seed = es.envs[0]['status'].seed
    assert first_seed == 7
    assert second_seed == 8",tests/es_manager/test_seed_iteration.py,
survived,"def subaru_checksum(address: int, sig: Signal, d: bytearray) -> int:
    s = 0
    addr = address
    while addr:
        s += addr & 0xFF
        addr >>= 8
    for i in range(1, len(d)):
        s += d[i]
    return s & 0xFF
",opendbc/can/packer.py,
survived,"def hkg_can_fd_checksum(address: int, sig: Signal, d: bytearray) -> int:
    crc = 0
    for i in range(2, len(d)):
        crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ d[i]]) & 0xFFFF
    crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ ((address >> 0) & 0xFF)]) & 0xFFFF
    crc = ((crc << 8) ^ CRC16_XMODEM[(crc >> 8) ^ ((address >> 8) & 0xFF)]) & 0xFFFF
    if len(d) == 8:
        crc ^= 0x5F29
    elif len(d) == 16:
        crc ^= 0x041D
    elif len(d) == 24:
        crc ^= 0x819D
    elif len(d) == 32:
        crc ^= 0x9F5B
    return crc
",opendbc/can/packer.py,
survived,"    def __init__(self, x: int, y: int):
        self.x = x
        self.y = y
",runtime/ffi/python/testmod.py,Point
survived,"    async def lineage_subtree(node_id: int, _: None = Depends(verify_token)) -> list[LineageNode]:
        """"""Return lineage up to ``node_id``.""""""
        path = Path(os.getenv(""ARCHIVE_PATH"", ""archive.db""))
        arch = Archive(path)
        nodes: list[LineageNode] = []
        found = False
        for a in arch.all():
            nodes.append(
                LineageNode(
                    id=a.id,
                    parent=a.meta.get(""parent""),
                    diff=a.meta.get(""diff"") or a.meta.get(""patch""),
                    pass_rate=a.score,
                )
            )
            if a.id == node_id:
                found = True
                break
        if not found:
            raise HTTPException(status_code=404)
        return nodes
",src/interface/api_server.py,
survived,"def _ensure(path: Path) -> None:
    with sqlite3.connect(path) as cx:
        cx.execute(
            """"""
            CREATE TABLE IF NOT EXISTS entries(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                parent TEXT,
                child TEXT,
                metrics TEXT,
                hash TEXT,
                ts REAL
            )
            """"""
        )
        cx.execute(
            ""CREATE TABLE IF NOT EXISTS merkle(date TEXT PRIMARY KEY, root TEXT)""
        )
",src/archive/archive.py,
survived,"def test_merkle_root_tracking(tmp_path: Path) -> None:
    db = tmp_path / ""arch.db""
    h1 = hashlib.sha256(json.dumps({""parent"": ""p"", ""child"": ""c1"", ""metrics"": {""s"": 1}}, sort_keys=True).encode()).hexdigest()
    insert(""p"", ""c1"", {""s"": 1}, db_path=db)
    h2 = hashlib.sha256(json.dumps({""parent"": ""c1"", ""child"": ""c2"", ""metrics"": {""s"": 2}}, sort_keys=True).encode()).hexdigest()
    insert(""c1"", ""c2"", {""s"": 2}, db_path=db)
    root = merkle_root(db_path=db)
    assert root == _manual_root([h1, h2])
",tests/test_archive_cron.py,
survived,"def test_cron_writes_root(tmp_path: Path, monkeypatch) -> None:
    db = tmp_path / ""hash.db""
    arch = HashArchive(db)
    tar = tmp_path / ""a.tar""
    tar.write_text(""a"", encoding=""utf-8"")
    arch.add_tarball(tar)
    out = tmp_path / ""root.json""
    monkeypatch.setenv(""ARCHIVE_PATH"", str(db))
    cid = publish_root(out_file=out)
    assert json.loads(out.read_text())[""cid""] == cid",tests/test_archive_cron.py,
survived,"        def labels(self, *_a: Any, **_kw: Any) -> ""_Metric"":
            return self
",alpha_factory_v1/backend/telemetry.py,_Metric
survived,"    def _get_metric(factory: Callable[..., Any], name: str, desc: str, labels: list[str] | None = None) -> Any:
        return _reg_metric(factory, name, desc, labels)
",alpha_factory_v1/backend/telemetry.py,
survived,"async def start_rest(app: Optional[""FastAPI""], port: int, loglevel: str) -> Optional[asyncio.Task]:
    """"""Run the FastAPI app on the given port if dependencies are available.""""""

    if not app or ""uvicorn"" not in globals():
        return None
    cfg = uvicorn.Config(app, host=""0.0.0.0"", port=port, log_level=loglevel.lower())
    task = asyncio.create_task(uvicorn.Server(cfg).serve())
    log.info(""REST UI ‚Üí  http://localhost:%d/docs"", port)
    return task
",alpha_factory_v1/backend/api_server.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/right_join.py,Customer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_sort.py,Item
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_items_iteration.py,Data
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_conditional_sum.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/join_multi.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_left_join.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join.py,Partsupp
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/left_join_multi.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/join_multi.py,Customer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/sort_stable.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/outer_join.py,Customer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/left_join.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join.py,Nation
survived,"def _mem_stub() -> object:
    vec = type(""Vec"", (), {""recent"": lambda *a, **k: [], ""search"": lambda *a, **k: []})()
    return type(""Mem"", (), {""vector"": vec})()
",tests/test_backend_orchestrator_dev.py,
survived,"    def test_missing_attributes_skips_version_check(self) -> None:
        fake_mod = types.SimpleNamespace(__version__=""0.0.17"")

        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            if name == ""agents"":
                return None
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [""openai_agents""]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
            mock.patch.object(check_env, ""check_openai_agents_version"", return_value=True) as chk,
        ):
            self.assertEqual(check_env.main([]), 0)
            chk.assert_not_called()
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"async def _shutdown() -> None:
    """"""Stop the orchestrator loop and wait for the thread to exit.""""""
    global orch, loop_thread
    if orch:
        orch.stop = True
    if loop_thread:
        loop_thread.join(timeout=1)
    orch = None
    loop_thread = None
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,
survived,"        def __init__(self, *a, **kw) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,AgentRuntime
survived,"def test_simulate_invalid_option() -> None:
    """"""Invoke simulate with an invalid export option.""""""
    res = CliRunner().invoke(
        cli.main,
        [""simulate"", ""--horizon"", ""1"", ""--offline"", ""--export"", ""xml""],
    )
    assert res.exit_code != 0
    assert ""Invalid value for '--export'"" in res.output",tests/test_demo_cli.py,
survived,"def test_transform_groups():
    result = transform_groups(MOCK_ENTRA_GROUPS, {
        gid: [u.id for u in users] for gid, users in MOCK_GROUP_MEMBERS.items()
    })
    assert len(result) == 2
    group1 = next(g for g in result if g[""id""] == ""11111111-1111-1111-1111-111111111111"")
    assert group1[""display_name""] == ""Security Team""
    assert group1[""member_ids""] == [
        ""ae4ac864-4433-4ba6-96a6-20f8cffdadcb"",
        ""11dca63b-cb03-4e53-bb75-fa8060285550"",
    ]
",tests/unit/cartography/intel/entra/test_groups.py,
survived,"def load_groups(
    neo4j_session: neo4j.Session,
    groups: List[Dict[str, Any]],
    update_tag: int,
    tenant_id: str,
) -> None:
    logger.info(f""Loading {len(groups)} Entra groups"")
    load(
        neo4j_session,
        EntraGroupSchema(),
        groups,
        lastupdated=update_tag,
        TENANT_ID=tenant_id,
    )
",cartography/intel/entra/groups.py,
survived,"    def resolve(
        self, packages: Iterable[str], include_hashes: bool = False
    ) -> Tuple[List[str], Dict[str, str], Optional[Dict[str, str]]]:
        """"""Return pinned requirements and license info for ``packages``.""""""

        pinned: Dict[str, str] = {}
        licenses: Dict[str, str] = {}
        hashes: Optional[Dict[str, str]] = {} if include_hashes else None
        visited: set[str] = set()

        for pkg in packages:
            base = pkg.split(""=="")[0].split("">="")[0].split(""<"")[0]
            base = base.split(""["")[0]
            self._collect_recursive(base, pinned, licenses, visited, include_hashes, hashes)

        reqs = [f""{name}=={ver}"" for name, ver in sorted(pinned.items())]
        return reqs, licenses, hashes",src/meta_agent/dependency_manager.py,DependencyManager
survived,"def test_bundle_generator_git(tmp_path: Path) -> None:
    remote = tmp_path / ""remote.git""
    subprocess.run([""git"", ""init"", ""--bare"", str(remote)], check=True)

    repo = tmp_path / ""repo""
    gen = BundleGenerator(repo)
    gen.generate(agent_code=""print('x')"", init_git=True, git_remote=str(remote))

    assert (repo / "".git"").exists()
    commit = subprocess.check_output(
        [""git"", ""-C"", str(repo), ""rev-parse"", ""HEAD""], text=True
    ).strip()
    with open(repo / ""bundle.json"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[""custom""][""git_commit""] == commit

    log = subprocess.check_output(
        [""git"", ""-C"", str(remote), ""log"", ""--oneline""], text=True
    )
    assert commit[:7] in log",tests/test_bundle_generator.py,
survived,"def test_git_manager_push(tmp_path: Path) -> None:
    remote = tmp_path / ""remote.git""
    subprocess.run([""git"", ""init"", ""--bare"", str(remote)], check=True)

    repo = tmp_path / ""repo""
    gm = GitManager(repo)
    gm.init()
    (repo / ""bar.txt"").write_text(""bar"")
    gm.commit_all(""first"")
    gm.add_remote(""origin"", str(remote))
    gm.push(""origin"", ""main"")

    log = subprocess.check_output(
        [""git"", ""-C"", str(remote), ""log"", ""--oneline""], text=True
    )
    assert ""first"" in log",tests/test_git_utils.py,
survived,"    def git_available() -> bool:
        """"""Return True if the ``git`` executable can be found.""""""
        return shutil.which(""git"") is not None
",src/meta_agent/git_utils.py,GitManager
survived,"    def validate(self) -> ValidationResult:
        errors: List[str] = []
        try:
            metadata = self._load_metadata()
        except Exception as exc:  # pragma: no cover - invalid json path rare
            errors.append(f""invalid bundle metadata: {exc}"")
            return ValidationResult(success=False, errors=errors, coverage=0.0)

        self._validate_checksums(metadata, errors)
        self._validate_requirements(errors)
        self._validate_agent(errors)

        if not errors:
            self._run_tests(errors)

        success = not errors
        return ValidationResult(success=success, errors=errors, coverage=0.0)",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def test_bundle_validator_success(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is True
    assert result.errors == []
",tests/test_bundle_validator.py,
survived,"    def __init__(self, *args: object, **kwargs: object) -> None:
        super().__init__(*args)
",tests/conftest.py,APITimeoutError
survived,"    def __init__(self, *args: object, **kwargs: object) -> None:
        super().__init__(*args)
",tests/conftest.py,APIConnectionError
survived,"def trim(s):
    start = 0
    while start < len(s) and (s[start:start + 1] == "" "" or s[start:start + 1] == ""\t""):
        start = start + 1
    end = len(s)
    while end > start and (s[end - 1:end] == "" "" or s[end - 1:end] == ""\t""):
        end = end - 1
    return s[start:end]
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,
survived,"def incr(i):
    return (int(i)) + 1
",tests/rosetta/transpiler/Python/church-numerals-1.py,
survived,"def succ(n):
    return lambda f: compose(f, n(f))
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def one():
    return id
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def main():
    res = []
    count = 0
    k = 11 * 11
    while count < 20:
        if k % 3 == 0 or k % 5 == 0 or k % 7 == 0:
            k = k + 2
            continue
        factors = primeFactors(k)
        if len(factors) > 1:
            s = str(k)
            includesAll = True
            prev = -1
            for f in factors:
                if f == prev:
                    continue
                fs = str(f)
                if indexOf(s, fs) == (-1):
                    includesAll = False
                    break
                prev = f
            if includesAll:
                res = res + [k]
                count = count + 1
        k = k + 2
    line = """"
    for e in res[0:10]:
        line = line + pad10(commatize(e)) + "" ""
    print(trimRightStr(line))
    line = """"
    for e in res[10:20]:
        line = line + pad10(commatize(e)) + "" ""
    print(trimRightStr(line))
",tests/rosetta/transpiler/Python/composite-numbers-k-with-no-single-digit-factors-whose-factors-are-all-substrings-of-k.py,
survived,"def modInv(a, m):
    r = egcd(a, m)
    if r[0] != 1:
        return 0
    x = r[1]
    if x < 0:
        return x + m
    return x
",tests/rosetta/transpiler/Python/chinese-remainder-theorem.py,
survived,"def toInt(x):
    counter = 0
    def fCounter(f):
        global counter
        counter = counter + 1
        return f
    x(fCounter)(id)
    return counter
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def pointStr(p):
    return ""("" + str(p.x) + "","" + str(p.y) + "")""
",tests/rosetta/transpiler/Python/convex-hull.py,
survived,"def test_cli_output_error(monkeypatch):
    def fail(*args, **kwargs):
        raise OSError(""boom"")

    monkeypatch.setattr(click, ""secho"", fail)
    cli = CLIOutput()
    with pytest.raises(CLIOutputError):
        cli.info(""hello"")",tests/ux/test_cli_output.py,
survived,"    def checksums(self) -> Dict[str, str]:
        return dict(self.metadata.custom.get(""checksums"", {}))",src/meta_agent/bundle.py,Bundle
survived,"def test_replay_closes_ledger(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with (
            patch.object(cli.logging, ""Ledger"") as led_cls,
            patch.object(cli.time, ""sleep"", return_value=None),
        ):
            led = led_cls.return_value
            led.__enter__.return_value = led
            led.__exit__.side_effect = lambda *_: led.close()
            led.tail.return_value = [{""ts"": 0.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            CliRunner().invoke(cli.main, [""replay""])
        led.close.assert_called_once()",tests/test_demo_cli.py,
survived,"        def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
            self.logged.append(env)
",tests/test_adapters.py,DummyLedger
survived,"        def start_merkle_task(self, *_a, **_kw):
            pass
",tests/test_adapters.py,DummyLedger
survived,"    def fake_find_spec(name, *args, **kwargs):
        if name in {""numpy"", ""pandas""}:
            return None
        return orig_find_spec(name, *args, **kwargs)
",tests/test_check_env_core.py,
survived,"        def register_agent(self, _agent) -> None:  # pragma: no cover - stub
            pass
",stubs/google_adk/__init__.py,Router
survived,"    def test_entry_point_resolves(self) -> None:
        eps = im.entry_points().select(group=""console_scripts"")
        match = [ep for ep in eps if ep.name == ""mats-bridge""]
        self.assertTrue(match, ""mats-bridge entry point not found"")
        self.assertEqual(
            match[0].value,
            ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge:main"",
        )
",tests/test_mats_bridge_entrypoint.py,TestMatsBridgeEntryPoint
survived,"def include_sqlalchemy_models(
    app: EnrichMCP,
    base: type[DeclarativeBase],
    *,
    session_key: str = ""session_factory"",
) -> dict[str, type]:
    """"""Register SQLAlchemy models with automatic resources and resolvers.""""""

    models: dict[str, type] = {}
    for mapper in base.registry.mappers:
        sa_model = mapper.class_
        if not issubclass(sa_model, EnrichSQLAlchemyMixin):
            continue
        enrich_cls = sa_model.__enrich_model__()
        model = type(
            enrich_cls.__name__,
            (enrich_cls,),
            {""__doc__"": enrich_cls.__doc__},
        )
        app.entity(model)
        models[sa_model.__name__] = model
        models[model.__name__] = model

    for mapper in base.registry.mappers:
        sa_model = mapper.class_
        if sa_model.__name__ not in models:
            continue
        enrich_model = models[sa_model.__name__]
        _register_default_resources(app, sa_model, enrich_model, session_key)
        _register_relationship_resolvers(app, sa_model, enrich_model, models, session_key)
        enrich_model.model_rebuild(_types_namespace=models)

    return models",src/enrichmcp/sqlalchemy/auto.py,
survived,"def test_allows_node_22() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    script = browser_dir / ""build.js""
    node_code = ""Object.defineProperty(process.versions,'node',{value:'22.0.0'});"" f"" import('./{script.name}')""
    res = subprocess.run(
        [""node"", ""-e"", node_code],
        cwd=browser_dir,
        text=True,
        capture_output=True,
    )
    assert res.returncode == 0, res.stderr
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_node_version.py,
survived,"def test_concurrent_experiments(tmp_path, monkeypatch) -> None:
    monkeypatch.setenv(""ARCHIVE_PATH"", str(tmp_path / ""arch.db""))
    monkeypatch.setenv(""SOLUTION_ARCHIVE_PATH"", str(tmp_path / ""sol.duckdb""))
    settings = config.Settings(bus_port=0)
    with mock.patch.object(orchestrator.Orchestrator, ""_init_agents"", lambda self: []):
        orch = orchestrator.Orchestrator(settings)

    import numpy as np

    monkeypatch.setattr(""src.evaluators.novelty.embed"", lambda _t: np.zeros((1, 1), dtype=""float32""))
    monkeypatch.setattr(""src.simulation.surrogate_fitness.aggregate"", lambda vals, **kw: [0.0 for _ in vals])
    times = []

    def dummy_run(*_a, **_kw):
        times.append(time.perf_counter())
        time.sleep(0.2)
        ind = orchestrator.mats.Individual([0.0])
        ind.score = 0.0
        return [ind]

    monkeypatch.setattr(orchestrator.mats, ""run_evolution"", dummy_run)

    def fn(genome: list[float]) -> tuple[float]:
        time.sleep(0.2)
        return (sum(genome),)

    async def run() -> None:
        await asyncio.gather(
            orch.evolve(""a"", fn, 1, experiment_id=""exp1"", population_size=2, generations=1),
            orch.evolve(""b"", fn, 1, experiment_id=""exp2"", population_size=2, generations=1),
        )

    asyncio.run(run())
    assert len(times) == 2
    assert abs(times[0] - times[1]) < 0.05
    assert ""exp1"" in orch.experiment_pops
    assert ""exp2"" in orch.experiment_pops
    assert orch.experiment_pops[""exp1""] is not orch.experiment_pops[""exp2""]

    specs = [json.loads(row[0])[""experiment_id""] for row in orch.archive.conn.execute(""SELECT spec FROM entries"")]
    assert {""exp1"", ""exp2""} <= set(specs)",tests/test_experiments.py,
survived,"def anthropic_rewrite(agents: List[int], model: str | None = None) -> List[int]:
    """"""Improve ``agents`` using the Anthropic API when available.""""""

    have_anthropic = importlib.util.find_spec(""anthropic"") is not None
    if have_anthropic and os.getenv(""ANTHROPIC_API_KEY""):
        try:  # pragma: no cover - optional integration
            import anthropic  # type: ignore

            claude_model = model or os.getenv(
                ""ANTHROPIC_MODEL"", ""claude-3-opus-20240229""
            )
            client = anthropic.Anthropic(api_key=os.getenv(""ANTHROPIC_API_KEY""))

            prompt = (
                ""Given the current integer policy ""
                f""{agents}, suggest a slightly improved list of integers.""
            )

            msg = client.messages.create(
                model=claude_model,
                max_tokens=20,
                messages=[{""role"": ""user"", ""content"": prompt}],
                system=""You rewrite policies for a simple number line game."",
            )

            text = msg.content[0].text if getattr(msg, ""content"", None) else """"
            result = _parse_numbers(text, agents)
            return result
        except Exception as exc:  # pragma: no cover - safety net
            logging.warning(f""anthropic_rewrite fallback due to error: {exc}"")

    return meta_rewrite(agents)",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/meta_rewrite.py,
survived,"def health() -> dict[str, str]:
    return {""status"": ""ok""}",backend/main.py,
survived,"    def fake_run(app, host, port, log_level=""info"", **kw):
        called['host'] = host
        called['port'] = port
",tests/test_external_integrations.py,
survived,"        def run(self, prompt: str):
            return ""ok""
",tests/test_external_integrations.py,Dummy
survived,"        def __init__(self):
            self.app = types.SimpleNamespace(middleware=lambda *_a, **_k: lambda f: f)
",tests/test_external_integrations.py,DummyRouter
survived,"            def __init__(self, *args: object, **kwargs: object) -> None:
                pass
",tests/test_orchestrator_grpc.py,TestServeGrpc._Msg
survived,"    def __exit__(self, exc_type, exc, tb) -> bool:
        try:
            self.close()
        except Exception as err:  # pragma: no cover - defensive
            logger.warning(""MemoryFabric: close failed ‚Üí %s"", err)
            return exc_type is None
        return False
",alpha_factory_v1/backend/memory_fabric.py,MemoryFabric
survived,"  async def deactivate(self):
    pass
",pylabrobot/temperature_controlling/temperature_controller_tests.py,_FakeBackend
survived,"def _fitness(item: Any) -> Iterable[float]:
    if isinstance(item, Mapping):
        vals = item.get(""fitness"") or item.get(""objective_values"")
        if isinstance(vals, Mapping):
            return list(vals.values())
        if isinstance(vals, Iterable):
            return list(vals)
    return list(getattr(item, ""fitness"", []))
",src/utils/visual.py,
survived,"def test_ai_search_dataset_missing(client, monkeypatch):
    from np_ocr import api as api_module

    monkeypatch.setattr(api_module, ""search_client"", types.SimpleNamespace(search_images_by_text=lambda *a, **k: []))

    response = client.post(
        ""/search"",
        data={""user_query"": ""foo"", ""user_id"": ""user"", ""case_name"": ""case""},
    )
    assert response.status_code == 404
",no-ocr-api/tests/test_ingest_search.py,
survived,"def test_vllm_call_image_not_found(client, monkeypatch, tmp_path):
    from np_ocr import api as api_module

    ds_path = tmp_path / ""storage/user/case/hf_dataset""
    ds_path.mkdir(parents=True)

    fake_dataset = FakeDataset([
        {""pdf_name"": ""a.pdf"", ""pdf_page"": 1, ""image"": Image.new(""RGB"", (10, 10))}
    ])

    monkeypatch.setattr(api_module, ""load_from_disk"", lambda *_: fake_dataset)
    monkeypatch.setattr(
        api_module,
        ""settings"",
        types.SimpleNamespace(
            STORAGE_DIR=str(tmp_path / ""storage""),
            HF_DATASET_DIRNAME=""hf_dataset"",
            VLLM_URL=""http://x"",
            VLLM_API_KEY=""k"",
            VLLM_MODEL=""m"",
        ),
    )
    monkeypatch.setattr(api_module, ""call_vllm"", lambda *a, **kw: api_module.ImageAnswer(answer=""ok""))

    response = client.post(
        ""/vllm_call"",
        data={
            ""user_query"": ""foo"",
            ""user_id"": ""user"",
            ""case_name"": ""case"",
            ""pdf_name"": ""not.pdf"",
            ""pdf_page"": 2,
        },
    )
    assert response.status_code == 404
",no-ocr-api/tests/test_ingest_search.py,
survived,"                        def select(self, *_):
                            return self
",no-ocr-api/tests/test_ingest_search.py,FakeTable.Limiter.Selector
survived,"def test_vllm_call_missing_dataset(client):
    response = client.post(
        ""/vllm_call"",
        data={
            ""user_query"": ""foo"",
            ""user_id"": ""user"",
            ""case_name"": ""case"",
            ""pdf_name"": ""x.pdf"",
            ""pdf_page"": 1,
        },
    )
    assert response.status_code == 404
",no-ocr-api/tests/test_ingest_search.py,
survived,"def test_transform_image_missing_cv2(monkeypatch) -> None:
    img = Image.new(""RGB"", (10, 10), ""red"")

    monkeypatch.setattr(""mistral_common.tokens.tokenizers.multimodal.is_cv2_installed"", lambda: False)

    with pytest.raises(ImportError) as exc_info:
        transform_image(img, (16, 16))

    assert ""pip install mistral-common[opencv]"" in str(exc_info.value)",tests/test_multimodal.py,
survived,"        def __init__(self, bootstrap_servers: str) -> None:
            pass
",tests/test_bus_large_payloads_property.py,Prod
survived,"    def _load_manifest(self) -> Dict[str, Any]:
        try:
            with open(self.manifest_path, ""r"", encoding=""utf-8"") as f:
                return json.load(f)
        except (IOError, json.JSONDecodeError):
            logger.warning(""Failed to read template registry manifest. Recreating."")
            return {}
",src/meta_agent/template_registry.py,TemplateRegistry
survived,"    def rollback(self, slug: str, version: str) -> bool:
        slug_sanitized = slug.replace("" "", ""_"").lower()
        manifest = self._load_manifest()
        entry = manifest.get(slug_sanitized)
        if not entry or version not in entry.get(""versions"", {}):
            return False
        entry[""current_version""] = version
        self._save_manifest(manifest)
        return True",src/meta_agent/template_registry.py,TemplateRegistry
survived,"def _create_repo(tmpdir: Path, content: str) -> Path:
    docs = tmpdir / ""docs""
    docs.mkdir()
    (docs / ""DISCLAIMER_SNIPPET.md"").write_text(SNIPPET_TEXT)
    (tmpdir / ""README.md"").write_text(content)
    return tmpdir
",tests/test_verify_disclaimer_snippet.py,
survived,"def generate_service_worker(root: Path, dist_dir: Path, manifest: dict) -> None:
    """"""Create ``sw.js`` using workbox and inject it into ``index.html``.""""""
    sw_src = root / ""sw.js""
    sw_dest = dist_dir / ""sw.js""
    version = json.loads((root / ""package.json"").read_text())[""version""]
    temp_sw = dist_dir / ""sw.build.js""
    temp_sw.write_text(sw_src.read_text().replace(""__CACHE_VERSION__"", version))
    node_script = f""""""
const {{injectManifest}} = require('workbox-build');
injectManifest({{
  swSrc: {json.dumps(str(temp_sw))},
  swDest: {json.dumps(str(sw_dest))},
  globDirectory: {json.dumps(str(dist_dir))},
  importWorkboxFrom: 'disabled',
  globPatterns: {json.dumps(manifest['precache'])},
  injectionPoint: 'self.__WB_MANIFEST',
}}).catch(err => {{console.error(err); process.exit(1);}});
""""""
    try:
        subprocess.run([""node"", ""-e"", node_script], check=True)
    except FileNotFoundError:
        print(
            ""[manual_build] node not found; skipping service worker generation"",
            file=sys.stderr,
        )
    except subprocess.CalledProcessError as exc:
        print(
            f""[manual_build] workbox build failed: {exc}; offline features disabled"",
            file=sys.stderr,
        )
    finally:
        temp_sw.unlink(missing_ok=True)
    sw_hash = sha384(sw_dest)
    index_path = dist_dir / ""index.html""
    text = index_path.read_text()
    text = text.replace("".register('sw.js')"", "".register('service-worker.js')"")
    text = text.replace(
        ""</body>"",
        f'<script src=""service-worker.js"" integrity=""{sw_hash}"" crossorigin=""anonymous""></script>\n</body>',
    )
    text = re.sub(r""(script-src 'self' 'wasm-unsafe-eval')"", rf""\1 '{sw_hash}'"", text)
    index_path.write_text(text)",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build/common.py,
survived,"def linear_curve(t: float) -> float:
    return max(0.0, min(1.0, t))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/forecast.py,
survived,"    async def ws_progress(websocket: WebSocket) -> None:
        """"""Stream year-by-year progress updates to the client.""""""
        await websocket.accept()
        _progress_ws.add(websocket)
        try:
            while True:
                await websocket.receive_text()
        except Exception:
            pass
        finally:
            _progress_ws.discard(websocket)
",src/interface/api_server.py,
survived,"    def test_ledger_env_override(self) -> None:
        with tempfile.TemporaryDirectory() as tmp_home, tempfile.TemporaryDirectory() as tmp:
            target = Path(tmp) / ""ledger.json""
            env = {""HOME"": tmp_home, ""CROSS_ALPHA_LEDGER"": str(target)}
            with patch.dict(os.environ, env, clear=False):
                path = stub._ledger_path(None)
            self.assertEqual(path, target.resolve())
            self.assertTrue(path.parent.exists())
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"    def test_discover_alpha_online(self) -> None:
        resp = types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""[]""))])
        openai_mock = types.SimpleNamespace(ChatCompletion=types.SimpleNamespace(create=Mock(return_value=resp)))
        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""x""}):
            with patch.object(stub, ""openai"", openai_mock, create=True):
                stub.discover_alpha(num=1, ledger=None, model=""gpt-4o-mini"")
        openai_mock.ChatCompletion.create.assert_called_once()
        kwargs = openai_mock.ChatCompletion.create.call_args.kwargs
        self.assertEqual(kwargs.get(""response_format""), {""type"": ""json_object""})
        self.assertEqual(kwargs.get(""timeout""), stub.OPENAI_TIMEOUT_SEC)
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"def test_other_dtype_annotation():
    def bar(x: i32[""batch""]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(bar, include_extras=True)[""x""])[1]
    assert spec.dtype == jnp.int32
    assert spec.before == (""batch"",)
",tests/test_dtype_typing.py,
survived,"    def __init__(self, payload=None, text=""ok"", status_code=200):
        self._payload = payload
        self.text = text
        self.status_code = status_code
",tests/test_openai_bridge_integration.py,DummyResponse
survived,"def _start_server(directory: Path):
    handler = partial(http.server.SimpleHTTPRequestHandler, directory=str(directory))
    server = http.server.ThreadingHTTPServer((""localhost"", 0), handler)
    thread = threading.Thread(target=server.serve_forever, daemon=True)
    thread.start()
    return server, thread
",tests/test_sw_offline_reload.py,
survived,"def load_documents(data_dir: str):
    """"""Load and preprocess all text files under *data_dir*.""""""
    corpus = []
    for filename in sorted(glob.glob(os.path.join(data_dir, ""*.txt""))):
        with open(filename, ""r"", encoding=""utf-8"", errors=""ignore"") as f:
            text = f.read().lower()
        tokens = [t for t in TOKEN_RE.findall(text) if t not in STOPWORDS]
        corpus.append(tokens)
    return corpus
",scripts/bbc_demo.py,
survived,"def test_bbc_demo_deterministic():
    args = argparse.Namespace(
        data_dir=BBC_DIR,
        iterations=2,
        display_topics=2,
        n_words=3,
        num_levels=3,
        alpha=10.0,
        gamma=1.0,
        eta=0.1,
        seed=0,
    )
    hlda = bbc_demo.run_demo(args)
    assert hlda.root_node.total_nodes == 15
    assert hlda.root_node.customers == 401
    assert hlda.num_documents == 401
",tests/test_bbc_demo.py,
survived,"    def fake_download(url: str, dest: Path) -> None:
        dest.parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(""ok"")
        calls.append((url, dest))
",tests/test_download_hf_gpt2.py,
survived,"def test_tracing_env_variable(monkeypatch: pytest.MonkeyPatch) -> None:
    endpoint = ""http://collector:4317""
    called: list[str] = []

    class DummyExporter:
        def __init__(self, endpoint: str | None = None, *args: Any, **_kw: Any) -> None:  # noqa: D401 - simple init
            called.append(endpoint or """")

    class DummyTracer:
        def __init__(self) -> None:
            self.spans: list[str] = []

        def start_as_current_span(self, name: str) -> Any:
            self.spans.append(name)
            return nullcontext()

    class DummyTrace:
        def __init__(self) -> None:
            self.tracer = DummyTracer()

        def set_tracer_provider(self, _provider: Any) -> None:
            pass

        def get_tracer(self, _name: str) -> DummyTracer:
            return self.tracer

    class DummyMetrics:
        def set_meter_provider(self, _provider: Any) -> None:  # noqa: D401 - simple stub
            pass

        def get_meter(self, _name: str) -> None:
            return None

    import importlib
    import alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils.tracing as tracing

    if not hasattr(tracing, ""OTLPSpanExporter""):
        pytest.skip(""OTLP exporter not available"")

    monkeypatch.setattr(tracing, ""OTLPSpanExporter"", DummyExporter)
    monkeypatch.setattr(tracing, ""OTLPMetricExporter"", DummyExporter)
    monkeypatch.setattr(tracing, ""trace"", DummyTrace())
    monkeypatch.setattr(tracing, ""metrics"", DummyMetrics())
    monkeypatch.setenv(""OTEL_EXPORTER_OTLP_ENDPOINT"", endpoint)

    tracing = importlib.reload(tracing)
    assert called == [endpoint, endpoint]
    assert tracing.tracer is not None
    with tracing.span(""demo""):
        pass
    dummy = cast(DummyTracer, tracing.tracer)
    assert ""demo"" in dummy.spans",tests/test_metrics.py,
survived,"        def set_meter_provider(self, _provider: Any) -> None:  # noqa: D401 - simple stub
            pass
",tests/test_metrics.py,DummyMetrics
survived,"    def fake_apply(diff_text, repo_path):
        applied.append(diff_text)
        diff_utils.apply_diff(diff_text, repo_dir=repo_path)
",tests/test_self_healer_pipeline.py,
survived,"def test_guardrail_config_from_dict():
    data = {""rules"": [{""name"": ""pii"", ""pattern"": ""ssn""}]}
    cfg = GuardrailConfig.from_dict(data)
    assert len(cfg.rules) == 1
    assert cfg.rules[0].name == ""pii""
",tests/test_guardrail_generator.py,
survived,"async def test_input_guardrail_exception_propagates():
    adapter = MockAdapter()
    router = GuardrailModelRouter({""a"": adapter}, default_model=""a"")

    async def bad_guard(_prompt: str):
        raise RuntimeError(""bad"")

    router.add_input_guardrail(bad_guard)

    with pytest.raises(RuntimeError):
        await router.invoke(""x"")
    assert not adapter.prompts
",tests/test_guardrail_router.py,
survived,"    def __init__(self, bus, ledger) -> None:  # type: ignore[override]
        super().__init__(""freeze"", bus, ledger)
",tests/test_agents.py,FreezeAgent
survived,"        def close(self) -> None:
            pass
",tests/test_agents.py,DummyLedger
survived,"def test_surrogate_pair_single_chunk() -> None:
    chunks = ['{""a"": ""\\ud83d\\ude00""}']
    parsed = _stream_to_dict({}, chunks)
    assert parsed == {""a"": ""üòÄ""}
",api/core/utils/streams_test.py,
survived,"    def _process_unicode_sequence(self) -> None:
        try:
            code = int(self._unicode_buffer, 16)
        except ValueError:
            self.current_chain += f""\\u{self._unicode_buffer}""
        else:
            if self._pending_surrogate is not None:
                if 0xDC00 <= code <= 0xDFFF:
                    high = self._pending_surrogate
                    code = ((high - 0xD800) << 10) + (code - 0xDC00) + 0x10000
                    self.current_chain += chr(code)
                    self._pending_surrogate = None
                else:
                    self.current_chain += chr(self._pending_surrogate)
                    self._pending_surrogate = None
                    if 0xD800 <= code <= 0xDBFF:
                        self._pending_surrogate = code
                    else:
                        self.current_chain += chr(code)
            else:
                if 0xD800 <= code <= 0xDBFF:
                    self._pending_surrogate = code
                else:
                    self.current_chain += chr(code)
        self._unicode_chars_left = None
        self._unicode_buffer = """"
",api/core/utils/streams.py,JSONStreamParser
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/machine/x/python/q3.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/machine/x/python/q2.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q2.py,Region
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Part
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/machine/x/python/q1.py,
survived,"def summarize_error(log: str) -> str:
    """"""Return a short summary of the failure log.""""""
    first_line = next((ln.strip() for ln in log.splitlines() if ln.strip()), """")
    return first_line[:80]
",alpha_factory_v1/demos/self_healing_repo/agent_core/llm_client.py,
survived,"    def _register_tool_def(self, fn: Callable[..., Any], tool_def: ToolDef) -> Callable[..., Any]:
        """"""Register ``fn`` as a tool using ``tool_def``.""""""

        desc = self._append_enrichparameter_hints(tool_def.final_description(self), fn)
        self.resources[tool_def.name] = fn
        mcp_tool = self.mcp.tool(name=tool_def.name, description=desc)
        return mcp_tool(fn)
",src/enrichmcp/app.py,EnrichMCP
survived,"def _verify(path: Path, name: str) -> bytes:
    data = path.read_bytes()
    expected = checksums.get(name)
    if expected:
        actual = sha384(path)
        if expected != actual:
            sys.exit(f""Checksum mismatch for {name}"")
    return data
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""13""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(13)",benchmarks/poly_mini/task_013.py,
survived,"def run() -> None:
    n = 24
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_024.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""5""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(5)",benchmarks/poly_mini/task_005.py,
survived,"def run() -> None:
    n = 22
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_022.py,
survived,"def test_filetools_adk_tasks(temp_file: Path) -> None:
    from src.self_edit.tools import FileToolsADK

    temp_file.write_text(""a\nb\nc\n"")
    adk = FileToolsADK()

    res = adk.view_task(path=str(temp_file), start=1, end=3)
    assert res == {""text"": ""b\nc""}

    adk.edit_task(path=str(temp_file), start=1, end=2, new_code=""X"")
    assert temp_file.read_text() == ""a\nX\nc""

    out = adk.replace_task(path=str(temp_file), pattern=""X"", repl=""Y"")
    assert out == {""count"": 1}
    assert temp_file.read_text() == ""a\nY\nc""",tests/test_self_edit_tools.py,
survived,"def test_run_macro_demo_download_failure_fallback(tmp_path: Path) -> None:
    """"""Failed downloads should copy placeholder CSVs.""""""
    offline_dir = tmp_path / ""data""
    env = {
        ""OPENAI_API_KEY"": ""dummy-key"",
        ""OFFLINE_DATA_DIR"": offline_dir.as_posix(),
    }
    _run_script(tmp_path, env=env, curl_rc=1)
    for f in [
        ""fed_speeches.csv"",
        ""yield_curve.csv"",
        ""stable_flows.csv"",
        ""cme_settles.csv"",
    ]:
        path = offline_dir / f
        assert path.exists(), f""missing {f}""
        assert path.stat().st_size > 0",tests/test_macro_launcher.py,
survived,"def test_add():
    assert calc.add(1, 1) == 2",tests/fixtures/self_heal_repo/test_calc.py,
survived,"    def resume(self) -> None:
        """"""Resume execution after a pause.""""""
        self.paused_at = None
        self.next_ts = 0
",alpha_factory_v1/backend/agent_runner.py,AgentRunner
survived,"def test_run_muzero_demo_invokes_docker(tmp_path: Path) -> None:
    repo_root = Path(__file__).resolve().parents[1]
    src = repo_root / ""alpha_factory_v1""
    dst = tmp_path / ""alpha_factory_v1""
    shutil.copytree(src, dst)

    script = dst / ""demos"" / ""muzero_planning"" / ""run_muzero_demo.sh""
    log_file = tmp_path / ""docker.log""
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()
    docker_stub = bin_dir / ""docker""
    docker_stub.write_text(
        f""#!/usr/bin/env bash\necho \""$@\"" >> '{log_file}'\nexit 0\n""
    )
    docker_stub.chmod(0o755)

    with socket.socket() as s:
        s.bind((""localhost"", 0))
        port = s.getsockname()[1]

    env = os.environ.copy()
    env.update({""PATH"": f""{bin_dir}:{env.get('PATH', '')}"", ""HOST_PORT"": str(port)})

    subprocess.run([""bash"", str(script)], check=True, env=env)

    assert log_file.read_text(), ""Docker stub was not invoked""
    assert ""compose"" in log_file.read_text()",tests/test_run_muzero_demo.py,
survived,"def test_malicious_message_blocked(tmp_path) -> None:
    if not hasattr(struct_pb2.Struct, ""get""):
        def _get(self: struct_pb2.Struct, key: str, default=None):
            try:
                return self[key]
            except Exception:
                return default

        struct_pb2.Struct.get = _get  # type: ignore[attr-defined]

    cfg = config.Settings(bus_port=0)
    bus = messaging.A2ABus(cfg)
    ledger = logging.Ledger(str(tmp_path / ""ledger.db""), broadcast=False)

    mem = memory_agent.MemoryAgent(bus, ledger, str(tmp_path / ""mem.log""))
    guardian = safety_agent.SafetyGuardianAgent(bus, ledger)
    chaos = chaos_agent.ChaosAgent(bus, ledger, burst=1)

    async def run() -> None:
        async with bus, ledger:
            await chaos.run_cycle()
            await asyncio.sleep(0)

    asyncio.run(run())

    assert mem.records
    assert mem.records[-1][""status""] == ""blocked""",tests/test_safety_block.py,
survived,"    def test_default_ledger_creation(self) -> None:
        with tempfile.TemporaryDirectory() as home:
            env = os.environ.copy()
            env[""HOME""] = home
            env.pop(""CROSS_ALPHA_LEDGER"", None)
            default = Path(home) / "".alpha_factory"" / ""cross_alpha_log.json""
            result = subprocess.run(
                [
                    sys.executable,
                    STUB,
                    ""-n"",
                    ""1"",
                    ""--seed"",
                    ""5"",
                    ""--model"",
                    ""gpt-4o-mini"",
                ],
                capture_output=True,
                text=True,
                env=env,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            self.assertTrue(default.exists())
            data = json.loads(default.read_text())
            self.assertIsInstance(data, list)
            self.assertEqual(len(data), 1)
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"    def test_stochastic_zero_noise(self):
        genes = {""temperature"": 0.7, ""top_p"": 0.9, ""max_tokens"": 128}
        self.assertAlmostEqual(
            gt.stochastic_fitness(genes, noise=0.0), gt.toy_fitness(genes), places=6
        )
",alpha_factory_v1/tests/test_genetic_tests.py,GeneticTestsTest
survived,"    def reset(self) -> float:
        """"""Reset the environment and return the starting price.""""""
        self.price = self.start_price
        return self.price
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv
survived,"    async def __aexit__(self, *_exc) -> None:
        return None
",alpha_factory_v1/backend/market_data.py,SimulatedMarketData
survived,"    async def __aenter__(self) -> ""SimulatedMarketData"":
        return self
",alpha_factory_v1/backend/market_data.py,SimulatedMarketData
survived,"    async def __aenter__(self) -> ""BinanceMarketData"":
        await self._client()
        return self
",alpha_factory_v1/backend/market_data.py,BinanceMarketData
survived,"    async def __aexit__(self, exc_type, exc, tb) -> None:
        if hasattr(self._backend, ""__aexit__""):
            await self._backend.__aexit__(exc_type, exc, tb)
",alpha_factory_v1/backend/market_data.py,MarketData
survived,"    def backend(self) -> str:
        """"""Current storage backend ('neo4j', 'networkx', or 'stub').""""""
        return self._backend
",alpha_factory_v1/backend/memory_graph.py,GraphMemory
survived,"    def __init__(self, resp: str):
        self.resp = resp
",alpha_factory_v1/tests/test_planner_agent.py,DummyModel
survived,"    def test_env_file(self):
        with tempfile.NamedTemporaryFile('w', delete=False) as fh:
            fh.write('FOO=bar\nLOGLEVEL=warning')
            path = fh.name
        args = _parse_with(['--env-file', path, '--loglevel', 'error'])
        for key in ('FOO', 'LOGLEVEL'):
            os.environ.pop(key, None)
        af_run.apply_env(args)
        os.unlink(path)
        self.assertEqual(os.environ['FOO'], 'bar')
        self.assertEqual(os.environ['LOGLEVEL'], 'ERROR')
",alpha_factory_v1/tests/test_cli.py,CliParseTest
survived,"    def __repr__(self) -> str:  # noqa: D401
        return f""GridWorldEnv(pos={self.pos})""",alpha_factory_v1/backend/environments/alpha_labyrinth.py,GridWorldEnv
survived,"    def update(self, **kw):
        for k, v in kw.items():
            if hasattr(self, k):
                setattr(self, k, v)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Config
survived,"    def __init__(self): self.pool: List[MiniWorld]=[]
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,POETGenerator
survived,"async def send_cmd(cmd:Dict[str,str]):
    A2ABus.publish(""orch"",cmd); return {""ok"":True}
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,
survived,"    def __init__(self, obs_dim: int, act_dim: int):
        super().__init__()
        self.repr = Repr(obs_dim, CFG.hidden)
        self.dyn  = Dyn(CFG.hidden, act_dim)
        self.pred = Pred(CFG.hidden, act_dim)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MuZeroTiny
survived,"    def initial(self, obs):
        h = self.repr(obs)
        v, p = self.pred(h)
        return h, v, p
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MuZeroTiny
survived,"    async def get_cash(self) -> float:
        return self.cash
",alpha_factory_v1/backend/broker/broker_sim.py,SimulatedBroker
survived,"def run_cycle(orchestrator: Orchestrator, fin_agent: AgentFin, res_agent: AgentRes,
              ene_agent: AgentEne, gdl_agent: AgentGdl, model: Model) -> None:
    """"""Execute one evaluation + commitment cycle.""""""

    bundle = orchestrator.collect_signals()
    delta_h = fin_agent.latent_work(bundle)
    delta_s = res_agent.entropy(bundle)
    beta = ene_agent.market_temperature()
    delta_g = delta_h - (delta_s / beta)

    log.info(""ŒîH=%s ŒîS=%s Œ≤=%s ‚Üí ŒîG=%s"", delta_h, delta_s, beta, delta_g)

    if delta_g < 0:
        orchestrator.post_alpha_job(id(bundle), delta_g)

    weight_update: Dict[str, Any] = {}
    if gdl_agent.provable(weight_update):
        model.commit(weight_update)
",alpha_factory_v1/demos/alpha_agi_business_3_v1/alpha_agi_business_3_v1.py,
survived,"    def ok(self) -> bool:
        return self.status_code < 400
",alpha_factory_v1/requests.py,Response
survived,"def on_join():
    domain = domain_var.get().strip()
    user = user_var.get().strip()
    ou = ou_var.get().strip()
    if not domain:
        messagebox.showerror(""Error"", ""Domain is required"")
        return
    if not user:
        messagebox.showerror(""Error"", ""Admin user is required"")
        return
    output, code = join_domain(domain, user, ou)
    if code == 0:
        messagebox.showinfo(""Success"", f""Successfully joined {domain}"")
    else:
        messagebox.showerror(""Join Failed"", output or ""Unknown error"")
",adconnection_gui.py,
survived,"def run_cmd(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print(e.output)
        sys.exit(e.returncode)
",adconnection_app.py,
survived,"async def make_client() -> tuple[AsyncClient, Any]:
    from src.interface import api_server

    transport = ASGITransport(app=cast(Any, api_server.app))
    client = AsyncClient(base_url=""http://test"", transport=transport)
    return client, api_server
",tests/test_api_server_cors.py,
survived,"def test_use_guards_sets_attribute():
    assert hasattr(GuardController.root, ""__guards__"")
    assert SimpleGuard in GuardController.root.__guards__
",tests/test_core/test_decorators/test_guard.py,
survived,"    def do_POST(self) -> None:  # noqa: D401
        self.send_response(200)
        self.send_header(""Content-Type"", ""application/json"")
        self.end_headers()
        self.wfile.write(b'{""choices"":[{""message"":{""content"":""ok""}}]}')
",tests/test_aiga_service_mixtral.py,_Handler
survived,"        def __init__(self, *a, **k):
            self.patch_file = os.environ.get(""PATCH_FILE"")
",tests/test_patcher_core_cli.py,StubAgent
survived,"    def fail_secho(*args, **kwargs):
        raise OSError(""boom"")
",tests/integration/test_ux_interactions.py,
survived,"    def fake_secho(message, **kwargs):
        messages.append(click.unstyle(message))
",tests/integration/test_ux_interactions.py,
survived,"def test_basic_ux_workflow(monkeypatch, capsys, capture_secho):
    # Simulate interactive choices
    inputs = iter([""1"", ""foo"", ""bar""])
    monkeypatch.setattr(""builtins.input"", lambda _: next(inputs))

    cli = CLIOutput()
    feedback = UserFeedback(cli_output=cli)
    interactive = Interactive()
    generator = DiagramGenerator()

    # interactive menu and form
    choice = interactive.menu(""Select"", [""diagram"", ""quit""])
    params = interactive.form([""a"", ""b""])

    assert choice == ""diagram""
    assert params == {""a"": ""foo"", ""b"": ""bar""}

    list(feedback.progress_iter(range(2), description=""progress""))
    out, err = capsys.readouterr()
    combined = click.unstyle(out + err)

    spec = {
        ""task_description"": ""Demo"",
        ""inputs"": {""q"": ""str""},
        ""outputs"": {""r"": ""str""},
    }
    diagram = generator.generate(spec)
    feedback.notify(""done"", NotificationSeverity.SUCCESS)

    assert ""done"" in capture_secho
    assert ""progress"" in combined
    assert diagram.startswith(""flowchart"")
",tests/integration/test_ux_interactions.py,
survived,"def capture_secho(monkeypatch):
    messages = []

    def fake_secho(message, **kwargs):
        messages.append(click.unstyle(message))

    monkeypatch.setattr(click, ""secho"", fake_secho)
    return messages
",tests/integration/test_ux_interactions.py,
survived,"        def intermediate(self, x):
            return x + 2 * self.w
",tests/test_scan.py,Module
survived,"    def test_policy_dispatch_discover(self):
        agent = bridge.BusinessAgent()
        with patch.object(bridge, ""trigger_discovery"", new=AsyncMock(return_value=""ok"")) as func:
            result = asyncio.run(agent.policy({""action"": ""discover""}, None))
        func.assert_awaited_once_with()
        self.assertEqual(result, ""ok"")
",tests/test_openai_bridge_integration.py,TestBusinessAgentIntegration
survived,"    async def stub(sim_id: str, _cfg: api.SimRequest) -> None:
        counter[""current""] += 1
        counter[""max""] = max(counter[""max""], counter[""current""])
        await asyncio.sleep(0.05)
        counter[""current""] -= 1
",tests/test_max_sim_tasks.py,
survived,"    async def list_runs(_: None = Depends(verify_token)) -> RunsResponse:
        return RunsResponse(ids=list(_simulations.keys()))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"        def __init__(self, app: FastAPI, limit: int = 60, window: int = 60) -> None:
            super().__init__(app)
            self.limit = int(os.getenv(""API_RATE_LIMIT"", str(limit)))
            self.window = window
            # Use TTLCache so inactive IP entries expire automatically.
            self.counters: TTLCache[str, deque[float]] = TTLCache(maxsize=1024, ttl=window)
            self.lock = asyncio.Lock()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,SimpleRateLimiter
survived,"    async def ws_progress(websocket: WebSocket) -> None:
        auth = websocket.headers.get(""authorization"")
        if not auth or not auth.startswith(""Bearer "") or auth.split("" "", 1)[1] != API_TOKEN:
            await websocket.close(code=1008)
            return
        await websocket.accept()
        _progress_ws.add(websocket)
        try:
            while True:
                await websocket.receive_text()
        except Exception:
            pass
        finally:
            _progress_ws.discard(websocket)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def get_population(sim_id: str, _: None = Depends(verify_token)) -> PopulationResponse:
        result = _simulations.get(sim_id)
        if result is None:
            raise HTTPException(status_code=404)
        return PopulationResponse(id=sim_id, population=result.population or [])
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def insight(req: InsightRequest, _: None = Depends(verify_token)) -> InsightResponse | JSONResponse:
        """"""Return aggregated forecast data across runs.""""""

        try:
            ids = req.ids or list(_simulations.keys())
            forecasts = [_simulations[i].forecast for i in ids if i in _simulations]
            if not forecasts:
                raise HTTPException(status_code=404)

            year_map: dict[int, list[float]] = {}
            for fc in forecasts:
                for point in fc:
                    year_map.setdefault(point.year, []).append(point.capability)
            agg = [InsightPoint(year=year, capability=sum(vals) / len(vals)) for year, vals in sorted(year_map.items())]
            return InsightResponse(forecast=agg)
        except HTTPException as exc:
            return problem_response(exc)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"def sbml_test_dir():
    old_cwd = os.getcwd()
    old_path = copy.copy(sys.path)
    yield
    os.chdir(old_cwd)
    sys.path = old_path
",tests/testSBMLSuiteJax.py,
survived,"    async def _run() -> None:
        await strat.handle(env)
        await summariser.run_cycle()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_openai_adk_integration.py,
survived,"        def close(self):
            pass
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_StubEnv
survived,"def register_demo_agents() -> None:
    """"""Register the demo agent with the framework.""""""

    register_agent(
        AgentMetadata(
            name=IncorporatorAgent.NAME,
            cls=IncorporatorAgent,
            version=""1.0.0"",
            capabilities=IncorporatorAgent.CAPABILITIES,
        )
    )
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=os.getenv(""OPENAI_API_KEY""))
    agent = MATSAgent()
    runtime.register(agent)
    try:
        from alpha_factory_v1.backend.adk_bridge import auto_register, maybe_launch

        auto_register([agent])
        maybe_launch()
    except Exception as exc:  # pragma: no cover - ADK optional
        print(f""ADK bridge unavailable: {exc}"")

    print(""Registered MATSAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,
survived,"    def test_parse_numbers_helper(self) -> None:
        from alpha_factory_v1.demos.meta_agentic_tree_search_v0.mats.meta_rewrite import (
            _parse_numbers,
        )

        text = ""[1, 2, -3]""
        res = _parse_numbers(text, [0, 0, 0])
        self.assertEqual(res, [1, 2, -3])
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"        async def stop_consumer(self) -> None:
            nonlocal stopped
            stopped = True
",tests/test_agent_manager_consumer.py,DummyBus
survived,"    def test_curve_helpers(self) -> None:
        self.assertEqual(forecast.linear_curve(-1.0), 0.0)
        self.assertEqual(forecast.linear_curve(2.0), 1.0)
        self.assertAlmostEqual(forecast.exponential_curve(0.0), 0.0)
        self.assertAlmostEqual(forecast.exponential_curve(1.0), 1.0)
        self.assertAlmostEqual(
            forecast.capability_growth(0.5, curve=""linear""), 0.5
        )
        val = forecast.capability_growth(0.2, curve=""exponential"")
        self.assertGreaterEqual(val, 0.0)
        self.assertLessEqual(val, 1.0)
",tests/test_forecast_functions.py,TestForecastFunctions
survived,"    def eval_fn(genome: list[float]) -> tuple[float, float, float]:
        x, y = genome
        return x**2, y**2, (x + y) ** 2
",src/interface/api_server.py,
survived,"def test_show_results_missing(tmp_path) -> None:
    with patch.object(cli.config, ""Settings"") as settings:
        settings.return_value.ledger_path = tmp_path / ""ledger.txt""
        out = CliRunner().invoke(cli.main, [""show-results""])
        assert ""No results"" in out.output
",tests/test_cli.py,
survived,"    async def step(self) -> None:
        return None
",tests/test_agents.py,DummyHB
survived,"def test_simulate_runs() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio"") as aio:
        aio.run.return_value = None
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(cli.main, [""simulate"", ""--horizon"", ""1"", ""--offline"", ""--pop-size"", ""1"", ""--generations"", ""1""])
        assert res.exit_code == 0
        aio.run.assert_called_once()",tests/test_cli.py,
survived,"    def fn(genome):
        x, y = genome
        return x ** 2, y ** 2
",tests/test_mats.py,
survived,"def get_stripped_lines(val: str):
    ## you don't want empty lines to add empty list after splitlines!
    val = val.strip()

    return [val_line.strip() for val_line in val.split(""\n"")]
",scripts/utils/lcb_runner.py,
survived,"    def read(self, *args):
        return self.inputs
",scripts/utils/lcb_runner.py,MockStdinWithBuffer
survived,"def grade_call_based(
    code: str, all_inputs: list, all_outputs: list, fn_name: str, timeout: int
):
    # call-based clean up logic
    # need to wrap in try-catch logic after to catch the correct errors, but for now this is fine.
    code = import_string + ""\n\n"" + code
    compiled_sol = compile_code(code, timeout)

    if compiled_sol is None:
        return

    method = get_function(compiled_sol, fn_name)

    if method is None:
        return

    all_inputs = [
        [json.loads(line) for line in inputs.split(""\n"")] for inputs in all_inputs
    ]

    all_outputs = [json.loads(output) for output in all_outputs]

    total_execution = 0
    all_results = []
    for idx, (gt_inp, gt_out) in enumerate(zip(all_inputs, all_outputs)):
        signal.alarm(timeout)
        faulthandler.enable()
        try:
            # can lock here so time is useful
            start = time.time()
            prediction = method(*gt_inp)
            total_execution += time.time() - start
            signal.alarm(0)

            # don't penalize model if it produces tuples instead of lists
            # ground truth sequences are not tuples
            if isinstance(prediction, tuple):
                prediction = list(prediction)

            tmp_result = prediction == gt_out

            # handle floating point comparisons

            all_results.append(tmp_result)

            if not tmp_result:
                return all_results, {
                    ""output"": truncatefn(prediction),
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                    ""error_code"": -2,
                    ""error_message"": ""Wrong Answer"",
                }
        except Exception as e:
            signal.alarm(0)
            if ""timeoutexception"" in repr(e).lower():
                all_results.append(-3)
                return all_results, {
                    ""error"": repr(e),
                    ""error_code"": -3,
                    ""error_message"": ""Time Limit Exceeded"",
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                }
            else:
                all_results.append(-4)
                return all_results, {
                    ""error"": repr(e),
                    ""error_code"": -4,
                    ""error_message"": ""Runtime Error"",
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                }

        finally:
            signal.alarm(0)
            faulthandler.disable()

    return all_results, {""execution time"": total_execution}
",scripts/utils/lcb_runner.py,
survived,"def run_test(sample, test=None, debug=False, timeout=6):
    """"""
    if test(generated_code) is not None it'll try to run the code.
    otherwise it'll just return an input and output pair.
    """"""
    signal.signal(signal.SIGALRM, timeout_handler)

    # Disable functionalities that can make destructive changes to the test.
    # max memory is set to 4GB
    reliability_guard()

    if debug:
        print(f""start = {datetime.now().time()}"")

    try:
        in_outs = json.loads(sample[""input_output""])
    except ValueError as e:
        raise e
        in_outs = None

    if in_outs:
        if in_outs.get(""fn_name"") is None:
            which_type = CODE_TYPE.standard_input  # Standard input
            method_name = None

        else:
            which_type = CODE_TYPE.call_based  # Call-based
            method_name = in_outs[""fn_name""]

    if debug:
        print(f""loaded input_output = {datetime.now().time()}"")

    if test is None:
        assert False, ""should not happen: test code is none""
        return in_outs, {""error"": ""no test code provided""}
    elif test is not None:
        results = []
        sol = import_string
        if debug:
            print(f""loading test code = {datetime.now().time()}"")

        if which_type == CODE_TYPE.call_based:
            signal.alarm(timeout)
            try:
                results, metadata = grade_call_based(
                    code=test,
                    all_inputs=in_outs[""inputs""],
                    all_outputs=in_outs[""outputs""],
                    fn_name=method_name,
                    timeout=timeout,
                )
                return results, metadata
            except Exception as e:
                return [-4], {
                    ""error_code"": -4,
                    ""error_message"": f""Error during testing: {e}"",
                }
            finally:
                signal.alarm(0)
        elif which_type == CODE_TYPE.standard_input:
            # sol
            # if code has if __name__ == ""__main__"": then remove it

            signal.alarm(timeout)
            try:
                results, metadata = grade_stdio(
                    code=test,
                    all_inputs=in_outs[""inputs""],
                    all_outputs=in_outs[""outputs""],
                    timeout=timeout,
                )
                return results, metadata
            except Exception as e:
                return [-4], {
                    ""error_code"": -4,
                    ""error_message"": f""Error during testing: {e}"",
                }
            finally:
                signal.alarm(0)
",scripts/utils/lcb_runner.py,
survived,"    async def _run_job(self, job: Job) -> None:
        try:
            await asyncio.to_thread(
                self_improver.improve_repo,
                job.repo,
                job.patch,
                job.metric,
                job.log,
            )
            self.tokens_used += job.tokens
        except Exception:  # noqa: BLE001
            await self.queue.put(job)
",src/scheduler.py,SelfImprovementScheduler
survived,"        async def _spawn():  # pragma: no cover - Rocketry callback
            await self._spawn_jobs()
",src/scheduler.py,SelfImprovementScheduler
survived,"def test_run_benchmarks(tmp_path: Path) -> None:
    result = subprocess.run(
        [sys.executable, str(Path('benchmarks') / 'run_benchmarks.py')],
        capture_output=True,
        text=True,
        check=True,
    )
    data = json.loads(result.stdout)
    assert any(d['task_id'].startswith('swebench_verified_mini') for d in data)
    assert any(d['task_id'].startswith('polyglot_lite') for d in data)
    for entry in data:
        assert 'time_ms' in entry and isinstance(entry['time_ms'], int)
        assert 'pass' in entry",tests/test_benchmarks.py,
survived,"def test_str_replace_basic(tmp_path: Path) -> None:
    p = tmp_path / ""f.txt""
    p.write_text(""foo bar foo"")
    n = str_replace(p, ""foo"", ""baz"")
    assert n == 2
    assert p.read_text() == ""baz bar baz""
",tests/test_file_ops.py,
survived,"def test_replace_regex(temp_file: Path) -> None:
    temp_file.write_text(""foo bar foo\n"")
    n = replace(temp_file, r""foo"", ""baz"")
    assert n == 2
    assert temp_file.read_text() == ""baz bar baz\n""
",tests/test_self_edit_tools.py,
survived,"def _edit_tool(ctx: RunContextWrapper | dict, path: str, start: int, end: Optional[int], new_code: str) -> str:
    edit(path, start, end, new_code)
    return ""ok""
",src/self_edit/tools.py,
survived,"    def _get_model(self, blank: bool = False) -> LogisticRegression:
        global _model
        if _model is not None and not blank:
            return _model
        model_path = os.path.join(self.MODEL_DIR, 'model.pkl')
        if not blank and os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                _model = pickle.load(f)
        else:
            _model = LogisticRegression(max_iter=1000)
        return _model
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"        async def send_transaction(self, tx: Any, *args: Any) -> None:
            calls.append((""sent"", tx.instructions[0].data.decode()))
",tests/test_ledger.py,DummyClient
survived,"def env_setup(monkeypatch):
    env = {
        ""COLPALI_TOKEN"": ""test-token"",
        ""VLLM_URL"": ""http://localhost"",
        ""COLPALI_BASE_URL"": ""http://localhost"",
        ""VLLM_API_KEY"": ""dummy"",
    }
    for k, v in env.items():
        monkeypatch.setenv(k, v)
",no-ocr-api/tests/test_utils.py,
survived,"                def raise_for_status(self):
                    called['raised'] = True
",alpha_factory_v1/tests/test_scripts_import_dashboard.py,ImportDashboardScriptTest.Resp
survived,"    def test_path_must_exist(self):
        with self.assertRaises(SystemExit):
            with mock.patch.object(sys, 'argv', ['run_tests.py', '/nope']):
                run_tests.main()
",alpha_factory_v1/tests/test_scripts_run_tests.py,RunTestsScriptTest
survived,"def run_tests(target: Path) -> int:
    """"""Execute tests under ``target``.

    ``pytest`` is preferred when available; otherwise ``unittest`` is used.
    The exit status of the invoked command is returned.
    """"""
    if importlib.util.find_spec(""pytest""):
        cmd = [sys.executable, ""-m"", ""pytest"", str(target)]
    else:
        cmd = [sys.executable, ""-m"", ""unittest"", ""discover"", str(target)]
    return subprocess.call(cmd)
",alpha_factory_v1/scripts/run_tests.py,
survived,"    def purge_old(self) -> None:
        """"""Remove records older than ``retention_days``.""""""
        if self.retention_days <= 0:
            return
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        cur = self.conn.cursor()
        cur.execute(""DELETE FROM telemetry WHERE timestamp < ?"", (cutoff.isoformat(),))
        self.conn.commit()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def __init__(
        self,
        name: str | None = None,
        instructions: str | None = None,
        tools: list | None = None,
    ) -> None:
        self.name = name or """"
        self.instructions = instructions or """"
        self.tools = tools or []
",src/agents/__init__.py,Agent
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def _render_agent_default(ctx: Dict[str, Any]) -> str:
    tools = ctx.get(""tools"") or []
    guardrails = ctx.get(""guardrails"") or []
    lines: List[str] = []
    lines.append('""""""')
    lines.append(""Auto-generated agent implementation"")
    lines.append('""""""')
    lines.append(""from agents import Agent"")
    lines.append("""")
    cls = ctx.get(""agent_class_name"", ""AgentImpl"")
    lines.append(f""class {cls}(Agent):"")
    lines.append(""    def __init__(self):"")
    lines.append(""        super().__init__("")
    lines.append(f""            name=\""{ctx.get('name')}\"","")
    lines.append(f""            instructions=\""\""\""{ctx.get('instructions')}\""\""\"","")
    lines.append(""        )"")
    lines.append("""")
    lines.append(""    def run(self, input):"")
    core_logic = ctx.get(""core_logic"", """")
    for line in str(core_logic).splitlines():
        lines.append(f""        {line}"")
    lines.append("""")
    lines.append(""    # Tools"")
    for tool in tools:
        for tl in str(tool).splitlines():
            lines.append(f""    {tl}"")
    lines.append("""")
    lines.append(""    # Guardrails"")
    for g in guardrails:
        for gl in str(g).splitlines():
            lines.append(f""    {gl}"")
    return ""\n"".join(lines)
",src/jinja2/__init__.py,
survived,"    def get_source(self, _environment: Any, template: str) -> str:
        path = os.path.join(self.searchpath, template)
        with open(path, ""r"", encoding=""utf-8"") as f:
            return f.read()
",src/jinja2/__init__.py,FileSystemLoader
survived,"def safe_load(stream: Any) -> Any:
    try:
        if hasattr(stream, ""read""):
            data = stream.read()
        else:
            data = str(stream)
        return json.loads(data)
    except Exception as e:
        raise YAMLError(str(e))
",src/yaml/__init__.py,
survived,"        async def wrapper(*args: Any, **kwargs: Any):
            tries = 0
            while True:
                try:
                    return await func(*args, **kwargs)
                except exceptions:
                    tries += 1
                    if tries >= max_tries:
                        raise
",src/backoff/__init__.py,
survived,"    def __init__(
        self, text: str, name: str = """", globals: Dict[str, Any] | None = None
    ) -> None:
        self.text = text
        self.name = name
        self.globals = globals or {}
",src/jinja2/__init__.py,Template
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def test_cli_dashboard_no_data(runner, tmp_path):
    db_path = tmp_path / ""tele.db""
    TelemetryDB(db_path).close()
    result = runner.invoke(cli, [""dashboard"", ""--db-path"", str(db_path)])
    assert result.exit_code == 0
    assert ""No telemetry data found."" in result.output
",tests/test_cli.py,
survived,"    def export(
        self,
        path: str | Path,
        *,
        fmt: str = ""json"",
        start: datetime | str | None = None,
        end: datetime | str | None = None,
        metrics: Iterable[str] | None = None,
        compress: bool | None = None,
    ) -> str:
        """"""Export telemetry data in ``fmt`` ('json' or 'csv').""""""
        fmt = fmt.lower()
        if fmt == ""json"":
            return self.export_json(
                path,
                start=start,
                end=end,
                metrics=metrics,
                compress=compress,
            )
        if fmt == ""csv"":
            return self.export_csv(
                path,
                start=start,
                end=end,
                metrics=metrics,
                compress=compress,
            )
        if fmt == ""pdf"":
            raise NotImplementedError(""PDF export not implemented"")
        raise ValueError(f""Unknown format: {fmt}"")
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def test_cli_generate_custom_metrics(runner, sample_json_file):
    result = runner.invoke(
        cli,
        [""generate"", ""--spec-file"", str(sample_json_file), ""--metric"", ""latency""],
    )
    assert result.exit_code == 0
    assert ""Telemetry:"" in result.output
    assert ""latency="" in result.output
    assert ""cost="" not in result.output
",tests/test_cli.py,
survived,"def make_client() -> TestClient:
    return TestClient(cast(Any, api.app))
",tests/test_metrics_exposure.py,
survived,"def test_cycle_detection():
    agent_a = Agent(name=""A"")
    agent_b = Agent(name=""B"")
    agent_a.handoffs.append(agent_b)
    agent_b.handoffs.append(agent_a)

    nodes = get_all_nodes(agent_a)
    edges = get_all_edges(agent_a)

    assert nodes.count('""A"" [label=""A""') == 1
    assert nodes.count('""B"" [label=""B""') == 1
    assert '""A"" -> ""B""' in edges
    assert '""B"" -> ""A""' in edges",tests/test_visualization.py,
survived,"        async def _run() -> None:
            await start_background_tasks()
            # Pre-set error count to threshold -1
            object.__setattr__(AGENT_REGISTRY[""fail""], ""err_count"", _ERR_THRESHOLD - 1)
            _HEALTH_Q.put((""fail"", 0.0, False))
            await asyncio.sleep(0.05)
            self.assertIs(AGENT_REGISTRY[""fail""].cls, StubAgent)
            await stop_background_tasks()
",tests/test_agents_registry.py,TestHealthQuarantine
survived,"def test_show_memory_missing(tmp_path) -> None:
    with patch.object(cli.config.CFG, ""memory_path"", str(tmp_path / ""mem.log"")):
        res = CliRunner().invoke(cli.main, [""show-memory""])
        assert ""No memory"" in res.output
",tests/test_cli.py,
survived,"    def load_env(seed: int):
        cfg = tmp_path / ""config.yaml""
        cfg.write_text(f""general:\n  seed: {seed}\n"")
        monkeypatch.chdir(tmp_path)
        monkeypatch.setenv(""NO_LLM"", ""1"")
        monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
        monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
        if module in sys.modules:
            del sys.modules[module]
        mod = importlib.import_module(module)
        env = mod.Orchestrator().envs[0]
        return env.size, sorted(env.obstacles)
",tests/test_world_model_config.py,
survived,"def inc(c):
    c[""n""] = c.n + 1
",tests/transpiler/x/py/record_assign.py,
survived,"def sum_rec(n, acc):
    if (n == 0):
        return acc
    return sum_rec((n - 1), (acc + n))
",tests/transpiler/x/py/tail_recursion.py,
survived,"def add(a, b):
    return a + b
",tests/transpiler/x/py/fun_call.py,
survived,"def test_delta_sector_to_dcf_npv() -> None:
    sector_state = {
        ""delta_revenue"": 1_000_000.0,
        ""margin"": 0.2,
        ""discount_rate"": 0.1,
        ""years"": 3,
    }
    result = delta_sector_to_dcf(sector_state)
    expected_npv = 497370.3981968444
    assert result[""npv""] == pytest.approx(expected_npv, rel=0.02)
",tests/test_finance_adapter.py,
survived,"def self_improver_cmd(repo_url: str, patch_file: str, metric_file: str, log_file: str) -> None:
    """"""Clone repo, apply patch, evaluate score delta and log it.""""""

    delta, _ = self_improver.improve_repo(repo_url, patch_file, metric_file, log_file)
    click.echo(f""score delta: {delta}"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def test_tree_has_single_path_false(self):
        """"""Tree branching results in ``False``.""""""
        tree = FPTree([[1, 2], [1, 3]], 1, None, None)
        self.assertFalse(tree.tree_has_single_path(tree.root))
",tests/test_pyfpgrowth.py,FPTreeTests
survived,"    def test_tree_has_single_path_true(self):
        """"""Tree with a single path is detected correctly.""""""
        tree = FPTree([[1], [1], [1]], 1, None, None)
        self.assertTrue(tree.tree_has_single_path(tree.root))
",tests/test_pyfpgrowth.py,FPTreeTests
survived,"    def norm(self) -> float:
        """"""Return the quaternion magnitude.""""""
        return float(np.sqrt(
            self.w * self.w + self.x * self.x + self.y * self.y + self.z * self.z,
            dtype=DTYPE,
        ))
",MPC_Controller/math_utils/orientation_tools.py,Quaternion
survived,"        def __init__(self):
            self.image_size_cache = {}
            self.class_embeddings_cache = {}
            self.image_embed_cache = {}
            self.cpu_image_embed_cache = {}
            self.before_unload_image_none = False
            self.after_unload = False
",tests/inference/models_predictions_tests/test_owlv2.py,DummyOwl
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_index.py,
survived,"    def needs_rebuild(self) -> bool:
        """"""Return True if stored checksums differ from source files.""""""
        if not self.index_path.exists():
            return True
        if not self._index:
            self.load()
        for item in self._index:
            template_path = self.registry.templates_dir / item[""path""]
            try:
                content = template_path.read_text(encoding=""utf-8"")
            except OSError:  # file removed
                return True
            checksum = sha256(content.encode(""utf-8"")).hexdigest()
            if checksum != item.get(""checksum""):
                return True
        # check for new templates not in index
        seen = {(i[""slug""], i[""version""]) for i in self._index}
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                if (slug, version_info[""version""]) not in seen:
                    return True
        return False
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self.index_path = self.registry.templates_dir / self.INDEX_FILE_NAME
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def load(self) -> None:
        if self.index_path.exists():
            try:
                with open(self.index_path, ""r"", encoding=""utf-8"") as f:
                    self._index = json.load(f)
            except (OSError, json.JSONDecodeError):  # pragma: no cover - corrupt file
                self._index = []
        else:
            self._index = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"def test_blocks_paywalled_excerpt(tmp_path):
    text = (""paywalled "" * 65).strip()
    f = tmp_path / ""secret.txt""
    f.write_text(text)

    assert dp_scrubber.scan_file(Path(f)) is True",tests/test_dp_scrubber.py,
survived,"def _fetch_spot_price(region: str = ""us-east-1"") -> float:
    """"""Return the current A10 spot price per hour in ``region``.""""""
    if boto3 is None:  # pragma: no cover - missing deps
        raise RuntimeError(""boto3 not available"")
    ec2 = boto3.client(""ec2"", region_name=region)
    history = ec2.describe_spot_price_history(
        InstanceTypes=[""g5.2xlarge""],
        ProductDescriptions=[""Linux/UNIX""],
        MaxResults=1,
    )
    price = float(history[""SpotPriceHistory""][0][""SpotPrice""])
    return price
",src/scheduler/spot_gpu.py,
survived,"    def allocate(
        self,
        top_children: Sequence[str],
        other_children: Sequence[str],
        *,
        dry_run: bool = False,
    ) -> Mapping[str, int]:
        """"""Return GPU allocation for ``top_children`` and ``other_children``.""""""
        price = self.price_fetcher(self.region)
        hourly_budget = self.budget_per_day / 24
        result: dict[str, int] = {}
        spent = 0.0
        for child in top_children:
            cost = 8 * price
            if spent + cost <= hourly_budget:
                result[child] = 8
                spent += cost
                if dry_run:
                    _log.info(
                        ""Allocate 8√óA10 to %s: cost %.2f/h (remaining %.2f/h)"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
            else:
                if dry_run:
                    _log.info(
                        ""Skip %s: need %.2f/h, remaining %.2f/h"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
        for child in other_children:
            cost = price
            if spent + cost <= hourly_budget:
                result[child] = 1
                spent += cost
                if dry_run:
                    _log.info(
                        ""Allocate 1√óA10 to %s: cost %.2f/h (remaining %.2f/h)"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
            else:
                if dry_run:
                    _log.info(
                        ""Skip %s: need %.2f/h, remaining %.2f/h"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
        if dry_run:
            _log.info(""Total hourly cost: %.2f of %.2f"", spent, hourly_budget)
        return result
",src/scheduler/spot_gpu.py,SpotGPUAllocator
survived,"def test_dry_run_respects_budget(caplog: pytest.LogCaptureFixture) -> None:
    alloc = SpotGPUAllocator(price_fetcher=lambda r: 0.5)
    caplog.set_level(logging.INFO)
    result = alloc.allocate([""a"", ""b""], [""c""], dry_run=True)
    assert result == {""a"": 8, ""b"": 8}
    msgs = [r.getMessage() for r in caplog.records]
    assert any(""Total hourly cost"" in m for m in msgs)
    end_msg = [m for m in msgs if ""Total hourly cost"" in m][0]
    assert ""8.00"" in end_msg and ""8.33"" in end_msg
    assert any(""Skip c"" in m for m in msgs)",tests/test_spot_gpu.py,
survived,"    async def broadcast_merkle_root(self) -> None:
        try:
            root = self.compute_merkle_root()
        except Exception as exc:  # pragma: no cover - corruption
            _log.warning(""Failed to compute Merkle root: %s"", exc)
            return
        if AsyncClient is None or not self.broadcast:
            _log.info(""Merkle root %s"", root)
            return
        try:
            client = AsyncClient(self.rpc_url or ""https://api.testnet.solana.com"")
            memo_prog = PublicKey(""MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr"")
            tx = Transaction().add(TransactionInstruction(program_id=memo_prog, data=root.encode(), keys=[]))
            signer = None
            if self.wallet:
                try:  # pragma: no cover - optional dependency
                    from solana.keypair import Keypair

                    signer = Keypair.from_secret_key(bytes.fromhex(self.wallet))
                except Exception as exc:  # noqa: BLE001 - invalid key
                    _log.warning(""Invalid wallet key: %s"", exc)
            if signer:
                await client.send_transaction(tx, signer)
            else:
                await client.send_transaction(tx)
            _log.info(""Broadcasted Merkle root %s"", root)
        except Exception as exc:  # pragma: no cover - network errors
            _log.warning(""Failed to broadcast Merkle root: %s"", exc)
        finally:
            try:
                await client.close()
            except Exception:  # pragma: no cover - ignore close errors
                pass
",src/archive/service.py,ArchiveService
survived,"    def start_merkle_task(self, interval: int = 86_400) -> None:
        if self._task is None:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:  # pragma: no cover - no loop in sync context
                _log.warning(""Merkle task requires a running event loop"")
                return
            self._task = loop.create_task(self._loop(interval))
",src/archive/service.py,ArchiveService
survived,"        def __init__(self) -> None:
            self.instructions = []
",tests/test_archive.py,DummyTx
survived,"def _offline() -> bool:
    return not os.getenv(""OPENAI_API_KEY"") or os.getenv(""AGI_INSIGHT_OFFLINE"") == ""1""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"    def create_app(self) -> ""FastAPI"":
        if FastAPI is None:
            raise RuntimeError(""FastAPI not installed"")

        app = FastAPI(title=""Dual Critic Service"")

        @app.post(""/critique"")
        async def _critique(req: CritiqueRequest = Body(...)) -> Any:  # noqa: D401
            result = self.score(req.context, req.response)
            return JSONResponse(result)

        CritiqueRequest.model_rebuild()

        return app
",src/critics/dual_critic_service.py,DualCriticService
survived,"def detect_anomalies(rows: Iterable[Dict[str, float]], *, z: float = 2.0) -> List[Dict[str, float]]:
    """"""Return rows with any metric deviating more than ``z`` standard deviations.""""""
    stats = aggregate_stats(rows)
    anomalies = []
    for row in rows:
        for m in _METRICS:
            mean = stats.get(f""{m}_mean"", 0.0)
            st = stats.get(f""{m}_stdev"", 0.0)
            if st and abs(row[m] - mean) > z * st:
                anomalies.append(row)
                break
    return anomalies
",src/analysis/meta_foresight.py,
survived,"    def query(self, limit: int = 100):
        """"""Alias of :meth:`read` for backward compatibility.""""""
        return self.read(limit)
",alpha_factory_v1/backend/memory.py,Memory
survived,"def post(
    url: str,
    *,
    json: dict | None = None,
    data: dict | bytes | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    """"""Perform a minimal HTTP POST request.""""""
    body = b""""
    req_headers = headers or {}
    if json is not None:
        body = json_d = json
        body = json.dumps(json_d).encode()
        req_headers.setdefault(""Content-Type"", ""application/json"")
    elif data is not None:
        if isinstance(data, (bytes, bytearray)):
            body = data
        else:
            body = _parse.urlencode(data).encode()
            req_headers.setdefault(
                ""Content-Type"", ""application/x-www-form-urlencoded""
            )

    req = _request.Request(url, data=body, headers=req_headers, method=""POST"")
    with _request.urlopen(req, timeout=timeout) as resp:
        text = resp.read().decode()
        return Response(resp.getcode(), text)
",alpha_factory_v1/requests.py,
survived,"    def test_post_json(self):
        self.server, self.thread, H, url = start_server()
        payload = {""x"": 1}
        resp = requests.post(url, json=payload)
        self.assertEqual(resp.json(), payload)
        self.assertEqual(H.received_body, json.dumps(payload).encode())
        self.assertEqual(H.received_headers.get(""Content-Type""), ""application/json"")
",alpha_factory_v1/tests/test_requests_shim.py,RequestsShimTest
survived,"    def test_raise_for_status(self):
        self.server, self.thread, H, url = start_server(status=404)
        resp = requests.get(url)
        with self.assertRaises(RuntimeError):
            resp.raise_for_status()
",alpha_factory_v1/tests/test_requests_shim.py,RequestsShimTest
survived,"    def setUp(self):
        self._orig_embed = mv._embed
        mv._embed = lambda texts: np.array(
            [[len(t), sum(t.encode())] for t in texts], dtype=""float32""
        )
",alpha_factory_v1/tests/test_vector_memory.py,VectorMemoryTest
survived,"    def setUp(self):
        AGENT_REGISTRY.clear()
",alpha_factory_v1/tests/test_register_decorator.py,RegisterDecoratorTest
survived,"    def flush(self) -> None:
        """"""Erase all stored events.""""""
        self.file.write_text("""")
",alpha_factory_v1/backend/memory.py,Memory
survived,"        def add_edge(self, u: str, v: str, **attrs: Any) -> None:
            self.edges[(u, v)] = attrs
",alpha_factory_v1/backend/agents/supply_chain_agent.py,_FakeGraph
survived,"def _parse_with(args):
    old = sys.argv
    sys.argv = ['run.py'] + args
    try:
        return af_run.parse_args()
    finally:
        sys.argv = old
",alpha_factory_v1/tests/test_cli.py,
survived,"    def __init__(self, settings: config.Settings | None = None) -> None:
        self.settings = settings or config.CFG
        insight_logging.setup(json_logs=self.settings.json_logs)
        bus = messaging.A2ABus(self.settings)
        ledger = Ledger(
            self.settings.ledger_path,
            rpc_url=self.settings.solana_rpc_url,
            wallet=self.settings.solana_wallet,
            broadcast=self.settings.broadcast,
            db=self.settings.db_type,
        )
        archive = ArchiveService(
            os.getenv(""ARCHIVE_PATH"", ""archive.db""),
            rpc_url=self.settings.solana_rpc_url,
            wallet=self.settings.solana_wallet,
            broadcast=self.settings.broadcast,
        )
        solution_archive = SolutionArchive(os.getenv(""SOLUTION_ARCHIVE_PATH"", ""solutions.duckdb""))
        registry = StakeRegistry()
        if resource is not None:
            try:
                limit = 8 * 1024 * 1024 * 1024
                resource.setrlimit(resource.RLIMIT_AS, (limit, limit))
            except Exception:  # pragma: no cover - unsupported platform
                pass
        super().__init__(
            bus,
            ledger,
            archive,
            solution_archive,
            registry,
            self.settings.island_backends,
            err_threshold=ERR_THRESHOLD,
            backoff_exp_after=BACKOFF_EXP_AFTER,
            promotion_threshold=PROMOTION_THRESHOLD,
        )
        for agent in self._init_agents():
            self.add_agent(agent)
",alpha_factory_v1/core/orchestrator.py,Orchestrator
survived,"def _arima_baseline(history: Sequence[float], months: int) -> list[float]:
    """"""Return a simple AR(1) baseline forecast.""""""
    if not history:
        return [0.0] * months
    if len(history) < 2:
        return [history[-1]] * months
    y = history[1:]
    x = history[:-1]
    denom = sum(v * v for v in x) or 1e-12
    phi = sum(xi * yi for xi, yi in zip(x, y)) / denom
    pred = history[-1]
    out = []
    for _ in range(months):
        pred = phi * pred
        out.append(pred)
    return out
",alpha_factory_v1/core/evaluators/lead_time.py,
survived,"    def _record_restart(self, runner: AgentRunner) -> None:
        super()._record_restart(runner)
        alerts.send_alert(
            f""{runner.agent.name} restarted"",
            self.settings.alert_webhook_url,
        )
",alpha_factory_v1/core/orchestrator.py,Orchestrator
survived,"    def slash(self, agent_id: str) -> None:
        self.registry.burn(agent_id, 0.1)
",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator
survived,"    async def run_forever(self) -> None:
        await self.bus.start()
        self.ledger.start_merkle_task(3600)
        self.archive.start_merkle_task(86_400)
        for r in self.runners.values():
            proposal = f""promote:{r.agent.name}""
            if self.registry.accepted(proposal):
                r.start(self.bus, self.ledger)
        self._monitor_task = asyncio.create_task(
            monitor_agents(
                self.runners,
                self.bus,
                self.ledger,
                err_threshold=self._err_threshold,
                backoff_exp_after=self._backoff_exp_after,
                on_restart=self._record_restart,
            )
        )
        try:
            while True:
                await asyncio.sleep(0.5)
        finally:
            if self._monitor_task:
                self._monitor_task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await self._monitor_task
            for r in self.runners.values():
                if r.task:
                    r.task.cancel()
                    with contextlib.suppress(asyncio.CancelledError):
                        await r.task
            await self.bus.stop()
            await self.ledger.stop_merkle_task()
            await self.archive.stop_merkle_task()
            self.ledger.close()
            self.archive.close()
            self.solution_archive.close()",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator
survived,"def _reset() -> None:
    sc._seen_request_ids.clear()
",tests/test_safety_compliance_reward.py,
survived,"def test_typical_payload_returns_float() -> None:
    payload = {""latency_ms"": 400, ""tokens"": 500, ""cost_usd"": 0.002, ""energy_j"": 20, ""value"": 0.8}
    value = er.reward(None, None, payload)
    assert isinstance(value, float)
    assert 0.0 <= value <= 1.0
",tests/test_efficiency_reward.py,
survived,"def test_repeated_solution_zero() -> None:
    _reset()
    ns.reward(None, None, ""idea"")
    value = ns.reward(None, None, ""idea"")
    assert 0.0 <= value <= 1.0
    assert value == 0.0",tests/test_novel_solution_reward.py,
survived,"    def start_merkle_task(self, *a, **kw) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_property.py,DummyLedger
survived,"    def subscribe(self, topic: str, handler) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_property.py,DummyBus
survived,"def test_clone_sample_repo_fallback(tmp_path, monkeypatch):
    target = tmp_path / ""repo""
    monkeypatch.setattr(entrypoint, ""CLONE_DIR"", str(target))
    monkeypatch.setattr(subprocess, ""run"", lambda *a, **k: SimpleNamespace(returncode=1))
    entrypoint.clone_sample_repo()
    assert (target / ""calc.py"").exists()
",tests/test_self_heal_clone.py,
survived,"def test_add():
    assert calc.add(1, 1) == 2",alpha_factory_v1/demos/self_healing_repo/sample_broken_calc/test_calc.py,
survived,"    def archive_accept(self, agent_id: str) -> None:
        """"""Burn 1% of ``agent_id`` stake when a candidate is accepted.""""""
        self.burn(agent_id, 0.01)
",src/governance/stake_registry.py,StakeRegistry
survived,"def test_single_file_format():
    source_files = {""Contract"": {""content"": ""contract C {}""}}
    assert not is_standard_json_contract(source_files)
",tests/test_utils.py,
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/supply_chain_agent.py,SupplyChainAgent
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/finance_agent.py,FinanceAgent
survived,"    async def emit(self, recipient: str, payload: Any) -> None:
        env = messaging.Envelope(self.name, recipient, payload, time.time())
        self.ledger.log(env)
        self.bus.publish(recipient, env)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent
survived,"    async def _stop() -> None:
        task = getattr(app_f.state, ""task"", None)
        if task:
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
            app_f.state.task = None
        app_f.state.orchestrator = None
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def _stop() -> None:
        if hasattr(app.state, ""task""):
            app.state.task.cancel()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"async def main(argv: list[str] | None = None) -> None:
    args = _build_parser().parse_args(argv)
    orch = orchestrator.Orchestrator()
    secs = [sector.Sector(""s%02d"" % i) for i in range(3)]
    sim = forecast.simulate_years(secs, args.horizon)
    for pt in sim:
        print(pt)
    await orch.run_forever()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def __init__(self, bus, ledger) -> None:
        super().__init__(""planning"", bus, ledger)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/planning_agent.py,PlanningAgent
survived,"    async def _on_envelope(self, env: messaging.Envelope) -> None:
        await self.handle(env)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent
survived,"            def _decorator(func):
                return func
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime
survived,"    def __init__(self, client_id: str, **kwargs):
        super().__init__(**kwargs)
        self.id = client_id
",app/services/client.py,GeminiClientWrapper
survived,"        def replacer(match: re.Match) -> str:
            outer_open_paren = match.group(1)
            display_text = match.group(2)

            new_target_url = simplify_link_target(display_text)
            new_link_segment = f""[`{display_text}`]({new_target_url})""

            if outer_open_paren:
                return f""{outer_open_paren}{new_link_segment})""
            else:
                return new_link_segment
",app/services/client.py,GeminiClientWrapper
survived,"    def get_player_location_on_minimap(self):
        """"""
        Get the player's location on the minimap by detecting a unique 4-pixel color.
        Return the player's location in minimap coordinates.
        """"""
        # Crop the minimap from the game screen
        x0, y0 = self.loc_minimap
        h, w, _ = self.img_route.shape
        img_minimap = self.img_frame[y0:y0 + h//4, x0:x0 + w//4]

        # Find pixels matching the player color
        mask = cv2.inRange(img_minimap,
                           self.cfg.minimap_player_color,
                           self.cfg.minimap_player_color)
        coords = cv2.findNonZero(mask)
        if coords is None or len(coords) < 4:
            logger.warning(""Fail to locate player location on minimap"")
            return None

        # Calculate the average location of the matching pixels
        avg = coords.mean(axis=0)[0]  # shape (1,2), so we take [0]
        loc_player_minimap = (int(round(avg[0] * self.cfg.minimap_upscale_factor)),
                              int(round(avg[1] * self.cfg.minimap_upscale_factor)))
        # Draw red circle to mark player's location on minimap
        cv2.circle(self.img_route_debug, loc_player_minimap,
                   radius=4, color=(0, 255, 255), thickness=2)

        return loc_player_minimap
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def limit_fps(self):
        '''
        Limit FPS
        '''
        # If the loop finished early, sleep to maintain target FPS
        target_duration = 1.0 / self.fps_limit  # seconds per frame
        frame_duration = time.time() - self.t_last_run
        if frame_duration < target_duration:
            time.sleep(target_duration - frame_duration)

        # Update FPS
        self.fps = round(1.0 / (time.time() - self.t_last_run))
        self.t_last_run = time.time()
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"def main(demo: str, *, print_only: bool = False) -> None:
    print(DISCLAIMER, file=sys.stderr)
    url = _demo_url(demo)
    if _remote_available(url):
        if print_only:
            print(url)
            return
        print(f""Opening {url}"")
        webbrowser.open(url)
        return

    repo_root = Path(__file__).resolve().parents[1]
    site_dir = repo_root / ""site"" / ""alpha_factory_v1"" / ""demos"" / demo
    local_page = site_dir / ""index.html""
    if not local_page.is_file():
        print(""Remote page unavailable. Building local copy..."", file=sys.stderr)
        if not _build_local_site(repo_root) or not local_page.is_file():
            print(
                f""Demo {demo} not found. Build the gallery with ./scripts/build_gallery_site.sh"",
                file=sys.stderr,
            )
            sys.exit(1)

    handler = partial(SimpleHTTPRequestHandler, directory=str(site_dir))
    with ThreadingHTTPServer((""127.0.0.1"", 0), handler) as httpd:
        port = httpd.server_address[1]
        local_url = f""http://127.0.0.1:{port}/index.html""
        print(f""Serving local copy at {local_url}"", file=sys.stderr)
        thread = threading.Thread(target=httpd.serve_forever, daemon=True)
        thread.start()
        try:
            if print_only:
                print(local_url)
            else:
                webbrowser.open(local_url)
            thread.join()
        except KeyboardInterrupt:
            pass
",scripts/open_subdir_demo.py,
survived,"        def __init__(self, *a, **k) -> None:
            last_runtime[""inst""] = self
            self.registered: list[object] = []
",tests/test_aiga_openai_bridge_offline.py,AgentRuntime
survived,"def test_aiga_openai_bridge_offline(monkeypatch: pytest.MonkeyPatch) -> None:
    port = _free_port()
    server = HTTPServer((""127.0.0.1"", port), _Handler)
    thread = threading.Thread(target=server.serve_forever)
    thread.start()

    stub = types.ModuleType(""openai_agents"")

    last_runtime: dict[str, object] = {}

    class Agent:
        pass

    class AgentRuntime:
        def __init__(self, *a, **k) -> None:
            last_runtime[""inst""] = self
            self.registered: list[object] = []

        def register(self, agent: object) -> None:
            self.registered.append(agent)

        def run(self) -> None:
            pass

    class OpenAIAgent:
        def __init__(self, *a, base_url: str | None = None, **_k) -> None:
            self.base_url = base_url.rstrip(""/"") if base_url else None

        def __call__(self, prompt: str) -> str:
            if not self.base_url:
                return ""no base url""
            r = requests.post(
                f""{self.base_url}/chat/completions"",
                json={""model"": ""stub"", ""messages"": [{""role"": ""user"", ""content"": prompt}]},
                timeout=5,
            )
            r.raise_for_status()
            return r.json()[""choices""][0][""message""][""content""]

    def Tool(*_a, **_k):
        def dec(f):
            return f
        return dec

    stub.Agent = Agent
    stub.AgentRuntime = AgentRuntime
    stub.OpenAIAgent = OpenAIAgent
    stub.Tool = Tool
    stub.last_runtime = last_runtime

    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    sys.modules.pop(""agents"", None)

    import alpha_factory_v1.backend  # noqa: F401 - trigger shim
    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)

    env_stub = types.ModuleType(""curriculum_env"")
    class DummyEnv:
        pass
    env_stub.CurriculumEnv = DummyEnv
    monkeypatch.setitem(
        sys.modules,
        ""alpha_factory_v1.demos.aiga_meta_evolution.curriculum_env"",
        env_stub,
    )

    evo_stub = types.ModuleType(""meta_evolver"")
    class _DummyEvolver:
        def __init__(self, *a, **k):
            pass

        def run_generations(self, *_a):
            pass

        def latest_log(self):
            return ""stub""

        best_architecture = ""arch""
        best_fitness = 1.0

    evo_stub.MetaEvolver = _DummyEvolver
    monkeypatch.setitem(
        sys.modules,
        ""alpha_factory_v1.demos.aiga_meta_evolution.meta_evolver"",
        evo_stub,
    )

    monkeypatch.setenv(""OPENAI_API_KEY"", """")
    monkeypatch.setenv(""OLLAMA_BASE_URL"", f""http://127.0.0.1:{port}/v1"")

    mod = importlib.import_module(
        ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge""
    )

    class DummyEvolver:
        def __init__(self, *a, llm=None, **_k) -> None:
            self.llm = llm

        def run_generations(self, *_a) -> None:
            pass

        def latest_log(self) -> str:
            return self.llm(""hi"") if self.llm else ""done""

        best_architecture = ""arch""
        best_fitness = 1.0

    mod.EVOLVER = DummyEvolver(llm=mod.LLM)

    try:
        mod.main()
        runtime = stub.last_runtime[""inst""]
        assert any(isinstance(a, mod.EvolverAgent) for a in runtime.registered)
        result = asyncio.run(mod.evolve(1))
        assert result == ""ok""
    finally:
        server.shutdown()
        thread.join()",tests/test_aiga_openai_bridge_offline.py,
survived,"    async def run() -> None:
        task = asyncio.create_task(runner.loop(bus, ledger))
        await asyncio.sleep(0.05)
        task.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await task
",tests/test_alert_webhook.py,
survived,"def test_patcher_cli_offline(tmp_path, monkeypatch):
    repo = tmp_path / ""repo""
    repo.mkdir()
    (repo / ""test_ok.py"").write_text(""def test_ok():\n    assert True\n"", encoding=""utf-8"")

    # stub openai to satisfy import in llm_client
    monkeypatch.setitem(sys.modules, ""openai"", types.ModuleType(""openai""))

    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)
    monkeypatch.setattr(sys, ""argv"", [""patcher_core.py"", ""--repo"", str(repo)])

    runpy.run_module(
        ""alpha_factory_v1.demos.self_healing_repo.patcher_core"", run_name=""__main__""
    )",tests/test_patcher_core_cli_offline.py,
survived,"    def test_delta_must_be_within_range(self) -> None:
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=-0.1, stake=1.0)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=1.1, stake=1.0)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"    def test_rounds_must_be_positive(self) -> None:
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=0, delta=0.5, stake=1.0)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=-5, delta=0.5, stake=1.0)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"        def __init__(self, *args: object, **kwargs: object) -> None:
            pass
",tests/test_gpt2_cli_demo.py,FakeTokenizer
survived,"def convert(src: Path, dest: Path | None = None) -> None:
    dest = dest or src
    try:
        from transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch import (
            convert_gpt2_checkpoint_to_pytorch,
        )
    except Exception as exc:  # pragma: no cover
        raise SystemExit(f""transformers with PyTorch is required: {exc}"")

    ckpt = src / ""model.ckpt""
    config = src / ""hparams.json""
    convert_gpt2_checkpoint_to_pytorch(str(ckpt), str(config), str(dest))
",scripts/convert_openai_gpt2.py,
survived,"    def test_short_readme_fails(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            d = os.path.join(tmp, ""demo_short"")
            os.mkdir(d)
            open(os.path.join(d, ""__init__.py""), ""w"").close()
            with open(os.path.join(d, ""README.md""), ""w"") as fh:
                fh.write(""x\n"")
            exit_code = validate_demos.main(tmp, min_lines=5)
            self.assertEqual(exit_code, 1)
",tests/test_demo_quality.py,TestValidateDemosFailures
survived,"def temp_path():
    path = REPO_ROOT / ""tmp_tools_undo.txt""
    try:
        yield path
    finally:
        if path.exists():
            path.unlink()
",tests/test_tools_undo.py,
survived,"def main() -> None:
    arch = HashArchive(""audit.db"")
    success = 0
    with tempfile.TemporaryDirectory() as tmp:
        for i in range(100):
            f = Path(tmp) / f""agent_{i}.tar""
            f.write_text(str(i), encoding=""utf-8"")
            cid = arch.add_tarball(f)
            if cid:
                success += 1
    root = arch.merkle_root()
    print(f""merkle_root={root}"")
    rate = success / 100
    print(f""pin_rate={rate:.2%}"")
    if root != EXPECTED_ROOT:
        raise SystemExit(""unexpected merkle root"")
    if rate < 0.99:
        raise SystemExit(""pin success below threshold"")
",scripts/audit_archive.py,
survived,"def archive() -> None:
    """"""Manage archived agent tarballs.""""""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def test_no_uncaught_error_on_setitem_failure() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        errors: list[str] = []
        page.on(""pageerror"", lambda err: errors.append(str(err)))
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""Object.defineProperty(localStorage,'setItem',{value:()=>{throw new Error('boom');},configurable:true});""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert not errors
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,
survived,"def toyota_checksum(address: int, sig, d: bytearray) -> int:
  s = len(d)
  addr = address
  while addr:
    s += addr & 0xFF
    addr >>= 8
  for i in range(len(d) - 1):
    s += d[i]
  return s & 0xFF",opendbc/car/toyota/toyotacan.py,
survived,"def honda_checksum(address: int, sig, d: bytearray) -> int:
  s = 0
  extended = address > 0x7FF
  addr = address
  while addr:
    s += addr & 0xF
    addr >>= 4
  for i in range(len(d)):
    x = d[i]
    if i == len(d) - 1:
      x >>= 4
    s += (x & 0xF) + (x >> 4)
  s = 8 - s
  if extended:
    s += 3
  return s & 0xF",opendbc/car/honda/hondacan.py,
survived,"def test_to_json_object_excludes_ref(client) -> None:
    class MyObj(weave.Object):
        @weave.op
        def predict(self, x: int) -> int:
            return x

    obj = MyObj()
    obj_rec = pydantic_object_record(obj)
    serialized = to_json(obj_rec, client._project_id(), client)
    assert ""ref"" not in serialized",tests/trace/test_serialize.py,
survived,"    def rebuild(self) -> None:
        """"""Rebuild the index from all registered templates.""""""
        self._index = []
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                version = version_info[""version""]
                path = self.registry.templates_dir / version_info[""path""]
                metadata_path = path.parent / METADATA_FILE_NAME
                try:
                    content = path.read_text(encoding=""utf-8"")
                except OSError:  # pragma: no cover - file missing
                    continue
                checksum = sha256(content.encode(""utf-8"")).hexdigest()
                try:
                    with open(metadata_path, ""r"", encoding=""utf-8"") as f:
                        metadata = json.load(f)
                except (OSError, json.JSONDecodeError):
                    metadata = {}
                self._index.append(
                    {
                        ""slug"": slug,
                        ""version"": version,
                        ""path"": str(path.relative_to(self.registry.templates_dir)),
                        ""checksum"": checksum,
                        ""metadata"": metadata,
                        ""content"": content,
                    }
                )
        self.save()
",src/meta_agent/template_index.py,TemplateIndex
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,
survived,"def _meta(slug: str) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug,
        description=""demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_index.py,
survived,"    def save(self) -> None:
        with open(self.index_path, ""w"", encoding=""utf-8"") as f:
            json.dump(self._index, f, indent=2)
",src/meta_agent/template_index.py,TemplateIndex
survived,"def test_execute_and_collect_success(monkeypatch, tmp_path):
    fake_exec = MagicMock()
    fake_exec.run_tests.return_value = ExecutionResult(0, ""out"", ""err"")
    module = ResultCollectionModule(fake_exec)
    result = module.execute_and_collect(tmp_path, timeout=5)
    assert isinstance(result, CollectionResult)
    assert result.exit_code == 0
    assert result.stdout == ""out""
    assert result.stderr == ""err""
    assert result.duration >= 0
    fake_exec.run_tests.assert_called_with(tmp_path, timeout=5)
",tests/unit/test_result_collection_module.py,
survived,"def test_execute_and_collect_propagates_error(monkeypatch, tmp_path):
    fake_exec = MagicMock()
    fake_exec.run_tests.side_effect = rc_mod.SandboxExecutionError(""boom"")
    module = ResultCollectionModule(fake_exec)
    with pytest.raises(rc_mod.SandboxExecutionError):
        module.execute_and_collect(tmp_path)",tests/unit/test_result_collection_module.py,
survived,"    def __init__(self, execution_module: Optional[ExecutionModule] = None) -> None:
        self.execution_module = execution_module or ExecutionModule()
",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule
survived,"def test_generate_html_report():
    module = ReportingModule()
    result = make_result()
    html_report = module.generate_report(result, output_format=""html"")
    assert html_report.startswith(""<html>"")
    assert ""PASSED"" in html_report
",tests/unit/test_reporting_module.py,
survived,"    def _validate_inputs(
        self,
        code_directory: Path,
        command: list[str],
        mem_limit: str,
        cpu_shares: int,
    ) -> None:
        """"""Validate inputs and resource limits for sandbox execution.""""""



        if not code_directory.is_dir():
            raise FileNotFoundError(f""Code directory not found: {code_directory}"")

        if not command or any(
            not isinstance(c, str) or any(x in c for x in ["";"", ""&"", ""|"", ""`"", ""\n""])
            for c in command
        ):
            raise ValueError(""Invalid command passed to sandbox"")

        if cpu_shares <= 0 or cpu_shares > MAX_CPU_SHARES:
            raise ValueError(""cpu_shares out of allowed range"")

        if mem_limit[-1].lower() not in {""m"", ""g""} or not mem_limit[:-1].isdigit():
            raise ValueError(""mem_limit must be like '256m' or '1g'"")
",src/meta_agent/sandbox/sandbox_manager.py,SandboxManager
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,
survived,"def test_invalid_resources(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python""], cpu_shares=-1)
",tests/unit/test_sandbox_manager.py,
survived,"def _oai_available() -> bool:
    for name in (""openai_agents"", ""agents""):
        try:
            spec = importlib.util.find_spec(name)
        except ValueError:
            spec = None
        if spec is None:
            continue
        mod = importlib.import_module(name)
        if Version(getattr(mod, ""__version__"", ""0"")) >= Version(""0.0.17""):
            return True
    return False
",tests/test_external_integrations.py,
survived,"def step(cells, ruleVal):
    newCells = """"
    i = 0
    while i < len(cells) - 2:
        bin = 0
        b = 2
        n = i
        while n < i + 3:
            bin = bin + btoi(cells[n:n + 1] == ""O"") * pow2(b)
            b = b - 1
            n = n + 1
        a = "".""
        if ((ruleVal / pow2(bin)) % 2 == 1):
            a = ""O""
        newCells = newCells + a
        i = i + 1
    return newCells
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/elliptic-curve-digital-signature-algorithm.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    dividend = 580
    divisor = 34
    res = egyptianDivide(dividend, divisor)
    print(str(dividend) + "" divided by "" + str(divisor) + "" is "" + str(res.q) + "" with remainder "" + str(res.r))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/egyptian-division.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    s = ""1223334444""
    counts = {}
    l = 0.0
    i = 0
    while i < len(s):
        ch = s[i:i + 1]
        if ch in counts:
            counts[ch] = counts[ch] + 1
        else:
            counts[ch] = 1
        l = l + 1.0
        i = i + 1
    hm = 0.0
    for ch in counts:
        c = float(counts[ch])
        hm = hm + c * log2(c)
    print(str(log2(l) - hm // l))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/entropy-2.py,
survived,"def outputState(state):
    line = """"
    i = 0
    while i < len(state):
        if state[i:i + 1] == ""1"":
            line = line + ""#""
        else:
            line = line + "" ""
        i = i + 1
    print(line)
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,
survived,"def exp(a, b):
    return powf(a, b)
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def _parse_args(argv: Optional[list[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Self‚ÄëHealing repo demo"")

    default_repo = Path(__file__).resolve().parent.parent.parent
    parser.add_argument(
        ""--repo"",
        type=Path,
        default=default_repo,
        help=f""Repository root (default: {default_repo})"",
    )
    parser.add_argument(""--max-turns"", type=int, default=6)
    parser.add_argument(
        ""--allow-local-code"",
        action=""store_true"",
        default=False,
        help=""Enable PythonTool local execution (DANGER)"",
    )
    return parser.parse_args(argv)
",alpha_factory_v1/demos/self_healing_repo_cli.py,
survived,"    async def get_results(sim_id: str, _: None = Depends(verify_token)) -> ResultsResponse:
        result = _simulations.get(sim_id)
        if result is None:
            raise HTTPException(status_code=404)
        return result
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"        async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -> Response:
            ip = request.client.host if request.client else ""unknown""
            now = time.time()
            async with self.lock:
                dq = self.counters.get(ip)
                if dq is None:
                    dq = deque()
                while dq and now - dq[0] > self.window:
                    dq.popleft()
                if len(dq) >= self.limit:
                    dq.append(now)
                    self.counters[ip] = dq
                    return Response(""Too Many Requests"", status_code=429)
                dq.append(now)
                self.counters[ip] = dq
            return await call_next(request)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,SimpleRateLimiter
survived,"    async def verify_token(
        credentials: HTTPAuthorizationCredentials = Depends(security),
    ) -> None:
        token = getattr(app_f.state, ""api_token"", API_TOKEN_DEFAULT)
        if credentials.credentials != token:
            raise HTTPException(status_code=403, detail=""Invalid token"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def _background_run(sim_id: str, cfg: SimRequest) -> None:
        secs = [sector.Sector(f""s{i:02d}"") for i in range(cfg.pop_size)]
        traj: list[ForecastTrajectoryPoint] = []
        for year in range(1, cfg.horizon + 1):
            t = year / cfg.horizon
            cap = forecast.capability_growth(t, cfg.curve, k=cfg.k, x0=cfg.x0)
            for sec in secs:
                if not sec.disrupted:
                    sec.energy *= 1.0 + sec.growth
                    if forecast.thermodynamic_trigger(sec, cap):
                        sec.disrupted = True
                        sec.energy += forecast._innovation_gain(
                            cfg.pop_size,
                            cfg.generations,
                            mut_rate=cfg.mut_rate,
                            xover_rate=cfg.xover_rate,
                        )
            snapshot = [sector.Sector(s.name, s.energy, s.entropy, s.growth, s.disrupted) for s in secs]
            point = forecast.TrajectoryPoint(year, cap, snapshot)
            traj.append(point)
            for ws in list(_progress_ws):
                try:
                    await ws.send_json({""id"": sim_id, ""year"": year, ""capability"": cap})
                except Exception:
                    _progress_ws.discard(ws)
            await asyncio.sleep(0)

        def eval_fn(genome: list[float]) -> tuple[float, float, float]:
            x, y = genome
            return x**2, y**2, (x + y) ** 2

        scenario = hashlib.sha1(sim_id.encode()).hexdigest()
        orch = getattr(app_f.state, ""orchestrator"", None)
        if orch is not None:
            pop = await orch.evolve(
                scenario,
                eval_fn,
                2,
                population_size=cfg.pop_size,
                generations=cfg.generations,
                experiment_id=sim_id,
            )
        else:
            pop = mats.run_evolution(
                eval_fn,
                2,
                population_size=cfg.pop_size,
                generations=cfg.generations,
                scenario_hash=scenario,
            )

        pop_data = [
            PopulationMember(
                effectiveness=ind.fitness[0],
                risk=ind.fitness[1],
                complexity=ind.fitness[2],
                rank=ind.rank,
            )
            for ind in pop
        ]

        result = ResultsResponse(
            id=sim_id,
            forecast=[ForecastPoint(year=p.year, capability=p.capability) for p in traj],
            population=pop_data,
        )
        _save_result(result)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"def convert_alpha(alpha: str, *, ledger: Path | None = None, model: str = ""gpt-4o-mini"") -> Dict[str, object]:
    """"""Return a plan dictionary and log to *ledger*.""""""
    plan: Dict[str, object] = SAMPLE_PLAN
    if ""openai"" in globals() and os.getenv(""OPENAI_API_KEY""):
        prompt = (
            f""Given the opportunity: {alpha}\n""
            ""Provide a short JSON plan with three concise steps to realise value.""
        )
        try:
            resp = openai.ChatCompletion.create(
                model=model,
                messages=[{""role"": ""user"", ""content"": prompt}],
            )
            plan = json.loads(resp.choices[0].message.content)  # type: ignore[index]
            if not isinstance(plan, dict):
                plan = SAMPLE_PLAN
        except Exception:
            plan = SAMPLE_PLAN
    (_ledger_path(ledger)).write_text(json.dumps(plan, indent=2))
    return plan
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py,
survived,"    def test_agent_compiles(self) -> None:
        py_compile.compile(AGENT, doraise=True)
",tests/test_aiga_evolver_agent.py,TestAIGAEvolverAgent
survived,"            def add(self, instr: Any) -> ""DummyTx"":
                self.instructions.append(instr)
                return self
",tests/test_ledger_broadcast.py,DummyTx
survived,"            def __init__(self, val: str) -> None:
                pass
",tests/test_ledger_client_close.py,DummyPk
survived,"    def passwords_per_seconds(self, seconds):
        return max(int(seconds * 1000), 1)
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    def create_from_params(cls, *args, **kwargs):
        self = cls(loading=True)
        self._load_wordlist()
        return self
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    async def create_item(name: str) -> Item:
        """"""Create item.""""""
        return Item(id=1, name=name)
",tests/test_mutability.py,
survived,"    async def update_item(item_id: int, patch: Item.PatchModel) -> Item:
        """"""Update item.""""""
        return Item(id=item_id, name=patch.name or ""n"")
",tests/test_mutability.py,
survived,"    def __init__(self, root: Path) -> None:
        self.root = root
        self.root.mkdir(parents=True, exist_ok=True)
",examples/basic_memory/memory.py,FileMemoryStore
survived,"    def new_id(self) -> str:  # pragma: no cover - simple wrapper
        return uuid4().hex
",examples/basic_memory/memory.py,FileMemoryStore
survived,"def odog_pairwise_distance_matrix(sampledffile, LocusFile, coofile,
                                  smalllargeNaN, outfilepath,
                                  missing_value_as = 9999999999):
    """"""Calculate a pairwise Euclidean distance matrix between ODOG samples.""""""
    if smalllargeNaN not in [""small"", ""large""]:
        raise ValueError(f""The smalllargeNaN {smalllargeNaN} is not 'small' or 'large'. Exiting."")

    for filepath in [sampledffile, LocusFile, coofile]:
        if not os.path.exists(filepath):
            raise IOError(f""The file {filepath} does not exist. Exiting."")

    cdf = pd.read_csv(sampledffile, sep=""\t"", index_col=0)
    ALGcomboix = algcomboix_file_to_dict(LocusFile)
    lil = load_npz(coofile).tolil()

    if lil.shape[0] != len(cdf):
        raise ValueError(
            f""The largest row index of the lil matrix, {lil.shape[0]}, is greater than the largest index of cdf, {max(cdf.index)}. Exiting."")
    if lil.shape[1] != len(ALGcomboix):
        raise ValueError(
            f""The largest column index of the lil matrix, {lil.shape[1]}, is greater than the length of ALGcomboix, {len(ALGcomboix)}. Exiting."")

    if smalllargeNaN == ""large"":
        print(""setting zeros to -1"")
        lil.data[lil.data == 0] = -1
        print(""Converting to a dense matrix. RAM will increase now."")
        matrix = lil.toarray()
        del lil
        print(f""setting zeros to {missing_value_as}"")
        matrix[matrix == 0] = missing_value_as
        print(""converting -1s to 0"")
        matrix[matrix == -1] = 0
        matrix = matrix + 1
        matrix = 1 / matrix
    else:
        matrix = lil.toarray()
        del lil

    dist_array = squareform(pdist(matrix, metric=""euclidean""))
    sample_names = cdf[""sample""] if ""sample"" in cdf.columns else cdf.index
    dist_df = pd.DataFrame(dist_array, index=sample_names, columns=sample_names)
    dist_df.to_csv(outfilepath, sep=""\t"")
    return 0
",dev_scripts/PhyloTreeUMAP.py,
survived,"def lead_time(truth: list[bool], pred: list[bool]) -> float:
    """"""Return ``pred`` onset minus ``truth`` onset.""""""
    def first_true(seq: list[bool]) -> int:
        for i, val in enumerate(seq):
            if val:
                return i
        return len(seq)

    return first_true(pred) - first_true(truth)
",src/simulation/replay.py,
survived,"def _append_metrics(path: Path, name: str, f1: float, auc: float, lead: float) -> None:
    import csv

    exists = path.exists()
    with path.open(""a"", newline="""", encoding=""utf-8"") as fh:
        writer = csv.writer(fh)
        if not exists:
            writer.writerow(_def_fields)
        writer.writerow([name, f1, auc, lead])
",src/simulation/replay.py,
survived,"def test_torch_backend(tmp_path):
    if not _have_torch() or os.environ.get(""RUN_ENGINE_TESTS"") != ""1"":
        print(""Skipping torch backend test"")
        return
    out_dir = tmp_path
    cmd = [
        ""./Release/Sibernetic"",
        ""-no_g"",
        ""-f"",
        ""configuration/test/test_energy"",
        ""-l_to"",
        f""lpath={out_dir}"",
        ""timelimit=0.001"",
        ""logstep=25"",
        ""backend=torch"",
    ]
    env = os.environ.copy()
    env[""PYTHONPATH""] = f""{os.getcwd()}:{env.get('PYTHONPATH', '')}""
    proc = subprocess.run(cmd, env=env)
    if proc.returncode != 0:
        print(""Torch backend run failed, skipping"")
        return

    pos = _load_matrix(""position_buffer.txt"", base=out_dir)
    base_pos = _load_matrix(""positions_step0.txt"")
    for g_row, b_row in zip(pos, base_pos):
        for gv, bv in zip(g_row, b_row):
            assert math.isfinite(gv)
            assert abs(gv - bv) < 1e-3

    vel = _load_matrix(""velocity_buffer.txt"", base=out_dir)
    base_vel = _load_matrix(""velocities_step0.txt"")
    for g_row, b_row in zip(vel, base_vel):
        for gv, bv in zip(g_row, b_row):
            assert math.isfinite(gv)
            assert abs(gv - bv) < 1e-3

    energy_file = os.path.join(out_dir, ""total_energy_distrib.txt"")
    if os.path.exists(energy_file):
        with open(energy_file) as f:
            energies = [float(line.strip()) for line in f if line.strip()]
        if len(energies) >= 2:
            start, end = energies[0], energies[-1]
            rel = abs(end - start) / (abs(start) + 1e-12)
            assert rel < 1.0",tests/test_torch_backend.py,
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(""--docs"", default=""docs"", help=""Documentation directory"")
    args = parser.parse_args()
    docs_dir = Path(args.docs)
    assets = gather_assets(docs_dir)
    sw_path = docs_dir / ""assets"" / ""service-worker.js""
    lines = [HEADER]
    for asset in assets:
        lines.append(f""          '{asset}',"")
    lines.append(FOOTER)
    sw_path.write_text(""\n"".join(lines))
    print(f""Wrote {sw_path}"")
",scripts/build_service_worker.py,
survived,"def gather_assets(docs_dir: Path) -> list[str]:
    base_assets = docs_dir / ""assets""
    assets: list[str] = []
    allowed = {"".js"", "".css"", "".svg"", "".json"", "".wasm"", "".tar"", "".cast""}
    pyodide_dir = base_assets / ""pyodide""
    if pyodide_dir.is_dir():
        order = [""pyodide.js"", ""pyodide.asm.wasm"", ""pyodide_py.tar""]
        for name in order:
            file = pyodide_dir / name
            if file.is_file() and file.suffix in allowed:
                rel = Path(""assets"") / ""pyodide"" / file.name
                assets.append(rel.as_posix())
    for item in sorted(docs_dir.iterdir()):
        if not item.is_dir() or item.name == ""assets"":
            continue
        a_dir = item / ""assets""
        if a_dir.is_dir():
            for file in sorted(a_dir.rglob(""*"")):
                if file.is_file() and file.suffix in allowed:
                    rel = Path("".."") / item.name / ""assets"" / file.relative_to(a_dir)
                    assets.append(rel.as_posix())
    return assets
",scripts/build_service_worker.py,
survived,"def test_readiness() -> None:
    resp = client.get(""/readiness"")
    assert resp.status_code == 200
    assert resp.text == ""ok""",tests/test_insight_health.py,
survived,"            def inc(self, *_a: Any) -> None: ...
",src/interface/api_server.py,_N
survived,"        def _get_metric(cls: Any, name: str, desc: str, labels: list[str]) -> Any:
            if name in getattr(_REG, ""_names_to_collectors"", {}):
                return _REG._names_to_collectors[name]
            return cls(name, desc, labels)
",src/interface/api_server.py,
survived,"def local_gpt2_tokenizer(tmp_path_factory):
    """"""Load a GPT2 tokenizer from a local JSON file to avoid network downloads.""""""

    config_src = Path(__file__).parent / ""gpt2_tokenizer_config.json""
    tmpdir = tmp_path_factory.mktemp(""gpt2_tok"")
    shutil.copy(config_src, tmpdir / ""tokenizer.json"")
    shutil.copy(config_src, tmpdir / ""tokenizer_config.json"")
    (tmpdir / ""config.json"").write_text(json.dumps({""model_type"": ""gpt2"", ""vocab_size"": 5027}))
    return AutoTokenizer.from_pretrained(str(tmpdir))",tests/conftest.py,
survived,"def test_bundle_metadata_defaults():
    meta = BundleMetadata()
    assert meta.schema_version == BUNDLE_SCHEMA_VERSION
    assert isinstance(meta.created_at, datetime)
    assert meta.custom == {}
",tests/test_bundle_metadata.py,
survived,"def test_bus_tls_accept(tmp_path: Path) -> None:
    """"""Envelope with valid token is accepted over TLS.""""""
    port = _free_port()
    cert, key, ca = _make_cert(tmp_path)
    cfg = config.Settings(bus_port=port, bus_cert=cert, bus_key=key, bus_token=""tok"")
    bus = messaging.A2ABus(cfg)
    received: list[messaging.Envelope] = []

    async def run() -> None:
        bus.subscribe(""b"", lambda e: received.append(e))
        await bus.start()
        try:
            creds = grpc.ssl_channel_credentials(root_certificates=ca)
            async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                stub = ch.unary_unary(""/bus.Bus/Send"")
                payload = {
                    ""sender"": ""a"",
                    ""recipient"": ""b"",
                    ""payload"": {""v"": 1},
                    ""ts"": 0.0,
                    ""token"": ""tok"",
                }
                await stub(json.dumps(payload).encode())
            await asyncio.sleep(0.05)
        finally:
            await bus.stop()

    asyncio.run(run())
    assert received and received[0].payload[""v""] == 1
",tests/test_bus_tls.py,
survived,"    def _write_detailed_large_objects(self, f):
        """"""
        ÂÜôÂÖ•Â§ßÂØπË±°ËØ¶ÁªÜÂàÜÊûê
        """"""
        f.write(""4. Â§ßÂØπË±°ËØ¶ÁªÜÂàÜÊûê\n"")
        f.write(""-"" * 50 + ""\n"")
        
        all_objects = muppy.get_objects()
        large_objects = []
        
        for obj in all_objects:
            try:
                size = asizeof.asizeof(obj)
                if size > 1024 * 1024:  # Â§ß‰∫é1MBÁöÑÂØπË±°
                    large_objects.append((obj, size))
            except:
                continue
        
        # ÊåâÂ§ßÂ∞èÊéíÂ∫è
        large_objects.sort(key=lambda x: x[1], reverse=True)
        
        f.write(f""Â§ßÂØπË±° (>1MB) Êï∞Èáè: {len(large_objects)}\n\n"")
        
        for i, (obj, size) in enumerate(large_objects[:20], 1):  # Âè™ÊòæÁ§∫Ââç20‰∏™
            size_mb = size / 1024 / 1024
            obj_type = type(obj).__name__
            
            f.write(f""{i:2d}. {obj_type} - {size_mb:.2f} MB\n"")
            
            # Â∞ùËØïËé∑ÂèñÊõ¥Â§ö‰ø°ÊÅØ
            try:
                if isinstance(obj, dict):
                    f.write(f""    Â≠óÂÖ∏È°πÊï∞: {len(obj)}\n"")
                    if obj:
                        sample_keys = list(obj.keys())[:3]
                        f.write(f""    Á§∫‰æãÈîÆ: {sample_keys}\n"")
                elif isinstance(obj, (list, tuple)):
                    f.write(f""    ÂÖÉÁ¥†Êï∞Èáè: {len(obj)}\n"")
                elif isinstance(obj, str):
                    f.write(f""    Â≠óÁ¨¶‰∏≤ÈïøÂ∫¶: {len(obj)}\n"")
                    if len(obj) > 100:
                        f.write(f""    ÂÜÖÂÆπÈ¢ÑËßà: {obj[:100]}...\n"")
                    else:
                        f.write(f""    ÂÜÖÂÆπ: {obj}\n"")
                elif hasattr(obj, '__dict__'):
                    f.write(f""    Â±ûÊÄßÊï∞Èáè: {len(obj.__dict__)}\n"")
                    if hasattr(obj, '__class__'):
                        f.write(f""    Á±ªÂêç: {obj.__class__.__name__}\n"")
            except:
                pass
            
            f.write(""\n"")
        
        f.write(""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper
survived,"    def set_current_graph(self) -> None:
        """"""Get the pkgx packages and dependencies""""""
        self.graph: CurrentGraph = self.current_graph(self.config.pm_config.pm_id)
        self.logger.log(f""Loaded {len(self.graph.package_map)} pkgx packages"")
",package_managers/pkgx/db.py,PkgxDB
survived,"    def diff_deps(
        self, import_id: str, pkg: PkgxPackage
    ) -> tuple[list[LegacyDependency], list[LegacyDependency]]:
        """"""
        Takes in a pkgx package and figures out what dependencies have changed.

        The process is:
           1. Build a view of what the package's dependencies are according to
              the parsed pkgx data, using priority-based deduplication
           2. Get this package's ID from CHAI
           3. Get this package's existing dependencies from CHAI
           4. Compare the two sets, and identify new and removed dependencies

        Note: The database has a unique constraint on (package_id, dependency_id),
        so if a package depends on the same dependency with multiple types (e.g.,
        both runtime and build), we choose the highest priority type:
        Runtime > Build > Test

        Returns:
          - new_deps: a list of new dependencies
          - removed_deps: a list of removed dependencies
        """"""
        new_deps: list[LegacyDependency] = []
        removed_deps: list[LegacyDependency] = []

        # First, collect all dependencies and deduplicate by dependency name
        # choosing the highest priority dependency type for each unique dependency
        dependency_map: dict[str, UUID] = {}

        # Priority order: Runtime > Build > Test
        priority_order = {
            self.config.dependency_types.runtime: 1,
            self.config.dependency_types.build: 2,
            self.config.dependency_types.test: 3,
        }

        def process_deps(dependencies: list[DependencyBlock], dep_type: UUID) -> None:
            """"""Helper to process dependencies of a given type with priority""""""
            for dep in dependencies:
                for dep_obj in dep.dependencies:
                    if not dep_obj.name:
                        continue

                    # Get the dependency package from cache
                    dependency = self.caches.package_map.get(dep_obj.name)
                    if not dependency:
                        self.logger.warn(
                            f""{dep_obj.name}, dep of {import_id} is not in cache""
                        )
                        continue

                    # If this dependency already exists in our map, choose higher priority
                    if dep_obj.name in dependency_map:
                        existing_priority = priority_order.get(
                            dependency_map[dep_obj.name], 999
                        )
                        new_priority = priority_order.get(dep_type, 999)

                        if (
                            new_priority < existing_priority
                        ):  # Lower number = higher priority
                            old_type_id = dependency_map[dep_obj.name]
                            dependency_map[dep_obj.name] = dep_type
                            self.logger.debug(
                                f""Updated dependency type for {dep_obj.name} from ""
                                f""{old_type_id} to {dep_type} (higher priority)""
                            )
                    else:
                        dependency_map[dep_obj.name] = dep_type

        # Process different types of dependencies with priority handling
        process_deps(pkg.dependencies, self.config.dependency_types.runtime)
        process_deps(pkg.build.dependencies, self.config.dependency_types.build)
        process_deps(pkg.test.dependencies, self.config.dependency_types.test)

        # Now build the actual set of dependencies with resolved types
        actual: set[tuple[UUID, UUID]] = set()
        for dep_name, dep_type in dependency_map.items():
            dependency = self.caches.package_map.get(dep_name)
            if dependency:  # Double-check it still exists
                actual.add((dependency.id, dep_type))

        # get the package ID for what we are working with
        package = self.caches.package_map.get(import_id)
        if not package:
            self.logger.warn(f""New package {import_id}, will grab its deps next time"")
            return [], []

        pkg_id: UUID = package.id

        # what are its existing dependencies?
        # specifically, existing dependencies IN THE SAME STRUCTURE as `actual`,
        # so we can do an easy comparison
        existing: set[tuple[UUID, UUID]] = {
            (dep.dependency_id, dep.dependency_type_id)
            for dep in self.caches.dependencies.get(pkg_id, set())
        }

        # we have two sets!
        # actual minus existing = new_deps
        # existing minus actual = removed_deps
        new = actual - existing
        removed = existing - actual

        new_deps: list[LegacyDependency] = [
            LegacyDependency(
                package_id=pkg_id,
                dependency_id=dep[0],
                dependency_type_id=dep[1],
                created_at=self.now,
                updated_at=self.now,
            )
            for dep in new
        ]

        # get the existing legacy dependency, and add it to removed_deps
        removed_deps: list[LegacyDependency] = []
        cache_deps: set[LegacyDependency] = self.caches.dependencies.get(pkg_id, set())
        for removed_dep_id, removed_dep_type in removed:
            try:
                existing_dep = next(
                    dep
                    for dep in cache_deps
                    if dep.dependency_id == removed_dep_id
                    and dep.dependency_type_id == removed_dep_type
                )
                removed_deps.append(existing_dep)
            except StopIteration as exc:
                cache_deps_str = ""\n"".join(
                    [
                        f""{dep.dependency_id} / {dep.dependency_type_id}""
                        for dep in cache_deps
                    ]
                )
                raise ValueError(
                    f""Removing {removed_dep_id} / {removed_dep_type} for {pkg_id} but not in Cache: \n{cache_deps_str}""
                ) from exc

        return new_deps, removed_deps
",package_managers/pkgx/diff.py,PkgxDiff
survived,"    def test_package_exists_dependency_change(self, mock_config, mock_logger):
        """"""Test scenario 3: Package existed in database and changed its dependencies""""""

        # Setup existing package and dependencies
        existing_pkg_id = uuid4()
        dep1_id = uuid4()
        dep2_id = uuid4()
        dep3_id = uuid4()

        existing_package = Package(
            id=existing_pkg_id,
            derived_id=""pkgx/dep-pkg"",
            name=""dep-pkg"",
            package_manager_id=mock_config.pm_config.pm_id,
            import_id=""dep-pkg"",
            readme="""",
        )

        # Create dependency packages
        dep1_pkg = Package(
            id=dep1_id, derived_id=""pkgx/dep1"", name=""dep1"", import_id=""dep1""
        )
        dep2_pkg = Package(
            id=dep2_id, derived_id=""pkgx/dep2"", name=""dep2"", import_id=""dep2""
        )
        dep3_pkg = Package(
            id=dep3_id, derived_id=""pkgx/dep3"", name=""dep3"", import_id=""dep3""
        )

        # Create existing dependencies (dep1 as runtime, dep2 as build)
        existing_dep1 = LegacyDependency(
            package_id=existing_pkg_id,
            dependency_id=dep1_id,
            dependency_type_id=mock_config.dependency_types.runtime,
        )
        existing_dep2 = LegacyDependency(
            package_id=existing_pkg_id,
            dependency_id=dep2_id,
            dependency_type_id=mock_config.dependency_types.build,
        )

        # Create cache
        cache = Cache(
            package_map={
                ""dep-pkg"": existing_package,
                ""dep1"": dep1_pkg,
                ""dep2"": dep2_pkg,
                ""dep3"": dep3_pkg,
            },
            url_map={},
            package_urls={},
            dependencies={existing_pkg_id: {existing_dep1, existing_dep2}},
        )

        # Create new package data with changed dependencies
        # Remove dep2, keep dep1, add dep3 as runtime
        new_pkg_data = create_pkgx_package(
            dependencies=[""dep1"", ""dep3""],  # runtime deps
            build_deps=[],  # no build deps (removes dep2)
        )

        # Test the diff
        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""dep-pkg"", new_pkg_data)

        # Assertions
        assert len(new_deps) == 1  # dep3 should be added
        assert new_deps[0].dependency_id == dep3_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.runtime

        assert len(removed_deps) == 1  # dep2 should be removed
        assert removed_deps[0].dependency_id == dep2_id
        assert removed_deps[0].dependency_type_id == mock_config.dependency_types.build
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading
survived,"    def diff_pkg_url(
        self, pkg_id: UUID, resolved_urls: dict[UUID, UUID]
    ) -> tuple[list[PackageURL], list[dict]]:
        """"""Takes in a package_id and resolved URLs from diff_url, and generates
        new PackageURL objects as well as a list of changes to existing ones""""""

        new_links: list[PackageURL] = []
        updates: list[dict] = []

        # what are the existing links?
        existing: set[UUID] = {
            pu.url_id for pu in self.caches.package_urls.get(pkg_id, set())
        }

        # for each URL type/URL for this package:
        for _url_type, url_id in resolved_urls.items():
            if url_id not in existing:
                # new link!
                new_links.append(
                    PackageURL(
                        id=uuid4(),
                        package_id=pkg_id,
                        url_id=url_id,
                        created_at=self.now,
                        updated_at=self.now,
                    )
                )
            else:
                # existing link - update timestamp
                existing_pu = next(
                    pu for pu in self.caches.package_urls[pkg_id] if pu.url_id == url_id
                )
                existing_pu.updated_at = self.now
                updates.append({""id"": existing_pu.id, ""updated_at"": self.now})

        return new_links, updates
",package_managers/pkgx/diff.py,PkgxDiff
survived,"    def test_missing_dependency_handling(self, mock_config, mock_logger):
        """"""Test how missing dependencies are handled""""""

        existing_pkg_id = uuid4()
        existing_package = Package(
            id=existing_pkg_id,
            derived_id=""pkgx/missing-dep-pkg"",
            name=""missing-dep-pkg"",
            import_id=""missing-dep-pkg"",
        )

        cache = Cache(
            package_map={""missing-dep-pkg"": existing_package},
            url_map={},
            package_urls={},
            dependencies={},
        )

        # Create package with dependency that doesn't exist in cache
        pkg_data = create_pkgx_package(dependencies=[""non-existent-dep""])

        diff = PkgxDiff(mock_config, cache, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""missing-dep-pkg"", pkg_data)

        # Should handle gracefully - no deps added for missing packages
        assert len(new_deps) == 0
        assert len(removed_deps) == 0
",tests/package_managers/pkgx/test_pkgx_diff.py,TestPkgxDifferentialLoading
survived,"    def test_sort_by_quality_index_asc(self):
        """"""Test sorting by quality index (lowest first).""""""
        models: list[ConciseModelResponse | ConciseLatestModelResponse] = [
            create_test_model(""model1"", quality_index=50),
            create_test_model(""model2"", quality_index=100),
            create_test_model(""model3"", quality_index=75),
        ]

        sorted_models = sort_models(models, ""quality_index"", ""asc"")

        assert [m.id for m in sorted_models] == [""model1"", ""model3"", ""model2""]
",api/api/routers/mcp/_utils/model_sorting_test.py,TestSortModels
survived,"def update_preset_column_config(
    preset_id: uuid.UUID,
    body: ColumnConfigurationDto,
    authenticated_entity: AuthenticatedEntity = Depends(
        IdentityManagerFactory.get_auth_verifier([""write:presets""])
    ),
    session: Session = Depends(get_session),
) -> PresetDto:
    tenant_id = authenticated_entity.tenant_id
    logger.info(""Updating preset column configuration"", extra={""preset_id"": preset_id})
    
    statement = (
        select(Preset)
        .where(Preset.tenant_id == tenant_id)
        .where(Preset.id == preset_id)
    )
    preset = session.exec(statement).first()
    if not preset:
        raise HTTPException(404, ""Preset not found"")

    # Get current options and remove any existing column config options
    current_options = [
        option for option in preset.options 
        if option.get(""label"", """").lower() not in [
            ""column_visibility"", 
            ""column_order"", 
            ""column_rename_mapping"", 
            ""column_time_formats"", 
            ""column_list_formats""
        ]
    ]

    # Add new column configuration options
    if body.column_visibility:
        current_options.append({
            ""label"": ""column_visibility"",
            ""value"": body.column_visibility
        })
    
    if body.column_order:
        current_options.append({
            ""label"": ""column_order"", 
            ""value"": body.column_order
        })
    
    if body.column_rename_mapping:
        current_options.append({
            ""label"": ""column_rename_mapping"",
            ""value"": body.column_rename_mapping
        })
    
    if body.column_time_formats:
        current_options.append({
            ""label"": ""column_time_formats"",
            ""value"": body.column_time_formats
        })
    
    if body.column_list_formats:
        current_options.append({
            ""label"": ""column_list_formats"",
            ""value"": body.column_list_formats
        })

    # Update the preset options
    preset.options = current_options
    session.commit()
    session.refresh(preset)
    
    logger.info(""Updated preset column configuration"", extra={""preset_id"": preset_id})
    return PresetDto(**preset.to_dict())
",keep/api/routes/preset.py,
survived,"    def column_visibility(self) -> Dict[str, bool]:
        """"""Get column visibility configuration from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_visibility""
        ]
        if not config:
            return {}
        return config[0].get(""value"", {})
",keep/api/models/db/preset.py,PresetDto
survived,"    def unnest(
        self,
        unnest_key: str,
        keep_empty: bool = False,
        expand_fields: Optional[List[str]] = None,
        recursive: bool = False,
        depth: Optional[int] = None,
        **kwargs
    ) -> pd.DataFrame:
        """"""
        Unnest list-like or dictionary values into multiple rows.

        Documentation: https://ucbepic.github.io/docetl/operators/unnest/

        Args:
            unnest_key: The column containing list-like or dictionary values to unnest
            keep_empty: Whether to keep rows with empty/null values (default: False)
            expand_fields: For dictionary values, which fields to expand (default: all)
            recursive: Whether to recursively unnest nested structures (default: False)
            depth: Maximum depth for recursive unnesting (default: 1, or unlimited if recursive=True)
            **kwargs: Additional configuration options

        Returns:
            pd.DataFrame: DataFrame with unnested values, where:
                - For lists: Each list element becomes a separate row
                - For dicts: Specified fields are expanded into the parent row

        Examples:
            >>> # Unnest a list column
            >>> df.semantic.unnest(
            ...     unnest_key=""tags""
            ... )
            # Input:  [{""id"": 1, ""tags"": [""a"", ""b""]}]
            # Output: [{""id"": 1, ""tags"": ""a""}, {""id"": 1, ""tags"": ""b""}]

            >>> # Unnest a dictionary column with specific fields
            >>> df.semantic.unnest(
            ...     unnest_key=""user_info"",
            ...     expand_fields=[""name"", ""age""]
            ... )
            # Input:  [{""id"": 1, ""user_info"": {""name"": ""Alice"", ""age"": 30, ""email"": ""alice@example.com""}}]
            # Output: [{""id"": 1, ""user_info"": {...}, ""name"": ""Alice"", ""age"": 30}]

            >>> # Recursive unnesting
            >>> df.semantic.unnest(
            ...     unnest_key=""nested_lists"",
            ...     recursive=True,
            ...     depth=2
            ... )
        """"""
        # Convert DataFrame to list of dicts
        input_data = self._df.to_dict(""records"")

        # Create unnest operation config
        unnest_config = {
            ""type"": ""unnest"",
            ""name"": f""semantic_unnest_{len(self._history)}"",
            ""unnest_key"": unnest_key,
            ""keep_empty"": keep_empty,
            ""recursive"": recursive,
            **kwargs,
        }

        # Add optional parameters if provided
        if expand_fields is not None:
            unnest_config[""expand_fields""] = expand_fields
        if depth is not None:
            unnest_config[""depth""] = depth

        # Create and execute unnest operation
        unnest_op = UnnestOperation(
            runner=self.runner,
            config=unnest_config,
            default_model=self.runner.config[""default_model""],
            max_threads=self.runner.max_threads,
            console=self.runner.console,
            status=self.runner.status,
        )
        results, cost = unnest_op.execute(input_data)

        return self._record_operation(results, ""unnest"", unnest_config, cost)
",docetl/apis/pd_accessors.py,SemanticAccessor
survived,"def test_semantic_split_delimiter():
    """"""Test semantic split operation with delimiter method.""""""
    df = pd.DataFrame({
        ""content"": [
            ""First paragraph.\n\nSecond paragraph.\n\nThird paragraph."",
            ""Another doc.\n\nWith multiple.\n\nParagraphs here.""
        ],
        ""id"": [1, 2]
    })
    
    result = df.semantic.split(
        split_key=""content"",
        method=""delimiter"", 
        method_kwargs={""delimiter"": ""\n\n"", ""num_splits_to_group"": 1}
    )
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) > len(df)  # Should create more rows
    assert ""content_chunk"" in result.columns
    assert f""semantic_split_0_id"" in result.columns
    assert f""semantic_split_0_chunk_num"" in result.columns
    
    # Check that each chunk contains one paragraph
    for chunk in result[""content_chunk""]:
        assert ""\n\n"" not in chunk  # Delimiter should be removed from chunks
",tests/test_pandas_accessors.py,
survived,"    def set_current_graph(self) -> None:
        """"""Get the debian packages and dependencies""""""
        self.graph: CurrentGraph = self.current_graph(self.config.pm_config.pm_id)
",package_managers/debian/db.py,DebianDB
survived,"    def test_build_package_to_source_mapping_no_binary_list(
        self, tmp_path, mock_logger
    ):
        """"""Test building mapping when source has no explicit binary list""""""

        # Create a test sources file with no Binary field
        sources_content = """"""Package: single-source
Vcs-Git: https://github.com/test/single-source.git
Homepage: https://example.com/single-source
""""""

        sources_file = tmp_path / ""sources""
        sources_file.write_text(sources_content)

        # Build mapping
        mapping = build_package_to_source_mapping(str(sources_file), mock_logger)

        # Verify mapping - should use source package name as binary name
        assert len(mapping) == 1
        assert ""single-source"" in mapping
        assert mapping[""single-source""].package == ""single-source""
        # URLs are normalized by the parser - expect normalized format
        assert mapping[""single-source""].vcs_git == ""github.com/test/single-source""
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"    def _generate_chai_urls(self, debian_data: DebianData) -> list[URLKey]:
        """"""Generate URLs for a debian package""""""
        urls = []

        # Homepage URL
        if debian_data.homepage:
            urls.append(URLKey(debian_data.homepage, self.config.url_types.homepage))

        # Source URL
        source_url = (
            debian_data.vcs_git if debian_data.vcs_git else debian_data.vcs_browser
        )
        if source_url:
            urls.append(URLKey(source_url, self.config.url_types.source))

        # Repository URL
        if is_github_url(source_url):
            urls.append(URLKey(source_url, self.config.url_types.repository))

        return urls",package_managers/debian/diff.py,DebianDiff
survived,"def main():
    data_dir = ""data/debian/latest""

    # Check if data files exist
    sources_file = os.path.join(data_dir, ""sources"")
    packages_file = os.path.join(data_dir, ""packages"")

    if not os.path.exists(sources_file):
        logger.log(f""ERROR: Sources file not found at {sources_file}"")
        logger.log(""Use --fetch to download the latest data"")
        return 1

    if not os.path.exists(packages_file):
        logger.log(f""ERROR: Packages file not found at {packages_file}"")
        logger.log(""Use --fetch to download the latest data"")
        return 1

    logger.log(f""Using sources file: {sources_file}"")
    logger.log(f""Using packages file: {packages_file}"")

    investigate_mapping(sources_file, packages_file)

    return 0
",package_managers/debian/scripts/investigate_sources.py,
survived,"def load_tasks():
    """"""Load tasks from file""""""
    global tasks
    try:
        if os.path.exists(TASKS_FILE):
            with open(TASKS_FILE, 'r') as f:
                tasks = json.load(f)
            logger.info(f""üìÇ Loaded {len(tasks)} tasks from {TASKS_FILE}"")
        else:
            logger.info(f""üìÇ No tasks file found, starting fresh"")
    except Exception as e:
        logger.warning(f""‚ö†Ô∏è Failed to load tasks: {e}"")
        tasks = {}
",server/utils.py,
survived,"def apply_patch_to_github_repo(repo, branch, patch_content, task):
    """"""Apply a git patch to a GitHub repository using the GitHub API""""""
    try:
        logger.info(f""üîß Parsing patch content..."")
        
        # Parse git patch format to extract file changes
        files_to_update = {}
        current_file = None
        new_content_lines = []
        
        # This is a simplified patch parser - for production you might want a more robust one
        lines = patch_content.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i]
            
            # Look for file headers in patch format
            if line.startswith('--- a/') or line.startswith('--- /dev/null'):
                # Next line should be +++ b/filename
                if i + 1 < len(lines) and lines[i + 1].startswith('+++ b/'):
                    current_file = lines[i + 1][6:]  # Remove '+++ b/'
                    logger.info(f""üìÑ Found file change: {current_file}"")
                    
                    # Get the original file content if it exists
                    try:
                        file_obj = repo.get_contents(current_file, ref=branch)
                        original_content = file_obj.decoded_content.decode('utf-8')
                        logger.info(f""üì• Got original content for {current_file}"")
                    except:
                        original_content = """"  # New file
                        logger.info(f""üìù New file: {current_file}"")
                    
                    # For simplicity, we'll reconstruct the file from the diff
                    # Skip to the actual diff content (after @@)
                    j = i + 2
                    while j < len(lines) and not lines[j].startswith('@@'):
                        j += 1
                    
                    if j < len(lines):
                        # Apply the diff changes
                        new_content = apply_diff_to_content(original_content, lines[j:], current_file)
                        if new_content is not None:
                            files_to_update[current_file] = new_content
                            logger.info(f""‚úÖ Prepared update for {current_file}"")
                    
                    i = j
            i += 1
        
        # Now update all the files via GitHub API
        updated_files = []
        commit_message = f""Claude Code: {task['prompt'][:100]}""
        
        for file_path, new_content in files_to_update.items():
            try:
                # Check if file exists
                try:
                    file_obj = repo.get_contents(file_path, ref=branch)
                    # Update existing file
                    repo.update_file(
                        path=file_path,
                        message=commit_message,
                        content=new_content,
                        sha=file_obj.sha,
                        branch=branch
                    )
                    logger.info(f""üìù Updated existing file: {file_path}"")
                except:
                    # Create new file
                    repo.create_file(
                        path=file_path,
                        message=commit_message,
                        content=new_content,
                        branch=branch
                    )
                    logger.info(f""üÜï Created new file: {file_path}"")
                
                updated_files.append(file_path)
                
            except Exception as file_error:
                logger.error(f""‚ùå Failed to update {file_path}: {file_error}"")
        
        return updated_files
        
    except Exception as e:
        logger.error(f""üí• Error applying patch: {str(e)}"")
        return []
",server/github_integration.py,
survived,"    def get_task_by_legacy_id(legacy_id: str) -> Optional[Dict]:
        """"""Get a task by its legacy UUID (for migration purposes)""""""
        try:
            result = supabase.table('tasks').select('*').eq('execution_metadata->>legacy_id', legacy_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error fetching task by legacy ID {legacy_id}: {e}"")
            raise
",server/database.py,DatabaseOperations
survived,"def get_git_diff(task_id):
    """"""Get git diff for a task (legacy endpoint for compatibility)""""""
    try:
        user_id = request.headers.get('X-User-ID')
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        task = DatabaseOperations.get_task_by_id(task_id, user_id)
        if not task:
            return jsonify({'error': 'Task not found'}), 404
        
        return jsonify({
            'status': 'success',
            'git_diff': task.get('git_diff', ''),
            'task_id': task_id
        })
        
    except Exception as e:
        logger.error(f""Error fetching git diff: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,
survived,"    def sort_by_duration(self) -> None:
        """"""Sort tasks by estimated duration (longest first) for optimal concurrent execution.""""""
        task_durations = []
        for task_path in self._tasks:
            try:
                task_paths_obj = TaskPaths(task_path)
                task = Task.from_yaml(task_paths_obj.task_config_path)
                duration = task.effective_estimated_duration_sec
            except Exception as e:
                self._logger.warning(
                    f""Failed to load task {task_path.name}: {e}. Using fallback duration.""
                )
                duration = 210.0
            task_durations.append((task_path, duration))

        task_durations.sort(key=lambda x: x[1], reverse=True)
        self._tasks = [task_path for task_path, _ in task_durations]

        # Log the task execution order
        table_data = []
        for i, (task_path, duration) in enumerate(task_durations, 1):
            task_name = task_path.name
            minutes = int(duration // 60)
            seconds = int(duration % 60)
            duration_str = f""{minutes}m {seconds}s""

            try:
                task_paths_obj = TaskPaths(task_path)
                task = Task.from_yaml(task_paths_obj.task_config_path)
                if task.estimated_duration_sec is not None:
                    source = ""historical""
                else:
                    source = ""calculated""
            except Exception:
                source = ""fallback""

            table_data.append([i, task_name, duration_str, source])

        headers = [""#"", ""Task Name"", ""Duration"", ""Source""]
        table = tabulate(table_data, headers=headers, tablefmt=""grid"")

        self._logger.info(""Processing tasks in duration order (longest first):"")
        self._logger.info(f""\n{table}"")
        self._logger.info(f""Total tasks: {len(task_durations)}"")",terminal_bench/dataset/dataset.py,Dataset
survived,"    def _set_dummy_api_key(self, monkeypatch, request):
        # MCP mode doesn't require API key, so we don't set it
        # This allows us to test MCP functionality without API key requirements
        pass
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_mcp_vs_rest_api_mode_exclusive(self, runner, temp_python_script):
        """"""Test that MCP and REST API modes are mutually exclusive in terms of requirements.""""""
        # Test that --mcp skips API key validation
        with patch.dict(""os.environ"", {}, clear=True):
            with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
                mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
                
                # MCP mode should work without API key
                result_mcp = runner.invoke(app, [
                    ""serve"", str(temp_python_script),
                    ""--mcp"", ""--verbose""
                ])
                assert ""MCP mode enabled"" in result_mcp.output
                
                # REST API mode should fail without API key
                result_rest = runner.invoke(app, [
                    ""serve"", str(temp_python_script),
                    ""--no-mcp"", ""--verbose""
                ])
                assert result_rest.exit_code == 1
                assert ""LANGFLOW_API_KEY"" in result_rest.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"    def flow_execution_help() -> str:
        """"""Get help on how to execute flows via MCP.""""""
        flow_list = list(graphs.keys())
        return f""""""
# Langflow MCP Server Help

This server exposes {len(flow_list)} Langflow flows as MCP tools.

## Available Flows:
{chr(10).join(f""- {flow_id}: {metas.get(flow_id, {}).get('title', flow_id)}"" for flow_id in flow_list)}

## How to Execute Flows:
Use the corresponding MCP tool for each flow. Each tool accepts:
- input_value: The main input text/data
- tweaks: Optional parameter modifications

## Getting Flow Information:
Use these MCP resources:
- flow://flows - List all flows
- flow://flows/{{flow_id}}/info - Get flow details  
- flow://flows/{{flow_id}}/schema - Get input/output schema

## Example Usage:
1. List flows: Read resource ""flow://flows""
2. Get flow info: Read resource ""flow://flows/my_flow/info""
3. Execute flow: Call tool ""execute_my_flow"" with input_value
""""""
",src/backend/base/langflow/cli/mcp_server.py,
survived,"    def test_mcp_custom_host_port(self, runner, temp_python_script):
        """"""Test MCP mode with custom host and port.""""""
        with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
            mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--mcp-transport"", ""sse"",
                ""--host"", ""0.0.0.0"",
                ""--port"", ""9000"",
                ""--verbose""
            ])
            
            # Verify custom host and port were passed
            mock_run_mcp.assert_called_once()
            run_args = mock_run_mcp.call_args
            assert run_args[1][""host""] == ""0.0.0.0""
            assert run_args[1][""port""] == 9000
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_mcp_server_creation_single_flow(self, mock_run_mcp, mock_create_mcp, runner, temp_python_script):
        """"""Test MCP server creation for single flow.""""""
        # Mock the MCP server creation
        mock_mcp_server = MagicMock()
        mock_create_mcp.return_value = mock_mcp_server
        mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
        
        result = runner.invoke(app, [
            ""serve"", str(temp_python_script),
            ""--mcp"", ""--mcp-name"", ""Test MCP Server"",
            ""--verbose""
        ])
        
        # Verify MCP server was created with correct parameters
        mock_create_mcp.assert_called_once()
        call_args = mock_create_mcp.call_args
        assert call_args[1][""server_name""] == ""Test MCP Server""
        assert ""graphs"" in call_args[1]
        assert ""metas"" in call_args[1]
        
        # Verify MCP server was run
        mock_run_mcp.assert_called_once()
        run_args = mock_run_mcp.call_args
        assert run_args[1][""mcp_server""] == mock_mcp_server
        assert run_args[1][""transport""] == ""stdio""  # default
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_mcp_transport_validation(self, runner, temp_python_script):
        """"""Test validation of MCP transport options.""""""
        # Test invalid transport
        result = runner.invoke(app, [
            ""serve"", str(temp_python_script), 
            ""--mcp"", ""--mcp-transport"", ""invalid""
        ])
        assert result.exit_code == 1
        assert ""Invalid MCP transport 'invalid'"" in result.output
        assert ""Must be one of: sse, stdio, websocket"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_mcp_server_keyboard_interrupt(self, runner, temp_python_script):
        """"""Test graceful handling of keyboard interrupt in MCP server.""""""
        with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
            mock_run_mcp.side_effect = KeyboardInterrupt(""User interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--verbose""
            ])
            
            assert result.exit_code == 0
            assert ""MCP server stopped"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"        def mock_tool_decorator(func):
            registered_tools.append(func)
            return func
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
deleted,"    def list_flows() -> str:
        """"""List all available flows with their metadata.""""""
        flows_info = []
        for flow_id, graph in graphs.items():
            meta = metas.get(flow_id, {})
            flow_info = FlowInfo(
                id=flow_id,
                title=getattr(meta, 'title', flow_id),
                description=getattr(meta, 'description', None),
                inputs=None,  # Could be expanded to include input schema
                outputs=None  # Could be expanded to include output schema
            )
            flows_info.append(flow_info.model_dump())
        
        return json.dumps(flows_info, indent=2)
",src/backend/base/langflow/cli/mcp_server.py,
survived,"def upload_registry(
    registry_path: Annotated[
        Path,
        typer.Option(
            help=""Path to the registry.json file"",
        ),
    ] = Path(""registry.json""),
) -> None:
    """"""
    Upload registry entries to Supabase database (reach out to admins for env variable access).
    """"""
    supabase = create_client(
        os.environ[""SUPABASE_URL""],
        os.environ[""SUPABASE_SERVICE_ROLE_KEY""],
    )

    rich_print(""Fetching existing registry entries..."")
    existing_entries_response = supabase.table(""registry"").select(""*"").execute()
    existing_entries = {
        (entry[""name""], entry[""version""]): SupabaseRegistry(**entry)
        for entry in existing_entries_response.data
    }

    rich_print(""Loading registry data..."")
    try:
        registry = Registry.from_file(registry_path)
    except FileNotFoundError:
        rich_print(f""[bold red]Registry file does not exist: {registry_path}[/]"")
        raise typer.Exit(1)
    except json.JSONDecodeError as e:
        rich_print(f""[bold red]Invalid JSON in registry file: {e}[/]"")
        raise typer.Exit(1)
    except Exception as e:
        rich_print(f""[bold red]Failed to parse registry data: {e}[/]"")
        raise typer.Exit(1)

    current_time = datetime.now().isoformat()
    registry_entries = []

    for entry in registry.datasets:
        registry_entry = SupabaseRegistry(
            name=entry.name,
            version=entry.version,
            description=entry.description,
            terminal_bench_version=entry.terminal_bench_version,
            github_url=entry.github_url,
            dataset_path=entry.dataset_path,
            branch=entry.branch,
            commit_hash=entry.commit_hash,
            updated_at=current_time,
        )

        entry_key = (registry_entry.name, registry_entry.version)
        if (
            entry_key not in existing_entries
            or registry_entry != existing_entries[entry_key]
        ):
            registry_entries.append(registry_entry.to_dict())

    if registry_entries:
        rich_print(
            f""Uploading {len(registry_entries)} new or changed registry entries...""
        )
        try:
            result = (
                supabase.table(""registry"")
                .upsert(registry_entries, on_conflict=""name,version"")
                .execute()
            )
        except Exception as e:
            rich_print(f""[bold red]Failed to upload registry entries: {e}[/]"")
            raise typer.Exit(1)
    else:
        rich_print(""[yellow]No new or changed registry entries to upload[/]"")
        return

    uploaded_count = len(registry_entries)
    rich_print(
        f""[bold green]Successfully uploaded {uploaded_count} registry entries to Supabase![/]""
    )",terminal_bench/cli/tb/admin.py,
survived,"    def to_dict(self) -> dict[str, Any]:
        """"""Convert the registry entry to a dictionary for Supabase upload.""""""
        return self.model_dump(exclude_none=True, mode=""json"")
",terminal_bench/cli/tb/admin.py,SupabaseRegistry
deleted,"    def test_moonshot_completion_mock(self, respx_mock):
        """"""
        Mock test for Moonshot completion using the model format from docs.
        This test mocks the actual HTTP request to test the integration properly.
        """"""

        litellm.disable_aiohttp_transport = (
            True  # since this uses respx, we need to set use_aiohttp_transport to False
        )

        # Set up environment variables for the test
        api_key = ""fake-moonshot-key""
        api_base = ""https://api.moonshot.ai/v1""
        model = ""moonshot/moonshot-v1-8k""
        model_name = ""moonshot-v1-8k""  # The actual model name without provider prefix

        # Mock the HTTP request to the moonshot API
        respx_mock.post(f""{api_base}/chat/completions"").respond(
            json={
                ""id"": ""chatcmpl-123"",
                ""object"": ""chat.completion"",
                ""created"": 1677652288,
                ""model"": model_name,
                ""choices"": [
                    {
                        ""index"": 0,
                        ""message"": {
                            ""role"": ""assistant"",
                            ""content"": '```python\nprint(""Hey from LiteLLM!"")\n```\n\nThis simple Python code prints a greeting message from LiteLLM.',
                        },
                        ""finish_reason"": ""stop"",
                    }
                ],
                ""usage"": {
                    ""prompt_tokens"": 9,
                    ""completion_tokens"": 12,
                    ""total_tokens"": 21,
                },
            },
            status_code=200,
        )

        # Make the actual API call through LiteLLM
        response = completion(
            model=model,
            messages=[
                {""role"": ""user"", ""content"": ""write code for saying hey from LiteLLM""}
            ],
            api_key=api_key,
            api_base=api_base,
        )

        # Verify response structure
        assert response is not None
        assert hasattr(response, ""choices"")
        assert len(response.choices) > 0
        assert hasattr(response.choices[0], ""message"")
        assert hasattr(response.choices[0].message, ""content"")
        assert response.choices[0].message.content is not None

        # Check for specific content in the response
        assert ""```python"" in response.choices[0].message.content
        assert ""Hey from LiteLLM"" in response.choices[0].message.content",tests/test_litellm/llms/moonshot/test_moonshot_chat_transformation.py,TestMoonshotConfig
survived,"    def test_csharp_real_world_example(self):
        # Based on a typical C# class with various method types
        patch = """"""
@@ -73,9 +73,7 @@ public UserService(IUserRepository repository)

@@ -87,7 +87,8 @@ public async Task<User> GetUserAsync(int id)

@@ -95,6 +95,7 @@ public bool ValidateUser(User user)

@@ -103,4 +107,23 @@ private void LogUserAction(string action)

@@ -115,6 +118,13 @@ public static UserService CreateDefault()

@@ -125,7 +125,7 @@ protected virtual void OnUserChanged(UserEventArgs e)

@@ -135,8 +135,8 @@ public void Dispose()

@@ -145,10 +145,10 @@ ~UserService()

@@ -155,12 +155,12 @@ public string Name
{
    get { return _name; }
    set { _name = value; }
}

@@ -168,15 +168,15 @@ public int Count => _users.Count;

@@ -180,18 +180,18 @@ public User this[int index] => _users[index];

""""""

        assert CSharpParser.extract_functions_from_patch(patch) == {
            ""UserService"",
            ""GetUserAsync"",
            ""ValidateUser"",
            ""LogUserAction"",
            ""CreateDefault"",
            ""OnUserChanged"",
            ""Dispose"",
            ""get"",
            ""set"",
            ""Count"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,CSharpParserTestCase
survived,"async def rollout_tau_bench_task(
    model: art.Model[TauBenchPolicyConfig],
    task_index: int,
) -> art.Trajectory:
    """"""
    Generate a trajectory for a single tau-bench task using the given model.
    This adapts the tau-bench evaluation loop for RL trajectory generation.
    """"""
    config = model.config.run_config
    
    # Get isolated environment for this task
    env = get_env(
        config.env,
        user_strategy=config.user_strategy,
        user_model=config.user_model,
        user_provider=config.user_model_provider,
        task_split=config.task_split,
        task_index=task_index,
    )
    
    # Create agent with the trainable model
    # For RL training, we need to override the model parameters
    agent = agent_factory(
        tools_info=env.tools_info,
        wiki=env.wiki,
        config=config,
    )
    
    # Override the agent's model if we're using a trainable model
    # Note: This will need to be adapted based on the specific agent implementation
    if model.trainable:
        try:
            if hasattr(agent, 'model'):
                setattr(agent, 'model', f""hosted_vllm/{model.name}"")
            if hasattr(agent, 'client'):
                client = getattr(agent, 'client')
                if hasattr(client, 'base_url'):
                    setattr(client, 'base_url', model.inference_base_url)
                    setattr(client, 'api_key', model.inference_api_key)
        except Exception as e:
            print(f""Warning: Could not override agent model parameters: {e}"")
    
    # Create trajectory object
    traj = art.Trajectory(
        messages_and_choices=[],
        reward=0,
        metadata={""task_index"": task_index, ""env"": config.env}
    )
    
    try:
        # Run the agent on the task
        result = agent.solve(
            env=env,
            task_index=task_index,
        )
        
        # Convert result to trajectory format
        traj.reward = result.reward
        traj.metadata.update(result.info)
        
        # Convert messages to the format expected by ART
        for msg in result.messages:
            traj.messages_and_choices.append(msg)
            
    except Exception as e:
        print(f""Error in rollout for task {task_index}: {e}"")
        traj.reward = 0.0
        traj.metadata[""error""] = str(e)
    
    traj.finish()
    return traj
",dev/tau-bench/run_rl.py,
survived,"    def test_env_group_with_mixed_datasets(self, mock_openai_client):
        """"""Test EnvGroup with environments having different dataset configurations.""""""
        # Environment with both train and eval datasets
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            eval_dataset=Dataset.from_dict({""question"": [""eq1""], ""answer"": [""ea1""]}),
            rubric=Rubric()
        )
        
        # Environment with only eval dataset
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q2""], ""answer"": [""a2""]}),
            eval_dataset=Dataset.from_dict({""question"": [""eq2""], ""answer"": [""ea2""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1, env2], env_names=[""task1"", ""task2""])
        
        # Should have concatenated train dataset from both envs
        train_dataset = env_group.get_dataset()
        assert len(train_dataset) == 2
        assert train_dataset[""task""][0] == ""task1""
        assert train_dataset[""task""][1] == ""task2""
        
        # Should have concatenated eval datasets from both
        eval_dataset = env_group.get_eval_dataset()
        assert len(eval_dataset) == 2
        assert eval_dataset[""task""][0] == ""task1""
        assert eval_dataset[""task""][1] == ""task2""",tests/test_env_group.py,TestEnvGroup
survived,"    async def test_a_generate_with_score_rollouts(self, mock_openai_client, sample_dataset):
        """"""Test async generate with scoring enabled.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        # Mock the rubric scoring
        env.rubric.score_rollouts = AsyncMock(return_value={
            ""reward"": [1.0]
        })
        
        inputs = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""answer"": [""Hi""]
        }
        
        results = await env.a_generate(inputs, score_rollouts=True)
        
        assert ""completion"" in results
        assert ""state"" in results
        assert ""reward"" in results
        assert results[""reward""] == [1.0]
",tests/test_environment.py,TestEnvironmentBase
survived,"    async def test_score_rollouts_with_mixed_return_types(self):
        """"""Test scoring when reward functions return different types.""""""
        def scalar_func(completion, **kwargs):
            return 0.5
        
        def list_func(completion, **kwargs):
            # This should not happen, but test robustness
            return [0.1, 0.2]  # Wrong return type
        
        rubric = Rubric(funcs=[scalar_func], weights=[1.0])
        
        results = await rubric.score_rollouts(
            prompts=[""test""],
            completions=[""test""],
            answers=[""test""],
            states=[{}],
            tasks=[""test""],
            infos=[{}]
        )
        
        assert results[""scalar_func""] == [0.5]
        assert results[""reward""] == [0.5]",tests/test_rubric.py,TestRubric
survived,"        def mock_apply_chat_template(conversation, tokenize=False, add_generation_prompt=True):
            # Convert messages to a string representation
            text = """"
            for msg in conversation:
                text += f""{msg['role']}: {msg['content']} ""
            return text.strip()
",tests/test_environment.py,TestEnvironmentBase
survived,"    def test_env_group_rubric_type(self, mock_openai_client):
        """"""Test that EnvGroup creates EnvGroupRubric.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1])
        
        assert isinstance(env_group.rubric, EnvGroupRubric)
        assert env_group.rubric.env_map[""env_0""] == env1
",tests/test_env_group.py,TestEnvGroup
survived,"    def test_generate_sync_wrapper(self, mock_openai_client, sample_dataset):
        """"""Test synchronous generate wrapper.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        # Mock the rubric scoring
        env.rubric.score_rollouts = AsyncMock(return_value={
            ""reward"": [1.0]
        })
        
        inputs = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""answer"": [""Hi""]
        }
        
        results = env.generate(inputs, client=env.client)
        
        assert ""completion"" in results
        assert ""state"" in results
        assert ""reward"" in results
",tests/test_environment.py,TestEnvironmentBase
survived,"        def func1(completion, **kwargs):
            return 0.8
",tests/test_env_group.py,TestEnvGroupRubric
survived,"async def main():
    """"""Main entry point""""""
    try:
        await run_sequence_demo()
    except KeyboardInterrupt:
        print(""\n\nDemo interrupted by user"")
        sys.exit(0)
    except Exception as e:
        print(f""\nError: {e}"")
        sys.exit(1)
",examples/python_mcp_chunk_stream.py,
survived,"def format_timestamp(iso_timestamp):
    """"""Convert ISO timestamp to readable format""""""
    dt = datetime.fromisoformat(iso_timestamp.replace('Z', '+00:00'))
    return dt.strftime(""%H:%M:%S.%f"")[:-3]
",examples/python_mcp_chunk_stream.py,
survived,"    async def test_create_project_with_minimal_input(
        self,
        db: DbSessionFactory,
        gql_client: AsyncGraphQLClient,
    ) -> None:
        """"""Test the create_project mutation with only required fields.""""""
        project_name = token_hex(8)

        mutation = """"""
            mutation CreateProject($input: CreateProjectInput!) {
                createProject(input: $input) {
                    project {
                        id
                        name
                        gradientStartColor
                        gradientEndColor
                    }
                    query {
                        __typename
                    }
                }
            }
        """"""

        result = await gql_client.execute(
            mutation,
            variable_values={
                ""input"": {
                    ""name"": project_name,
                }
            },
        )

        assert result.errors is None
        assert result.data is not None
        create_project_data = result.data[""createProject""]
        
        project_data = create_project_data[""project""]
        assert project_data[""name""] == project_name
        # Should use default gradient colors from the database
        assert project_data[""gradientStartColor""] == ""#5bdbff""
        assert project_data[""gradientEndColor""] == ""#1c76fc""
        
        # Verify the project was actually created in the database
        project_id = project_data[""id""]
        decoded_id = GlobalID.from_id(project_id)
        
        async with db() as session:
            project = await session.get(models.Project, int(decoded_id.node_id))
            assert project is not None
            assert project.name == project_name
            assert project.description is None",tests/unit/server/api/mutations/test_project_mutations.py,TestProjectMutations
survived,"    def should_glean(self, gleaning_config: Optional[Dict[str, Any]], output: Dict[str, Any]) -> bool:
        """"""Determine whether to execute a gleaning round based on an optional conditional expression.""""""
        if not gleaning_config or ""if"" not in gleaning_config:
            return True

        condition = gleaning_config.get(""if"")
        if not isinstance(condition, str):
            raise ValueError(f""Invalid gleaning condition (should be a string): {condition}"")

        try:
            return safe_eval(condition, output)
        except Exception as exc:
            self.console.log(
                f""[bold red]Error evaluating gleaning condition '{condition}': {exc}; executing gleaning round anyway[/bold red]""
            )
            return False
",docetl/operations/utils/api.py,ValidationHandler
deleted,"    def _handle_validation_retries(
        self,
        response: Any,
        output_schema: Dict[str, Any],
        output_mode: OutputMode,
        validation_config: Dict[str, Any],
        model: str,
        op_type: str,
        messages: List[Dict[str, str]],
        tools: Optional[str],
        scratchpad: Optional[str],
        litellm_completion_kwargs: Dict[str, Any],
        op_config: Dict[str, Any],
    ) -> tuple[Any, float, bool]:
        """"""Handle validation retries.""""""
        additional_cost = 0.0
        num_tries = validation_config.get(""num_retries"", 2) + 1
        validation_fn = validation_config.get(""validation_fn"")
        val_rule = validation_config.get(""val_rule"")

        # Try validation
        i = 0
        validation_result = False
        while not validation_result and i < num_tries:
            parsed_output, validation_result = validation_fn(response)
            if validation_result:
                return response, additional_cost, True

            # Append the validation result to messages
            messages.append({""role"": ""assistant"", ""content"": json.dumps(parsed_output)})
            messages.append({
                ""role"": ""user"",
                ""content"": f""Your output {parsed_output} failed my validation rule: {str(val_rule)}\n\nPlease try again."",
            })
            
            self.console.log(
                f""[bold red]Validation failed:[/bold red] {val_rule}\n""
                f""\t[yellow]Output:[/yellow] {parsed_output}\n""
                f""\t({i + 1}/{num_tries})""
            )
            i += 1

            response = self.llm_handler.make_completion_call(
                model, op_type, messages, output_mode, output_schema, tools, scratchpad, litellm_completion_kwargs, op_config
            )
            additional_cost += completion_cost(response)

        return response, additional_cost, validation_result
",docetl/operations/utils/api.py,ValidationHandler
deleted,"    def build_tool_schema(output_schema: Dict[str, Any], scratchpad: Optional[str] = None, model: str = """") -> Dict[str, Any]:
        """"""Build a tool schema from an output schema.""""""
        props = {key: convert_val(value) for key, value in output_schema.items()}
        
        if scratchpad is not None:
            props[""updated_scratchpad""] = {""type"": ""string""}

        parameters = {""type"": ""object"", ""properties"": props}
        parameters[""required""] = list(props.keys())

        # Some models don't support additionalProperties
        if ""gemini"" not in model and ""claude"" not in model:
            parameters[""additionalProperties""] = False

        return parameters
",docetl/operations/utils/api.py,OutputSchemaBuilder
survived,"    async def test_rollout_with_sampling_args(self, mock_singleturn_env):
        """"""Test rollout with custom sampling arguments.""""""
        prompt = [{""role"": ""user"", ""content"": ""Hello""}]
        answer = ""Hi""
        sampling_args = {""temperature"": 0.8, ""max_tokens"": 100}
        
        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=answer,
            sampling_args=sampling_args
        )
        
        assert isinstance(completion, list)
        assert completion[0][""content""] == ""This is a test response""
        
        # Verify sampling args were passed
        call_args = mock_singleturn_env.client.chat.completions.create.call_args
        assert ""temperature"" in call_args.kwargs
        assert ""max_tokens"" in call_args.kwargs
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"    def add_text_response(self, prompt, response, finish_reason=""stop""):
        """"""Add a mapped response for specific prompt.""""""
        self.text_completions[prompt] = {
            ""text"": response,
            ""finish_reason"": finish_reason
        }
",tests/conftest.py,MockAsyncOpenAI
survived,"        def error_func(completion, **kwargs):
            raise ValueError(""Test error"")
",tests/test_rubric.py,TestRubric
survived,"    def test_rubric_group_score_rollouts_with_kwargs(self):
        """"""Test scoring rollouts with additional kwargs.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_parse_simple_xml(self, xml_parser):
        """"""Test parsing simple XML with basic fields.""""""
        xml_text = """"""
        <reasoning>
        Let me think about this problem step by step.
        </reasoning>
        <answer>
        The final answer is 42.
        </answer>
        """"""
        result = xml_parser.parse(xml_text)
        assert result.reasoning == ""Let me think about this problem step by step.""
        assert result.answer == ""The final answer is 42.""
",tests/test_xml_parser.py,TestXMLParser
survived,"    def test_get_methods(self):
        """"""Test getter methods.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        rubric = Rubric(funcs=[func1, func2], weights=[0.8, 0.2])
        
        assert rubric.get_reward_funcs() == [func1, func2]
        assert rubric.get_reward_weights() == [0.8, 0.2]
        assert rubric.get_reward_func_names() == [""func1"", ""func2""]
",tests/test_rubric.py,TestRubric
survived,"    async def test_immediate_completion(self, mock_multiturn_env):
        """"""Test completion detection on first turn.""""""
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Quick question""}],
            response=""Immediate DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Quick question""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""target_answer""
        )
        
        # Should complete immediately
        assert len(completion) == 1
        assert completion[0][""content""] == ""Immediate DONE""
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def extract_boxed(text):
        """"""Simple boxed answer extractor for testing.""""""
        import re
        match = re.search(r'\\boxed\{([^}]+)\}', text)
        return match.group(1) if match else text
",tests/conftest.py,
survived,"    def test_rubric_initialization_empty(self):
        """"""Test Rubric initialization with no parameters.""""""
        rubric = Rubric()
        
        assert rubric.reward_funcs == []
        assert rubric.reward_weights == []
        assert isinstance(rubric.parser, Parser)
",tests/test_rubric.py,TestRubric
survived,"        def func2(completion, **kwargs):
            return len(completion) * 0.1
",tests/test_rubric.py,TestRubric
survived,"    def test_parse_returns_text_as_is(self, basic_parser):
        """"""Test that parse method returns text unchanged.""""""
        text = ""This is a test string""
        result = basic_parser.parse(text)
        assert result == text
",tests/test_parser.py,TestParser
survived,"        def reward_func2(completion, **kwargs):
            return 0.5
",tests/test_rubric.py,TestRubric
survived,"    def test_abstract_methods_not_implemented(self):
        """"""Test that MultiTurnEnv cannot be instantiated directly (abstract class).""""""
        # MultiTurnEnv is abstract and should not be instantiable without implementing abstract methods
        with pytest.raises(TypeError):
            # This should fail because MultiTurnEnv has abstract methods
            MultiTurnEnv(
                model=""test-model"",
                parser=Parser(),
                rubric=Rubric()
            )
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    async def test_run_rollouts(self, mock_openai_client):
        """"""Test running multiple rollouts.""""""
        env = TestEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""test""], ""answer"": [""test""]}),
            parser=Parser(),
            rubric=Rubric()
        )
        
        prompts = [
            [{""role"": ""user"", ""content"": ""Hello""}],
            [{""role"": ""user"", ""content"": ""Hi""}]
        ]
        answers = [""response1"", ""response2""]
        tasks = [""default"", ""default""]
        infos = [{}, {}]
        
        # Mock the rollout method calls
        results = await env.run_rollouts(
            client=mock_openai_client,
            model=""test-model"",
            prompts=prompts,
            answers=answers,
            tasks=tasks,
            infos=infos
        )
        
        assert len(results) == 2
        assert all(len(result) == 2 for result in results)  # Each result is (completion, state)",tests/test_environment.py,TestEnvironmentBase
survived,"    async def test_env_response_integration(self, mock_multiturn_env):
        """"""Test that environment responses are properly integrated.""""""
        # Set up responses for the conversation turns
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Start conversation""}],
            response=""First response""
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {""role"": ""user"", ""content"": ""Start conversation""},
                {""role"": ""assistant"", ""content"": ""First response""},
                {""role"": ""user"", ""content"": ""Continue (turn 1)""}
            ],
            response=""Final response DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Start conversation""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""target_answer""
        )
        
        # Verify environment responses are included
        assert len(completion) >= 3
        user_messages = [msg for msg in completion if msg[""role""] == ""user""]
        assert len(user_messages) >= 1
        assert ""Continue (turn 1)"" in user_messages[0][""content""]
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"        def comprehensive_func(prompt, completion, answer, state, task, info, **kwargs):
            return len(completion) + len(answer) + len(task)
",tests/test_rubric.py,TestRubric
survived,"        def func3(completion, **kwargs):
            return 0.3
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_generate_sync_wrapper(self, mock_singleturn_env):
        """"""Test the synchronous generate wrapper.""""""
        inputs = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""answer"": [""Hi""],
            ""info"": [{}]
        }
        
        # Mock the rubric.score_rollouts method
        mock_singleturn_env.rubric.score_rollouts = AsyncMock(return_value={
            ""rewards"": [1.0],
            ""scores"": [{""correctness"": 1.0}]
        })
        
        results = mock_singleturn_env.generate(inputs)
        
        assert ""completion"" in results
        assert ""state"" in results
        assert ""rewards"" in results
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"        def accuracy_func(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0
",tests/test_rubric.py,TestRubric
survived,"def xml_parser_with_alternatives():
    """"""Return an XMLParser instance with alternative field names.""""""
    return XMLParser(
        fields=[""reasoning"", (""code"", ""answer"")],
        answer_field=""answer""
    )
",tests/conftest.py,
survived,"def test_scenarios():
    """"""
    Collection of test scenarios for URL loading.
    Each scenario represents a different case we want to test.
    """"""
    scenarios = {
        ""new_urls"": {
            ""import_id"": ""github.com/certifi/python-certifi"",
            ""package_id"": UUID(""e0f18184-e743-40fb-8add-a5ccaac026a4""),
            # What transformer found
            ""transformer_urls"": [
                (""github.com/certifi/python-certifi"", [""homepage"", ""source""])
            ],
            # What's in DB (nothing, completely new)
            ""db_state"": {""urls"": {}, ""package_urls"": {}},
            ""expected_behavior"": {
                ""new_urls_created"": 2,  # homepage and source
                ""new_package_urls_created"": 2,
                ""urls_updated"": 0,
            },
        },
        ""existing_package_some_urls"": {
            ""import_id"": ""github.com/pyca/cryptography"",
            ""package_id"": UUID(""f0f18184-e743-40fb-8add-a5ccaac026a5""),
            # Transformer found homepage, source, and repository
            ""transformer_urls"": [
                (""github.com/pyca/cryptography"", [""homepage"", ""source"", ""repository""])
            ],
            # DB only has homepage
            ""db_state"": {
                ""urls"": {
                    (""github.com/pyca/cryptography"", ""homepage""): {
                        ""id"": UUID(""22222222-2222-2222-2222-222222222222""),
                        ""url"": ""github.com/pyca/cryptography"",
                    }
                },
                ""package_urls"": {
                    UUID(""f0f18184-e743-40fb-8add-a5ccaac026a5""): [
                        {
                            ""id"": UUID(""33333333-3333-3333-3333-333333333333""),
                            ""url_id"": UUID(""22222222-2222-2222-2222-222222222222""),
                        }
                    ]
                },
            },
            ""expected_behavior"": {
                ""new_urls_created"": 2,  # source and repository
                ""new_package_urls_created"": 2,
                ""urls_updated"": 1,  # homepage timestamp updated
            },
        },
        ""all_urls_exist"": {
            ""import_id"": ""github.com/requests/requests"",
            ""package_id"": UUID(""a0a18184-e743-40fb-8add-a5ccaac026a6""),
            # Transformer found these
            ""transformer_urls"": [
                (""github.com/requests/requests"", [""homepage"", ""source""])
            ],
            # DB has exact same ones
            ""db_state"": {
                ""urls"": {
                    (""github.com/requests/requests"", ""homepage""): {
                        ""id"": UUID(""44444444-4444-4444-4444-444444444444""),
                        ""url"": ""github.com/requests/requests"",
                    },
                    (""github.com/requests/requests"", ""source""): {
                        ""id"": UUID(""55555555-5555-5555-5555-555555555555""),
                        ""url"": ""github.com/requests/requests"",
                    },
                },
                ""package_urls"": {
                    UUID(""a0a18184-e743-40fb-8add-a5ccaac026a6""): [
                        {
                            ""id"": UUID(""66666666-6666-6666-6666-666666666666""),
                            ""url_id"": UUID(""44444444-4444-4444-4444-444444444444""),
                        },
                        {
                            ""id"": UUID(""77777777-7777-7777-7777-777777777777""),
                            ""url_id"": UUID(""55555555-5555-5555-5555-555555555555""),
                        },
                    ]
                },
            },
            ""expected_behavior"": {
                ""new_urls_created"": 0,
                ""new_package_urls_created"": 0,
                ""urls_updated"": 2,  # Both timestamps updated
            },
        },
        ""no_urls_in_db"": {
            ""import_id"": ""github.com/numpy/numpy"",
            ""package_id"": UUID(""b0b18184-e743-40fb-8add-a5ccaac026a7""),
            # Transformer found URLs but DB has no record of this package
            ""transformer_urls"": [
                (""github.com/numpy/numpy"", [""homepage"", ""repository"", ""documentation""])
            ],
            # DB is empty for this package
            ""db_state"": {""urls"": {}, ""package_urls"": {}},
            ""expected_behavior"": {
                ""new_urls_created"": 3,
                ""new_package_urls_created"": 3,
                ""urls_updated"": 0,
            },
        },
    }

    return scenarios
",tests/package_managers/pkgx/test_pkgx_load_urls.py,
survived,"def package_ids():
    """"""Fixture providing consistent package IDs for testing.""""""
    return {
        ""main"": uuid4(),
        ""dep"": uuid4(),
    }
",tests/package_managers/crates/test_diff_deps.py,
survived,"def ids():
    """"""Fixture providing consistent IDs for testing.""""""
    return {
        ""homepage_url_type"": uuid4(),
        ""package_manager"": uuid4(),
        ""pkg1"": uuid4(),
        ""pkg2"": uuid4(),
        ""pkg3"": uuid4(),
        ""canon1"": uuid4(),
        ""canon2"": uuid4(),
        ""canon3"": uuid4(),
        ""url1"": uuid4(),
        ""url2"": uuid4(),
        ""url3"": uuid4(),
    }
",tests/ranker/test_dedupe.py,
survived,"def homebrew_formula():
    """"""
    Factory fixture to create Actual homebrew formula objects.

    Returns a function that creates Actual objects.
    """"""

    def create_formula(
        formula_name,
        dependencies=None,
        build_dependencies=None,
        test_dependencies=None,
        recommended_dependencies=None,
        optional_dependencies=None,
    ):
        return Actual(
            formula=formula_name,
            description=""Test formula"",
            license=""MIT"",
            homepage="""",
            source="""",
            repository="""",
            dependencies=dependencies or [],
            build_dependencies=build_dependencies or [],
            test_dependencies=test_dependencies or [],
            recommended_dependencies=recommended_dependencies or [],
            optional_dependencies=optional_dependencies or [],
        )

    return create_formula
",tests/package_managers/homebrew/test_diff_dep.py,
survived,"            def track_bulk_update(mapper, mappings):
                urls_updated.extend(mappings)
",tests/package_managers/pkgx/test_pkgx_load_urls.py,TestPkgxLoader
survived,"    def get_url_type_by_name(name):
        if hasattr(mock_url_types, name):
            return getattr(mock_url_types, name)
        return None
",tests/conftest.py,
survived,"def map_config_with_tools():
    return {
        ""type"": ""map"",
        ""name"": ""word_count"",
        ""prompt"": ""Count the number of words in the following text: '{{ input.text }}'"",
        ""output"": {""schema"": {""word_count"": ""integer""}},
        ""model"": ""gpt-4o-mini"",
        ""tools"": [
            {
                ""required"": True,
                ""code"": """"""
def count_words(text):
    return {""word_count"": len(text.split())}
                """""",
                ""function"": {
                    ""name"": ""count_words"",
                    ""description"": ""Count the number of words in a text string."",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""text"": {
                                ""type"": ""string"",
                            }
                        },
                        ""required"": [""text""],
                    },
                },
            }
        ],
        ""validate"": [""len(output['text']) > 0""],
        ""num_retries_on_validate_failure"": 3,
    }
",tests/basic/test_basic_map.py,
survived,"    def test_go_function_variables(self):
        patch = """"""
@@ -152,10 +152,6 @@ var handler = func(w http.ResponseWriter, r *http.Request) {

@@ -152,10 +152,6 @@ var callback = func() error {

@@ -152,10 +152,6 @@ processor := func(data []byte) []byte {

@@ -152,10 +152,6 @@ validator := func(input string) bool {

@@ -152,10 +152,6 @@ var transformer = func(x int) int {

@@ -152,10 +152,6 @@ mapper := func(items []string) map[string]int {

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""handler"",
            ""callback"",
            ""processor"",
            ""validator"",
            ""transformer"",
            ""mapper"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase
survived,"    def test_send_html_email_with_both_body_and_html(self, mock_smtp_class, smtp_provider):
        """"""Test that HTML takes precedence when both body and html are provided.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send email with both body and html
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            body=""Plain text content"",
            html=""<p>HTML content</p>"",
        )

        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        
        # Verify HTML content is used (not plain text)
        email_content = call_args[0][2]
        assert ""Content-Type: text/html"" in email_content
        assert ""<p>HTML content</p>"" in email_content
        assert ""Content-Type: text/plain"" not in email_content
        
        # Verify return value contains both
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test Subject"",
            ""body"": ""Plain text content"",
            ""html"": ""<p>HTML content</p>"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    async def get_pairs_by_chain_and_pair(self, parameters: dict):
        url = f""{self.base_url}/pairs/{parameters['chainId']}/{parameters['pairId']}""
        return await self._fetch(url, ""fetch pairs"")
",python/src/plugins/dexscreener/goat_plugins/dexscreener/service.py,DexscreenerService
survived,"    async def get_trading_signal(self, parameters: dict):
        """"""Get trading signals and alerts based on onchain data and patterns""""""
        url = f""{self.base_url}/signals""
        params = {
            ""start_date"": parameters[""start_date""],
            ""end_date"": parameters[""end_date""]
        }
        if parameters.get(""token_address""):
            params[""token_address""] = parameters[""token_address""]
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"    def supports_chain(self, chain) -> bool:
        # farcaster is chain-agnostic
        return True
",python/src/plugins/farcaster/goat_plugins/farcaster/__init__.py,FarcasterPlugin
survived,"def main(_):
    base_temp_dir = FLAGS.temp_dir or tempfile.gettempdir()
    with tempfile.TemporaryDirectory(dir=base_temp_dir) as temp_dir:
        zip_path = os.path.join(temp_dir, ""test_dataset.zip"")
        download_file(FLAGS.url, zip_path)
        
        extract_dir = os.path.join(temp_dir, ""extracted"")
        os.makedirs(extract_dir, exist_ok=True)
        extract_zip(zip_path, extract_dir)
        
        results = test_parsing_equality(extract_dir)
        
        print(""\nTest Summary:"")
        print(f""  Passed: {results['passed']}"")
        print(f""  Failed: {results['failed']}"")
        
        if results[""errors""]:
            print(""\nFailures:"")
            for name, error in results[""errors""]:
                print(f""  {name}: {error}"")
        
        if FLAGS.keep_files:
            keep_dir = os.path.join(os.getcwd(), ""test_dataset"")
            print(f""\nKeeping files in {keep_dir}"")
            if not os.path.exists(keep_dir):
                os.makedirs(keep_dir)
            os.system(f""cp -r {extract_dir}/* {keep_dir}/"")
        
        return 0 if results[""failed""] == 0 else 1
",tests/replay_parser_test.py,
survived,"    def test_certificate_installation(self):
        """"""Test certificate installation creates temporary file and sets environment variables""""""
        test_cert = ""-----BEGIN CERTIFICATE-----\ntest\n-----END CERTIFICATE-----""
        
        with patch('tempfile.NamedTemporaryFile') as mock_temp_file, \
             patch.dict('os.environ', {}, clear=True):
            
            mock_file = mock_open()
            mock_temp_file.return_value.__enter__.return_value = mock_file.return_value
            mock_file.return_value.name = ""/tmp/test_cert.pem""
            
            from source_file.proxy import _install_ca_certificate
            
            result_path = _install_ca_certificate(test_cert)
            
            mock_file.return_value.write.assert_called_once_with(test_cert)
            mock_file.return_value.flush.assert_called_once()
            
            assert os.environ.get(""REQUESTS_CA_BUNDLE"") == ""/tmp/test_cert.pem""
            assert os.environ.get(""CURL_CA_BUNDLE"") == ""/tmp/test_cert.pem""
            assert os.environ.get(""SSL_CERT_FILE"") == ""/tmp/test_cert.pem""
",airbyte-integrations/connectors/source-file/unit_tests/test_proxy_certificate_support.py,TestProxyCertificateSupport
deleted,"def retrieve_stock_data(ticker: str, days: int = 30) -> tuple[float, float]:
    end_date = datetime.today()
    start_date = end_date - timedelta(days=days)
    data = yf.download(ticker, start=start_date, end=end_date)
    current_price = data['Close'].iloc[-1]
    start_price = data['Close'].iloc[0]
    percent_change = ((current_price - start_price) / start_price) * 100
    return current_price, percent_change
",financial_analysis/functions.py,
survived,"def rugcheck(options: RugCheckPluginOptions) -> RugCheckPlugin:
    return RugCheckPlugin(options)",python/src/plugins/rugcheck/goat_plugins/rugcheck/__init__.py,
survived,"    async def _make_request(self, endpoint: str):
        headers = {
            ""Content-Type"": ""application/json"",
        }
        if self.jwt_token:
            headers[""Authorization""] = f""Bearer {self.jwt_token}""
        async with aiohttp.ClientSession() as session:
            url = f""{self.base_url}{endpoint}""
            async with session.get(url, headers=headers) as response:
                if not response.ok:
                    if response.status == 429:
                        raise Exception(""RugCheck API rate limit exceeded"")
                    raise Exception(f""RugCheck API request failed: {response.status}"")
                return await response.json()
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService
deleted,"    def _get_github_master_metadata_url(self, connector: Connector) -> str:
        return f""{GITHUB_URL_PREFIX_FOR_CONNECTORS}/{connector.technical_name}/{METADATA_FILE_NAME}""
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck
survived,"async def test_quick_reconnection(setup_ycell):
    """"""Test that quick reconnection properly handles cleanup task cancellation""""""
    # Setup
    cell_id = CellId_t(""test_cell"")
    file_key = MarimoFileKey(""test_file"")
    key = CellIdAndFileKey(cell_id, file_key)
    
    # Create initial ycell
    ydoc = Doc[Text]()
    ycell = YCell(ydoc=ydoc, clients=1)
    ycells[key] = ycell
    
    # Start cleanup task
    cleanup_task = asyncio.create_task(clean_cell(key))
    
    # Simulate quick reconnection by creating a new client before cleanup finishes
    ycells[key].clients += 1
    
    # Cancel cleanup task (simulating what happens in ycell_provider)
    cleanup_task.cancel()
    try:
        await cleanup_task
    except asyncio.CancelledError:
        pass
    
    # Verify state
    assert len(ycells) == 1
    assert ycells[key].clients == 2  # Original client + reconnected client",marimo/_server/api/endpoints/tests/test_ws_rtc.py,
survived,"    def _messages_into(self, messages: List[common.Message]) -> List[Dict[str, Any]]:
        ollama_messages = []
        for message in messages:
            content_parts = []
            tool_calls = []
            
            for block in message.content:
                if isinstance(block, common.TextRaw):
                    content_parts.append(block.text)
                elif isinstance(block, common.ToolUse):
                    tool_calls.append({
                        ""type"": ""function"",
                        ""function"": {
                            ""name"": block.name,
                            ""arguments"": block.input
                        }
                    })
                elif isinstance(block, common.ToolResult):
                    content_parts.append(f""Tool result: {block.content}"")
            
            ollama_message: Dict[str, Any] = {
                ""role"": message.role,
                ""content"": "" "".join(content_parts) if content_parts else """"
            }
            
            if tool_calls:
                ollama_message[""tool_calls""] = tool_calls
                
            ollama_messages.append(ollama_message)
        
        return ollama_messages
",agent/llm/ollama_client.py,OllamaLLM
deleted,"def test_scrape_url_with_parse_pdf_true():
    if TEST_API_KEY:
        app = FirecrawlApp(api_url=API_URL, api_key=TEST_API_KEY)
        response = app.scrape_url('https://arxiv.org/pdf/astro-ph/9301001.pdf', parse_pdf=True)
        assert response is not None
        assert 'markdown' in response
        assert len(response['markdown']) > 100
",apps/python-sdk/firecrawl/__tests__/v1/e2e_withAuth/test.py,
deleted,"def test_scrape_options_with_parse_pdf():
    if TEST_API_KEY:
        from firecrawl.firecrawl import ScrapeOptions
        app = FirecrawlApp(api_url=API_URL, api_key=TEST_API_KEY)
        scrape_options = ScrapeOptions(parsePDF=False, formats=['markdown'])
        response = app.search(""firecrawl"", limit=1, scrape_options=scrape_options)
        assert response is not None
        assert 'data' in response
",apps/python-sdk/firecrawl/__tests__/v1/e2e_withAuth/test.py,
survived,"    def __init__(self, block, layers, fc_layers=1):
        self.inplanes = 64
        super(SixDRepNet360, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AvgPool2d(7)

        self.linear_reg = nn.Linear(512*block.expansion,6)
      
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, (2. / n) ** 0.5)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
",face_recognition/6d_repnet_360/convert_to_onnx.py,SixDRepNet360
survived,"    def _perform_conversion(self, file_path: str, converter, format_msg: str) -> List[Tuple[str, Dict[str, Any]]]:
        """"""Perform the actual conversion using the specified converter.""""""
        pages_data = []
        try:
            result = converter.convert(file_path)
            markdown_content = result.document.export_to_markdown()
            
            metadata = {""source"": file_path}
            # Return the *DoclingDocument* object as third tuple element so downstream
            # chunkers that understand the element tree can use it.  Legacy callers that
            # expect only (markdown, metadata) can simply ignore the extra value.
            pages_data.append((markdown_content, metadata, result.document))
            print(f""Successfully converted {file_path} with docling {format_msg}."")
            return pages_data
        except Exception as e:
            print(f""Error processing {file_path} with docling: {e}"")
            return []",rag_system/ingestion/document_converter.py,DocumentConverter
survived,"        def _pdf_has_text(path: str) -> bool:
            try:
                doc = fitz.open(path)
                for page in doc:
                    if page.get_text(""text"").strip():
                        return True
            except Exception:
                pass
            return False
",rag_system/ingestion/document_converter.py,DocumentConverter
survived,"                def __init__(self, provider, session, init_timestamp, kwargs):
                    self.provider = provider
                    self.session = session
                    self.init_timestamp = init_timestamp
                    self.kwargs = kwargs
                    self.stream = None
                    # Create a new LLM event for this stream
                    self.llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                    if session is not None:
                        self.llm_event.session_id = session.session_id
                        self.llm_event.agent_id = check_call_stack_for_agent_id()
                        self.llm_event.model = kwargs.get(""model"", ""command-r-plus"")
                        self.llm_event.prompt = kwargs.get(""message"", """")
                        self.llm_event.completion = """"  # Initialize empty completion
                        logger.info(f""Initialized async stream LLM event with session_id: {session.session_id}"")
",agentops/llms/providers/cohere.py,CohereProvider.AsyncStreamWrapper
survived,"def test_litellm_integration():
    """"""Integration test demonstrating all four LiteLLM call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()
    
    # Initialize LiteLLM provider
    from agentops.llms.providers.litellm import LiteLLMProvider
    provider = LiteLLMProvider(None)  # LiteLLM doesn't need a client
    provider.override()
    
    # Pass session to provider
    provider.client = session

    def sync_no_stream():
        litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync no stream"", ""role"": ""user""}],
            session=session
        )

    def sync_stream():
        stream_response = litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass

    async def async_no_stream():
        await litellm.acompletion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from async no stream"", ""role"": ""user""}],
            session=session
        )

    async def async_stream():
        async_stream_response = await litellm.acompletion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from async streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        # Handle streaming response
        if isinstance(async_stream_response, str):
            _ = async_stream_response
        else:
            async for chunk in async_stream_response:
                _ = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta, 'content') else ''

    async def run_async_tests():
        await async_no_stream()
        await async_stream()

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during LiteLLM test: {str(e)}"")
        raise

    session.end_session(""Success"")
    analytics = session.get_analytics()
    print(analytics)
    # Verify that all LLM calls were tracked
    assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""
",tests/core_manual_tests/providers/litellm_canary.py,
survived,"    async def async_stream():
        async_stream_response = await litellm.acompletion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from async streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        # Handle streaming response
        if isinstance(async_stream_response, str):
            _ = async_stream_response
        else:
            async for chunk in async_stream_response:
                _ = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta, 'content') else ''
",tests/core_manual_tests/providers/litellm_canary.py,
survived,"def test_anthropic_integration():
    """"""Integration test demonstrating all four Anthropic call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()

    # Initialize clients and provider
    anthropic_client = anthropic.Anthropic(api_key=os.getenv(""ANTHROPIC_API_KEY""))
    async_anthropic_client = anthropic.AsyncAnthropic(api_key=os.getenv(""ANTHROPIC_API_KEY""))
    from agentops.llms.providers.anthropic import AnthropicProvider
    provider = AnthropicProvider(anthropic_client)
    provider.override()
    
    # Pass session to provider
    provider.client = session

    def sync_no_stream():
        anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from sync no stream"",
                }
            ],
            session=session
        )

    def sync_stream():
        stream_response = anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from sync streaming"",
                }
            ],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass

    async def async_no_stream():
        await async_anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from async no stream"",
                }
            ],
            session=session
        )

    async def async_stream():
        async_stream_response = await async_anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from async streaming"",
                }
            ],
            stream=True,
            session=session
        )
        async for _ in async_stream_response:
            pass

    async def run_async_tests():
        await async_no_stream()
        await async_stream()

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during Anthropic test: {str(e)}"")
        raise

    session.end_session(""Success"")
    analytics = session.get_analytics()
    print(analytics)
    # Verify that all LLM calls were tracked
    assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""
",tests/core_manual_tests/providers/anthropic_canary.py,
survived,"    async def async_stream():
        stream_response = await client.chat_stream(
            model=""mistral-tiny"",
            messages=[ChatMessage(role=""user"", content=""Hello from async streaming"")]
        )
        async for chunk in stream_response:
            _ = chunk.delta.content if hasattr(chunk.delta, 'content') else ''
",tests/core_manual_tests/providers/mistral_canary.py,
survived,"def test_parallel_dataset_creation():
  """"""Test dataset creation with parallel processing.""""""
  threads = 4
  parsed_data = test_dataset_creation(threads=threads)

  print(f""Successfully processed dataset with {threads} threads"")
  return parsed_data
",tests/dataset_creation_test.py,
survived,"def main():
    # Set up argument parser
    parser = argparse.ArgumentParser(description=""Codebase Context Agent using Claude 3.7"")
    parser.add_argument(""-p"", ""--prompt"", required=True, help=""The user's request"")
    parser.add_argument(""-d"", ""--directory"", default=os.getcwd(), help=""Directory to search in (defaults to current working directory)"")
    parser.add_argument(""-g"", ""--globs"", nargs=""*"", default=[], help=""List of glob patterns to filter files (optional)"")
    parser.add_argument(""-e"", ""--extensions"", nargs=""*"", default=[], help=""List of file extensions to filter files (optional)"")
    parser.add_argument(""-q"", ""--quiet"", action=""store_true"", help=""Quiet mode (don't show logging)"")
    parser.add_argument(""-l"", ""--limit"", type=int, default=100, help=""Maximum number of files to return"")
    parser.add_argument(""-f"", ""--file-line-limit"", type=int, default=500, help=""Maximum number of lines per file"")
    parser.add_argument(""-c"", ""--compute"", type=int, default=10, help=""Maximum number of agent loops (default: 10)"")
    args = parser.parse_args()

    # Configure the API key
    ANTHROPIC_API_KEY = os.getenv(""ANTHROPIC_API_KEY"")
    if not ANTHROPIC_API_KEY:
        console.print(
            ""[red]Error: ANTHROPIC_API_KEY environment variable is not set[/red]""
        )
        console.print(
            ""Please get your API key from https://console.anthropic.com/settings/keys""
        )
        console.print(""Then set it with: export ANTHROPIC_API_KEY='your-api-key-here'"")
        sys.exit(1)

    client = Anthropic(api_key=ANTHROPIC_API_KEY)

    # Set global USER_PROMPT
    global USER_PROMPT
    USER_PROMPT = args.prompt

    # Configure quiet mode
    if args.quiet:
        console.quiet = True

    # Create a single combined prompt based on the full template
    completed_prompt = (
        AGENT_PROMPT.replace(""{{user_request}}"", args.prompt)
        .replace(""{{directory}}"", args.directory)
        .replace(""{{globs}}"", str(args.globs))
        .replace(""{{extensions}}"", str(args.extensions))
        .replace(""{{file_line_limit}}"", str(args.file_line_limit))
        .replace(""{{limit}}"", str(args.limit))
    )
    
    # Initialize messages with proper typing for Anthropic chat
    messages = [{""role"": ""user"", ""content"": completed_prompt}]

    compute_iterations = 0
    break_loop = False
    # Main agent loop
    while True:
        if break_loop or compute_iterations >= args.compute or len(RELEVANT_FILES) >= args.limit:
            break

        console.rule(
            f""[yellow]Agent Loop {compute_iterations+1}/{args.compute}[/yellow]""
        )
        compute_iterations += 1

        try:
            # Generate content with tool support
            response = client.messages.create(
                model=""claude-3-7-sonnet-20250219"",
                system=""You are a codebase context builder. Use the available tools to search, filter and determine which files in the codebase are relevant to the prompt (user query)."",
                messages=messages,
                tools=TOOLS,
                max_tokens=4000,
                thinking={
                    ""type"": ""enabled"",
                    ""budget_tokens"": 2000
                },
            )

            # Extract thinking block and other content
            thinking_block = None
            tool_use_block = None
            text_block = None
            
            if response.content:
                # Get the message content
                for content_block in response.content:
                    if content_block.type == ""thinking"":
                        thinking_block = content_block
                        previous_thinking = thinking_block
                    elif content_block.type == ""tool_use"":
                        tool_use_block = content_block
                        # Access the proper attributes directly
                        tool_name = content_block.name
                        tool_input = content_block.input
                        tool_id = content_block.id
                    elif content_block.type == ""text"":
                        text_block = content_block
                        console.print(f""[cyan]Model response:[/cyan] {content_block.text}"")
                
                # Handle text responses if there was no tool use
                if not tool_use_block and text_block:
                    messages.append({
                        ""role"": ""assistant"", 
                        ""content"": [
                            *([thinking_block] if thinking_block else []), 
                            {""type"": ""text"", ""text"": text_block.text}
                        ]
                    })
                    break_loop = True
                    continue
                
                # We need a tool use block to proceed
                if tool_use_block:
                    console.print(
                        f""[blue]Tool Call:[/blue] {tool_name}({json.dumps(tool_input, indent=2)})""
                    )

                    try:
                        # Execute the appropriate tool based on name
                        if tool_name == ""git_list_files"":
                            directory = tool_input.get(""directory"", args.directory)
                            globs = tool_input.get(""globs"", args.globs)
                            extensions = tool_input.get(""extensions"", args.extensions)
                            result = git_list_files(
                                reasoning=tool_input[""reasoning""],
                                directory=directory,
                                globs=globs,
                                extensions=extensions,
                            )
                        elif tool_name == ""check_file_paths_line_length"":
                            result = check_file_paths_line_length(
                                reasoning=tool_input[""reasoning""],
                                file_paths=tool_input[""file_paths""],
                                file_line_limit=args.file_line_limit,
                            )
                        elif tool_name == ""determine_if_files_are_relevant"":
                            result = determine_if_files_are_relevant(
                                reasoning=tool_input[""reasoning""],
                                file_paths=tool_input[""file_paths""],
                            )
                        elif tool_name == ""add_relevant_files"":
                            result = add_relevant_files(
                                reasoning=tool_input[""reasoning""],
                                file_paths=tool_input[""file_paths""],
                            )
                            # Check if we've reached the limit
                            if len(RELEVANT_FILES) >= args.limit:
                                console.print(f""[green]Reached file limit of {args.limit}. Stopping.[/green]"")
                                break_loop = True
                        else:
                            raise Exception(f""Unknown tool call: {tool_name}"")

                        console.print(
                            f""[blue]Tool Call Result:[/blue] {tool_name}(...)""
                        )

                        # Append the tool result to messages
                        messages.append(
                            {
                                ""role"": ""assistant"",
                                ""content"": [
                                    *([thinking_block] if thinking_block else []),
                                    {
                                        ""type"": ""tool_use"",
                                        ""id"": tool_id,
                                        ""name"": tool_name,
                                        ""input"": tool_input
                                    }
                                ]
                            }
                        )

                        messages.append(
                            {
                                ""role"": ""user"",
                                ""content"": [
                                    {
                                        ""type"": ""tool_result"",
                                        ""tool_use_id"": tool_id,
                                        ""content"": json.dumps(result)
                                    }
                                ]
                            }
                        )

                    except Exception as e:
                        error_msg = f""Error executing {tool_name}: {e}""
                        console.print(f""[red]{error_msg}[/red]"")

                        # Append the error to messages
                        messages.append(
                            {
                                ""role"": ""assistant"",
                                ""content"": [
                                    *([thinking_block] if thinking_block else []),
                                    {
                                        ""type"": ""tool_use"",
                                        ""id"": tool_id,
                                        ""name"": tool_name,
                                        ""input"": tool_input
                                    }
                                ]
                            }
                        )

                        messages.append(
                            {
                                ""role"": ""user"",
                                ""content"": [
                                    {
                                        ""type"": ""tool_result"",
                                        ""tool_use_id"": tool_id,
                                        ""content"": str(error_msg)
                                    }
                                ]
                            }
                        )

        except Exception as e:
            console.print(f""[red]Error in agent loop: {str(e)}[/red]"")
            raise e

    # Print the final list of relevant files
    console.rule(""[green]Relevant Files[/green]"")
    for i, file_path in enumerate(RELEVANT_FILES, 1):
        console.print(f""{i}. {file_path}"")
",sfa_codebase_context_agent_v3.py,
survived,"def check_file_paths_line_length(reasoning: str, file_paths: List[str], file_line_limit: int = 500) -> Dict[str, int]:
    """"""Checks the line length of each file and returns a dictionary of file paths and their line counts.
    
    Args:
        reasoning: Explanation of why we're checking line lengths
        file_paths: List of file paths to check
        file_line_limit: Maximum number of lines per file
        
    Returns:
        Dictionary mapping file paths to their total line counts
    """"""
    try:
        console.log(f""[blue]Check File Paths Line Length Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Checking {len(file_paths)} files with line limit {file_line_limit}[/dim]"")
        
        result = {}
        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    if line_count <= file_line_limit:
                        result[file_path] = line_count
                    else:
                        console.log(f""[yellow]Skipping {file_path}: {line_count} lines exceed limit of {file_line_limit}[/yellow]"")
            except Exception as e:
                console.log(f""[red]Error reading file {file_path}: {str(e)}[/red]"")
        
        console.log(f""[dim]Found {len(result)} files within line limit[/dim]"")
        return result
    except Exception as e:
        console.log(f""[red]Error checking file paths: {str(e)}[/red]"")
        return {}
",sfa_codebase_context_agent_v3.py,
survived,"def _find_json_loaders(data: Any, path: List[str]) -> List[JsonLoaderNode]:  # noqa: ANN401
    logger = main_logger
    loaders: List[JsonLoaderNode] = []
    if isinstance(data, dict):
        if ""type"" in data and data[""type""] == ""JsonFileSchemaLoader"":
            ref = f""#/{'/'.join(path)}""
            if ""file_path"" in data:
                loaders.append(JsonLoaderNode(ref, data[""file_path""]))
            else:
                logger.info(f""    !! JsonFileSchemaLoader missing file_path: {ref}"")
        else:
            for key, value in data.items():
                loaders += _find_json_loaders(value, path + [key])
    elif isinstance(data, list):
        for i, value in enumerate(data):
            loaders += _find_json_loaders(value, path + [f""Array[{str(i)}]""])
    return loaders
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,
survived,"    async def _run(self) -> StepResult:
        connector = self.context.connector
        python_path = connector.python_source_dir_path
        schemas_path = python_path / SCHEMAS_DIR_NAME
        logger = self.logger

        manifest = connector.manifest_path.read_text()

        if manifest.find(""JsonFileSchemaLoader"") != -1:
            return StepResult(
                step=self,
                status=StepStatus.SKIPPED,
                stderr=""Skipping: the manifest is still using JSON Schema loader."",
            )

        if schemas_path.exists():
            logger.info(f""    Removing schemnas dir: {schemas_path}"")
            shutil.rmtree(schemas_path)

        return StepResult(step=self, status=StepStatus.SUCCESS)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RemoveUnusedJsonSchamas
survived,"def copy_directory(src: Path, dest: Path) -> None:
    if dest.exists():
        shutil.rmtree(dest)
    shutil.copytree(src, dest)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,
survived,"    def test_multiple_human_input_rounds(self, mock_input):
        """"""Test multiple rounds of human input with Flow status management.""""""
        from crewai.agents.agent_builder.base_agent_executor_mixin import CrewAgentExecutorMixin
        
        executor = CrewAgentExecutorMixin()
        executor.crew = MagicMock()
        executor.crew._train = False
        executor._printer = MagicMock()
        
        formatter = event_listener.formatter
        
        original_paused_state = formatter._live_paused
        
        try:
            pause_calls = []
            resume_calls = []
            
            def track_pause():
                pause_calls.append(True)
                
            def track_resume():
                resume_calls.append(True)
            
            with patch.object(formatter, 'pause_live_updates', side_effect=track_pause), \
                 patch.object(formatter, 'resume_live_updates', side_effect=track_resume):
                
                result1 = executor._ask_human_input(""Test result 1"")
                assert result1 == 'feedback'
                
                result2 = executor._ask_human_input(""Test result 2"")
                assert result2 == ''
                
                assert len(pause_calls) == 2
                assert len(resume_calls) == 2
        finally:
            formatter._live_paused = original_paused_state
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration
survived,"    def fingerprint(self) -> Fingerprint:
        """"""
        Get the crew's fingerprint.

        Returns:
            Fingerprint: The crew's fingerprint
        """"""
        return self.security_config.fingerprint
",src/crewai/crew.py,Crew
survived,"    def _generate_uuid(cls, seed: str) -> str:
        """"""
        Generate a deterministic UUID based on a seed string.

        Args:
            seed (str): The seed string to use for UUID generation

        Returns:
            str: A string representation of the UUID consistently generated from the seed
        """"""
        if not isinstance(seed, str):
            raise ValueError(""Seed must be a string"")
        
        if not seed.strip():
            raise ValueError(""Seed cannot be empty or whitespace"")
            
        # Create a deterministic UUID using v5 (SHA-1)
        # Custom namespace for CrewAI to enhance security
        CREW_AI_NAMESPACE = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')
        return str(uuid.uuid5(CREW_AI_NAMESPACE, seed))
",src/crewai/security/fingerprint.py,Fingerprint
deleted,"    async def init_runtime_embeddings_model(
        self,
        model_info: persistence_model.EmbeddingsModel | sqlalchemy.Row[persistence_model.EmbeddingsModel] | dict,
    ):
        """"""ÂàùÂßãÂåñËøêË°åÊó∂ Embeddings Ê®°Âûã""""""
        if isinstance(model_info, sqlalchemy.Row):
            model_info = persistence_model.EmbeddingsModel(**model_info._mapping)
        elif isinstance(model_info, dict):
            model_info = persistence_model.EmbeddingsModel(**model_info)

        requester_inst = self.requester_dict[model_info.requester](ap=self.ap, config=model_info.requester_config)

        await requester_inst.initialize()

        runtime_embeddings_model = requester.RuntimeEmbeddingsModel(
            model_entity=model_info,
            token_mgr=token.TokenManager(
                name=model_info.uuid,
                tokens=model_info.api_keys,
            ),
            requester=requester_inst,
        )

        return runtime_embeddings_model
",pkg/provider/modelmgr/modelmgr.py,ModelManager
survived,"def mock_catalog(mock_stream):
    """"""Create a mock AirbyteCatalog for testing.""""""
    catalog = Mock(spec=AirbyteCatalog)
    catalog.streams = [mock_stream]
    return catalog
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"def test_set_primary_keys(input_keys, expected_output):
    """"""Test that set_primary_keys properly converts and updates the primary key overrides.""""""
    with patch.object(Source, ""_discover"", return_value=Mock()):
        source = Source(executor=Mock(), name=""test-source"")

        source.set_primary_keys(kwargs=input_keys)

        assert source._primary_key_overrides == expected_output

        update_keys = {""stream3"": ""pk3""}
        expected_after_update = expected_output.copy()
        expected_after_update[""stream3""] = [""pk3""]

        source.set_primary_keys(kwargs=update_keys)
        assert source._primary_key_overrides == expected_after_update
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"    def default_tool(input_text: str) -> str:
        """"""A default tool.""""""
        return f""Result: {input_text}""
",tests/tools/test_tool_usage_limit.py,
survived,"        def _run(self, input_text: str) -> str:
            return f""Processed {input_text}""
",tests/tools/test_tool_usage_limit.py,UnlimitedTool
survived,"    def save_cache(self, cache: Cache) -> None:
        key = f""{cache.cache_type}_{cache.hash}""
        self.saved_caches[key] = cache
",tests/_save/loaders/test_loader.py,MockLoader
survived,"    def test_load_cache(self) -> None:
        """"""Test the load_cache method.""""""
        loader = MockPersistenceLoader(""test"", self.save_path)
        
        # Create and save a cache
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        loader.saved_caches[""Pure_hash1""] = cache
        
        # Create a placeholder file to trigger cache_hit
        cache_path = loader.build_path(""hash1"", ""Pure"")
        with open(cache_path, ""w"") as f:
            f.write(""placeholder"")
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")",tests/_save/loaders/test_loader.py,TestBasePersistenceLoader
survived,"def create_project_toml(plugin_dir: Path, plugin_name: str, is_evm: bool) -> None:
    """"""Create the pyproject.toml file for the plugin.""""""
    # Base dependencies
    dependencies = '''python = ""^3.10""
goat-sdk = ""^0.1.0""'''
    
    # Add EVM dependency if needed
    if is_evm:
        dependencies += '\ngoat-sdk-wallet-evm = ""^0.1.0""'
    
    # Dev dependencies
    dev_dependencies = '''ruff = ""^0.8.6""
goat-sdk = { path = ""../../goat-sdk"", develop = true }'''
    
    # Add EVM dev dependency if needed
    if is_evm:
        dev_dependencies += '\ngoat-sdk-wallet-evm = { path = ""../../wallets/evm"", develop = true }'
    
    toml_content = f'''[tool.poetry]
name = ""goat-sdk-plugin-{plugin_name}""
version = ""0.1.0""
description = ""Goat plugin for {plugin_name}""
authors = [""Your Name <your_email@example.com>""]
readme = ""README.md""
keywords = [""goat"", ""sdk"", ""agents"", ""ai"", ""{plugin_name}""]
homepage = ""https://ohmygoat.dev/""
repository = ""https://github.com/goat-sdk/goat""
packages = [
    {{ include = ""goat_plugins/{plugin_name}"" }},
]

[tool.poetry.dependencies]
{dependencies}

[tool.poetry.group.test.dependencies]
pytest = ""^8.3.4""
pytest-asyncio = ""^0.25.0""

[tool.poetry.urls]
""Bug Tracker"" = ""https://github.com/goat-sdk/goat/issues""

[tool.pytest.ini_options]
addopts = [
  ""--import-mode=importlib"",
]
pythonpath = ""src""
asyncio_default_fixture_loop_scope = ""function""

[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""

[tool.poetry.group.dev.dependencies]
{dev_dependencies}

[tool.ruff]
line-length = 120
target-version = ""py312""
'''
    
    with open(plugin_dir / ""pyproject.toml"", ""w"") as f:
        f.write(toml_content)
",python/create_plugin.py,
survived,"    def send_transaction(self, transaction: SolanaTransaction) -> Dict[str, str]:
        """"""Send a transaction on the Solana chain.

        Args:
            transaction: Transaction parameters including instructions and optional lookup tables

        Returns:
            Dict containing the transaction hash
        """"""
        pass
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaWalletClient
survived,"    def __init__(self):
        pass
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaOptions
survived,"    def get_address(self) -> str:
        """"""Get the wallet's public address.""""""
        pass
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaWalletClient
survived,"    def _get_body_content(self, driver, return_html):
        body_element = driver.find_element(By.TAG_NAME, ""body"")

        return (
            body_element.get_attribute(""outerHTML"")
            if return_html
            else body_element.text
        )
",crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py,SeleniumScrapingTool
survived,"def test_scrape_with_css_selector(_mocked_chrome_driver):
    html_content = ""<html><body><div>test content</div><div class='test'>test content in a specific div</div></body></html>""
    mock_driver = mock_driver_with_html(html_content)
    tool = initialize_tool_with(mock_driver)

    result = tool._run(website_url=""https://example.com"", css_element=""div.test"")

    assert ""test content in a specific div"" in result
    mock_driver.get.assert_called_once_with(""https://example.com"")
    mock_driver.find_elements.assert_called_with(""css selector"", ""div.test"")
    mock_driver.close.assert_called_once()
",tests/tools/selenium_scraping_tool_test.py,
survived,"    def fingerprint(self) -> Fingerprint:
        """"""
        Get the crew's fingerprint.

        Returns:
            Fingerprint: The crew's fingerprint
        """"""
        return self.security_config.fingerprint
",src/crewai/crew.py,Crew
survived,"    def validate_fingerprint(cls, values):
        """"""Ensure fingerprint is properly initialized.""""""
        if isinstance(values, dict):
            # Handle case where fingerprint is not provided or is None
            if 'fingerprint' not in values or values['fingerprint'] is None:
                values['fingerprint'] = Fingerprint()
            # Handle case where fingerprint is a string (seed)
            elif isinstance(values['fingerprint'], str):
                if not values['fingerprint'].strip():
                    raise ValueError(""Fingerprint seed cannot be empty"")
                values['fingerprint'] = Fingerprint.generate(seed=values['fingerprint'])
        return values
",src/crewai/security/security_config.py,SecurityConfig
survived,"    def validate_fingerprint(cls, values):
        """"""Ensure fingerprint is properly initialized.""""""
        if isinstance(values, dict):
            # Handle case where fingerprint is not provided or is None
            if 'fingerprint' not in values or values['fingerprint'] is None:
                values['fingerprint'] = Fingerprint()
            # Handle case where fingerprint is a string (seed)
            elif isinstance(values['fingerprint'], str):
                values['fingerprint'] = Fingerprint.generate(seed=values['fingerprint'])
        return values
",src/crewai/security/security_config.py,SecurityConfig
survived,"    def validate_fingerprint(cls, values):
        """"""Ensure fingerprint is properly initialized.""""""
        if isinstance(values, dict):
            # Handle case where fingerprint is not provided or is None
            if 'fingerprint' not in values or values['fingerprint'] is None:
                values['fingerprint'] = Fingerprint()
            # Handle case where fingerprint is a string (seed)
            elif isinstance(values['fingerprint'], str):
                if not values['fingerprint'].strip():
                    raise ValueError(""Fingerprint seed cannot be empty"")
                values['fingerprint'] = Fingerprint.generate(seed=values['fingerprint'])
        return values
",src/crewai/security/security_config.py,SecurityConfig
survived,"    def __init__(self, base_url, max_pages=500, timeout=10, delay=0.5):
        self.base_url = base_url.rstrip('/')
        self.domain = urlparse(base_url).netloc
        self.max_pages = max_pages
        self.timeout = timeout
        self.delay = delay
        
        self.visited_pages = set()
        self.checked_links = set()
        self.dead_links = []
        self.pages_to_visit = deque([base_url])
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; DeadLinkChecker/1.0)'
        })
",scripts/check_dead_links.py,DeadLinkChecker
survived,"    def normalize_url(self, url):
        """"""Normalize URL for comparison.""""""
        parsed = urlparse(url)
        normalized = f""{parsed.scheme}://{parsed.netloc}{parsed.path}""
        if parsed.query:
            normalized += f""?{parsed.query}""
        return normalized
",scripts/check_dead_links.py,DeadLinkChecker
survived,"def generate_tools_code_sync(tools: List[ToolDefinition]) -> str:
    """"""
    Synchronous wrapper for generate_tools_code.
    
    Args:
        tools: List of tool definitions
        
    Returns:
        Python code implementing all tools
    """"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(generate_tools_code(tools))
",meta_agent/generators/tool_generator.py,
survived,"    def __init__(self) -> None:
        self.messages: list[tuple[str, dict]] = []
",tests/_messaging/test_console_output_worker.py,MockStream
survived,"    def test_noop_stream(self) -> None:
        # Test that NoopStream implements Stream
        stream = NoopStream()

        # Should not raise any exceptions
        stream.write(""test_op"", {""key"": ""value""})
        stream.stop()

        # cell_id should be None by default
        assert stream.cell_id is None

        # Set cell_id
        stream.cell_id = ""test_cell""
        assert stream.cell_id == ""test_cell""
",tests/_messaging/test_types.py,TestStream
deleted,"    def test_marimo_ancestor_stopped_error(self) -> None:
        error = MarimoAncestorStoppedError(
            msg=""Execution stopped by ancestor"",
            raising_cell=""cell1"",
        )

        # Test properties
        assert error.type == ""ancestor-stopped""
        assert error.describe() == ""Execution stopped by ancestor""
        assert error.raising_cell == ""cell1""
",tests/_messaging/test_errors.py,TestErrorClasses
survived,"    def test_post_init_strips_trailing_quotes(self) -> None:
        # Test that __post_init__ strips trailing quotes
        option1 = CompletionOption(
            name='test_string""',
            type=""string"",
            completion_info=None,
        )
        assert option1.name == ""test_string""

        option2 = CompletionOption(
            name=""test_string'"",
            type=""string"",
            completion_info=None,
        )
        assert option2.name == ""test_string""

        # Test with multiple quotes
        option3 = CompletionOption(
            name='test_string""""""',
            type=""string"",
            completion_info=None,
        )
        assert option3.name == 'test_string'

        # Test with no quotes
        option4 = CompletionOption(
            name=""test_string"",
            type=""string"",
            completion_info=None,
        )
        assert option4.name == ""test_string""",tests/_messaging/test_completion_option.py,TestCompletionOption
survived,"        def __init__(self) -> None:
            self.written_data: list[tuple[str, KnownMimeType]] = []
",tests/_messaging/test_types.py,TestStdoutStderr.MockStderr
survived,"    def test_print_override_with_thread_no_context(self) -> None:
        # Test print_override when in a marimo thread but no context
        thread_id = threading.get_ident()
        THREADS.add(thread_id)

        try:
            with patch(""marimo._messaging.print_override._original_print"") as mock_print:
                with patch(
                    ""marimo._messaging.print_override.get_context"",
                    side_effect=ContextNotInitializedError,
                ):
                    print_override(""Hello, world!"")

                    # Original print should be called as a fallback
                    mock_print.assert_called_once_with(""Hello, world!"")
        finally:
            # Clean up
            if thread_id in THREADS:
                THREADS.remove(thread_id)
",tests/_messaging/test_print_override.py,TestPrintOverride
survived,"    def test_add_output_to_buffer_merge(self) -> None:
        # Test merging output for an existing cell with same stream and mimetype
        outputs_buffered_per_cell = {
            ""cell1"": [
                ConsoleMsg(
                    stream=CellChannel.STDOUT,
                    cell_id=""cell1"",
                    data=""Hello"",
                    mimetype=""text/plain"",
                )
            ]
        }
        msg = ConsoleMsg(
            stream=CellChannel.STDOUT,
            cell_id=""cell1"",
            data="" World"",
            mimetype=""text/plain"",
        )

        _add_output_to_buffer(msg, outputs_buffered_per_cell)

        assert len(outputs_buffered_per_cell[""cell1""]) == 1
        assert outputs_buffered_per_cell[""cell1""][0].data == ""Hello World""
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker
survived,"    def test_kernel_message_type(self) -> None:
        # Test that KernelMessage can be used as a type annotation
        def accepts_kernel_message(message: KernelMessage) -> KernelMessage:
            return message

        # Create a valid kernel message
        message: KernelMessage = (""test_op"", {""key"": ""value""})

        assert accepts_kernel_message(message) == message",tests/_messaging/test_types.py,TestKernelMessage
deleted,"    def test_http_request_context_manager_with_none(self) -> None:
        # Test that http_request_context can set None
        with http_request_context(None):
            # Request should be None within the context
            ctx_request = HTTP_REQUEST_CTX.get()
            assert ctx_request is None

        # Request should be unset outside the context
        with pytest.raises(LookupError):
            HTTP_REQUEST_CTX.get()
",tests/_messaging/test_context.py,TestHTTPRequestContext
survived,"def test_print_experimental_features() -> None:
    """"""Test the print_experimental_features function.""""""
    # Test with no experimental features
    with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
        config = merge_default_config({})
        print_experimental_features(config)
        mock_print_tabbed.assert_not_called()
    
    # Test with experimental features that have been released
    with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
        config = merge_default_config({""experimental"": {""rtc"": True, ""chat_sidebar"": True}})
        print_experimental_features(config)
        mock_print_tabbed.assert_not_called()
    
    # Test with experimental features that have not been released
    with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
        with patch(""marimo._server.print._utf8"") as mock_utf8:
            mock_utf8.return_value = ""UTF8_EMOJI""
            with patch(""marimo._server.print.green"") as mock_green:
                mock_green.return_value = ""GREEN_TEXT""
                config = merge_default_config({""experimental"": {""new_feature"": True}})
                print_experimental_features(config)
                mock_print_tabbed.assert_called_once()
                mock_utf8.assert_called_once_with(""üß™"")
                mock_green.assert_called_once_with(""Experimental features (use with caution)"")",tests/_server/test_print.py,
survived,"def test_smart_wallet_balance(smart_api, test_wallet_options, test_keypair):
    """"""Test getting wallet balance.""""""
    # Create wallet and client
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        test_wallet_options[""chain""],
        test_keypair,
        test_wallet_options[""provider""],
        test_wallet_options[""options""][""ensProvider""]
    )
    
    # Get balance
    balance = client.balance_of(wallet[""address""])
    assert ""value"" in balance
    assert ""symbol"" in balance
    assert balance[""symbol""] == ""ETH""
    assert ""decimals"" in balance
    assert balance[""decimals""] == 18
    assert ""name"" in balance
    assert balance[""name""] == ""Ethereum""
    assert ""in_base_units"" in balance
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def test_base_url_configuration(custodial_api):
    """"""Test base URL configuration.""""""
    assert custodial_api.base_url == ""https://staging.crossmint.com/api/v1-alpha2""
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def test_user_id():
    """"""Fixture providing test user ID for wallet creation.""""""
    return 12345",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_url_encoding_special_chars(custodial_api):
    """"""Test URL parameter encoding with special characters.""""""
    special_chars = ""test:user+@example.com""
    encoded = quote(special_chars)
    with pytest.raises(Exception) as exc:
        custodial_api.get_wallet(f""email:{encoded}:solana-custodial-wallet"")
    # Verify the special characters were properly encoded
    assert ""+"" not in str(exc.value)
    assert ""@"" not in str(exc.value)
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def test_solana_transaction():
    """"""Fixture providing a test Solana transaction.""""""
    return {
        ""instructions"": []  # Empty instructions for basic test
    }
",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_request_timeout_handling(custodial_api):
    """"""Test request timeout handling.""""""
    with pytest.raises(Exception) as exc:
        custodial_api._request(
            ""/wallets"",
            method=""GET"",
            timeout=0.001  # Very short timeout
        )
    assert ""timeout"" in str(exc.value).lower() or ""timed out"" in str(exc.value).lower()",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def run():
    source = SourceBoxDataExtract()
    launch(source, sys.argv[1:])",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/run.py,
survived,"    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:
        """"""
        :return: string if single primary key, list of strings if composite primary key, list of list of strings if composite primary key consisting of nested fields.
          If the stream has no primary keys, return None.
        """"""
        return ""id""
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIAskFolder
survived,"def create_account_agent() -> Agent:
    """"""
    Create an account management agent.
    
    Returns:
        An Agent instance specialized in account management.
    """"""
    instructions = """"""
    You are an account management specialist who can help customers with account-related issues.
    You can assist with questions about account creation, profile updates, security settings, and account recovery.
    Always prioritize account security and verify the customer's identity before making changes.
    Provide clear guidance on how customers can manage their account settings.
    """"""
    
    return Agent(
        name=""AccountManager"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for account management, profile updates, or security questions.""
    )
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"def create_blog_agent() -> Agent:
    """"""
    Create a blog agent that writes engaging blog posts.
    
    Returns:
        An Agent instance specialized in blog writing.
    """"""
    instructions = """"""
    You are a blog writing specialist who excels at creating engaging, informative blog posts.
    Your task is to:
    1. Understand the blog request and research provided
    2. Use the generate_blog_outline tool to create a structured outline
    3. Write a comprehensive blog post based on the outline and research
    4. Use the format_blog_as_markdown tool to format the post properly
    5. Ensure the blog is engaging, informative, and well-structured
    
    Your blog posts should be conversational yet informative, with a clear introduction,
    well-developed body sections, and a compelling conclusion.
    """"""
    
    # Create the blog agent with function tools
    return Agent(
        name=""BlogSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[generate_blog_outline, format_blog_as_markdown],
        handoff_description=""Use this agent to write engaging blog posts based on research.""
    )
",openai-agents-examples/13_research_blog_system.py,
survived,"async def run_protected_agent(prompt: str) -> str:
    """"""
    Run the protected agent with the given prompt.
    
    Args:
        prompt: The user's query or prompt
        
    Returns:
        The agent's response as a string, or a rejection message if the input is filtered
    """"""
    # Create the protected agent
    agent = create_protected_agent()
    
    try:
        # Run the agent with the prompt
        result = await Runner.run(agent, prompt)
        return result.final_output
    except Exception as e:
        # Check if it's a guardrail rejection
        if ""guardrail rejected"" in str(e).lower():
            return f""Input rejected by guardrails: {str(e)}""
        # Other exception
        return f""Error: {str(e)}""
",openai-agents-examples/10_agent_with_guardrails.py,
survived,"def test_custom_tools():
    """"""Test that the custom tools work correctly.""""""
    # Test currency conversion
    currency_result = convert_currency(CurrencyConversionInput(
        amount=100,
        from_currency=""USD"",
        to_currency=""EUR""
    ))
    assert ""USD"" in currency_result
    assert ""EUR"" in currency_result
    
    # Test stock price
    stock_result = get_stock_price(StockPriceInput(symbol=""AAPL""))
    assert ""AAPL"" in stock_result
    assert ""$"" in stock_result
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def create_travel_assistant() -> Agent:
    """"""
    Create a travel assistant agent with function tools.
    
    Returns:
        An Agent instance with function tools for travel assistance.
    """"""
    instructions = """"""
    You are a helpful travel assistant that can provide information about weather, 
    distances between locations, and current time.
    Use the tools available to you to provide accurate information when asked.
    If you don't have a tool for the specific request, acknowledge the limitations
    and provide the best information you can.
    """"""
    
    # Create the agent with function tools
    return Agent(
        name=""TravelAssistant"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[get_current_weather, calculate_distance, get_current_time]
    )
",openai-agents-examples/05_agent_with_function_tools.py,
survived,"def test_research_tools():
    """"""Test that the research tools work correctly.""""""
    # Test search tool
    search_result = search_for_information(""artificial intelligence ethics"")
    assert ""ethics"" in search_result.lower()
    assert ""principles"" in search_result.lower()
    
    # Test analysis tool
    analysis_result = analyze_topic(""artificial intelligence ethics"")
    assert ""analysis"" in analysis_result.lower()
    assert ""key aspects"" in analysis_result.lower()
",openai-agents-examples/13_research_blog_system.py,
survived,"def run_sync_agent(prompt: str, agent: Optional[Agent] = None) -> str:
    """"""
    Run an agent synchronously with the given prompt.
    
    Args:
        prompt: The user's query or prompt
        agent: Optional pre-configured agent. If None, a health advisor agent is created.
        
    Returns:
        The agent's response as a string
    """"""
    # Create agent if not provided
    if agent is None:
        agent = create_health_agent()
    
    # Run the agent synchronously with the prompt
    result = Runner.run_sync(agent, prompt)
    
    # Return the response
    return result.final_output
",openai-agents-examples/03_sync_agent.py,
survived,"def create_billing_agent() -> Agent:
    """"""
    Create a billing specialist agent.
    
    Returns:
        An Agent instance specialized in billing issues.
    """"""
    instructions = """"""
    You are a billing specialist who can help customers with billing-related issues.
    You can assist with questions about invoices, payment methods, refunds, and subscription plans.
    Be helpful, clear, and concise in your responses.
    Always verify the customer's information before providing specific account details.
    """"""
    
    return Agent(
        name=""BillingSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for questions about billing, payments, invoices, or subscription issues.""
    )
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"    def __init__(self):
        """"""Initialize the content moderation guardrail.""""""
        # List of terms to filter out (simplified for example purposes)
        self.filtered_terms = [
            ""hack"", ""exploit"", ""bypass"", ""illegal"", ""steal"", ""attack"",
            ""malware"", ""virus"", ""phishing"", ""scam"", ""fraud""
        ]
",openai-agents-examples/10_agent_with_guardrails.py,ContentModerationGuardrail
survived,"def test_create_protected_agent():
    """"""Test that the protected agent is created with the correct configuration.""""""
    agent = create_protected_agent()
    assert agent.name == ""ProtectedAssistant""
    assert ""helpful assistant"" in agent.instructions.lower()
    assert agent.model == ""gpt-4o-mini""
    assert len(agent.input_guardrails) == 2
",openai-agents-examples/10_agent_with_guardrails.py,
survived,"async def test_chat_clear_messages():
    def mock_model(
        messages: List[ChatMessage], config: ChatModelConfig
    ) -> str:
        del messages, config
        return ""Mock response""

    chat = ui.chat(mock_model)
    chat._chat_history = [
        ChatMessage(role=""user"", content=""Hello""),
        ChatMessage(role=""assistant"", content=""Hi there!""),
    ]

    # Simulate clearing messages
    chat._value = []
    assert chat.value == []
    assert chat._chat_history == []
",tests/_plugins/ui/_impl/chat/test_chat.py,
survived,"def test_verify_subdomain():
    assert verify_subdomain(""abcd1234"") is True
    assert verify_subdomain(""12345678"") is True
    assert verify_subdomain(""abcdefgh"") is True
    
    assert verify_subdomain(""short"") is False  # Too short
    assert verify_subdomain(""toolong123456"") is False  # Too long
    assert verify_subdomain(""invalid#$"") is False  # Invalid characters
",backend/tests/test_utils_extended.py,
survived,"async def test_write_basic_file():
    mock_redis = AsyncMock()
    mock_redis.set.return_value = True
    
    subdomain = ""abcd1234""
    await write_basic_file(subdomain, mock_redis)
    
    mock_redis.set.assert_called_once()
    call_args = mock_redis.set.call_args[0]
    assert call_args[0] == f""files:{subdomain}""
    
    file_data = json.loads(call_args[1])
    assert ""index.html"" in file_data
    assert file_data[""index.html""][""status_code""] == 200
    assert isinstance(file_data[""index.html""][""headers""], list)
    assert len(file_data[""index.html""][""headers""]) >= 2  # At least 2 default headers
    
    with patch(""config.config.include_server_domain"", True):
        with patch(""config.config.server_domain"", ""example.com""):
            await write_basic_file(subdomain, mock_redis)
            call_args = mock_redis.set.call_args[0]
            file_data = json.loads(call_args[1])
            headers = file_data[""index.html""][""headers""]
            server_headers = [h for h in headers if h[""header""] == ""Server""]
            assert len(server_headers) == 1
            assert server_headers[0][""value""] == ""example.com""",backend/tests/test_utils_extended.py,
survived,"async def test_websocket_invalid_token():
    with client.websocket_connect(""/api/ws"") as websocket:
        websocket.send_text(""invalid-token"")
        
        data = websocket.receive_json()
        assert data[""cmd""] == ""error""
        assert ""Invalid token"" in data[""msg""]
",backend/tests/test_endpoints.py,
survived,"    def _run(self, api_method: str, instruction: str, **kwargs: Any) -> Any:
        """"""Execute a Stagehand command using the specified API method.
        
        Args:
            api_method: The Stagehand API to use ('act', 'extract', or 'observe')
            instruction: An atomic instruction for Stagehand to execute
            **kwargs: Additional keyword arguments passed to the Stagehand API
            
        Returns:
            The result from the Stagehand API call
            
        Raises:
            ValueError: If an invalid api_method is provided
            RuntimeError: If the Stagehand API call fails
        """"""
        try:
            # Initialize Stagehand with the OpenAI API key
            st = stagehand.Stagehand(api_key=self.api_key)
            
            # Call the appropriate Stagehand API based on the method
            if api_method == ""act"":
                return st.act(instruction)
            elif api_method == ""extract"":
                return st.extract(instruction)
            elif api_method == ""observe"":
                return st.observe(instruction)
            else:
                raise ValueError(f""Unknown api_method: {api_method}"")
                
        except Exception as e:
            raise RuntimeError(f""Stagehand API call failed: {str(e)}"")",crewai_tools/tools/stagehand_tool/stagehand_tool.py,StagehandTool
survived,"def test_solana_smart_wallet_error_handling(smart_api):
    """"""Test error handling for Solana smart wallet operations.""""""
    # Test invalid wallet type
    with pytest.raises(Exception) as exc:
        smart_api.create_wallet(
            wallet_type=""invalid-wallet-type"",
            linked_user=""email:test@example.com""
        )
    assert ""error"" in str(exc.value).lower()
    
    # Test invalid transaction
    wallet = smart_api.create_wallet(
        wallet_type=WalletType.SOLANA_SMART_WALLET,
        linked_user=""email:test@example.com""
    )
    
    # Test with invalid transaction format
    invalid_tx = SolanaSmartWalletTransactionParams(
        transaction=""invalid-transaction""
    )
    with pytest.raises(Exception) as exc:
        smart_api.create_transaction_for_smart_wallet(
            wallet[""address""],
            invalid_tx,
            ""solana""
        )
    assert ""error"" in str(exc.value).lower() or ""invalid"" in str(exc.value).lower()
",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,
survived,"def test_solana_smart_wallet_with_phone(smart_api, test_phone, test_solana_wallet_options):
    """"""Test Solana smart wallet creation with phone.""""""
    wallet = smart_api.create_wallet(
        wallet_type=WalletType.SOLANA_SMART_WALLET,
        linked_user=f""phoneNumber:{test_phone}""
    )
    assert wallet[""type""] == ""solana-smart-wallet""
    assert ""linkedUser"" in wallet
",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,
survived,"    def my_tool_with_default(question: str) -> str:
        """"""This tool uses the default result_as_answer value.""""""
        return question
",tests/tools/test_base_tool.py,
survived,"    def from_dict(cls, data):
        """"""Create a task from dictionary.""""""
        task = cls(
            title=data[""title""],
            description=data.get(""description""),
            user_id=data.get(""user_id""),
            status=data.get(""status"", ""pending""),
            id=data.get(""id"")
        )
        task.created_at = data.get(""created_at"", task.created_at)
        task.updated_at = data.get(""updated_at"", task.updated_at)
        return task",codebase-architectures/vertical-slice-architecture/features/tasks/model.py,Task
survived,"def validate_required_fields(data, required_fields):
    """"""Validate that all required fields are present in the data.""""""
    if not isinstance(data, dict):
        raise ValueError(""Data must be a dictionary"")
    
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        raise ValueError(f""Missing required fields: {', '.join(missing_fields)}"")
    
    return True
",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def delete(self, table_name, item_id):
        """"""Delete an item from a table.""""""
        if table_name not in self.data or item_id not in self.data[table_name]:
            Logger.warning(self.logger, f""Cannot delete: Item with ID {item_id} not found in '{table_name}'"")
            return False
        
        del self.data[table_name][item_id]
        Logger.info(self.logger, f""Deleted item with ID {item_id} from '{table_name}'"")
        return True
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase
survived,"def logout_user(token: str) -> bool:
    """"""
    Logout a user by revoking their token.
    
    Args:
        token: The token to revoke
        
    Returns:
        True if the token was revoked, False otherwise
    """"""
    return revoke_token(token)
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"    def __init__(self, username, email, name=None, id=None):
        self.id = id or generate_id()
        self.username = username
        self.email = email
        self.name = name
        self.created_at = get_timestamp()
        self.updated_at = self.created_at
",codebase-architectures/vertical-slice-architecture/features/users/model.py,User
survived,"def mark_all_notifications_as_read(user_id: str) -> int:
    """"""
    Mark all notifications for a user as read.
    
    Args:
        user_id: The ID of the user
        
    Returns:
        Number of notifications marked as read
    """"""
    if user_id not in NOTIFICATION_STORE:
        return 0
    
    count = 0
    for notification in NOTIFICATION_STORE[user_id]:
        if not notification[""is_read""]:
            notification[""is_read""] = True
            count += 1
    
    return count
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,
survived,"    def delete_category(category_id):
        """"""Delete a category.""""""
        try:
            # Check if category exists
            category_data = db.get(""categories"", category_id)
            if not category_data:
                Logger.warning(app_logger, f""Cannot delete: Category not found: {category_id}"")
                return False
            
            # Check if category has products
            products = db.query(""products"", lambda p: p[""category_id""] == category_id)
            if products:
                raise ValueError(f""Cannot delete category: {len(products)} products are associated with this category"")
            
            # Delete category
            result = db.delete(""categories"", category_id)
            Logger.info(app_logger, f""Deleted category: {category_id}"")
            return result
        except Exception as e:
            Logger.error(app_logger, f""Error deleting category: {str(e)}"", exc_info=True)
            raise",codebase-architectures/layered-architecture/services/category_service.py,CategoryService
survived,"    def process(self, input_result):
        """"""
        Process the data from the input stage.
        
        Args:
            input_result: Result from the input stage
        
        Returns:
            dict: Stage result with processed data and metadata
        """"""
        # Check if input stage had errors
        if input_result[""metadata""][""status""] in [""error"", ""validation_failed""]:
            self.metadata[""status""] = ""skipped""
            self.metadata[""errors""].append(""Input stage had errors, processing skipped"")
            return self._create_result()
        
        # Get data from input stage
        self.data = input_result[""data""]
        self.metadata[""input_metadata""] = input_result[""metadata""]
        
        # Initialize processing
        self.metadata[""status""] = ""processing""
        self.metadata[""started_at""] = datetime.now().isoformat()
        
        return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage
survived,"    def __init__(self):
        """"""Initialize the input stage.""""""
        self.data = None
        self.metadata = {
            ""stage"": ""input"",
            ""status"": ""initialized"",
            ""errors"": []
        }
",codebase-architectures/pipeline-architecture/pipeline/input_stage.py,InputStage
survived,"    def get_by_sku(sku):
        """"""Get a product by SKU.""""""
        try:
            products = db.query(""products"", lambda p: p[""sku""] == sku)
            if not products:
                Logger.warning(app_logger, f""Product with SKU '{sku}' not found"")
                return None
            return products[0]
        except Exception as e:
            Logger.error(app_logger, f""Error getting product by SKU: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/product_service.py,ProductService
survived,"    def format_as_detailed_report(self):
        """"""
        Format the data as a detailed report.
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data to format"")
            return self._create_result()
        
        try:
            # Create detailed report
            report = {
                ""report_type"": ""detailed"",
                ""generated_at"": datetime.now().isoformat(),
                ""data_source"": self.metadata.get(""input_metadata"", {}).get(""source"", ""unknown""),
                ""record_count"": len(self.data) if isinstance(self.data, list) else 1,
                ""data"": self.data
            }
            
            # Add analysis if available
            if self.analysis:
                report[""analysis""] = self.analysis
            
            # Add processing information
            if ""processing_metadata"" in self.metadata:
                report[""processing_info""] = {
                    ""steps"": self.metadata[""processing_metadata""].get(""processing_steps"", []),
                    ""filters"": self.metadata[""processing_metadata""].get(""filters_applied"", []),
                    ""transformations"": self.metadata[""processing_metadata""].get(""transformations_applied"", []),
                    ""processing_time_seconds"": self.metadata[""processing_metadata""].get(""processing_time_seconds"")
                }
            
            # Store the detailed report
            self.detailed_report = report
            
            # Update metadata
            self.metadata[""output_formats""].append(""detailed_report"")
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Detailed report formatting error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/output_stage.py,OutputStage
survived,"    def __init__(self):
        self.data = {}
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"    def update_profile(token: str, profile_data: Dict) -> Dict:
        """"""
        Update a user's profile.
        
        Args:
            token: Authentication token
            profile_data: The profile data to update
            
        Returns:
            Response with success status and updated user data or error message
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Update profile
        success, result = update_user_profile(user_data[""id""], profile_data)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Profile updated successfully"",
                ""data"": result
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": result.get(""error"", ""Profile update failed""),
                ""data"": None
            }
",codebase-architectures/atomic-composable-architecture/endpoints/user_api.py,UserAPI
survived,"    def calculate_statistics(self, numeric_fields=None):
        """"""
        Calculate statistics for numeric fields in the data.
        
        Args:
            numeric_fields: List of field names to calculate statistics for
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data to process"")
            return self._create_result()
        
        try:
            # Determine fields to analyze
            if numeric_fields is None:
                # Try to automatically detect numeric fields
                if isinstance(self.data, list) and len(self.data) > 0:
                    sample = self.data[0]
                    numeric_fields = [
                        field for field, value in sample.items()
                        if isinstance(value, (int, float)) or (
                            isinstance(value, str) and value.replace('.', '', 1).isdigit()
                        )
                    ]
            
            # Calculate statistics
            stats = {}
            if isinstance(self.data, list) and numeric_fields:
                for field in numeric_fields:
                    try:
                        # Extract numeric values
                        values = []
                        for item in self.data:
                            if field in item:
                                value = item[field]
                                if isinstance(value, (int, float)):
                                    values.append(value)
                                elif isinstance(value, str) and value.replace('.', '', 1).isdigit():
                                    values.append(float(value))
                        
                        # Calculate statistics if we have values
                        if values:
                            field_stats = {
                                ""count"": len(values),
                                ""min"": min(values),
                                ""max"": max(values),
                                ""sum"": sum(values),
                                ""mean"": statistics.mean(values),
                                ""median"": statistics.median(values)
                            }
                            
                            # Add standard deviation if we have enough values
                            if len(values) > 1:
                                field_stats[""std_dev""] = statistics.stdev(values)
                            
                            stats[field] = field_stats
                    except Exception as e:
                        self.metadata[""errors""].append(f""Error calculating statistics for field '{field}': {str(e)}"")
            
            # Add statistics to data
            if not hasattr(self, ""analysis""):
                self.analysis = {}
            self.analysis[""statistics""] = stats
            
            # Update metadata
            self.metadata[""processing_steps""].append(""calculate_statistics"")
            self.metadata[""statistics_fields""] = list(stats.keys())
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Statistics calculation error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage
survived,"    def from_dict(cls, data):
        """"""Create a user from dictionary.""""""
        user = cls(
            username=data[""username""],
            email=data[""email""],
            name=data.get(""name""),
            id=data.get(""id"")
        )
        user.created_at = data.get(""created_at"", user.created_at)
        user.updated_at = data.get(""updated_at"", user.updated_at)
        return user",codebase-architectures/vertical-slice-architecture/features/users/model.py,User
survived,"def main():
    """"""Run the application.""""""
    Logger.info(app_logger, ""Starting Layered Architecture Example"")
    
    display_header(""Layered Architecture Example"")
    
    # Create categories
    display_header(""Creating Categories"")
    
    electronics_result = CategoryAPI.create_category(""Electronics"", ""Electronic devices and gadgets"")
    display_result(electronics_result)
    
    books_result = CategoryAPI.create_category(""Books"", ""Books and e-books"")
    display_result(books_result)
    
    clothing_result = CategoryAPI.create_category(""Clothing"", ""Apparel and accessories"")
    display_result(clothing_result)
    
    # Try to create a duplicate category
    duplicate_result = CategoryAPI.create_category(""Electronics"", ""Duplicate category"")
    display_result(duplicate_result)
    
    # Get all categories
    display_header(""All Categories"")
    categories_result = CategoryAPI.get_all_categories()
    display_result(categories_result)
    
    # Create products
    display_header(""Creating Products"")
    
    # Get category IDs
    categories = categories_result[""data""]
    electronics_id = next((c[""id""] for c in categories if c[""name""] == ""Electronics""), None)
    books_id = next((c[""id""] for c in categories if c[""name""] == ""Books""), None)
    
    # Create products
    laptop_result = ProductAPI.create_product(
        ""Laptop"", 
        999.99, 
        electronics_id, 
        ""High-performance laptop"", 
        ""TECH-001""
    )
    display_result(laptop_result)
    
    phone_result = ProductAPI.create_product(
        ""Smartphone"", 
        499.99, 
        electronics_id, 
        ""Latest smartphone model"", 
        ""TECH-002""
    )
    display_result(phone_result)
    
    book_result = ProductAPI.create_product(
        ""Programming Book"", 
        29.99, 
        books_id, 
        ""Learn programming with this book"", 
        ""BOOK-001""
    )
    display_result(book_result)
    
    # Try to create a product with invalid price
    invalid_result = ProductAPI.create_product(
        ""Invalid Product"", 
        ""not-a-price"", 
        electronics_id
    )
    display_result(invalid_result)
    
    # Get products by category
    display_header(""Electronics Products"")
    electronics_products = ProductAPI.get_products_by_category(electronics_id)
    display_result(electronics_products)
    
    # Update a product
    display_header(""Updating a Product"")
    if laptop_result.get(""success"") and ""data"" in laptop_result:
        laptop_id = laptop_result[""data""][""id""]
        update_result = ProductAPI.update_product(
            laptop_id,
            price=899.99,
            description=""High-performance laptop with discount""
        )
        display_result(update_result)
    
    # Try to delete a category with products
    display_header(""Trying to Delete a Category with Products"")
    delete_result = CategoryAPI.delete_category(electronics_id)
    display_result(delete_result)
    
    # Delete a product
    display_header(""Deleting a Product"")
    if phone_result.get(""success"") and ""data"" in phone_result:
        phone_id = phone_result[""data""][""id""]
        delete_product_result = ProductAPI.delete_product(phone_id)
        display_result(delete_product_result)
    
    # Get all products
    display_header(""All Remaining Products"")
    all_products = ProductAPI.get_all_products()
    display_result(all_products)
    
    Logger.info(app_logger, ""Layered Architecture Example completed"")
",codebase-architectures/layered-architecture/main.py,
survived,"def mark_alert_as_read(user_id: str, notification_id: str) -> bool:
    """"""
    Mark an alert as read.
    
    Args:
        user_id: The ID of the user
        notification_id: The ID of the notification
        
    Returns:
        True if the alert was marked as read, False otherwise
    """"""
    return mark_notification_as_read(user_id, notification_id)
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,
survived,"    def get_user_tasks(user_id):
        """"""Get all tasks for a specific user.""""""
        all_tasks = db.get_all(""tasks"")
        return [task for task in all_tasks if task.get(""user_id"") == user_id]
",codebase-architectures/vertical-slice-architecture/features/tasks/service.py,TaskService
survived,"def send_sms_notification(phone_number: str, message: str) -> bool:
    """"""
    Send an SMS notification (mock implementation).
    
    Args:
        phone_number: The recipient's phone number
        message: The SMS message
        
    Returns:
        True if the SMS was sent successfully (always True in this mock)
    """"""
    # In a real application, this would send an actual SMS
    print(f""[SMS] To: {phone_number}"")
    print(f""[SMS] Message: {message}"")
    return True
",codebase-architectures/atomic-composable-architecture/modules/notifications.py,
survived,"    def update_user(user_id, user_data):
        """"""Update a user.""""""
        existing_user = db.get(""users"", user_id)
        if not existing_user:
            return None
        
        # Check if username is being changed and already exists
        if ""username"" in user_data and user_data[""username""] != existing_user[""username""]:
            all_users = db.get_all(""users"")
            if any(user[""username""] == user_data[""username""] for user in all_users if user[""id""] != user_id):
                raise ValueError(f""Username '{user_data['username']}' already exists"")
        
        # Update fields
        for key, value in user_data.items():
            if key not in [""id"", ""created_at""]:
                existing_user[key] = value
        
        # Update timestamp
        existing_user[""updated_at""] = get_timestamp()
        
        # Save to database
        db.update(""users"", user_id, existing_user)
        return existing_user
",codebase-architectures/vertical-slice-architecture/features/users/service.py,UserService
survived,"    def add_stage(self, name: str, stage: PipelineStage) -> None:
        """"""
        Add a stage to the pipeline.
        
        Args:
            name: The name of the stage
            stage: The stage to add
        """"""
        self.stages[name] = stage
        self.stage_order.append(name)
        console.log(f""[pipeline] Added stage: {name}"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/pipeline_manager.py,Pipeline
survived,"    def view_file(path: str, view_range=None) -> FileOperationResult:
        """"""
        View the contents of a file.

        Args:
            path: The path to the file to view
            view_range: Optional start and end lines to view [start, end]

        Returns:
            FileOperationResult with content or error message
        """"""
        try:
            # Normalize the path
            path = normalize_path(path)

            if not os.path.exists(path):
                error_msg = f""File {path} does not exist""
                console.log(f""[view_file] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            with open(path, ""r"") as f:
                lines = f.readlines()

            if view_range:
                start, end = view_range
                # Convert to 0-indexed for Python
                start = max(0, start - 1)
                if end == -1:
                    end = len(lines)
                else:
                    end = min(len(lines), end)
                lines = lines[start:end]

            content = """".join(lines)

            # Display the file content (only for console, not returned to Claude)
            display_file_content(path, content)

            return FileOperationResult(True, f""Successfully viewed file {path}"", content)
        except Exception as e:
            error_msg = f""Error viewing file: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[view_file] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/service.py,FileOperationService
survived,"    def to_response(self) -> Dict[str, Any]:
        """"""
        Convert the result to a response for Claude.
        
        Returns:
            Dictionary with result or error to send back to Claude
        """"""
        if self.success:
            return {""result"": self.data if self.data is not None else self.message}
        else:
            return {""error"": self.message}
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,FileOperationResult
deleted,"def file_exists(path: str) -> bool:
    """"""
    Check if a file exists.

    Args:
        path: The path to check

    Returns:
        True if the file exists, False otherwise
    """"""
    return os.path.exists(path) and os.path.isfile(path)",example-agent-codebase-arch/atomic-composable-architecture/atom/path_utils.py,
survived,"def main():
    """"""Main entry point for the application.""""""
    # Set up argument parser
    parser = argparse.ArgumentParser(description=""Claude 3.7 File Editor Agent"")
    parser.add_argument(
        ""--prompt"",
        ""-p"",
        required=True,
        help=""The prompt for what file operations to perform"",
    )
    parser.add_argument(
        ""--max-loops"",
        ""-l"",
        type=int,
        default=15,
        help=""Maximum number of tool use loops (default: 15)"",
    )
    parser.add_argument(
        ""--thinking"",
        ""-t"",
        type=int,
        default=DEFAULT_THINKING_TOKENS,
        help=f""Maximum thinking tokens (default: {DEFAULT_THINKING_TOKENS})"",
    )
    parser.add_argument(
        ""--efficiency"",
        ""-e"",
        action=""store_true"",
        help=""Enable token-efficient tool use (beta feature)"",
    )
    args = parser.parse_args()

    console.print(Panel.fit(""Claude 3.7 File Editor Agent (Pipeline Architecture)""))
    console.print(f""\n[bold]Prompt:[/bold] {args.prompt}\n"")
    console.print(f""[dim]Thinking tokens: {args.thinking}[/dim]"")
    console.print(f""[dim]Max loops: {args.max_loops}[/dim]"")
    
    if args.efficiency:
        console.print(f""[dim]Token-efficient tools: Enabled[/dim]\n"")
    else:
        console.print(f""[dim]Token-efficient tools: Disabled[/dim]\n"")

    # For testing purposes, we'll just print a success message
    console.print(""[green]Successfully loaded the Pipeline Architecture implementation![/green]"")
    console.print(""[yellow]This is a mock implementation for testing the architecture structure.[/yellow]"")
    console.print(""[yellow]In a real implementation, this would connect to the Claude API.[/yellow]"")

    # Display mock token usage
    display_token_usage(1000, 500)
",example-agent-codebase-arch/pipeline-architecture/main.py,
survived,"    def __init__(self, command: str, path: str = None, **kwargs):
        """"""
        Initialize a tool use request.
        
        Args:
            command: The command to execute
            path: The path to operate on
            **kwargs: Additional arguments for the command
        """"""
        self.command = command
        self.path = path
        self.kwargs = kwargs
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,ToolUseRequest
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the result to a dictionary.
        
        Returns:
            Dictionary representation of the result
        """"""
        return {
            ""success"": self.success,
            ""message"": self.message,
            ""data"": self.data
        }
",example-agent-codebase-arch/layered-architecture/models/tool_models.py,FileOperationResult
survived,"    def __init__(self, success: bool, message: str, data: Any = None):
        """"""
        Initialize a file operation result.
        
        Args:
            success: Whether the operation was successful
            message: A message describing the result
            data: Optional data returned by the operation
        """"""
        self.success = success
        self.message = message
        self.data = data
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,FileOperationResult
deleted,"def is_valid_path(path: str) -> bool:
    """"""
    Check if a path is valid.

    Args:
        path: The path to check

    Returns:
        True if the path is valid, False otherwise
    """"""
    return path is not None and path.strip() != """"
",example-agent-codebase-arch/atomic-composable-architecture/atom/path_utils.py,
survived,"def test_create_crew_with_trailing_slash_creates_valid_project(mock_load_env, mock_write_env, mock_copy_template, temp_dir):
    mock_load_env.return_value = {}
    
    with tempfile.TemporaryDirectory() as work_dir:
        os.chdir(work_dir)
        create_crew(""test-project/"", skip_provider=True)
        
        project_path = Path(work_dir) / ""test_project""
        assert project_path.exists()
        assert (project_path / ""src"" / ""test_project"").exists()
        
        mock_copy_template.assert_called()
        copy_calls = mock_copy_template.call_args_list
        
        for call in copy_calls:
            args = call[0]
            if len(args) >= 5:
                folder_name_arg = args[4]
                assert not folder_name_arg.endswith(""/""), f""folder_name should not end with slash: {folder_name_arg}""
",tests/cli/test_create_crew.py,
survived,"    def save_state(
        self,
        flow_uuid: str,
        method_name: str,
        state_data: Union[Dict[str, Any], BaseModel]
    ) -> None:
        """"""Persist the flow state after method completion.
        
        Args:
            flow_uuid: Unique identifier for the flow instance
            method_name: Name of the method that just completed
            state_data: Current state data (either dict or Pydantic model)
        """"""
        pass
",src/crewai/flow/persistence/base.py,FlowPersistence
survived,"def test_xai_create_with_completion():
    """"""Test that create_with_completion works with XAI provider""""""
    client = instructor.from_provider(""xai/grok-3-mini"", mode=instructor.Mode.XAI_JSON)
    
    user, raw_response = client.chat.completions.create_with_completion(
        response_model=User,
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""You are a helpful assistant that extracts information."",
            },
            {
                ""role"": ""user"",
                ""content"": ""Extract: Jason is 25 years old."",
            },
        ],
    )
    
    assert isinstance(user, User)
    assert user.name.lower() == ""jason""
    assert user.age == 25
    assert hasattr(user, ""_raw_response""), (
        ""The raw response should be available from XAI""
    )
    assert raw_response is not None
    assert user._raw_response == raw_response
",tests/llm/test_xai/test_raw_response.py,
survived,"    def call(
        self,
        messages: Union[str, List[Dict[str, str]]],
        tools: Optional[List[dict]] = None,
        callbacks: Optional[List[Any]] = None,
        available_functions: Optional[Dict[str, Any]] = None,
    ) -> Union[str, Any]:
        """"""Call the LLM with the given messages.
        
        Args:
            messages: Input messages for the LLM.
                     Can be a string or list of message dictionaries.
                     If string, it will be converted to a single user message.
                     If list, each dict must have 'role' and 'content' keys.
            tools: Optional list of tool schemas for function calling.
                  Each tool should define its name, description, and parameters.
            callbacks: Optional list of callback functions to be executed
                      during and after the LLM call.
            available_functions: Optional dict mapping function names to callables
                               that can be invoked by the LLM.
            
        Returns:
            Either a text response from the LLM (str) or
            the result of a tool function call (Any).
        """"""
        pass
",src/crewai/llm.py,BaseLLM
survived,"def test_custom_llm_with_jwt_auth():
    """"""Test a custom LLM implementation with JWT authentication.""""""
    jwt_llm = JWTAuthLLM(jwt_token=""example.jwt.token"")
    
    # Test that create_llm returns the JWT-authenticated LLM instance directly
    result_llm = create_llm(jwt_llm)
    
    assert result_llm is jwt_llm
    
    # Test calling the JWT-authenticated LLM
    response = result_llm.call(""Test message"")
    
    # Verify that the JWT-authenticated LLM was called
    assert len(jwt_llm.calls) > 0
    # Verify that the response from the JWT-authenticated LLM was used
    assert response == ""Response from JWT-authenticated LLM""",tests/custom_llm_test.py,
