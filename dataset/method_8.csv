status,method,filepath,class_name
survived,"def _is_private(n: str) -> bool:
    return n.startswith(""_"") and not n.startswith(""__"") and not n.endswith(""__"")
",dev/check_function_signatures.py,
survived,"    def format(self, github: bool = False) -> str:
        message = "" "".join(self.lines)
        if github:
            return f""::warning file={self.file_path},line={self.line},col={self.column}::{message}""
        else:
            return f""{self.file_path}:{self.line}:{self.column}: {message}""
",dev/check_function_signatures.py,Error
survived,"def serialize_for_json(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, torch.Tensor):
        return obj.cpu().numpy().tolist()
    raise TypeError(f""Object of type {obj.__class__.__name__} is not JSON serializable"")",triton_viz/visualizer/draw.py,
survived,"def add_3d_slices(input1, input2, output):
    # Get tensor shapes
    slice_z, slice_y, slice_x = input1.shape

    # Compute strides
    stride_z, stride_y, stride_x = input1.stride()

    # Determine grid size
    grid = (
        triton.cdiv(slice_x, BLOCK_SIZE_X),
        triton.cdiv(slice_y, BLOCK_SIZE_Y),
        triton.cdiv(slice_z, BLOCK_SIZE_Z),
    )

    # Launch kernel
    add_3d_slices_kernel[grid](
        input1,
        input2,
        output,
        stride_x,
        stride_y,
        stride_z,
        slice_x,
        slice_y,
        slice_z,
        BLOCK_SIZE_X=BLOCK_SIZE_X,
        BLOCK_SIZE_Y=BLOCK_SIZE_Y,
        BLOCK_SIZE_Z=BLOCK_SIZE_Z,
    )
",examples/3dims.py,
survived,"def add_3d_slices_kernel(
    input_ptr1,
    input_ptr2,
    output_ptr,
    stride_x,
    stride_y,
    stride_z,
    slice_x,
    slice_y,
    slice_z,
    BLOCK_SIZE_X: tl.constexpr,
    BLOCK_SIZE_Y: tl.constexpr,
    BLOCK_SIZE_Z: tl.constexpr,
):
    # Compute the 3D position in the output tensor
    pid_x = tl.program_id(0)
    pid_y = tl.program_id(1)
    pid_z = tl.program_id(2)

    # Compute the starting position for this block
    x_start = pid_x * BLOCK_SIZE_X
    y_start = pid_y * BLOCK_SIZE_Y
    z_start = pid_z * BLOCK_SIZE_Z

    # Compute offsets within the block
    x_offsets = x_start + tl.arange(0, BLOCK_SIZE_X)
    y_offsets = y_start + tl.arange(0, BLOCK_SIZE_Y)
    z_offsets = z_start + tl.arange(0, BLOCK_SIZE_Z)

    # Create a mask to handle boundary conditions
    mask = (
        (x_offsets < slice_x)
        & (y_offsets < slice_y)[:, None]
        & (z_offsets < slice_z)[:, None, None]
    )

    # Compute the input and output offsets
    offsets = (
        z_offsets[:, None, None] * stride_z
        + y_offsets[:, None] * stride_y
        + x_offsets * stride_x
    )

    # Load input slices
    slice1 = tl.load(input_ptr1 + offsets, mask=mask)
    slice2 = tl.load(input_ptr2 + offsets, mask=mask)

    # Perform addition
    result = slice1 + slice2

    # Store the result
    tl.store(output_ptr + offsets, result, mask=mask)
",examples/3dims.py,
survived,"    def test_1d_array_raises_error(self, func):
        """"""Test that 1D arrays raise an appropriate error.""""""
        data_1d = np.array([1, 2, 3, 4, 5], dtype=np.float64)

        with pytest.raises(ValueError, match=""requires at least a 2D array""):
            func(data_1d)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"    def __init__(
        self,
        func: Callable,
        signature: tuple[list[tuple], str],
        **kwargs,
    ):
        self.signature = signature
        super().__init__(func, **kwargs)
",numbagg/decorators.py,ndmovematrix
survived,"    def test_rolling_comparison_with_pandas(self, move_func, window):
        """"""Compare rolling functions with pandas.""""""
        np.random.seed(42)
        n_vars = 4
        n_obs = 20
        data = np.random.randn(n_vars, n_obs)

        # NumBagg result
        numbagg_result = move_func(data, window=window, min_count=window)

        # Pandas result - need to transpose for pandas (wants observations as rows)
        df = pd.DataFrame(data.T)
        if move_func == move_nancorrmatrix:
            pandas_result = df.rolling(window, min_periods=window).corr()
        else:
            pandas_result = df.rolling(window, min_periods=window).cov()

        # Compare each window
        for t in range(window - 1, n_obs):
            # Extract pandas matrix for this timepoint
            pandas_matrix = pandas_result.loc[t].values
            # Compare
            assert_allclose(numbagg_result[t], pandas_matrix, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"def move_nancovmatrix(a, window, min_count, out):
    """"""
    Moving window covariance matrix gufunc.

    For 2D input, computes covariance between variables (rows) across observations (columns in the window).
    """"""
    n_vars = a.shape[0]
    n_obs = a.shape[1]
    min_count = max(min_count, 1)

    # Initialize running statistics
    sums = np.zeros(n_vars, dtype=a.dtype)
    counts = np.zeros(n_vars, dtype=np.int64)

    # Initialize pairwise statistics
    prods = np.zeros((n_vars, n_vars), dtype=a.dtype)
    pair_counts = np.zeros((n_vars, n_vars), dtype=np.int64)

    for t in range(n_obs):
        # Remove old values when window slides
        if t >= window:
            for i in range(n_vars):
                old_val = a[i, t - window]
                if not np.isnan(old_val):
                    sums[i] -= old_val
                    counts[i] -= 1

                    # Update pairwise products
                    for j in range(n_vars):
                        old_val_j = a[j, t - window]
                        if not np.isnan(old_val_j):
                            prods[i, j] -= old_val * old_val_j
                            pair_counts[i, j] -= 1

        # Add new values
        for i in range(n_vars):
            new_val = a[i, t]
            if not np.isnan(new_val):
                sums[i] += new_val
                counts[i] += 1

                # Update pairwise products
                for j in range(n_vars):
                    new_val_j = a[j, t]
                    if not np.isnan(new_val_j):
                        prods[i, j] += new_val * new_val_j
                        pair_counts[i, j] += 1

        # Compute covariance matrix for current window
        for i in range(n_vars):
            for j in range(n_vars):
                n = pair_counts[i, j]
                if n >= min_count:
                    if n > 1:
                        # Unbiased covariance with ddof=1
                        cov = (prods[i, j] - sums[i] * sums[j] / n) / (n - 1)
                        out[t, i, j] = cov
                    else:
                        # n == 1, covariance is undefined (requires at least 2 points)
                        out[t, i, j] = np.nan
                else:
                    out[t, i, j] = np.nan",numbagg/moving_matrix.py,
survived,"    def test_array_alpha(self, func):
        """"""Test with alpha as an array rather than scalar.""""""
        data = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        alpha_array = np.array([0.1, 0.5, 0.9, 0.3])

        result = func(data, alpha=alpha_array)

        # Should work and produce expected shape
        assert result.shape == (4, 2, 2)

        # Should be different from constant alpha
        result_constant = func(data, alpha=0.5)
        assert not np.allclose(result, result_constant)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions
survived,"    def test_correlation_bounds(self):
        """"""Test that correlation values are properly bounded between -1 and 1.""""""
        # Create data with some negative correlation
        np.random.seed(42)
        data = np.random.randn(3, 100)
        data[1] = -data[0] + 0.1 * np.random.randn(100)  # Strong negative correlation

        result = move_exp_nancorrmatrix(data, alpha=0.5)

        # All correlation values should be between -1 and 1
        finite_mask = np.isfinite(result)
        assert np.all(result[finite_mask] >= -1.0)
        assert np.all(result[finite_mask] <= 1.0)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions
survived,"    def test_generate_for_model_migration(self):
        """"""Test model migration YAML generation""""""
        files = [
            'src/models/old_model.py',
            'src/api/old_api.py',
            'tests/test_old.py'
        ]
        
        config = self.generator.generate_for_model_migration(
            'gpt-3.5',
            'gpt-4',
            files
        )
        
        assert config['provider'] == 'claude'
        assert config['metadata']['migration'] == 'gpt-3.5 -> gpt-4'
        assert len(config['tasks']) == 3
        
        # Check migration prompts
        for task in config['tasks']:
            assert 'Migrate code from gpt-3.5 to gpt-4' in task['prompt']
            assert task['file'] in files
        
        assert config['options']['timeout'] == 180  # Longer timeout for migration
        assert config['options']['output_dir'] == './migration-gpt-3.5-to-gpt-4'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _compare_metadata(self, model_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """"""Compare model metadata""""""
        metadata = {}
        
        for model, data in model_data.items():
            model_meta = {
                'has_config': data.get('config') is not None,
                'file_count': len(data.get('files', [])),
                'config_keys': list(data['config'].keys()) if data.get('config') else []
            }
            
            # Extract specific metadata if available
            if data.get('config'):
                config = data['config']
                important_keys = [
                    'model_type', 'architecture', 'license', 
                    'training_data', 'created_by', 'version'
                ]
                
                for key in important_keys:
                    if key in config:
                        model_meta[key] = config[key]
            
            metadata[model] = model_meta
        
        return metadata
",src/haconiwa/scan/comparator.py,ModelComparator
survived,"        def build_tree(node: Dict[str, Any], prefix: str = """", is_last: bool = True):
            """"""Recursively build tree representation""""""
            items = [(k, v) for k, v in node.items() if k != '__files__']
            files = node.get('__files__', [])
            
            # Add directories
            for i, (key, value) in enumerate(items):
                is_last_item = i == len(items) - 1 and not files
                
                connector = ""â””â”€â”€ "" if is_last_item else ""â”œâ”€â”€ ""
                lines.append(f""{prefix}{connector}{key}/"")
                
                if isinstance(value, dict):
                    extension = ""    "" if is_last_item else ""â”‚   ""
                    build_tree(value, prefix + extension, is_last_item)
            
            # Add files
            for i, file_info in enumerate(files):
                is_last_file = i == len(files) - 1
                connector = ""â””â”€â”€ "" if is_last_file else ""â”œâ”€â”€ ""
                
                if isinstance(file_info, dict):
                    name = file_info.get('name', 'Unknown')
                    size = file_info.get('size', 0)
                    size_str = self._format_size(size)
                    lines.append(f""{prefix}{connector}{name} ({size_str})"")
                else:
                    lines.append(f""{prefix}{connector}{file_info}"")
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def save_yaml(self, config: Dict[str, Any], output_path: Path) -> Path:
        """"""Save configuration to YAML file""""""
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
        
        return output_path
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator
survived,"    def test_scan_guide_types(self, runner, temp_model_dir):
        """"""Test different guide types""""""
        guide_types = [""development"", ""usage"", ""integration"", ""quickstart""]
        
        for guide_type in guide_types:
            result = runner.invoke(
                scan_app,
                [""guide"", ""o1-mini"", ""--path"", str(temp_model_dir), ""--type"", guide_type]
            )
            
            assert result.exit_code == 0
            assert ""o1-mini"" in result.stdout
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def _extract_model_name(self, path: Path) -> str:
        """"""Extract model name from path""""""
        # Try to get from config files first
        for config_file in self.config_files:
            config_path = path / config_file
            if config_path.exists():
                try:
                    if config_file.endswith('.json'):
                        with open(config_path, 'r') as f:
                            config = json.load(f)
                            if 'model_name' in config:
                                return config['model_name']
                            if 'name' in config:
                                return config['name']
                except:
                    pass
        
        # Fallback to directory name
        return path.name
",src/haconiwa/scan/analyzer.py,ModelAnalyzer
survived,"    def _format_summary(self, data: Any) -> str:
        """"""Format as a concise summary""""""
        if not isinstance(data, dict):
            return str(data)
        
        lines = [""="" * 50]
        
        # Handle model search results
        if 'model_name' in data:
            lines.append(f""Model Search: {data['model_name']}"")
            lines.append(""="" * 50)
            lines.append(f""Total files found: {data.get('total_files', 0)}"")
            
            if 'matches' in data:
                lines.append(""\nMatches by category:"")
                for category, files in data['matches'].items():
                    lines.append(f""  {category}: {len(files)} files"")
        
        # Handle content search results
        elif 'pattern' in data and 'matches' in data:
            lines.append(f""Content Search: {data['pattern']}"")
            lines.append(""="" * 50)
            lines.append(f""Total matches: {data.get('total_matches', 0)}"")
            lines.append(f""Files searched: {data.get('files_searched', 0)}"")
            
            if data['matches']:
                lines.append(""\nTop matches:"")
                for match in data['matches'][:5]:
                    lines.append(f""  {match['file']}:{match['line_number']}"")
        
        # Handle analysis results
        elif 'categories' in data and 'providers' in data:
            lines.append(""Model Analysis Summary"")
            lines.append(""="" * 50)
            lines.append(f""Base path: {data.get('base_path', 'Unknown')}"")
            lines.append(f""Total models: {data.get('total_models', 0)}"")
            
            if data.get('total_size', 0) > 0:
                size_gb = data['total_size'] / (1024 ** 3)
                lines.append(f""Total size: {size_gb:.2f} GB"")
            
            if 'insights' in data:
                lines.append(""\nInsights:"")
                for insight in data['insights']:
                    lines.append(f""  â€¢ {insight}"")
        
        # Handle list results
        elif isinstance(data, list) and data:
            lines.append(f""Results: {len(data)} items"")
            lines.append(""="" * 50)
            for i, item in enumerate(data[:10], 1):
                if isinstance(item, dict):
                    name = item.get('name', item.get('path', 'Unknown'))
                    lines.append(f""{i}. {name}"")
                else:
                    lines.append(f""{i}. {item}"")
            
            if len(data) > 10:
                lines.append(f""... and {len(data) - 10} more"")
        
        lines.append(""="" * 50)
        return ""\n"".join(lines)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def _format_table(self, data: Any) -> str:
        """"""Format as ASCII table""""""
        if not isinstance(data, (list, dict)):
            return str(data)
        
        # Convert dict to list of items
        if isinstance(data, dict) and 'matches' not in data:
            data = [{'key': k, 'value': v} for k, v in data.items()]
        
        # Handle different data structures
        if isinstance(data, dict) and 'matches' in data:
            # Model search results
            rows = []
            for category, files in data['matches'].items():
                for file in files:
                    rows.append({
                        'Category': category,
                        'File': file['name'],
                        'Path': file['path'],
                        'Type': file['type']
                    })
            return self._create_table(rows)
        
        elif isinstance(data, list) and data:
            # List of models or other items
            if isinstance(data[0], dict):
                return self._create_table(data)
            else:
                # Simple list
                return ""\n"".join(f""â€¢ {item}"" for item in data)
        
        return str(data)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def test_ignore_patterns(self, temp_model_dir):
        """"""Test ignore patterns functionality""""""
        # Create files that should be ignored
        (temp_model_dir / ""models"" / ""__pycache__"").mkdir(parents=True)
        (temp_model_dir / ""models"" / "".git"").mkdir(parents=True)
        (temp_model_dir / ""models"" / ""test.pyc"").touch()
        
        scanner = ModelScanner(temp_model_dir)
        
        # These should not appear in results
        results = scanner.search_by_model_name(""pycache"")
        assert results['total_files'] == 0
        
        results = scanner.search_by_model_name(""git"")
        assert results['total_files'] == 0
",tests/test_scan/test_scanner.py,TestModelScanner
survived,"    def _generate_usage_guide(self, model_info: Dict[str, Any]) -> str:
        """"""Generate a usage guide""""""
        lines = [
            f""# Usage Guide: {model_info['name']}"",
            f""\nGenerated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"",
            ""\n## Quick Start""
        ]
        
        # Basic usage
        lines.extend([
            ""\n### Basic Usage"",
            ""\n```python"",
            f""# Using {model_info['name']}"",
            """",
            ""# 1. Import necessary libraries"",
            ""import json"",
            ""from pathlib import Path"",
            """",
            ""# 2. Load model configuration"",
            ""config_path = Path('config.json')"",
            ""with open(config_path, 'r') as f:"",
            ""    config = json.load(f)"",
            """",
            ""# 3. Initialize and use model"",
            ""# Add framework-specific code here"",
            ""```""
        ])
        
        # Common use cases
        lines.extend([
            ""\n## Common Use Cases"",
            f""\nBased on the model structure, {model_info['name']} can be used for:""
        ])
        
        if 'llm' in str(model_info['categories']).lower():
            lines.extend([
                ""\n### Text Generation"",
                ""- Content creation"",
                ""- Code generation"",
                ""- Language translation"",
                ""- Text summarization""
            ])
        
        if 'vision' in str(model_info['categories']).lower():
            lines.extend([
                ""\n### Computer Vision"",
                ""- Image classification"",
                ""- Object detection"",
                ""- Image generation"",
                ""- Visual analysis""
            ])
        
        # Parameters
        if model_info['config']:
            lines.extend([
                ""\n## Model Parameters"",
                ""\nKey configuration options:""
            ])
            
            for key, value in list(model_info['config'].items())[:10]:
                lines.append(f""- `{key}`: {value}"")
        
        # Examples from files
        if model_info['examples']:
            lines.extend([
                ""\n## Code Examples"",
                ""\nExample files available:""
            ])
            
            for example in model_info['examples'][:3]:
                lines.append(f""\n### {example['name']}"")
                if example.get('content'):
                    lines.append(""```python"")
                    lines.append(example['content'][:300])
                    if len(example['content']) > 300:
                        lines.append(""# ... (truncated)"")
                    lines.append(""```"")
        
        # Tips
        lines.extend([
            ""\n## Tips and Tricks"",
            ""\n1. **Performance Optimization**:"",
            ""   - Use appropriate batch sizes"",
            ""   - Enable GPU acceleration if available"",
            ""   - Consider model quantization for deployment"",
            """",
            ""2. **Error Handling**:"",
            ""   - Validate input data formats"",
            ""   - Handle out-of-memory errors gracefully"",
            ""   - Implement timeout mechanisms"",
            """",
            ""3. **Best Results**:"",
            ""   - Preprocess input data appropriately"",
            ""   - Use recommended hyperparameters"",
            ""   - Fine-tune for specific use cases""
        ])
        
        return ""\n"".join(lines)
",src/haconiwa/scan/guide_generator.py,GuideGenerator
survived,"    def process_dimension_data(
        summary_df, func, dimension, all_libs, matrix_shape_exclusions
    ):
        """"""Process data for a single dimension (1D or 2D) for a specific function.""""""
        if summary_df.empty:
            return {f""{dimension}_{lib}"": ""n/a"" for lib in all_libs}

        return {
            f""{dimension}_{lib}"": get_column_value(
                summary_df, func, lib, dimension, matrix_shape_exclusions
            )
            for lib in all_libs
        }
",numbagg/test/run_benchmarks.py,
survived,"    def test_consistency_vs_basic_matrix_full_window(self):
        """"""Test consistency between moving and basic matrix functions when using full window.""""""
        np.random.seed(42)

        # Create data in both conventions
        data_moving = np.random.randn(50, 4)  # (obs, vars) for moving
        data_basic = data_moving.T  # (vars, obs) for basic

        # Basic correlation using all data
        corr_basic = nancorrmatrix(data_basic)

        # Moving correlation using full window
        corr_moving = move_nancorrmatrix(data_moving, window=50)

        # Last timestep should match basic result
        assert_allclose(corr_moving[-1], corr_basic, rtol=1e-14)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices
survived,"    def test_three_series_consistency(self):
        """"""Test consistency for a 3x3 matrix case.""""""
        np.random.seed(444)

        # Create three time series
        n_obs = 30
        a1 = np.random.randn(n_obs)
        a2 = np.random.randn(n_obs) * 1.5 + 0.5
        a3 = np.random.randn(n_obs) * 0.8 - 0.2

        alpha = 0.35

        # Test all pairwise combinations
        pairs = [(a1, a2, 0, 1), (a1, a3, 0, 2), (a2, a3, 1, 2)]

        # Compute matrix result once
        # Exponential moving functions expect (obs, vars) format
        data_matrix = np.column_stack([a1, a2, a3])
        cov_matrix_result = move_exp_nancovmatrix(data_matrix, alpha=alpha)
        corr_matrix_result = move_exp_nancorrmatrix(data_matrix, alpha=alpha)

        for series1, series2, i, j in pairs:
            # Compute pairwise results
            cov_nonmatrix = move_exp_nancov(series1, series2, alpha=alpha)
            corr_nonmatrix = move_exp_nancorr(series1, series2, alpha=alpha)

            # Extract from matrix results
            cov_from_matrix = cov_matrix_result[:, i, j]
            corr_from_matrix = corr_matrix_result[:, i, j]

            # They should match
            assert_allclose(
                cov_nonmatrix,
                cov_from_matrix,
                rtol=1e-10,
                err_msg=f""Covariance mismatch for series {i},{j}"",
            )
            assert_allclose(
                corr_nonmatrix,
                corr_from_matrix,
                rtol=1e-10,
                err_msg=f""Correlation mismatch for series {i},{j}"",
            )

            # Also check symmetry
            assert_allclose(
                cov_matrix_result[:, i, j], cov_matrix_result[:, j, i], rtol=1e-10
            )
            assert_allclose(
                corr_matrix_result[:, i, j], corr_matrix_result[:, j, i], rtol=1e-10
            )
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def test_simple_matrix(self, func, expected_diag):
        """"""Test simple 2x2 matrix calculation with exponential decay.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 2], [2, 4], [3, 6], [4, 8]], dtype=np.float64)
        alpha = 0.5
        result = func(data, alpha=alpha)

        # Check shape - should be (time, vars, vars)
        assert result.shape == (4, 2, 2)

        # Check diagonal at the end
        final_result = result[-1]
        if expected_diag is not None:
            assert_allclose(
                np.diag(final_result), [expected_diag, expected_diag], rtol=1e-10
            )
        else:
            # For covariance, just check diagonal is non-negative
            assert np.all(np.diag(final_result) >= 0)

        # Check symmetry at each time step
        for t in range(result.shape[0]):
            assert_allclose(result[t], result[t].T, rtol=1e-10)

        # For perfect linear relationship, correlation should be 1
        if func == move_exp_nancorrmatrix:
            # Check that off-diagonal elements approach 1 as we get more data
            assert_allclose(final_result, [[1.0, 1.0], [1.0, 1.0]], rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def close_all_connections(self) -> None:
        """"""Close all active SQLite connections.

        This method ensures that all SQLite database connections are properly
        closed to prevent file locking issues, especially on Windows systems.
        It should be called during cleanup or when the ContextManager is no
        longer needed.

        Side Effects:
            Closes all connections tracked in self._active_connections.
            Clears the connections set after closing.
        """"""
        import platform

        connections_to_close = list(self._active_connections)
        for conn in connections_to_close:
            try:
                conn.close()
            except Exception:  # nosec B110
                # Ignore errors during cleanup
                pass
        self._active_connections.clear()

        # On Windows, add a small delay to ensure file handles are released
        if platform.system() == ""Windows"" and connections_to_close:
            import time

            time.sleep(0.1)
",ocode_python/core/context_manager.py,ContextManager
survived,"    async def test_bash_tool_windows_shell_preparation(self, mock_which, mock_platform):
        """"""Test Windows shell command preparation.""""""
        mock_which.side_effect = {
            ""cmd"": ""C:\\Windows\\System32\\cmd.exe"",
            ""powershell"": (
                ""C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe""
            ),
            ""pwsh"": None,
        }.get

        bash_tool = BashTool()

        # Test default cmd.exe handling
        result = bash_tool._prepare_shell_command(""echo test"", ""bash"")
        assert result == [""C:\\Windows\\System32\\cmd.exe"", ""/c"", ""echo test""]

        # Test PowerShell handling
        result = bash_tool._prepare_shell_command(""echo test"", ""powershell"")
        assert result == [
            ""C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe"",
            ""-Command"",
            ""echo test"",
        ]
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility
survived,"    async def test_windows_script_execution_no_chmod(
        self, mock_chmod, mock_tempfile, mock_platform
    ):
        """"""Test that chmod is not called on Windows for script files.""""""
        mock_file = MagicMock()
        mock_file.name = ""C:\\temp\\script.sh""
        mock_file.__enter__.return_value = mock_file
        mock_tempfile.return_value = mock_file

        bash_tool = BashTool()

        # This would normally trigger script execution, but we'll just test the setup
        with patch.object(bash_tool, ""execute"") as mock_execute:
            mock_execute.return_value = MagicMock(success=True)

            # The actual script execution path includes chmod logic
            # We're testing that on Windows, chmod should not be called
            # This is tested indirectly through the platform check in the code

        # On Windows, chmod should not be called
        mock_chmod.assert_not_called()
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility
survived,"    def test_claude_desktop_with_new_options(self):
        """"""Test claude-desktop install with new uv options.""""""
        from pathlib import Path

        command, bound, _ = install_app.parse_args(
            [
                ""claude-desktop"",
                ""server.py"",
                ""--python"",
                ""3.10"",
                ""--project"",
                ""/my/project"",
                ""--with-requirements"",
                ""reqs.txt"",
            ]
        )

        assert bound.arguments[""python""] == ""3.10""
        assert bound.arguments[""project""] == Path(""/my/project"")
        assert bound.arguments[""with_requirements""] == Path(""reqs.txt"")
",tests/cli/test_install.py,TestClaudeDesktopInstall
survived,"    def test_build_uv_command_with_all_options(self):
        """"""Test building uv command with all options.""""""
        cmd = _build_uv_command(
            ""server.py"",
            python_version=""3.10"",
            project=Path(""/my/project""),
            with_packages=[""pandas"", ""numpy""],
            with_requirements=Path(""reqs.txt""),
            with_editable=Path(""/local/pkg""),
            no_banner=True,
        )
        expected = [
            ""uv"",
            ""run"",
            ""--python"",
            ""3.10"",
            ""--project"",
            ""/my/project"",
            ""--with"",
            ""fastmcp"",
            ""--with-editable"",
            ""/local/pkg"",
            ""--with"",
            ""pandas"",
            ""--with"",
            ""numpy"",
            ""--with-requirements"",
            ""reqs.txt"",
            ""fastmcp"",
            ""run"",
            ""server.py"",
            ""--no-banner"",
        ]
        assert cmd == expected
",tests/cli/test_cli.py,TestMainCLI
survived,"    def test_run_command_parsing_with_new_options(self):
        """"""Test run command parsing with new uv options.""""""
        command, bound, _ = app.parse_args(
            [
                ""run"",
                ""server.py"",
                ""--python"",
                ""3.11"",
                ""--with"",
                ""pandas"",
                ""--with"",
                ""numpy"",
                ""--project"",
                ""/path/to/project"",
                ""--with-requirements"",
                ""requirements.txt"",
            ]
        )

        assert command is not None
        assert bound.arguments[""server_spec""] == ""server.py""
        assert bound.arguments[""python""] == ""3.11""
        assert bound.arguments[""with_packages""] == [""pandas"", ""numpy""]
        assert bound.arguments[""project""] == Path(""/path/to/project"")
        assert bound.arguments[""with_requirements""] == Path(""requirements.txt"")
",tests/cli/test_cli.py,TestRunCommand
survived,"    def test_with_requirements_option(self):
        """"""Test --with-requirements option for all install commands.""""""
        commands_to_test = [
            [""claude-code"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""claude-desktop"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""cursor"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""mcp-json"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
        ]

        for cmd_args in commands_to_test:
            command, bound, _ = install_app.parse_args(cmd_args)
            assert command is not None
            assert str(bound.arguments[""with_requirements""]) == ""requirements.txt""
",tests/cli/test_install.py,TestInstallCommandParsing
survived,"    def test_noncontextual_pipeline_decay_with_rate(self):
        """"""Test non-contextual pipeline decay with explicit rate.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        pipeline = NonContextualAgentPipeline([], agent)

        pipeline.decay(decay_rate=0.7)
",tests/test_agent_pipeline.py,TestCoverage
survived,"    def predict(self, X):
        self.predict_calls.append(X)
        return np.zeros(len(X))
",tests/test_learner_pipeline.py,MockLearner
survived,"    def decay(
        self,
        X: Any,
        decay_rate: Optional[float] = None,
    ) -> None:
        """"""Decay all arms of the wrapped agent.

        Parameters
        ----------
        X : Any
            Input data to transform and use for decaying the arms.
            Will be transformed through the pipeline steps to ContextType.
        decay_rate : Optional[float], default=None
            Decay rate to use for decaying the arms.
        """"""
        X_transformed = self.transform(X)
        self._agent.decay(X_transformed, decay_rate=decay_rate)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def test_with_normal_regressor(self):
        """"""Test pipeline with real NormalRegressor.""""""
        # Pre-fit the scaler
        scaler = StandardScaler()
        scaler.fit(np.random.randn(50, 3))  # Fit on dummy data

        pipeline = LearnerPipeline(
            steps=[(""scale"", scaler)],
            learner=NormalRegressor(alpha=1.0, beta=1.0)
        )

        # Generate training data
        X = np.random.randn(20, 3)
        y = np.random.randn(20)

        # Train
        pipeline.partial_fit(X, y)

        # Test all methods work
        samples = pipeline.sample(X[:5], size=10)
        assert samples.shape == (10, 5)  # (size, n_samples)

        predictions = pipeline.predict(X[:5])
        assert predictions.shape == (5,)

        # Decay should work
        pipeline.decay(X[:5], decay_rate=0.95)
",tests/test_learner_pipeline.py,TestLearnerPipelineIntegration
survived,"    def test_contextual_agent_dispatch(self):
        """"""Test factory dispatches to ContextualAgentPipeline for ContextualAgent.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling())
        steps = [(""identity"", FunctionTransformer())]

        pipeline = AgentPipeline(steps, agent)

        assert isinstance(pipeline, ContextualAgentPipeline)
        assert pipeline._agent is agent
",tests/test_agent_pipeline.py,TestAgentPipelineFactory
survived,"    def test_pull_with_top_k(self):
        """"""Test pull method with top_k.""""""
        arms = make_arms(range(5))
        agent = ContextualAgent(arms, ThompsonSampling(), random_seed=42)
        steps = [(""identity"", FunctionTransformer())]

        pipeline = ContextualAgentPipeline(steps, agent)

        X = np.array([[1.0, 2.0], [3.0, 4.0]])
        action_lists = pipeline.pull(X, top_k=3)

        assert len(action_lists) == 2
        assert all(len(actions) == 3 for actions in action_lists)
",tests/test_agent_pipeline.py,TestContextualAgentPipeline
survived,"    def test_invalid_agent_type(self):
        """"""Test error handling for invalid agent types.""""""
        # This would be caught by type checker, but test runtime behavior
        steps = [(""identity"", FunctionTransformer())]

        # Mock object that doesn't have the expected interface
        class MockAgent:
            pass

        mock_agent = MockAgent()

        # The factory function should handle invalid agent types
        # In practice, this would be a type error at development time
        # Since isinstance check won't match, it will try to create ContextualAgentPipeline
        # which will fail on first attribute access
        pipeline = AgentPipeline(steps, mock_agent)  # type: ignore
        # The error will happen when trying to use the agent
        with pytest.raises(AttributeError):
            _ = pipeline.arms
",tests/test_agent_pipeline.py,TestErrorHandling
survived,"    def setup_method(self):
        """"""Set up test pipeline.""""""
        self.mock_learner = MockLearner()
        # Pre-fit the scaler for testing
        scaler = StandardScaler()
        scaler.fit(np.random.randn(100, 2))  # Fit on dummy data
        self.pipeline = LearnerPipeline(steps=[(""scale"", scaler)], learner=self.mock_learner)
",tests/test_learner_pipeline.py,TestLearnerPipelineInterface
survived,"    def test_dict_vectorizer_integration(self):
        """"""Test integration with DictVectorizer.""""""
        # Pre-fit vectorizer
        vectorizer = DictVectorizer()
        historical_dicts = [
            {""user"": ""A"", ""item"": ""X""},
            {""user"": ""B"", ""item"": ""Y""},
        ]
        vectorizer.fit(historical_dicts)

        arms = [
            Arm(i, learner=NormalRegressor(alpha=1.0, beta=1.0, sparse=True))
            for i in range(3)
        ]
        agent = ContextualAgent(arms, ThompsonSampling(), random_seed=42)

        steps = [(""vectorize"", vectorizer)]
        pipeline = ContextualAgentPipeline(steps, agent)

        X = [{""user"": ""A"", ""item"": ""X""}, {""user"": ""B"", ""item"": ""Y""}]

        actions = pipeline.pull(X)
        assert len(actions) == 2

        y = np.array([1.0, 2.0])
        pipeline.update(X, y)
",tests/test_agent_pipeline.py,TestTransformationFlow
survived,"    def __init__(self, steps: List[Tuple[str, Any]], learner: Learner[Any]) -> None:
        # Validate steps (can be empty)
        if steps:
            names, _ = zip(*steps)
            # Validate names are unique
            if len(set(names)) != len(names):
                raise ValueError(""Step names must be unique"")

        # Validate learner has required Learner methods
        required_methods = [""partial_fit"", ""sample"", ""predict"", ""decay""]
        missing_methods = [
            method for method in required_methods if not hasattr(learner, method)
        ]
        if missing_methods:
            raise ValueError(
                f""Learner must implement the Learner protocol. ""
                f""Missing methods: {missing_methods}""
            )

        # Store transformer steps and learner separately
        self.steps = steps  # Transformer steps only
        self._learner: Learner[Any] = learner
",bayesianbandits/pipelines/_learner.py,LearnerPipeline
survived,"    def test_empty_steps_error(self):
        """"""Test empty steps raise error.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling())

        with pytest.raises(ValueError, match=""Pipeline steps cannot be empty""):
            ContextualAgentPipeline([], agent)
",tests/test_agent_pipeline.py,TestContextualAgentPipeline
survived,"    def _format_prp_documentation(self, docs: List[str]) -> str:
        """"""Format documentation links for PRP.""""""
        return ""\n"".join(f""- {doc}"" for doc in docs)
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
deleted,"    def _analyze_naming_conventions(self, project_path: str, file_patterns: List[str]) -> Dict[str, Any]:
        """"""Analyze naming conventions used in the project.""""""
        conventions = {""style"": ""snake_case"", ""patterns"": [], ""exceptions"": []}
        # Implementation would analyze actual naming patterns
        return conventions
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def __init__(self, project_path: str, llm: str = ""gpt-4o-mini""):
        self.project_path = project_path
        self.llm = llm
        self.context_data = {}
        self.setup_agents()
",examples/python/concepts/context-engineering-workflow.py,ContextEngineeringWorkflow
survived,"    def setup_agents(self):
        """"""Setup the multi-agent team with Context Engineering support.""""""
        
        # 1. Product Manager Agent - Defines requirements
        self.product_manager = Agent(
            name=""Product Manager"",
            role=""Product Requirements Specialist"",
            goal=""Define clear, comprehensive product requirements and user stories"",
            backstory=""""""You are an experienced product manager who specializes in 
            creating detailed, actionable requirements. You understand that clear 
            requirements are the foundation of successful implementation."""""",
            instructions=""""""
            When defining requirements:
            1. Be specific and measurable
            2. Include user acceptance criteria
            3. Consider edge cases and error scenarios
            4. Define success metrics
            5. Specify technical constraints
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 2. Context Engineering Agent - Generates comprehensive context
        self.context_engineer = create_context_agent(
            llm=self.llm,
            name=""Context Engineering Specialist"",
            verbose=True
        )
        
        # 3. Software Architect Agent - Designs implementation using context
        self.architect = Agent(
            name=""Software Architect"",
            role=""System Architecture Specialist"", 
            goal=""Design robust, scalable system architecture based on comprehensive context"",
            backstory=""""""You are a senior software architect with expertise in 
            designing systems that follow established patterns and best practices. 
            You excel at creating architectures that integrate seamlessly with 
            existing codebases."""""",
            instructions=""""""
            When designing architecture:
            1. Follow the patterns identified in the context analysis
            2. Ensure compatibility with existing systems
            3. Design for scalability and maintainability
            4. Consider security and performance implications
            5. Document architectural decisions and rationale
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 4. Senior Developer Agent - Implements using context-enhanced guidance
        self.developer = Agent(
            name=""Senior Developer"",
            role=""Implementation Specialist"",
            goal=""Implement features following context-guided best practices"",
            backstory=""""""You are a senior developer who excels at implementing 
            features that follow established codebase patterns. You understand 
            that consistency and quality are paramount."""""",
            instructions=""""""
            When implementing features:
            1. Follow the codebase patterns identified in context analysis
            2. Maintain consistency with existing code style
            3. Implement comprehensive error handling
            4. Write clean, maintainable code
            5. Follow the implementation blueprint exactly
            """""",
            llm=self.llm,
            verbose=True
        )
        
        # 5. QA Engineer Agent - Validates using context-generated criteria
        self.qa_engineer = Agent(
            name=""QA Engineer"",
            role=""Quality Assurance Specialist"",
            goal=""Ensure implementation meets all quality criteria and requirements"",
            backstory=""""""You are an experienced QA engineer who specializes in 
            comprehensive testing and validation. You understand that quality 
            is built in, not bolted on."""""",
            instructions=""""""
            When validating implementations:
            1. Use the validation criteria from context analysis
            2. Test both happy path and edge cases
            3. Verify integration with existing systems
            4. Check for security vulnerabilities
            5. Validate performance requirements
            """""",
            llm=self.llm,
            verbose=True
        )
",examples/python/concepts/context-engineering-workflow.py,ContextEngineeringWorkflow
survived,"    def _format_implementation_patterns(self, code_patterns: Dict[str, Any]) -> str:
        """"""Format implementation patterns for context document.""""""
        return ""Follow existing class and function patterns identified in codebase analysis.""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _extract_architecture_guidance(self, context_data: Dict[str, Any]) -> str:
        """"""Extract architecture guidance from context data.""""""
        return ""Follow established architectural patterns identified in the analysis.""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _analyze_docstring_format(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze docstring format conventions.""""""
        return {""format"": ""google"", ""completeness"": ""partial""}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def test_backward_compatibility():
    """"""Test that existing PraisonAI functionality still works.""""""
    print(""\nðŸ§ª Testing Backward Compatibility..."")
    
    try:
        # Test that we can still import and use existing agents
        from praisonaiagents import Agent, ImageAgent
        
        # Test basic Agent still works
        basic_agent = Agent(name=""Test Agent"")
        print(""âœ… Basic Agent still works"")
        
        # Test ImageAgent still works
        image_agent = ImageAgent(name=""Test Image Agent"")
        print(""âœ… ImageAgent still works"")
        
        # Test that we can import other PraisonAI components
        from praisonaiagents import Task, PraisonAIAgents
        print(""âœ… Task and PraisonAIAgents can still be imported"")
        
        # Test that __all__ exports are working
        import praisonaiagents
        expected_exports = [
            'Agent', 'ImageAgent', 'ContextAgent', 'create_context_agent',
            'PraisonAIAgents', 'Task'
        ]
        
        for export in expected_exports:
            assert hasattr(praisonaiagents, export), f""Missing export: {export}""
        
        print(""âœ… All expected exports are available"")
        
        return True
        
    except Exception as e:
        print(f""âŒ Backward compatibility test failed: {e}"")
        return False
",test_context_agent.py,
survived,"        async def mock_llm_call(*args, **kwargs):
            return llm.LLMResponse(
                raw_response="""",
                prompt=[],
                response='{""result"": ""test""}',
                tool_calls=None,
                prompt_tokens=10,
                completion_tokens=20,
                reasoning=None,
            )
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking
survived,"    def _message(self) -> str:
        return ""Should use `Mlflow` in class name, not `MLflow` or `MLFlow`.""",dev/clint/src/clint/rules/mlflow_class_name.py,MlflowClassName
survived,"    def check(node: ast.Name) -> bool:
        return node.id in IncorrectTypeAnnotation.MAPPING
",dev/clint/src/clint/rules/incorrect_type_annotation.py,IncorrectTypeAnnotation
survived,"    def _message(self) -> str:
        return ""This example has a syntax error.""",dev/clint/src/clint/rules/example_syntax_error.py,ExampleSyntaxError
survived,"    def example(cls) -> ""ModelVersionTagDeletedPayload"":
        return cls(
            name=""example_model"",
            version=""1"",
            key=""example_key"",
        )
",mlflow/webhooks/types.py,ModelVersionTagDeletedPayload
survived,"    def test_webhook(
        self, webhook_id: str, event: Optional[WebhookEvent] = None
    ) -> WebhookTestResult:
        """"""
        Test a webhook by sending a test event to the specified URL.

        Args:
            webhook_id: Webhook ID.
            event: Optional event type to test. If not specified, uses the first event from webhook.

        Returns:
            WebhookTestResult indicating success/failure and response details
        """"""
        raise NotImplementedError(f""{self.__class__.__name__} does not support test_webhook"")",mlflow/store/model_registry/abstract_store.py,AbstractStore
survived,"    def example(cls) -> ""RegisteredModelCreatedPayload"":
        return cls(
            name=""example_model"",
            tags={""example_key"": ""example_value""},
            description=""An example registered model"",
        )
",mlflow/webhooks/types.py,RegisteredModelCreatedPayload
survived,"def test_webhook_test_insecure_endpoint(mlflow_client: MlflowClient, app_client: AppClient) -> None:
    # Create webhook for testing
    webhook = mlflow_client.create_webhook(
        name=""test_webhook"",
        url=app_client.get_url(""/insecure-webhook""),
        events=[WebhookEvent.MODEL_VERSION_CREATED],
    )

    # Test the webhook
    result = mlflow_client.test_webhook(webhook.webhook_id)

    # Check that the test was successful
    assert result.success is True
    assert result.response_status == 200
    assert result.error_message is None

    # Check that the test payload was received
    logs = app_client.get_logs()
    assert len(logs) == 1
    assert logs[0][""endpoint""] == ""/insecure-webhook""
    assert logs[0][""payload""] == {
        ""name"": ""example_model"",
        ""version"": ""1"",
        ""source"": ""runs:/abcd1234abcd5678/model"",
        ""run_id"": ""abcd1234abcd5678"",
        ""tags"": {""example_key"": ""example_value""},
        ""description"": ""An example model version"",
    }
",tests/webhooks/test_e2e.py,
survived,"def test_import_json_command_missing_name_key(tmp_path):
    """"""Test handling JSON with missing 'name' key using 'id' instead.""""""
    # Create JSON with id instead of name (common in Knowledge Graph Memory Server)
    data_with_id = [
        {
            ""type"": ""entity"",
            ""id"": ""test_entity_id"",
            ""entityType"": ""test"",
            ""observations"": [""Test observation with id""],
        },
        {
            ""type"": ""entity"",
            ""entityName"": ""test_entity_2"",
            ""entityType"": ""test"",
            ""observations"": [""Test observation with entityName""],
        },
        {
            ""type"": ""entity"",
            ""name"": ""test_entity_title"",
            ""entityType"": ""test"",
            ""observations"": [""Test observation with name""],
        },
    ]

    json_file = tmp_path / ""missing_name.json""
    with open(json_file, ""w"", encoding=""utf-8"") as f:
        for item in data_with_id:
            f.write(json.dumps(item) + ""\n"")

    # Set up test environment
    monkeypatch = pytest.MonkeyPatch()
    monkeypatch.setenv(""HOME"", str(tmp_path))

    # Run import - should not fail even without 'name' key
    result = runner.invoke(import_app, [""memory-json"", str(json_file)])
    assert result.exit_code == 0
    assert ""Import complete"" in result.output
    assert ""Created 3 entities"" in result.output",tests/cli/test_import_memory_json.py,
survived,"        def w_get_blueval(vm: 'SPyVM', w_oparg: W_OpArg) -> W_Dynamic:
            if w_oparg.color != 'blue':
                raise SPyRuntimeError('oparg is not blue')
            return w_oparg.w_blueval
",spy/vm/opimpl.py,W_OpArg
survived,"    def w_NEW(vm: 'SPyVM', wop_cls: W_OpArg, *args_wop: W_OpArg) -> 'W_OpImpl':
        """"""
        Operator for creating OpImpl instances with different argument counts.
        - OpImpl(func) -> Simple OpImpl
        - OpImpl(func, args) -> OpImpl with pre-filled arguments
        """"""
        from spy.vm.function import W_Func
        from spy.vm.list import W_OpArgList

        w_type = wop_cls.w_blueval
        assert isinstance(w_type, W_Type)

        if len(args_wop) == 1:
            # Simple case: OpImpl(func)
            @builtin_func(w_type.fqn, 'new1')
            def w_new1(vm: 'SPyVM', w_cls: W_Type, w_func: W_Func) -> W_OpImpl:
                return W_OpImpl(w_func)
            return W_OpImpl(w_new1)

        elif len(args_wop) == 2:
            # OpImpl(func, args) case
            @builtin_func(w_type.fqn, 'new2')
            def w_new2(vm: 'SPyVM', w_cls: W_Type,
                       w_func: W_Func, w_args: W_OpArgList) -> W_OpImpl:
                # Convert from applevel w_args into interp-level args_w
                args_w = w_args.items_w[:]
                return W_OpImpl(w_func, args_w)
            return W_OpImpl(w_new2)
        else:
            return W_OpImpl.NULL
",spy/vm/opimpl.py,W_OpImpl
survived,"    def test_relationship_to_correlation(self):
        # Test relationship between covariance and correlation
        from numbagg import nancorrmatrix

        np.random.seed(42)
        data = np.random.randn(4, 50)

        cov_matrix = nancovmatrix(data)
        corr_matrix = nancorrmatrix(data)

        # Correlation = Covariance / (std_i * std_j)
        stds = np.sqrt(np.diag(cov_matrix))
        expected_corr = cov_matrix / np.outer(stds, stds)

        assert_allclose(corr_matrix, expected_corr, rtol=1e-10)
",numbagg/test/test_nancovmatrix.py,TestNanCovMatrix
survived,"def nancovmatrix(a, out):
    """"""
    Compute covariance matrix treating NaN as missing values.

    For 2D input, computes covariance between variables (rows) across observations (columns).
    Uses pairwise complete observations (like pandas.DataFrame.cov).
    """"""
    n_vars, n_obs = a.shape

    # Compute covariance matrix
    for i in range(n_vars):
        for j in range(i, n_vars):  # Only compute upper triangle
            # Find pairwise complete observations and compute sums in one pass
            sum_i = 0.0
            sum_j = 0.0
            count = 0

            for k in range(n_obs):
                val_i = a[i, k]
                val_j = a[j, k]
                if not np.isnan(val_i) and not np.isnan(val_j):
                    sum_i += val_i
                    sum_j += val_j
                    count += 1

            if count > 1:
                # Compute means using only pairwise complete observations
                mean_i = sum_i / count
                mean_j = sum_j / count

                # Compute covariance in second pass
                cov_sum = 0.0
                for k in range(n_obs):
                    val_i = a[i, k]
                    val_j = a[j, k]
                    if not np.isnan(val_i) and not np.isnan(val_j):
                        cov_sum += (val_i - mean_i) * (val_j - mean_j)

                # Use count - 1 for sample covariance
                out[i, j] = cov_sum / (count - 1)
                out[j, i] = out[i, j]  # Symmetric
            else:
                out[i, j] = np.nan
                out[j, i] = np.nan",numbagg/funcs.py,
survived,"    def __call__(
        self,
        a: np.ndarray,
        axis: int | tuple[int, ...] | None = None,
        **kwargs,
    ):
        if axis is None:
            axis = -1

        if isinstance(axis, tuple):
            if len(axis) != 1:
                raise ValueError(
                    f""Matrix function requires exactly one axis, got {len(axis)}""
                )
            axis = axis[0]

        # Handle 1D input by treating it as a single variable
        if a.ndim == 1:
            a = a.reshape(1, -1)

        # Move the correlation axis to the last position
        a = np.moveaxis(a, axis, -1)

        gufunc = self.gufunc(target=self.target)
        # axes specifies which axes contain the core dimensions
        # For our signature ""(n,m)->(n,n)"":
        # - Input has 2 core dims: second-to-last (n) and last (m)
        # - Output has 2 core dims: last two dimensions (n,n)
        result = gufunc(a, axes=[(-2, -1), (-2, -1)], **kwargs)

        # Return result as-is, let numba handle the output shape
        return result
",numbagg/decorators.py,ndmatrix
survived,"        def __get__(self, obj: Any, objtype: Any = None) -> Any:
            warnings.warn(
                ""`jaxls.Factor` has been renamed to `jaxls.Cost`"",
                DeprecationWarning,
                stacklevel=2,
            )
            return Cost
",src/jaxls/__init__.py,_FactorDescriptor
survived,"    def _field_has_secrets(self, field_name: str) -> bool:
        """"""Check if a field contains secrets based on the schema's secret_fields.""""""
        secret_fields = self.model_json_schema().get(""secret_fields"", [])

        # Check if field_name matches any secret field pattern
        for secret_field in secret_fields:
            if secret_field == field_name:
                return True
            elif secret_field.startswith(f""{field_name}.""):
                # This field contains nested secrets
                return True
            elif secret_field.endswith("".*""):
                # Handle wildcard patterns like ""field.*""
                prefix = secret_field[:-2]  # Remove .*
                if field_name == prefix:
                    return True

        return False
",src/prefect/blocks/core.py,Block
survived,"        def test_resource_with_path(x: int) -> str:
            return f""test resource with {x}""
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    async def test_list_tools_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.list_tools()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""tools/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_tools"", times=1)

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(method=""tools/list"", times=3)
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(hook=""on_list_tools"", times=1)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    async def sample_tool(context: Context) -> None:
        await context.sample(""hello"")
",tests/server/middleware/test_middleware.py,
survived,"    async def _list_resources(self, apply_middleware: bool = True) -> list[Resource]:
        """"""
        List all available resources.
        """"""

        if (resources := self._cache.get(""resources"")) is self._cache.NOT_FOUND:
            resources: list[Resource] = []

            # iterate such that new mounts overwrite older ones
            for mounted_server in self._mounted_servers:
                try:
                    if apply_middleware:
                        server_resources = (
                            await mounted_server.server._middleware_list_resources()
                        )
                    else:
                        server_resources = await mounted_server.server._list_resources()
                    # Apply prefix to each resource key if prefix exists
                    if mounted_server.prefix:
                        for resource in server_resources:
                            resource = resource.with_key(
                                add_resource_prefix(
                                    resource.key,
                                    mounted_server.prefix,
                                    self.resource_prefix_format,
                                )
                            )
                            resources.append(resource)
                    else:
                        resources.extend(server_resources)
                except Exception as e:
                    logger.warning(
                        f""Failed to get resources from mounted server '{mounted_server.prefix}': {e}""
                    )
                    continue
            resources.extend(self._resource_manager.get_resources().values())
            self._cache.set(""resources"", resources)
        return resources
",src/fastmcp/server/server.py,FastMCP
survived,"            async def record_and_call(
                context: MiddlewareContext, call_next: Callable
            ) -> Any:
                result = await call_next(context)

                self.calls.append(Recording(hook=name, context=context, result=result))

                return result
",tests/server/middleware/test_middleware.py,RecordingMiddleware
survived,"def compile(query: str, target: str = None) -> str:
    """"""
    Compile a Wvlet query into SQL.
    
    Args:
        query (str): The Wvlet query to compile.
        target (str): Optional target database (e.g., 'trino', 'duckdb').
    
    Returns:
        str: The compiled SQL query.
    
    Raises:
        ValueError: If the compilation fails.
    """"""
    compiler = WvletCompiler(target=target)
    return compiler.compile(query)",sdks/python/wvlet/__init__.py,
survived,"def build_default_regex_config(
    additional_patterns: Dict[str, str] | None = None,
) -> GuardrailConfig:
    """"""Return a :class:`GuardrailConfig` with default regex rules.

    Args:
        additional_patterns: Optional extra name->pattern mappings to include.

    Returns:
        GuardrailConfig: Config populated with regex guardrail rules.
    """"""
    patterns: Dict[str, str] = {**DEFAULT_REGEX_PATTERNS}
    if additional_patterns:
        patterns.update(additional_patterns)

    config = GuardrailConfig()
    for name, pattern in patterns.items():
        # validate pattern by compiling; GuardrailRule will also validate
        re.compile(pattern)
        config.add_rule(GuardrailRule(name=name, pattern=pattern))
    return config",src/meta_agent/generators/regex_patterns.py,
survived,"    def __init__(self, **data: Any) -> None:  # pragma: no cover - exercised in tests
        super().__init__(**data)
        if not self.openai_api_key:
            self.openai_api_key = get_secret(""OPENAI_API_KEY"")
        if not self.openai_api_key:
            _log.warning(""OPENAI_API_KEY missing â€“ offline mode enabled"")
            self.offline = True
",src/utils/config.py,Settings
survived,"  def test_all_dbcs(self, subtests):
    # Asserts no exceptions on all DBCs
    for dbc in ALL_DBCS:
      with subtests.test(dbc=dbc):
        CANDefine(dbc)",opendbc/can/tests/test_define.py,TestCANDefine
survived,"async def test_shutdown_without_signal(monkeypatch: pytest.MonkeyPatch) -> None:
    loop = asyncio.get_running_loop()

    def boom(*_: object) -> None:
        raise NotImplementedError

    monkeypatch.setattr(loop, ""add_signal_handler"", boom)

    shutdown_event = asyncio.Event()
    dummy = DummyManager()

    async def factory() -> qm.QueueManager:
        return cast(qm.QueueManager, dummy)

    async def run_manager(manager: DummyManager, *args: object, **kwargs: object) -> None:
        await manager.run()

    def setup_shutdown_handlers_stub(
        manager: DummyManager, shutdown: asyncio.Event
    ) -> DummyManager:
        manager.shutdown = shutdown
        return manager

    monkeypatch.setattr(supervisor, ""run_manager"", run_manager)
    monkeypatch.setattr(supervisor, ""setup_shutdown_handlers"", setup_shutdown_handlers_stub)

    task = asyncio.create_task(
        supervisor.runit(
            factory,
            dequeue_timeout=timedelta(seconds=1),
            batch_size=1,
            restart_delay=timedelta(seconds=0),
            restart_on_failure=False,
            shutdown=shutdown_event,
            mode=types.QueueExecutionMode.continuous,
            max_concurrent_tasks=None,
            shutdown_on_listener_failure=False,
        )
    )

    await asyncio.sleep(0.1)
    shutdown_event.set()
    await task",test/windows/test_shutdown.py,
survived,"def run_claude_json(
    prompt: str,
    allowed_tools: Optional[List[str]] = None,
    cli: str = ""claude"",
) -> dict:
    """"""Run Claude and parse JSON output.""""""
    output = run_claude(prompt, ""json"", allowed_tools, cli)
    return json.loads(output)",claude_testing_v1.py,
survived,"def _files_from_diff(diff: str) -> list[str]:
    files: set[str] = set()
    for line in diff.splitlines():
        if line.startswith(""+++"") or line.startswith(""---""):
            parts = line.split(maxsplit=1)
            if len(parts) != 2:
                continue
            path = parts[1]
            if path.startswith(""a/"") or path.startswith(""b/""):
                path = path[2:]
            files.add(path)
    return list(files)
",src/agents/self_improver_agent.py,
survived,"def test_blocked_payload_not_stored(tmp_path) -> None:
    """"""ChaosAgent payloads should be blocked and skipped by the memory.""""""

    cfg = config.Settings(bus_port=0)
    bus = CaptureBus(cfg)
    ledger = logging.Ledger(str(tmp_path / ""ledger.db""), broadcast=False)

    mem = FilteringMemoryAgent(bus, ledger, str(tmp_path / ""mem.log""))
    guardian = safety_agent.SafetyGuardianAgent(bus, ledger)
    chaos = chaos_agent.ChaosAgent(bus, ledger, burst=1)

    async def run() -> None:
        async with bus, ledger:
            await chaos.run_cycle()
            await asyncio.sleep(0)

    asyncio.run(run())

    memory_events = [env for topic, env in bus.published if topic == ""memory""]
    assert memory_events
    assert memory_events[-1].payload[""status""] == ""blocked""
    assert mem.records == []",tests/test_safety_agent.py,
survived,"    def save(
        self,
        response: List[bytes],
        name: str = None,
        dir: str = os.getcwd(),
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your amazing images! ðŸ’¾

        Args:
            response (List[bytes]): List of image data
            name (str, optional): Base name for saved files
            dir (str, optional): Where to save the images
            filenames_prefix (str, optional): Prefix for filenames

        Returns:
            List[str]: List of saved filenames
        """"""
        assert isinstance(response, list), f""Response should be of {list} not {type(response)}""
        name = self.prompt if name is None else name

        if not os.path.exists(dir):
            os.makedirs(dir)

        filenames = []
        count = 0
        for image in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            with open(absolute_path_to_file, ""wb"") as fh:
                fh.write(image)
        return filenames
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager
survived,"    def __init__(
        self,
        model: str = ""flux-schnell"",
        timeout: int = 60,
        proxies: dict = {},
    ):
        """"""Initialize your PixelMuse provider with custom settings! âš™ï¸

        Args:
            model (str): Which model to use (default: flux-schnell)
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
            logging (bool): Enable fire logging (default: True)
        """"""
        self.api_endpoint = ""https://www.pixelmuse.studio/api/predictions""
        self.headers = {
            ""accept"": ""*/*"",
            ""accept-language"": ""en-US,en;q=0.9"",
            ""content-type"": ""application/json"",
            ""origin"": ""https://www.pixelmuse.studio"",
            ""referer"": ""https://www.pixelmuse.studio/"",
            ""sec-ch-ua"": '""Chromium"";v=""134"", ""Not:A-Brand"";v=""24"", ""Google Chrome"";v=""134""',
            ""sec-ch-ua-mobile"": ""?0"",
            ""sec-ch-ua-platform"": '""Windows""',
            ""sec-fetch-dest"": ""empty"",
            ""sec-fetch-mode"": ""cors"",
            ""sec-fetch-site"": ""same-origin"",
            ""user-agent"": LitAgent().random(),
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.model = model
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""webp""
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager
survived,"    def generate(
        self, 
        prompt: str, 
        amount: int = 1,
        caption_model: str = ""sdxl"",
        selected_ratio: str = ""1024"",
        negative_prompt: str = """"
    ) -> List[str]:
        """"""Generate some fire images! ðŸŽ¨

        Args:
            prompt (str): Your lit image description
            amount (int): How many images to generate (default: 1)
            caption_model (str): Which model to use (default: ""sdxl"")
            selected_ratio (str): Image size ratio (default: ""1024"")
            negative_prompt (str): What you don't want in the image (default: """")

        Returns:
            List[str]: Your generated image URLs
        """"""
        assert bool(prompt), ""Yo fam, prompt can't be empty! ðŸš«""
        assert isinstance(amount, int), f""Amount gotta be an integer, not {type(amount)} ðŸ¤”""
        assert amount > 0, ""Amount gotta be greater than 0! ðŸ“ˆ""

        self.prompt = prompt
        response: List[str] = []

        payload = {
            ""captionInput"": prompt,
            ""captionModel"": caption_model,
            ""selectedRatio"": selected_ratio,
            ""selectedSamples"": str(amount),
            ""negative_prompt"": negative_prompt
        }

        try:
            resp = self.scraper.post(self.url, json=payload, timeout=self.timeout)
            resp.raise_for_status()

            response_data = resp.json()
            imgs = response_data.get(""imgs"", [])
            
            if imgs:
                response.extend(imgs)

        except requests.RequestException as e:
            raise

        return response
",webscout/Provider/TTI/artbit.py,ArtbitImager
survived,"    def __init__(self, timeout: int = 60, proxies: dict = None):
        """"""Initialize your FastFluxImager provider with custom settings

        Examples:
            >>> provider = FastFluxImager(timeout=120)
            >>> provider = FastFluxImager(proxies={""http"": ""http://proxy:8080""})

        Args:
            timeout (int): HTTP request timeout in seconds (default: 60)
            proxies (dict, optional): Proxy configuration for requests
            logging (bool): Enable/disable logging (default: True)
        """"""
        self.api_endpoint = ""https://api.fastflux.co/v1/images/generate""
        self.headers = {
            ""accept"": ""application/json, text/plain, */*"",
            ""content-type"": ""application/json"",
            ""origin"": ""https://fastflux.co"",
            ""referer"": ""https://fastflux.co/"",
            ""user-agent"": agent.random()
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        if proxies:
            self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
        self.logging = True
",webscout/Provider/TTI/fastflux.py,FastFluxImager
survived,"    def save(
        self,
        response: List[bytes],
        name: Optional[str] = None,
        dir: Optional[Union[str, Path]] = None,
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire generated images! ðŸ’¾""""""
        save_dir = dir if dir else os.getcwd()
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        name = self.prompt if name is None else name
        filenames = []
        count = 0

        for image in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(save_dir, filenames_prefix + name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            filepath = complete_path()
            filenames.append(os.path.basename(filepath))

            with open(filepath, ""wb"") as fh:
                fh.write(image)

        return filenames",webscout/Provider/TTI/magicstudio.py,MagicStudioImager
survived,"    def refresh_token(self, refresh_token: str) -> tuple[str, str]:
        """"""Refresh authentication token""""""
        payload = {
            ""grant_type"": ""refresh_token"",
            ""refresh_token"": refresh_token,
        }
        
        response = self.session.post(self.token_refresh_url, data=payload, timeout=self.timeout)
        response_data = response.json()
        
        return response_data.get(""id_token""), response_data.get(""refresh_token"")
",webscout/Provider/TTI/aiarta.py,AIArtaImager
survived,"    def create(*_a: object, **_k: object) -> None:
        raise FailError(""boom"")
",tests/test_llm_client_error_handling.py,
survived,"    def func() -> str:
        calls[""n""] += 1
        raise ValueError(""fail"")
",tests/test_retry_wrapper.py,
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"            async def close(self) -> None:
                pass
",src/meta_agent/services/llm_service.py,AiohttpPlaceholder.ClientSession
survived,"    async def close(self) -> None:
        pass
",src/aiohttp/__init__.py,ClientSession
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        close_fn = getattr(self._session, ""close"", None)
        if close_fn is None:
            return
        if asyncio.iscoroutinefunction(close_fn):
            await close_fn()
        else:
            close_fn()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"async def test_send_success(telemetry_client):
    result = await telemetry_client.send(""trace"", {""data"": 1})
    assert result == {""ok"": True}
",tests/unit/test_telemetry_client.py,
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    def __post_init__(self) -> None:
        if self.auth_token:
            self.headers[""Authorization""] = f""Bearer {self.auth_token}""
        self.headers.setdefault(""Content-Type"", ""application/json"")
",src/meta_agent/services/telemetry_client.py,EndpointConfig
survived,"def test_error_output_stderr(capsys):
    cli = CLIOutput()
    cli.error(""oops"")
    out, err = capsys.readouterr()
    assert out == """"
    assert ""oops"" in click.unstyle(err)",tests/ux/test_cli_output.py,
survived,"    def error(self, message: str, *, level: int = 1) -> None:
        """"""Output an error message.""""""
        self._echo(message, fg=""red"", bold=True, err=True, level=level)",src/meta_agent/ux/cli_output.py,CLIOutput
survived,"    def test_no_broker(self):
        os.environ.pop(""ALPHA_KAFKA_BROKER"", None)
        orig = base_mod.KafkaProducer
        base_mod.KafkaProducer = object  # dummy
        self.assertIsNone(base_mod._kafka_producer())
        base_mod.KafkaProducer = orig
",tests/test_base_helpers.py,TestKafkaProducer
survived,"    def setUp(self):
        self.agent = EnergyAgent()
",tests/test_energy_agent_behavior.py,TestEnergyAgentBehavior
survived,"    def test_ping_capability_present(self):
        # diagnostics capability should map to the ping agent
        from alpha_factory_v1.backend.agents.ping_agent import PingAgent
        meta = AgentMetadata(
            name=PingAgent.NAME,
            cls=PingAgent,
            version=""0"",
            capabilities=PingAgent.CAPABILITIES,
            compliance_tags=[],
        )
        register_agent(meta)
        agents = capability_agents(""diagnostics"")
        self.assertIn(""ping"", agents)
",tests/test_agents_registry.py,TestAgentRegistryFunctions
survived,"    def test_quickstart_wrapper(self) -> None:
        """"""The demo quick_start script delegates to the repo quickstart.""""""
        script = Path(""alpha_factory_v1/demos/quick_start.sh"")
        self.assertTrue(script.exists())
        content = script.read_text()
        self.assertTrue(content.startswith(""#!/usr/bin/env bash""))
        self.assertIn(""../quickstart.sh"", content)
",tests/test_demos.py,TestDemos
survived,"            def __init__(self) -> None:
                self.called = False
",tests/test_alpha_agi_business_3_v1.py,TestAlphaAgiBusiness3Demo.CaptureOrch
survived,"    def test_run_cycle_commits(self) -> None:
        model = DummyModel()
        demo.run_cycle(
            demo.Orchestrator(),
            demo.AgentFin(),
            demo.AgentRes(),
            demo.AgentEne(),
            demo.AgentGdl(),
            model,
        )
        self.assertTrue(model.committed)
",tests/test_alpha_agi_business_3_v1.py,TestAlphaAgiBusiness3Demo
survived,"        async def get_one():
            it = data_feeds.stream_macro_events(live=False)
            return await anext(it)
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"    def test_generated_model_name(self):
        """"""Test that generated EnrichModel has correct name.""""""

        class Base(DeclarativeBase):
            pass

        class Customer(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""customers""
            id: Mapped[int] = mapped_column(primary_key=True)

        CustomerEnrichModel = Customer.__enrich_model__()
        assert CustomerEnrichModel.__name__ == ""CustomerEnrichModel""
",tests/test_sqlalchemy_integration.py,TestEdgeCases
survived,"def emit_docker(fp:Path=Path(""Dockerfile"")): fp.write_text(DOCKERFILE); print(""Dockerfile â†’"",fp)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def act(self, obs):
        # epsilonâ€‘greedy w/ MCTS fallback
        if random.random()<0.1:
            return random.randint(0,3)
        return mcts_policy(self.net, obs, CFG.mcts_simulations)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Learner
survived,"def _utcnow_ms() -> str:
    return time.strftime(""%Y-%m-%dT%H:%M:%S"", time.gmtime()) + f"".{int((time.time()%1)*1000):03d}Z""
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,
survived,"def _str_tkn(text: str) -> int:
    # naÃ¯ve token estimate â‰ˆâ€‘ 1 token / 4 chars in English
    return max(1, math.ceil(len(text)/4))
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,
survived,"    def forward(self, x): return torch.tanh(self.l(x))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Repr
survived,"    def __exit__(self, exc_type, exc, tb):
        if signal:
            resource.setrlimit(resource.RLIMIT_CPU, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
        return False  # do not suppress
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,SafeExec
survived,"    def log():
        if not path.exists():
            return ""(no events yet)""
        return flask.escape(path.read_text(""utf-8""))
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,
survived,"def _main():
    p=argparse.ArgumentParser(prog=""alpha_asi_world_model_demo"")
    p.add_argument(""--demo"",action=""store_true"")
    p.add_argument(""--emit-docker"",action=""store_true"")
    p.add_argument(""--emit-helm"",action=""store_true"")
    p.add_argument(""--emit-notebook"",action=""store_true"")
    p.add_argument(""--host"",default=""127.0.0.1"")
    p.add_argument(""--port"",type=int,default=7860)
    args=p.parse_args()
    if args.emit_docker: emit_docker()
    elif args.emit_helm: emit_helm()
    elif args.emit_notebook: emit_notebook()
    elif args.demo:
        uvicorn.run(""alpha_asi_world_model_demo:app"",host=args.host,port=args.port,log_level=""info"")
    else: p.print_help()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def handle(self, msg: dict):  # to be overridden
        raise NotImplementedError
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Agent
survived,"def _boot(path: str):
    module_path, cls_name = (MODROOT + path).rsplit(""."", 1)
    try:
        cls = getattr(importlib.import_module(module_path), cls_name)
        inst: Agent = cls()  # type: ignore
        LOG.info(""[BOOT] loaded real agent %s"", inst.name)
    except Exception as exc:
        class Stub(Agent):
            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] â† %s"", cls_name, _msg)
        inst = Stub(cls_name)
        LOG.warning(""[BOOT] stubbed %s (%s)"", cls_name, exc)
    AGENTS[inst.name] = inst
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def score(self, metrics: Dict[str,float]) -> float:
        return (
            self.latency * (1/ (1+metrics.get(""latency"",0))) +
            self.cost    * (1/ (1+metrics.get(""cost"",0))) +
            self.carbon  * (1/ (1+metrics.get(""carbon"",0))) +
            self.risk    * (1- metrics.get(""risk"",0))
        )
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,ObjectiveWeights
survived,"    def __enter__(self):
        if resource:
            resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_sec, self.cpu_sec))
            resource.setrlimit(resource.RLIMIT_AS, (self.mem_mb*1024*1024, self.mem_mb*1024*1024))
        return self
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,SafeExec
survived,"    def run(self, prompt: str, context: Optional[Iterable[Dict[str,str]]]=None, **kw) -> Dict[str,Any]:
        ctx: List[Dict[str,str]] = list(context or [])
        ctx.append({""role"":""user"", ""content"": prompt})
        t0 = time.perf_counter()
        output = self.lm.chat(ctx, **kw)
        latency = time.perf_counter()-t0
        tokens_in = _str_tkn(prompt)
        tokens_out = _str_tkn(output)
        cost = self._estimate_cost(tokens_in,tokens_out)
        carbon = cost*0.00015 # placeholder multiplier (avg kgCO2 per $ cloud)
        risk = self._risk_assess(prompt, output)
        metrics = dict(latency=latency, cost=cost, carbon=carbon, risk=risk)
        score = self.objectives.score(metrics)
        self.tracer.log(""run"", prompt=prompt[:120], response=output[:120], metrics=metrics, score=score)
        return {""response"": output, ""metrics"": metrics, ""score"": score}
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,Agent
survived,"    def propose(self)->MiniWorld:
        size=random.randint(5,9)
        obstacles={(random.randint(1,size-2),random.randint(1,size-2)) for _ in range(random.randint(0,size))}
        env=MiniWorld(size,list(obstacles),(size-1,size-1))
        self.pool.append(env)
        return env
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,POETGenerator
survived,"    def train_once(self)->float:
        if len(self.buffer)<CFG.train_batch: return 0.0
        obs,rew=zip(*random.sample(self.buffer, CFG.train_batch))
        obs_t=torch.tensor(obs, device=CFG.device, dtype=torch.float32)
        rew_t=torch.tensor(rew, device=CFG.device)
        _,v,_=self.net.initial(obs_t)
        loss=F.mse_loss(v.squeeze(),rew_t)
        self.opt.zero_grad(); loss.backward(); self.opt.step()
        return float(loss.item())
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Learner
survived,"    def read_json_file(file_path: str) -> str:
        """"""Read and pretty print a JSON file.""""""
        with open(file_path, ""r"", encoding=""utf-8"") as f:
            data = json.load(f)
        return json.dumps(data, indent=2, ensure_ascii=False)
",datamax/parser/json_parser.py,JsonParser
survived,"async def run_memory_chat() -> None:
    """"""Run an interactive chat session with conversation memory enabled.""""""
    load_dotenv()
    config_file = os.path.join(os.path.dirname(__file__), ""config.json"")

    print(""Initializing chat..."")
    client = MCPClient.from_config_file(config_file)

    openai_key = os.getenv(""OPENAI_API_KEY"")
    ollama_model = os.getenv(""OLLAMA_MODEL"", ""llama3"")

    llm = ChatOpenAI(model=""gpt-4o"") if openai_key else ChatOllama(model=ollama_model)

    agent = MCPAgent(
        llm=llm,
        client=client,
        max_steps=15,
        memory_enabled=True,
        system_prompt=SYSTEM_MESSAGE,
    )

    print(""\n===== Interactive MCP Chat ====="")
    print(""Type 'exit' or 'quit' to end the conversation"")
    print(""Type 'clear' to clear conversation history"")
    print(""Type 'history' to display the conversation so far"")
    print(""=================================\n"")

    try:
        while True:
            user_input = input(""\nYou: "")
            command = user_input.lower()

            if command in (""exit"", ""quit""):
                print(""Ending conversation..."")
                break

            if command == ""clear"":
                agent.clear_conversation_history()
                print(""Conversation history cleared."")
                continue

            if command == ""history"":
                for msg in agent.conversation_history:
                    role = msg.get(""role"", ""assistant"").capitalize()
                    print(f""{role}: {msg['content']}"")
                continue

            print(""\nAssistant: "", end="""", flush=True)
            try:
                response = await agent.run(user_input)
                print(response)
            except Exception as exc:
                print(f""\nError: {exc}"")
    finally:
        if client and client.sessions:
            await client.close_all_sessions()
",examples/openai_chat_agent/app.py,
survived,"def test_root_serves_index() -> None:
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface import api_server

    client = TestClient(api_server.app)
    resp = client.get(""/"")
    assert resp.status_code == 200
    assert ""<div id=\""root\""></div>"" in resp.text",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_static.py,
survived,"    def __init__(self):
        super().__init__()
        self.sub1 = DummySub()
        self.sub2 = DummySub()
",tests/test_multi_contributor.py,DummyModel
survived,"    def __call__(self, text, return_tensors=None):
        return {""input_ids"": torch.tensor([[1]])}
",tests/test_multi_contributor.py,DummyTokenizer
survived,"def commit_activations(activations_path: str, challenge: int = CHALLENGE) -> str:
    """"""Return polynomial commitment of activations stored in JSON file.""""""
    with open(activations_path, ""r"") as f:
        data = json.load(f)
    arr = np.array(data[""input_data""], dtype=np.int64).reshape(-1)
    val = _poly_eval(arr, challenge, PRIME)
    return hex(val)
",src/zklora/polynomial_commit.py,
survived,"    def forward(self, x):
        return x * 2
",tests/test_multi_contributor.py,DummySub
survived,"def populated_db(temp_db_path):
    items = [
        {""id"": ""python-1"", ""text"": ""Python programming is fun"", ""tags"": [""python"", ""programming"", ""fun""]},
        {""id"": ""sql-1"", ""text"": ""SQL databases are powerful"", ""tags"": [""sql"", ""database"", ""programming""]},
        {""id"": ""testing-1"", ""text"": ""Testing code is important"", ""tags"": [""testing"", ""code"", ""programming""]},
        {""id"": ""regex-1"", ""text"": ""Regular expressions can be complex"", ""tags"": [""regex"", ""programming"", ""advanced""]},
        {""id"": ""learning-1"", ""text"": ""Learning new technologies is exciting"", ""tags"": [""learning"", ""technology"", ""fun""]},
    ]

    for item in items:
        command = AddCommand(
            id=item[""id""],
            text=item[""text""],
            tags=item[""tags""],
            db_path=temp_db_path,
        )
        add(command)

    return temp_db_path
",src/mcp_server_pocket_pick/tests/functionality/test_list_ids.py,
survived,"def test_agent_runner_shared() -> None:
    assert demo_orchestrator.AgentRunner is orchestrator_utils.AgentRunner
    assert core_orchestrator.AgentRunner is orchestrator_utils.AgentRunner",tests/test_orchestrator_utils.py,
survived,"def _make_agent() -> safety_agent.SafetyGuardianAgent:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    return safety_agent.SafetyGuardianAgent(bus, led)
",tests/test_codegen_safety.py,
survived,"def _ensure_offline():
    DATA_DIR.mkdir(exist_ok=True)
    for name, url in OFFLINE_URLS.items():
        path = DATA_DIR / name
        if path.exists():
            continue
        try:
            with urlopen(url, timeout=5) as r, open(path, ""wb"") as f:
                f.write(r.read())
        except Exception:
            row = _DEFAULT_ROWS[name]
            with open(path, ""w"", newline="""") as f:
                writer = csv.DictWriter(f, row.keys())
                writer.writeheader()
                writer.writerow(row)
",alpha_factory_v1/demos/macro_sentinel/data_feeds.py,
survived,"    def __init__(self, env_id: str = ""CartPole-v1"") -> None:
        self.env = gym.make(env_id, render_mode=""rgb_array"")
        obs_dim = math.prod(self.env.observation_space.shape)
        self.action_dim = self.env.action_space.n
        self.net = MiniMuNet(obs_dim, self.action_dim)
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,MiniMu
survived,"            def __init__(self, *a, **k):
                self.calls = []
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest.DummyMetric
survived,"    def test_check_cmd(self):
        with mock.patch('shutil.which', return_value='/bin/foo'):
            self.assertTrue(preflight.check_cmd('foo'))
        with mock.patch('shutil.which', return_value=None):
            self.assertFalse(preflight.check_cmd('foo'))
",alpha_factory_v1/tests/test_preflight.py,PreflightTest
survived,"    def tearDown(self):
        sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest
survived,"    def __init__(self):
        self.next_ts = 0
        self.period = 1
        self.last_beat = time.time()
        self.inst = SimpleNamespace()
        self.spec = None
",alpha_factory_v1/tests/test_orchestrator_rest.py,DummyRunner
survived,"    def test_requires_token(self):
        with mock.patch.dict(os.environ, {}, clear=True):
            with self.assertRaises(SystemExit):
                import_dashboard.main()
",alpha_factory_v1/tests/test_import_dashboard.py,ImportDashboardTest
survived,"    def test_uses_pytest_when_available(self):
        with mock.patch('importlib.util.find_spec', return_value=object()):
            with mock.patch('subprocess.call', return_value=0) as call:
                argv = sys.argv
                sys.argv = ['run_tests.py']
                try:
                    with self.assertRaises(SystemExit):
                        run_tests.main()
                finally:
                    sys.argv = argv
                call.assert_called_once()
                cmd = call.call_args.args[0]
                self.assertIn('pytest', cmd)
",alpha_factory_v1/tests/test_run_tests_script.py,RunTestsScriptTest
survived,"def test_show_results_export_formats(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.tail.return_value = [SAMPLE_LEDGER_ROW]
            res_json = CliRunner().invoke(cli.main, [""show-results"", ""--export"", ""json""])
            res_csv = CliRunner().invoke(cli.main, [""show-results"", ""--export"", ""csv""])
    assert res_json.output.startswith(""["")
    assert ""ts,sender,recipient,payload"" in res_csv.output
",tests/test_cli_runner_ext.py,
survived,"        def _fake_import(name: str, *args: object, **kwargs: object) -> object:
            if name == ""agents"":
                raise ModuleNotFoundError
            return orig_import_module(name, *args, **kwargs)
",tests/test_build_core_agent.py,TestBuildCoreAgent
survived,"    def test_stub_when_sdk_missing(self) -> None:
        os.environ.pop(""OPENAI_API_KEY"", None)
        sys.modules.pop(""agents"", None)
        sys.modules.pop(""alpha_factory_v1.backend.agent_factory"", None)
        importlib.invalidate_caches()

        orig_import_module = importlib.import_module

        def _fake_import(name: str, *args: object, **kwargs: object) -> object:
            if name == ""agents"":
                raise ModuleNotFoundError
            return orig_import_module(name, *args, **kwargs)

        with mock.patch(""importlib.import_module"", side_effect=_fake_import):
            af = orig_import_module(""alpha_factory_v1.backend.agent_factory"")
            af = importlib.reload(af)
            agent = af.build_core_agent(name=""t"", instructions=""demo"")

        self.assertTrue(hasattr(agent, ""run""))
        self.assertEqual(agent.run(""hi""), ""[t-stub] echo: hi"")
        self.assertFalse(any(isinstance(t, af.ComputerTool) for t in af.DEFAULT_TOOLS))
",tests/test_build_core_agent.py,TestBuildCoreAgent
survived,"def test_docker_compose_config() -> None:
    subprocess.run([""docker"", ""compose"", ""-f"", str(COMPOSE_FILE), ""config""], check=True, capture_output=True)",tests/test_macro_compose_config.py,
survived,"def state(f, x, y):
    while y < 0:
        y = y + f.h
    while x < 0:
        x = x + f.w
    return f.s[y % f.h][x % f.w]
",tests/rosetta/transpiler/Python/conways-game-of-life.py,
survived,"def cfSqrt2(nTerms):
    f = []
    n = 0
    while n < nTerms:
        f = f + [newTerm(2, 1)]
        n = n + 1
    if nTerms > 0:
        f[0][""a""] = 1
    return f
",tests/rosetta/transpiler/Python/continued-fraction.py,
survived,"def doPos(x):
    pass
",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"def longestSeq(dir):
    pd = 0
    longSeqs = [[2]]
    currSeq = [2]
    i = 1
    while i < len(primes):
        d = primes[i] - primes[i - 1]
        if (dir == ""ascending"" and d <= pd) or (dir == ""descending"" and d >= pd):
            if len(currSeq) > len(longSeqs[0]):
                longSeqs = [currSeq]
            else:
                if len(currSeq) == len(longSeqs[0]):
                    longSeqs = longSeqs + [currSeq]
            currSeq = [primes[i - 1], primes[i]]
        else:
            currSeq = currSeq + [primes[i]]
        pd = d
        i = i + 1
    if len(currSeq) > len(longSeqs[0]):
        longSeqs = [currSeq]
    else:
        if len(currSeq) == len(longSeqs[0]):
            longSeqs = longSeqs + [currSeq]
    print(""Longest run(s) of primes with "" + dir + "" differences is "" + str(len(longSeqs[0])) + "" :"")
    for ls in longSeqs:
        diffs = []
        j = 1
        while j < len(ls):
            diffs = diffs + [ls[j] - ls[j - 1]]
            j = j + 1
        k = 0
        while k < len(ls) - 1:
            print(str(ls[k]) + "" ("" + str(diffs[k]) + "") "", (""true"" if False else ""false""))
            k = k + 1
        print(str(ls[len(ls) - 1]))
    print("""")
",tests/rosetta/transpiler/Python/consecutive-primes-with-ascending-or-descending-differences.py,
survived,"def real(f):
    r = 0.0
    i = len(f) - 1
    while i > 0:
        r = (float(f[i][""b""])) // ((float(f[i][""a""])) + r)
        i = i - 1
    if len(f) > 0:
        r = r + (float(f[0][""a""]))
    return r
",tests/rosetta/transpiler/Python/continued-fraction.py,
survived,"def example8(b1, b2):
    if b1:
        None
    if b2:
        None",tests/rosetta/transpiler/Python/conditional-structures-8.py,
survived,"def _load_yaml(path: Path) -> dict[str, Any]:
    text = path.read_text(encoding=""utf-8"")
    try:
        import yaml  # type: ignore

        data: Any = yaml.safe_load(text)
    except Exception:
        data = json.loads(text)
    return cast(dict[str, Any], data or {})
",src/simulation/replay.py,
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Delete cache entries older than ``stale_after``.""""""",src/cachier/cores/base.py,_BaseCore
survived,"    def add(x):
        return x + 1
",tests/test_cleanup.py,
survived,"    async def dummy_stop() -> None:
        nonlocal called
        called = True
        bus._consumer_task = None
",tests/test_eventbus.py,
survived,"def list_envs() -> List[str]:
    """"""Return available environment names.""""""
    return sorted(_ENV_REG.keys())
",alpha_factory_v1/backend/world_model.py,
survived,"    def __init__(self, *args, **kwargs):
        pass
",stubs/openai_agents/__init__.py,OpenAIAgent
survived,"def test_build_vocab():
    corpus = [
        [""first"", ""document"", ""hello"", ""world""],
        [""second"", ""document"", ""world"", ""big"", ""bright""],
    ]

    vocab, index = run_hlda.build_vocab(corpus)
    expected_vocab = [""big"", ""bright"", ""document"", ""first"", ""hello"", ""second"", ""world""]
    expected_index = {w: i for i, w in enumerate(expected_vocab)}
    assert vocab == expected_vocab
    assert index == expected_index
",tests/test_run_hlda_utils.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join_multi.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/update_stmt.py,Person
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join_multi.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/join_multi.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join_multi.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_join.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/join_multi.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_sort.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join.py,Auto1
survived,"        def multi_cell(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyFPDF
deleted,"def test_question_mark_meridiem(now):
    assert timefhuman(
        'Are you free this Wed at 3p? Or maybe Fri at 5p?',
        tfhConfig(now=now)
    ) == [
        datetime.datetime(2018, 8, 8, 15, 0),
        datetime.datetime(2018, 8, 10, 17, 0)
    ]",tests/test_e2e.py,
survived,"    def Gauge(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Gauge, name, desc, labels)
",alpha_factory_v1/backend/agents/__init__.py,
survived,"    def visit_Assign(self, node: ast.Assign) -> None:
        if len(node.targets) != 1:
            return
        target = node.targets[0]
        if isinstance(target, ast.Name):
            name = self.name_map.get(target.id, target.id)
            if isinstance(node.value, ast.Constant) and node.value.value is None:
                return
            if (
                isinstance(node.value, ast.Call)
                and isinstance(node.value.func, ast.Name)
                and node.value.func.id == ""TypeVar""
            ):
                return
            if (
                isinstance(node.value, ast.Call)
                and isinstance(node.value.func, ast.Name)
                and node.value.func.id in self.dataclasses
                and not node.value.args
                and len(node.value.keywords) == 1
                and node.value.keywords[0].arg is None
                and isinstance(node.value.keywords[0].value, ast.Call)
                and isinstance(node.value.keywords[0].value.func, ast.Name)
                and node.value.keywords[0].value.func.id == ""_fetch""
            ):
                typ = node.value.func.id
                fetch_expr = self.convert_expr(node.value.keywords[0].value)
                self.seen_assigns.add(name)
                self.assign_values[name] = f""({fetch_expr}) as {typ}""
                var_kw = ""var"" if name == ""next"" else ""let""
                self.emit(f""{var_kw} {name}: {typ} = ({fetch_expr}) as {typ}"")
                return
            expr = self.convert_expr(node.value)
            if name in self.seen_assigns:
                if self.assign_values.get(name) == expr:
                    return
                self.assign_values[name] = expr
                self.emit(f""{name} = {expr}"")
                return
            self.seen_assigns.add(name)
            self.assign_values[name] = expr
            var_kw = ""var"" if name == ""next"" else ""let""
            self.emit(f""{var_kw} {name} = {expr}"")
            return
        if isinstance(target, ast.Attribute) and isinstance(target.value, ast.Name) and target.value.id == ""self"":
            self.emit(f""{target.attr} = {self.convert_expr(node.value)}"")
            return
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def visit_Assert(self, node: ast.Assert) -> None:
        expr = self.convert_expr(node.test)
        self.emit(f""expect {expr}"")
",tools/any2mochi/py/py2mochi.py,Converter
survived,"        def get_tool_stats_route() -> dict[str, Any]:
            result = self._get_tool_stats()
            return result.model_dump()
",src/serena/dashboard.py,SerenaDashboardAPI
deleted,"    def __init__(self) -> None:
        self.count = 0
        self.input_chars = 0
        self.output_chars = 0
",src/serena/analytics.py,ToolStatsEntry
survived,"        def clear_tool_stats_route() -> dict[str, str]:
            self._clear_tool_stats()
            return {""status"": ""cleared""}
",src/serena/dashboard.py,SerenaDashboardAPI
deleted,"def get_tool_stats() -> dict[str, dict[str, int]]:
    with _lock:
        return {name: entry.to_dict() for name, entry in _tool_stats.items()}
",src/serena/analytics.py,
survived,"def test_sqlalchemy_type_to_python_extra_types():
    assert _sqlalchemy_type_to_python(JSON()) is dict
    assert _sqlalchemy_type_to_python(LargeBinary()) is bytes
    assert _sqlalchemy_type_to_python(Time()) is time

    class MyInt(Integer):
        pass

    assert _sqlalchemy_type_to_python(MyInt()) is int

    class Custom(TypeEngine):
        pass

    assert _sqlalchemy_type_to_python(Custom()) is Any",tests/test_sqlalchemy_utils.py,
survived,"def test_main_ollama(monkeypatch: pytest.MonkeyPatch) -> None:
    base = _run_main(monkeypatch, openai_key="""", base_url=""http://ollama"")
    assert base == ""http://ollama""",tests/test_agent_experience_entrypoint.py,
survived,"def test_main_openai(monkeypatch: pytest.MonkeyPatch) -> None:
    base = _run_main(monkeypatch, openai_key=""dummy"", base_url=""http://ollama"")
    assert base is None
",tests/test_agent_experience_entrypoint.py,
survived,"def continuous_retry(*, retry_delay: float = 1.0):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            while True:
                try:
                    return func(*args, **kwargs)
                except Exception as exc:
                    logger.exception(
                        ""%s failed with %s â€” retrying in %.2f s"",
                        func.__name__,
                        exc,
                        retry_delay,
                    )
                    time.sleep(retry_delay)

        return wrapper

    return decorator",autogpt_platform/backend/backend/util/retry.py,
survived,"def get_metric(factory: Callable[..., Any], name: str, desc: str, labels: list[str] | None = None) -> Any:
    """"""Return an existing metric or create a new one.

    This avoids depending on ``REGISTRY._names_to_collectors`` which may
    change between ``prometheus_client`` versions.
    """"""
    metric = METRICS.get(name)
    if metric is not None:
        return metric
    metric = factory(name, desc, labels) if labels is not None else factory(name, desc)
    METRICS[name] = metric
    return metric",alpha_factory_v1/backend/metrics_registry.py,
survived,"def prefer_medium_model() -> ModelPreferences:
    """"""Balanced model preferences for general use.""""""

    return ModelPreferences(
        hints=[
            ModelHint(name=""gpt-4o-2024-05-13""),
            ModelHint(name=""claude-3-sonnet-20240229""),
            ModelHint(name=""llama-3-70b""),
        ],
        costPriority=0.5,
        speedPriority=0.6,
        intelligencePriority=0.6,
    )
",src/enrichmcp/context.py,
survived,"    def freeze(self) -> None:
        self._frozen = True
",weave/trace/weave_client.py,AttributesDict
survived,"    def my_op():
        call = call_context.get_current_call()
        call.summary[""foo""] = 1
        call.summary[""bar""] = 2
        return ""done""
",tests/trace/test_current_call.py,
deleted,"async def test_truss_server_passes_ping_options():
    model = """"""
    import fastapi

    class Model:
        async def websocket(self, websocket: fastapi.WebSocket):
            try:
                while True:
                    text = await websocket.receive_text()
                    if text == ""done"":
                        return
                    await websocket.send_text(text)
            except fastapi.WebSocketDisconnect:
                pass
    """"""
    config = """"""
    runtime:
      transport:
        kind: websocket
        ping_interval: 1
        ping_timeout: 1
    """"""
    with ensure_kill_all(), _temp_truss(model, config) as tr:
        container, urls = tr.docker_run_for_test()
        async with websockets.connect(urls.websockets_url) as websocket:
            await websocket.send(""hello"")
            assert await websocket.recv() == ""hello""

            await asyncio.sleep(2)

            await websocket.send(""world"")
            assert await websocket.recv() == ""world""

            await websocket.send(""done"")
            with pytest.raises(websockets.exceptions.ConnectionClosed):
                await websocket.recv()",truss/tests/test_model_inference.py,
survived,"def main():
    parser = argparse.ArgumentParser(
        description=""Run hierarchical LDA on the BBC tech dataset""
    )
    parser.add_argument(
        ""--data-dir"",
        default=os.path.join(os.path.dirname(__file__), "".."", ""data"", ""bbc"", ""tech""),
        help=""Directory containing BBC .txt files"",
    )
    parser.add_argument(""--iterations"", type=int, default=100, help=""Number of Gibbs samples"")
    parser.add_argument(
        ""--display-topics"", type=int, default=50, help=""Report topics every N iterations""
    )
    parser.add_argument(
        ""--n-words"", type=int, default=5, help=""Number of words to display per topic""
    )
    parser.add_argument(
        ""--num-levels"", type=int, default=3, help=""Depth of the topic hierarchy""
    )
    parser.add_argument(""--alpha"", type=float, default=10.0, help=""Alpha hyperparameter"")
    parser.add_argument(""--gamma"", type=float, default=1.0, help=""Gamma hyperparameter"")
    parser.add_argument(""--eta"", type=float, default=0.1, help=""Eta hyperparameter"")
    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")

    args = parser.parse_args()
    run_demo(args)
",scripts/run_bbc_demo.py,
survived,"    def get_new_leaf(self):
        ''' Keeps adding nodes along the path until a leaf node is generated'''
        node = self
        for l in range(self.level, self.num_levels-1):
            node = node.add_child()
        return node
",src/hlda/sampler.py,NCRPNode
survived,"    def sample_path(self, d):

        # define a path starting from the leaf node of this doc
        path = np.zeros(self.num_levels, dtype=object)
        node = self.document_leaves[d]
        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4
            path[level] = node
            node = node.parent

        # remove this document from the path, deleting empty nodes if necessary
        self.document_leaves[d].drop_path()

        ############################################################
        # calculates the prior p(c_d | c_{-d}) in eq. (4)
        ############################################################

        node_weights = {}
        self.calculate_ncrp_prior(node_weights, self.root_node, 0.0)

        ############################################################
        # calculates the likelihood p(w_d | c, w_{-d}, z) in eq. (4)
        ############################################################

        level_word_counts = {}
        for level in range(self.num_levels):
            level_word_counts[level] = {}
        doc_levels = self.levels[d]
        doc = self.corpus[d]

        # remove doc from path
        for n in range(len(doc)): # for each word in the doc

            # count the word at each level
            level = doc_levels[n]
            w = doc[n]
            if w not in level_word_counts[level]:
                level_word_counts[level][w] = 1
            else:
                level_word_counts[level][w] += 1

            # remove word count from the node at that level
            level_node = path[level]
            level_node.word_counts[w] -= 1
            level_node.total_words -= 1
            assert level_node.word_counts[w] >= 0
            assert level_node.total_words >= 0

        self.calculate_doc_likelihood(node_weights, level_word_counts)

        ############################################################
        # pick a new path
        ############################################################

        nodes = np.array(list(node_weights.keys()))
        weights = np.array([node_weights[node] for node in nodes])
        weights = np.exp(weights - np.max(weights)) # normalise so the largest weight is 1
        weights = weights / np.sum(weights)

        choice = self.random_state.multinomial(1, weights).argmax()
        node = nodes[choice]

        # if we picked an internal node, we need to add a new path to the leaf
        if not node.is_leaf():
            node = node.get_new_leaf()

        # add the doc back to the path
        node.add_path()                     # add a customer to the path
        self.document_leaves[d] = node      # store the leaf node for this doc

        # add the words
        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4
            word_counts = level_word_counts[level]
            for w in word_counts:
                node.word_counts[w] += word_counts[w]
                node.total_words += word_counts[w]
            node = node.parent
",src/hlda/sampler.py,HierarchicalLDA
survived,"def test_devicon_capitalized_extension_returns_default(monkeypatch):
    monkeypatch.setenv('XDG_DOWNLOAD_DIR', '/tmp/downloads')
    devicons = reload_devicons('es')
    file = MockFile('example.PY')
    assert devicons.devicon(file) == 'î˜’'
",tests/test_devicons.py,
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return int(s.getsockname()[1])
",tests/test_start_alpha_business.py,
survived,"def test_sample_notebook(tmp_path):
    """"""Integration test for notebook conversion.

    Ensures that with ``process_notebooks`` enabled only code cells are
    transformed, while markdown cells and already f-string formatted code stay
    untouched.
    """"""
    folder = os.path.dirname(__file__)
    src = os.path.join(folder, ""samples_in"", ""simple.ipynb"")
    expected = os.path.join(folder, ""expected_out"", ""simple.ipynb"")
    nb_path = tmp_path / ""simple.ipynb""
    shutil.copy2(src, nb_path)

    result = _fstringify_file(str(nb_path), State(process_notebooks=True))
    assert result and result.n_changes == 1

    with open(nb_path) as f:
        converted = json.load(f)
    with open(expected) as f:
        expect = json.load(f)

    assert converted == expect",test/integration/test_notebook_file.py,
survived,"def test_attention_paged_decode_prefill_in_chunks(prefix_size, chunk_size):
    B = Axis(""batch"", 2)
    Pos = Axis(""position"", prefix_size + 4 * chunk_size)
    Embed = Axis(""embed"", 16)

    cfg = AttentionConfig(Embed=Embed, num_heads=2, num_kv_heads=2, rope=None, attn_backend=AttentionBackend.VANILLA)
    attn_key, x_key = jrandom.split(jrandom.PRNGKey(0))
    attn = Attention.init(cfg, key=attn_key)
    x = hax.random.normal(x_key, (B, Pos, Embed)) * 0.2
    full_out = attn(x, AttentionMask.causal(), key=jrandom.PRNGKey(1))

    cache = _build_page_cache(cfg, B, Pos)
    prefix = x[Pos, 0:prefix_size]
    prefill_chunk, cache = _jit_paged_decode(
        attn, prefix, pos_ids=hax.arange(Pos.resize(prefix_size), dtype=jnp.int32), cache=cache
    )

    out_chunks = [prefill_chunk]
    for i in range(prefix_size, Pos.size, chunk_size):
        x_tok = x[Pos, hax.dslice(i, chunk_size)]
        sub_pos = x_tok.resolve_axis(""position"")
        pos_ids_tok = hax.arange(sub_pos, dtype=jnp.int32, start=i)
        out_tok, cache = _jit_paged_decode(attn, x_tok, pos_ids_tok, cache)
        out_chunks.append(out_tok)

    decoded_arr = hax.concatenate(""position"", out_chunks)
    assert_trees_all_close(full_out, decoded_arr, atol=1e-4, rtol=1e-4)
",tests/test_attention.py,
survived,"    def tearDown(self):
        self.patcher.stop()
",tests/test_sys_fn_kdb.py,TestKdbIPC
survived,"    def test_qcli_and_qclic(self):
        klong = KlongInterpreter()
        klong('c::.qcli(1234)')
        proxy = klong('c')
        self.assertTrue(proxy.connection.is_open())
        r = klong('c(""1+1"")')
        self.assertEqual(r, 'EXEC: 1+1')
        self.assertEqual(proxy.connection.conn.queries[-1], ' 1+1')
        klong('.qclic(c)')
        self.assertFalse(proxy.connection.is_open())
",tests/test_sys_fn_kdb.py,TestKdbIPC
survived,"def test_distribution_zip(tmp_path: Path) -> None:
    zip_path = BROWSER_DIR / ""insight_browser.zip""
    if zip_path.exists():
        zip_path.unlink()
    result = subprocess.run([
        ""npm"",
        ""run"",
        ""build:dist"",
    ], cwd=BROWSER_DIR, capture_output=True, text=True)
    assert result.returncode == 0, result.stderr
    assert zip_path.exists(), ""insight_browser.zip missing""
    assert zip_path.stat().st_size <= 3 * 1024 * 1024, ""zip size exceeds 3 MiB""
    with zipfile.ZipFile(zip_path) as zf:
        names = zf.namelist()
    expected = {
        ""index.html"",
        ""insight.bundle.js"",
        ""service-worker.js"",
        ""manifest.json"",
        ""style.css"",
        ""insight_browser_quickstart.pdf"",
    }
    # ensure expected files exist
    for name in expected:
        assert name in names, f""{name} missing from zip""
    # ensure assets directory exists and contains files
    assert any(n.startswith(""assets/"") for n in names), ""assets directory missing""
    # ensure no unexpected files
    allowed_prefixes = {""assets/""}
    for name in names:
        if name in expected:
            continue
        if any(name.startswith(p) for p in allowed_prefixes):
            continue
        pytest.fail(f""Unexpected file {name} in zip"")",tests/test_distribution_zip.py,
survived,"    def tearDown(self):
        """"""Close the Gym environment after each test.""""""
        self.mu.env.close()
",tests/test_muzero_planning.py,TestMiniMu
survived,"            def __init__(self) -> None:
                self.instructions = []
",tests/test_merkle_broadcast.py,TestMerkleBroadcast.DummyTx
survived,"    def test_broadcast_error(self) -> None:
        led = self._ledger()
        env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
        led.log(env)
        captured, DummyClient, DummyTx, DummyInstr, DummyPk = self._dummy_classes(True)
        with (
            mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
            mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
            mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
            mock.patch.object(insight_logging, ""_log"") as log,
        ):
            asyncio.run(led.broadcast_merkle_root())
        log.warning.assert_called()  # ensure warning emitted",tests/test_merkle_broadcast.py,TestMerkleBroadcast
deleted,"def create_app(graphdb_filename, config_filename):
    handler_definitions = yaml.safe_load(open(config_filename))

    edge_events = list(get_handler_instances(handler_definitions, ""edge_handlers""))
    vertex_events = list(get_handler_instances(handler_definitions, ""vertex_handlers""))
    vertex_reverse_geocoders = list(
        get_handler_instances(handler_definitions, ""vertex_reverse_geocoders"")
    )

    print(""edge event handlers:"")
    for e in edge_events:
        print(f""   {e}"")
    print(""vertex event handlers:"")
    for v in vertex_events:
        print(f""   {v}"")
    print(""vertex reverse geocoders:"")
    for g in vertex_reverse_geocoders:
        print(f""   {g}"")

    rs = RouteServer(graphdb_filename, vertex_events, edge_events, vertex_reverse_geocoders)
    app = Flask(__name__)

    @app.route(""/bounds"")
    def bounds():
        cb = request.args.get(""callback"")
        data = rs.bounds(jsoncallback=cb)
        mimetype = ""application/javascript"" if cb else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/vertices"")
    def vertices():
        return Response(rs.vertices(), mimetype=""text/plain"")

    @app.route(""/get_vertex_id"")
    def get_vertex_id():
        lat = float(request.args[""lat""])
        lon = float(request.args[""lon""])
        return Response(rs.get_vertex_id(lat, lon), mimetype=""application/json"")

    @app.route(""/path"")
    def path_route():
        args = request.args
        data = rs.path(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/geompath"")
    def geompath_route():
        args = request.args
        data = rs.geompath(
            lat1=float(args[""lat1""]),
            lon1=float(args[""lon1""]),
            lat2=float(args[""lat2""]),
            lon2=float(args[""lon2""]),
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/path_retro"")
    def path_retro_route():
        args = request.args
        data = rs.path_retro(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
        )
        return Response(data, mimetype=""application/json"")

    @app.route(""/path_raw"")
    def path_raw_route():
        args = request.args
        currtime = int(args.get(""currtime"")) if args.get(""currtime"") else None
        data = rs.path_raw(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")

    @app.route(""/path_raw_retro"")
    def path_raw_retro_route():
        args = request.args
        currtime = int(args[""currtime""])
        data = rs.path_raw_retro(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")

    return app
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"def load_weights(path: str | Path | None = None) -> dict[str, float | Sequence[float]]:
    """"""Return weight configuration loaded from YAML.""""""
    p = Path(path) if path is not None else _DEFAULT_YAML
    try:
        data = yaml.safe_load(p.read_text(encoding=""utf-8""))
    except Exception:
        data = {}
    return data or {}
",src/simulation/surrogate_fitness.py,
survived,"def _manual_nsga2_ranks(values: list[tuple[float, ...]]) -> list[int]:
    n = len(values)
    ranks = [0] * n
    S = [set() for _ in range(n)]
    dominated = [0] * n
    for i, a in enumerate(values):
        for j, b in enumerate(values):
            if i == j:
                continue
            if all(ai <= bj for ai, bj in zip(a, b)) and any(ai < bj for ai, bj in zip(a, b)):
                S[i].add(j)
            elif all(bj <= ai for ai, bj in zip(a, b)) and any(bj < ai for ai, bj in zip(a, b)):
                dominated[i] += 1
        if dominated[i] == 0:
            ranks[i] = 0
    fronts = [[i for i, d in enumerate(dominated) if d == 0]]
    i = 0
    while i < len(fronts):
        nxt: list[int] = []
        for p in fronts[i]:
            for q in S[p]:
                dominated[q] -= 1
                if dominated[q] == 0:
                    ranks[q] = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return ranks
",tests/test_surrogate_fitness.py,
survived,"def load_capsule_facts(base: str | Path | None = None) -> MutableMapping[str, CapsuleFacts]:
    """"""Return mapping of sector name to :class:`CapsuleFacts`.""""""

    base_path = Path(base or Path(__file__).parent)
    facts: MutableMapping[str, CapsuleFacts] = {}
    for entry in base_path.iterdir():
        if not entry.is_dir():
            continue
        yaml_path = entry / ""facts.yml""
        if not yaml_path.exists():
            continue
        try:
            data = yaml.safe_load(yaml_path.read_text(encoding=""utf-8"")) or {}
        except Exception:
            continue
        facts[entry.name] = CapsuleFacts(
            market_size=float(data.get(""market_size"", 0.0)),
            efficiency_gain=float(data.get(""efficiency_gain"", 0.0)),
            llm_score=(float(data.get(""llm_score"")) if data.get(""llm_score"") is not None else None),
        )
    return facts
",src/capsules/__init__.py,
survived,"def test_recognize_vosk_verbose(audio_data):
    recognizer = Recognizer()
    actual = recognizer.recognize_vosk(audio_data, verbose=True)

    assert actual == {""text"": ""one two three""}",tests/recognizers/test_vosk.py,
survived,"def test_verify_assets(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    content = b""data""
    asset_path = tmp_path / ""file.txt""
    asset_path.write_bytes(content)
    digest = base64.b64encode(hashlib.sha384(content).digest()).decode()
    monkeypatch.setattr(fa, ""ASSETS"", {""file.txt"": ""cid""})
    monkeypatch.setattr(fa, ""CHECKSUMS"", {""file.txt"": f""sha384-{digest}""})
    assert fa.verify_assets(tmp_path) == []
    asset_path.write_text(""bad"")
    assert fa.verify_assets(tmp_path) == [""file.txt""]",tests/test_fetch_assets.py,
survived,"    def test_runtime_port_env(self, monkeypatch: ""pytest.MonkeyPatch"") -> None:
        """"""AgentRuntime receives AGENTS_RUNTIME_PORT.""""""
        import importlib
        import sys
        import types

        captured: dict[str, int] = {}

        class DummyRuntime:
            def __init__(self, *a: object, port: int = 5001, **_k: object) -> None:
                captured[""port""] = port

            def register(self, *_a: object, **_k: object) -> None:
                pass

            def run(self) -> None:
                pass

        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = DummyRuntime
        stub.OpenAIAgent = object

        def _tool(*_a: object, **_k: object) -> Callable[[object], object]:
            def dec(f: object) -> object:
                return f

            return dec

        stub.Tool = _tool
        monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
        monkeypatch.delitem(sys.modules, ""agents"", raising=False)
        monkeypatch.setenv(""AGENTS_RUNTIME_PORT"", ""6101"")

        mod = importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_opportunity_stub"")
        importlib.reload(mod)
        mod.main([])
        self.assertEqual(captured[""port""], 6101)
",tests/test_alpha_opportunity_stub.py,TestAlphaOpportunityStub
survived,"            def dec(f: object) -> object:
                return f
",tests/test_alpha_opportunity_stub.py,TestAlphaOpportunityStub
survived,"def cosine(a: np.ndarray, b: np.ndarray) -> float:
    return float(a @ b / (np.linalg.norm(a) * np.linalg.norm(b)))
",tests/test_embedding_orthogonaliser.py,
survived,"def validate_template(content: str) -> Tuple[bool, str]:
    """"""Check that the template is valid Jinja2.""""""
    try:
        Environment().parse(content)
    except TemplateSyntaxError as e:  # pragma: no cover - jinja2 handled
        return False, str(e)
    return True, """"
",src/meta_agent/template_creator.py,
survived,"async def test_broadcast_merkle_root_local_validator(
    tmp_path: Path, validator: str
) -> None:
    ledger = Ledger(str(tmp_path / ""ledger.db""), rpc_url=validator, broadcast=True)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    resp = requests.post(
        validator, json={""jsonrpc"": ""2.0"", ""id"": 1, ""method"": ""getLatestBlockhash""}
    )
    start_slot = resp.json()[""result""][""context""][""slot""]
    try:
        await ledger.broadcast_merkle_root()
        for _ in range(20):
            time.sleep(1)
            resp = requests.post(
                validator,
                json={""jsonrpc"": ""2.0"", ""id"": 1, ""method"": ""getLatestBlockhash""},
            )
            if resp.json()[""result""][""context""][""slot""] > start_slot:
                break
        else:
            raise AssertionError(""no new block produced"")
    finally:
        await ledger.stop_merkle_task()
        ledger.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_ledger_local_validator.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q13.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q22.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q14.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q2.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q2.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Order
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto11
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto10
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q32.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q10.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q32.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto6
survived,"def test_Q27_selects_minimal_company__link_and_title():
    assert result == Auto1(
        producing_company=""Best Film"",
        link_type=""follows"",
        complete_western_sequel=""Western Sequel"",
    )
",tests/dataset/job/compiler/py/q27.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto10
survived,"def test_Q32_finds_movie_link_for_10_000_mile_club():
    assert result == Auto1(
        link_type=""sequel"", first_movie=""Movie A"", second_movie=""Movie C""
    )
",tests/dataset/job/compiler/py/q32.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto9
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q31.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto15
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto11
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto16
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q27.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto14
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q13.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto7
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q2.py,Auto5
survived,"def test_Q30_finds_violent_horror_thriller_movies_with_male_writer():
    assert result == [
        Auto1(
            movie_budget=""Horror"",
            movie_votes=2000,
            writer=""John Writer"",
            complete_violent_movie=""Violent Horror"",
        )
    ]
",tests/dataset/job/compiler/py/q30.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,CustomerAddres
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q76.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q81.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,CustomerDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CustomerAddress
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q33.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogReturn
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q98.py,_Group
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q3.py,_Group
survived,"def _q0():
    _groups = {}
    _order = []
    for r in records:
        _k = Auto3(w_state=r.w_state, i_item_id=r.i_item_id)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            w_state=g.key[""w_state""],
            i_item_id=g.key[""i_item_id""],
            sales_before=sum([x.net if x.sold_date < sales_date else 0.0 for x in g]),
            sales_after=sum([x.net if x.sold_date >= sales_date else 0.0 for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q40.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q13.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q75.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q10.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q65.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,Auto2
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q99.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,CustomerAddress
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q49.py,Web
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q16.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,StoreReturn
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,HouseholdDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,Customer
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": item,
                ""on"": lambda ss, i: ss.item == i.i_item_sk and i.i_manager_id == 1,
            },
            {""items"": date_dim, ""on"": lambda ss, i, d: ss.sold_date == d.d_date_sk},
        ],
        {""select"": lambda ss, i, d: (ss, i, d)},
    )
    _groups = _group_by(_rows, lambda ss, i, d: Auto2(brand_id=i.i_brand_id))
    _items1 = _groups
    return [
        Auto1(brand_id=g.key[""brand_id""], ext_price=sum([x[0].price for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q55.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q86.py,WebSale
survived,"def _q3():
    _groups = {}
    _order = []
    for u in union:
        _k = u.get(""item"") if isinstance(u, dict) else getattr(u, ""item"")
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(u)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            i_item_id=g.key,
            total_sales=_sum(
                [
                    x.get(""total"") if isinstance(x, dict) else getattr(x, ""total"")
                    for x in g
                ]
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q56.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,Auto2
survived,"def _q0():
    _src = catalog_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": inventory,
                ""on"": lambda cs, inv: inv.inv_item_sk == cs.cs_item_sk,
            },
            {
                ""items"": warehouse,
                ""on"": lambda cs, inv, w: w.w_warehouse_sk == inv.inv_warehouse_sk,
            },
            {""items"": item, ""on"": lambda cs, inv, w, i: i.i_item_sk == cs.cs_item_sk},
            {
                ""items"": customer_demographics,
                ""on"": lambda cs, inv, w, i, cd: cd.cd_demo_sk == cs.cs_bill_cdemo_sk,
            },
            {
                ""items"": household_demographics,
                ""on"": lambda cs, inv, w, i, cd, hd: hd.hd_demo_sk
                == cs.cs_bill_hdemo_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda cs, inv, w, i, cd, hd, d1: d1.d_date_sk
                == cs.cs_sold_date_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda cs, inv, w, i, cd, hd, d1, d2: d2.d_date_sk
                == inv.inv_date_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda cs, inv, w, i, cd, hd, d1, d2, d3: d3.d_date_sk
                == cs.cs_ship_date_sk,
            },
        ],
        {
            ""select"": lambda cs, inv, w, i, cd, hd, d1, d2, d3: (
                cs,
                inv,
                w,
                i,
                cd,
                hd,
                d1,
                d2,
                d3,
            ),
            ""where"": lambda cs, inv, w, i, cd, hd, d1, d2, d3: (
                (
                    (
                        (
                            d1.d_week_seq == d2.d_week_seq
                            and inv.inv_quantity_on_hand < cs.cs_quantity
                        )
                        and d3.d_date > d1.d_date + 5
                    )
                    and hd.hd_buy_potential == ""5001-10000""
                )
                and d1.d_year == 2000
            )
            and cd.cd_marital_status == ""M"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda cs, inv, w, i, cd, hd, d1, d2, d3: Auto2(
            item_desc=i.i_item_desc,
            warehouse=w.w_warehouse_name,
            week_seq=d1.d_week_seq,
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            i_item_desc=g.key[""item_desc""],
            w_warehouse_name=g.key[""warehouse""],
            d_week_seq=g.key[""week_seq""],
            no_promo=len([x for x in g if x[0].cs_promo_sk == None]),
            promo=len([x for x in g if x[0].cs_promo_sk != None]),
            total_cnt=len(g),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q72.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q76.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,CustomerAddres
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,StoreReturn
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,Auto1
survived,"def test_TPCDS_Q53_simplified():
    assert result == [
        Auto1(i_manufact_id=1, sum_sales=20.0),
        Auto1(i_manufact_id=2, sum_sales=53.0),
    ]
",tests/dataset/tpc-ds/compiler/py/q53.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,CustomerAddress
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q29.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,S
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Customer
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q32.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q28.py,Auto1
survived,"def _q2():
    _groups = {}
    _order = []
    for ss in store_sales:
        _k = ss.ss_customer_sk
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(ss)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto2(cust=g.key, sales=sum([x.ss_quantity * x.ss_sales_price for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q23.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,DateDim
survived,"def test_TPCDS_Q85_sample():
    assert result == 85.0
",tests/dataset/tpc-ds/compiler/py/q85.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q45.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q32.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q49.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q30.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,DateDim
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q63.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto8
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"def _q0():
    _src = catalog_sales
    _rows = _query(
        _src,
        [
            {""items"": item, ""on"": lambda cs, i: cs.cs_item_sk == i.i_item_sk},
            {
                ""items"": date_dim,
                ""on"": lambda cs, i, d: cs.cs_sold_date_sk == d.d_date_sk,
            },
        ],
        {
            ""select"": lambda cs, i, d: (cs, i, d),
            ""where"": lambda cs, i, d: (
                i.i_category in [""A"", ""B"", ""C""] and d.d_date >= ""2000-02-01""
            )
            and d.d_date <= ""2000-03-02"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda cs, i, d: Auto3(
            id=i.i_item_id,
            desc=i.i_item_desc,
            cat=i.i_category,
            _class=i.i_class,
            price=i.i_current_price,
        ),
    )
    _items1 = _groups
    return [
        Auto2(
            i_item_id=g.key[""id""],
            i_item_desc=g.key[""desc""],
            i_category=g.key[""cat""],
            i_class=g.key[""_class""],
            i_current_price=g.key[""price""],
            itemrevenue=sum([x[0].cs_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q28.py,StoreSale
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": customer_demographics,
                ""on"": lambda ss, cd: ss.ss_cdemo_sk == cd.cd_demo_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda ss, cd, d: ss.ss_sold_date_sk == d.d_date_sk,
            },
            {""items"": item, ""on"": lambda ss, cd, d, i: ss.ss_item_sk == i.i_item_sk},
            {
                ""items"": promotion,
                ""on"": lambda ss, cd, d, i, p: ss.ss_promo_sk == p.p_promo_sk,
            },
        ],
        {
            ""select"": lambda ss, cd, d, i, p: (ss, cd, d, i, p),
            ""where"": lambda ss, cd, d, i, p: (
                (
                    (cd.cd_gender == ""M"" and cd.cd_marital_status == ""S"")
                    and cd.cd_education_status == ""College""
                )
                and (p.p_channel_email == ""N"" or p.p_channel_event == ""N"")
            )
            and d.d_year == 1998,
        },
    )
    _groups = _group_by(_rows, lambda ss, cd, d, i, p: Auto2(i_item_id=i.i_item_id))
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: g.key[""i_item_id""])
    return [
        Auto1(
            i_item_id=g.key[""i_item_id""],
            agg1=_avg([x[0].ss_quantity for x in g]),
            agg2=_avg([x[0].ss_list_price for x in g]),
            agg3=_avg([x[0].ss_coupon_amt for x in g]),
            agg4=_avg([x[0].ss_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q7.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q51.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,StoreSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q43.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,WebSale
survived,"def test_TPCDS_Q50_simplified():
    assert result == [
        Auto1(s_store_name=""Main"", d30=1, d31_60=1, d61_90=1, d91_120=1, d_gt_120=1)
    ]
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q44.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,ShipMode
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Customer
survived,"def distinct(xs):
    out = []
    for x in xs:
        if not x in out:
            out = out + [x]
    return out
",tests/dataset/tpc-ds/compiler/py/q38.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,WebSale
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q79.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q92.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,CatalogSale
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q57.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,StoreReturn
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q90.py,WebPage
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q8.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q56.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Store
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Auto3
survived,"def _q2():
    _src = store_returns
    _rows = _query(
        _src,
        [
            {
                ""items"": date_dim,
                ""on"": lambda sr, d: d.d_date_sk == sr.sr_returned_date_sk,
            }
        ],
        {""select"": lambda sr, d: (sr, d)},
    )
    _groups = _group_by(_rows, lambda sr, d: sr.s_store_sk)
    _items3 = _groups
    return [
        Auto3(
            s_store_sk=g.key,
            returns=_sum([x[0].sr_return_amt for x in g]),
            profit_loss=_sum([x[0].sr_net_loss for x in g]),
        )
        for g in _items3
    ]
",tests/dataset/tpc-ds/compiler/py/q77.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,StoreSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"def test_TPCDS_Q38_simplified():
    assert result == 1
",tests/dataset/tpc-ds/compiler/py/q38.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q42.py,_Group
survived,"def test_TPCDS_Q78_simplified():
    assert result == [
        Auto1(
            ss_sold_year=1998,
            ss_item_sk=1,
            ss_customer_sk=1,
            ratio=1.25,
            store_qty=10,
            store_wholesale_cost=50.0,
            store_sales_price=100.0,
            other_chan_qty=8,
            other_chan_wholesale_cost=40.0,
            other_chan_sales_price=80.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q78.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Auto2
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q71.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,TimeDim
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q37.py,_Group
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q54.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Store
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,DateDim
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q34.py,
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q37.py,_Group
survived,"def _q0():
    _src = web_sales
    _rows = _query(
        _src,
        [
            {""items"": item, ""on"": lambda ws, i: ws.ws_item_sk == i.i_item_sk},
            {
                ""items"": date_dim,
                ""on"": lambda ws, i, d: ws.ws_sold_date_sk == d.d_date_sk,
            },
        ],
        {
            ""select"": lambda ws, i, d: (ws, i, d),
            ""where"": lambda ws, i, d: (
                i.i_category in [""A"", ""B"", ""C""] and d.d_date >= ""2001-01-15""
            )
            and d.d_date <= ""2001-02-14"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda ws, i, d: Auto3(
            id=i.i_item_id,
            desc=i.i_item_desc,
            cat=i.i_category,
            _class=i.i_class,
            price=i.i_current_price,
        ),
    )
    _items1 = _groups
    return [
        Auto2(
            i_item_id=g.key[""id""],
            i_item_desc=g.key[""desc""],
            i_category=g.key[""cat""],
            i_class=g.key[""_class""],
            i_current_price=g.key[""price""],
            itemrevenue=sum([x[0].ws_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q12.py,
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q52.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,Warehouse
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q90.py,HouseholdDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q63.py,Sale
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q70.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,Auto2
survived,"def test_TPCDS_Q87_sample():
    assert result == 87.0
",tests/dataset/tpc-ds/compiler/py/q87.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q73.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,CatalogSale
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q60.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,Auto2
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q73.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q40.py,Auto2
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q57.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q32.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,TimeDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,StoreSale
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,Inventory
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,Store
survived,"    def __init__(self, request=None):
        self.request = request
",openai/__init__.py,APIConnectionError
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _load_metadata(self) -> BundleMetadata:
        with open(self.bundle_dir / ""bundle.json"", encoding=""utf-8"") as f:
            data = json.load(f)
        return BundleMetadata(**data)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    failing_test = bundle_dir / ""tests"" / ""test_main.py""
    failing_test.write_text(""def test_fail():\n    assert False"")
    # update checksum so validation reaches test execution
    import hashlib
    import json

    bundle_file = bundle_dir / ""bundle.json""
    data = json.loads(bundle_file.read_text())
    digest = hashlib.sha256(failing_test.read_bytes()).hexdigest()
    data[""custom""][""checksums""][""tests/test_main.py""] = digest
    bundle_file.write_text(json.dumps(data))
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_unpinned_requirement(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""requirements.txt"").write_text(""pytest>=8"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""unpinned requirement"" in e for e in result.errors)
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_checksum_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""agent.py"").write_text(""broken"")
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""checksum mismatch"" in e for e in result.errors)
",tests/test_bundle_validator.py,
deleted,"    def messages(self) -> List[ChatMessage]:
        return list(self._messages)
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,ChatFormatter
deleted,"    def __init__(
        self, system_message: str, user_input: str, thinking_instructions: str | None
    ) -> None:
        super().__init__(system_message, user_input, thinking_instructions)
        if self.thinking_instructions is None:
            raise ValueError(
                ""thinking_instructions are required when strategy is final_and_intermediate""
            )
",libs/core/kiln_ai/adapters/chat/chat_formatter.py,FinalAndIntermediateFormatter
survived,"    def create_response(
        self,
        response_data: GraphQLHTTPResponse,
        sub_response: Response,
        is_strict: bool,
    ) -> Response:
        sub_response.text = self.encode_json(response_data)
        sub_response.content_type = (
            ""application/graphql-response+json"" if is_strict else ""application/json""
        )
        return sub_response
",src/graphql_server/webob/views.py,GraphQLView
survived,"    def __init__(
        self,
        graphiql: Optional[bool] = None,
        graphql_ide: Optional[GraphQL_IDE] = ""graphiql"",
        allow_queries_via_get: bool = True,
        result_override: ResultOverrideFunction = None,
        multipart_uploads_enabled: bool = False,
    ) -> None:
        self.view = GraphQLView(
            schema=schema,
            graphiql=graphiql,
            graphql_ide=graphql_ide,
            allow_queries_via_get=allow_queries_via_get,
            multipart_uploads_enabled=multipart_uploads_enabled,
        )
        self.view.result_override = result_override
",src/tests/http/clients/webob.py,WebobHttpClient
survived,"def test_service_worker_integrity() -> None:
    dist = BROWSER / ""dist""
    html = (dist / ""index.html"").read_text()
    match = re.search(r'<script[^>]*src=[""\']service-worker.js[""\'][^>]*>', html)
    assert match, ""service-worker.js script tag missing""
    tag = match.group(0)
    integrity = re.search(r'integrity=[""\']([^""\']+)[""\']', tag)
    assert integrity, ""integrity attribute missing""
    expected = sha384(dist / ""sw.js"")
    assert integrity.group(1) == expected",tests/test_sw_integrity.py,
survived,"    def _ensure(self) -> None:
        self.conn.execute(
            """"""
            CREATE TABLE IF NOT EXISTS solutions(
                sector TEXT,
                approach TEXT,
                score DOUBLE,
                band INTEGER,
                data TEXT,
                ts DOUBLE
            )
            """"""
        )
        self.conn.execute(
            ""CREATE INDEX IF NOT EXISTS idx_bins ON solutions(sector, approach, band)""
        )
        if isinstance(self.conn, sqlite3.Connection):
            self.conn.commit()
",src/archive/solution_archive.py,SolutionArchive
survived,"def _run_tests(repo: Path) -> int:
    cmd = [
        ""docker"",
        ""run"",
        ""--rm"",
        ""-v"",
        f""{repo}:/work"",
        ""-w"",
        ""/work"",
        IMAGE,
        ""pytest"",
        ""-q"",
    ]
    proc = subprocess.run(cmd, capture_output=True, text=True)
    return proc.returncode
",src/self_evolution/harness.py,
survived,"def test_vote_and_merge_reverts_on_failure(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    diff = """"""--- a/metric.txt
+++ b/metric.txt
@@
-1
+0
""""""
    reg = StakeRegistry()
    reg.set_stake(""orch"", 1.0)
    with (
        patch.object(harness, ""_run_tests"", return_value=1),
        patch.object(harness, ""run_preflight""),
        patch.object(
            harness.patcher_core, ""apply_patch"", lambda d, repo_path: (Path(repo_path) / ""metric.txt"").write_text(""0\n"")
        ),
    ):
        accepted = harness.vote_and_merge(repo, diff, reg)
    assert not accepted
    assert (repo / ""metric.txt"").read_text().strip() == ""1""",tests/test_self_evolution.py,
survived,"    def __init__(self, repo: str | Path, log_dir: str | Path, registry: StakeRegistry | None = None) -> None:
        self.repo = Path(repo)
        self.log_dir = Path(log_dir)
        self.registry = registry or StakeRegistry()
        if ""meta"" not in self.registry.stakes:
            self.registry.set_stake(""meta"", 1.0)
",src/agents/meta_refinement_agent.py,MetaRefinementAgent
survived,"    def _create_patch(self, bottleneck: str) -> str:
        goal = f""optimise around {bottleneck}""
        metric = self.repo / ""metric.txt""
        if metric.exists():
            try:
                current = int(float(metric.read_text().strip()))
            except Exception:
                current = 0
            new_val = current + 1
            diff = (
                ""--- a/metric.txt\n""
                ""+++ b/metric.txt\n""
                ""@@\n""
                f""-{current}\n""
                f""+{new_val}\n""
            )
            return diff
        return propose_diff(str(metric), goal)
",src/agents/meta_refinement_agent.py,MetaRefinementAgent
survived,"def server() -> Iterator[str]:
    port = _free_port()
    config = uvicorn.Config(evolution_worker.app, host=""127.0.0.1"", port=port, log_level=""warning"")
    server = uvicorn.Server(config)
    thread = threading.Thread(target=server.run, daemon=True)
    thread.start()
    for _ in range(50):
        if server.started:
            break
        time.sleep(0.1)
    yield f""http://127.0.0.1:{port}""
    server.should_exit = True
    thread.join(timeout=5)
",tests/test_evolution_worker_safe_extract.py,
survived,"    def adder(x):
        return x + n
",tests/human/python/closure.py,
survived,"def boom():
    print(""boom"")
    return True
",tests/human/x/python/bool_chain.py,
survived,"def test_pareto_entropy(tmp_path: Path) -> None:
    js_out = tmp_path / ""entropy.js""
    subprocess.run([
        ""tsc"",
        ""--target"",
        ""es2020"",
        ""--module"",
        ""es2020"",
        ENTROPY_TS,
        ""--outFile"",
        js_out,
    ], check=True)

    script = tmp_path / ""run.mjs""
    script.write_text(
        f""import {{ paretoEntropy }} from '{js_out.resolve().as_posix()}';\n""
        ""const pts = [{logic:0.1,feasible:0.1},{logic:0.9,feasible:0.9}];\n""
        ""console.log(paretoEntropy(pts,2).toFixed(2));\n"",
        encoding=""utf-8"",
    )
    res = subprocess.run([""node"", script], capture_output=True, text=True, check=True)
    assert res.stdout.strip() == ""1.00""",tests/test_entropy_ts.py,
survived,"def test_debate_arena() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#arena-panel"")
        page.wait_for_selector(""#arena-panel button"")
        page.click(""#arena-panel button"")
        page.wait_for_selector(""#debate-panel li"")
        page.wait_for_selector(""#ranking li"")
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_debate_arena.py,
survived,"        def patched_session_request(session_self, method, url, *a, **kw):
            if self._proxies and 'proxies' not in kw:
                kw['proxies'] = self._proxies
            return self._original_session_request(session_self, method, url, *a, **kw)
",webscout/Provider/TTI/base.py,_GlobalProxyManager
survived,"    def invoke(self, ctx: click.Context) -> Any:  # pragma: no cover - CLI
        click.echo(DISCLAIMER)
        return super().invoke(ctx)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,DisclaimerGroup
survived,"    def test_experience_stream_yields_event(self) -> None:
        async def get_event():
            gen = demo.experience_stream()
            return await anext(gen)
        evt = asyncio.run(get_event())
        self.assertIsInstance(evt, dict)
        self.assertIn(""kind"", evt)
        self.assertIn(""payload"", evt)
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def test_valid_expression(self) -> None:
        self.assertEqual(safe_eval(""2 + 3 * 4 - 5""), 9)
",tests/test_safe_eval_security.py,TestSafeEval
survived,"    def test_cli_args_override_env(self) -> None:
        env = {""PORT"": ""1111"", ""METRICS_PORT"": ""2222"", ""A2A_PORT"": ""3333"", ""CYCLE"": ""4""}
        argv = [""--port"", ""9000"", ""--metrics-port"", ""9001"", ""--agents"", ""X,Y""]
        with patch.dict(os.environ, env, clear=True):
            args = edge_runner.parse_args(argv)
        self.assertEqual(args.port, 9000)
        self.assertEqual(args.metrics_port, 9001)
        self.assertEqual(args.agents, ""X,Y"")
        # Unspecified flags fall back to environment defaults
        self.assertEqual(args.a2a_port, 3333)
        self.assertEqual(args.cycle, 4)
",tests/test_edge_runner_cli.py,TestParseArgs
survived,"    def test_search_text_glob_with_special_chars(self):
        """"""Glob patterns containing regex special characters should match literally.""""""
        content = """"""
        def func_square():
            print(""value[42]"")

        def func_curly():
            print(""value{bar}"")
        """"""

        matches_square = search_text(r""*\[42\]*"", content=content, is_glob=True)
        assert len(matches_square) == 1
        assert ""[42]"" in matches_square[0].lines[0].line_content

        matches_curly = search_text(""*{bar}*"", content=content, is_glob=True)
        assert len(matches_curly) == 1
        assert ""{bar}"" in matches_curly[0].lines[0].line_content
",test/serena/test_text_utils.py,TestSearchText
survived,"def _remote_available_space(address: str, path: str) -> int | None:
    """"""Return available bytes on remote machine or ``None`` on failure.""""""
    try:
        result = subprocess.run(
            [""ssh"", ""-o"", ""ConnectTimeout=5"", address, ""df"", ""-PB1"", path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
    except subprocess.CalledProcessError:
        return None

    try:
        lines = result.stdout.strip().splitlines()
        if len(lines) >= 2:
            return int(lines[1].split()[3])
    except Exception:
        return None

    return None
",pioreactor/actions/leader/backup_database.py,
survived,"def test_skip_backup_when_worker_has_no_space(tmp_path):
    db_path = tmp_path / ""db.sqlite""
    config[""storage""][""database""] = str(db_path)

    conn = sqlite3.connect(db_path)
    conn.execute(""CREATE TABLE t(id INTEGER)"")
    conn.commit()
    conn.close()

    output = tmp_path / ""backup.sqlite""

    with (
        patch(
            ""pioreactor.actions.leader.backup_database.long_running_managed_lifecycle"",
            dummy_lifecycle,
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.create_logger"",
            return_value=MagicMock(),
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.get_active_workers_in_inventory"",
            return_value=[""worker1""],
        ),
        patch(
            ""pioreactor.actions.leader.backup_database._remote_available_space"",
            return_value=0,
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.rsync"",
        ) as mock_rsync,
    ):
        backup_database(str(output), force=True, backup_to_workers=1)
        mock_rsync.assert_not_called()",pioreactor/tests/test_backup_database.py,
survived,"def test_tracehub_broadcast():
    event = asyncio.run(_run_broadcast())
    assert event[""label""] == ""hi""
    assert event[""type""] == ""tool_call""
",tests/test_trace_hub.py,
survived,"    async def _run_input_guardrails_with_queue(
        cls,
        agent: Agent[Any],
        guardrails: list[InputGuardrail[TContext]],
        input: str | list[TResponseInputItem],
        context: RunContextWrapper[TContext],
        streamed_result: RunResultStreaming,
        parent_span: Span[Any],
    ):
        queue = streamed_result._input_guardrail_queue

        # We'll run the guardrails and push them onto the queue as they complete
        guardrail_tasks = [
            asyncio.create_task(
                RunImpl.run_single_input_guardrail(agent, guardrail, input, context)
            )
            for guardrail in guardrails
        ]
        guardrail_results = []
        try:
            for done in asyncio.as_completed(guardrail_tasks):
                result = await done
                if result.output.tripwire_triggered:
                    _error_tracing.attach_error_to_span(
                        parent_span,
                        SpanError(
                            message=""Guardrail tripwire triggered"",
                            data={
                                ""guardrail"": result.guardrail.get_name(),
                                ""type"": ""input_guardrail"",
                            },
                        ),
                    )
                queue.put_nowait(result)
                guardrail_results.append(result)
        except Exception:
            for t in guardrail_tasks:
                t.cancel()
            raise

        streamed_result.input_guardrail_results = guardrail_results
",src/agents/run.py,DefaultAgentRunner
survived,"    async def _run_input_guardrails(
        cls,
        agent: Agent[Any],
        guardrails: list[InputGuardrail[TContext]],
        input: str | list[TResponseInputItem],
        context: RunContextWrapper[TContext],
    ) -> list[InputGuardrailResult]:
        if not guardrails:
            return []

        guardrail_tasks = [
            asyncio.create_task(
                RunImpl.run_single_input_guardrail(agent, guardrail, input, context)
            )
            for guardrail in guardrails
        ]

        guardrail_results = []

        for done in asyncio.as_completed(guardrail_tasks):
            result = await done
            if result.output.tripwire_triggered:
                # Cancel all guardrail tasks if a tripwire is triggered.
                for t in guardrail_tasks:
                    t.cancel()
                _error_tracing.attach_error_to_current_span(
                    SpanError(
                        message=""Guardrail tripwire triggered"",
                        data={""guardrail"": result.guardrail.get_name()},
                    )
                )
                raise InputGuardrailTripwireTriggered(result)
            else:
                guardrail_results.append(result)

        return guardrail_results
",src/agents/run.py,DefaultAgentRunner
survived,"    async def _run_single_turn_streamed(
        cls,
        streamed_result: RunResultStreaming,
        agent: Agent[TContext],
        hooks: RunHooks[TContext],
        context_wrapper: RunContextWrapper[TContext],
        run_config: RunConfig,
        should_run_agent_start_hooks: bool,
        tool_use_tracker: AgentToolUseTracker,
        all_tools: list[Tool],
        previous_response_id: str | None,
    ) -> SingleStepResult:
        if should_run_agent_start_hooks:
            await asyncio.gather(
                hooks.on_agent_start(context_wrapper, agent),
                (
                    agent.hooks.on_start(context_wrapper, agent)
                    if agent.hooks
                    else _coro.noop_coroutine()
                ),
            )

        output_schema = cls._get_output_schema(agent)

        streamed_result.current_agent = agent
        streamed_result._current_agent_output_schema = output_schema

        system_prompt = await agent.get_system_prompt(context_wrapper)

        handoffs = cls._get_handoffs(agent)
        model = cls._get_model(agent, run_config)
        model_settings = agent.model_settings.resolve(run_config.model_settings)
        model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)

        final_response: ModelResponse | None = None

        input = ItemHelpers.input_to_new_input_list(streamed_result.input)
        input.extend([item.to_input_item() for item in streamed_result.new_items])

        # 1. Stream the output events
        async for event in model.stream_response(
            system_prompt,
            input,
            model_settings,
            all_tools,
            output_schema,
            handoffs,
            get_model_tracing_impl(
                run_config.tracing_disabled, run_config.trace_include_sensitive_data
            ),
            previous_response_id=previous_response_id,
        ):
            if isinstance(event, ResponseCompletedEvent):
                usage = (
                    Usage(
                        requests=1,
                        input_tokens=event.response.usage.input_tokens,
                        output_tokens=event.response.usage.output_tokens,
                        total_tokens=event.response.usage.total_tokens,
                        input_tokens_details=event.response.usage.input_tokens_details,
                        output_tokens_details=event.response.usage.output_tokens_details,
                    )
                    if event.response.usage
                    else Usage()
                )
                final_response = ModelResponse(
                    output=event.response.output,
                    usage=usage,
                    response_id=event.response.id,
                )
                context_wrapper.usage.add(usage)

            streamed_result._event_queue.put_nowait(RawResponsesStreamEvent(data=event))

        # 2. At this point, the streaming is complete for this turn of the agent loop.
        if not final_response:
            raise ModelBehaviorError(""Model did not produce a final response!"")

        # 3. Now, we can process the turn as we do in the non-streaming case
        single_step_result = await cls._get_single_step_result_from_response(
            agent=agent,
            original_input=streamed_result.input,
            pre_step_items=streamed_result.new_items,
            new_response=final_response,
            output_schema=output_schema,
            all_tools=all_tools,
            handoffs=handoffs,
            hooks=hooks,
            context_wrapper=context_wrapper,
            run_config=run_config,
            tool_use_tracker=tool_use_tracker,
        )

        RunImpl.stream_step_result_to_queue(single_step_result, streamed_result._event_queue)
        return single_step_result
",src/agents/run.py,DefaultAgentRunner
survived,"    async def step(self) -> None:
        await self.publish(""alpha.opportunity"", {""alpha"": ""supply-chain bottleneck detected""})
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,AlphaOpportunityAgent
survived,"def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Run alpha_agi_business_v1 locally"")
    parser.add_argument(
        ""--bridge"",
        action=""store_true"",
        help=""Launch OpenAI Agents bridge if available"",
    )
    args = parser.parse_args(argv)

    check_env.main([])

    if args.bridge:
        _start_bridge()

    alpha_agi_business_v1.main([])
",alpha_factory_v1/demos/alpha_agi_business_v1/run_business_v1_local.py,
survived,"def _read_log(limit: int) -> List[Dict[str, str]]:
    path = _ledger_path(None)
    try:
        data = json.loads(Path(path).read_text())
        if isinstance(data, dict):
            data = [data]
        return data[-limit:]
    except Exception:  # pragma: no cover - missing or invalid log
        return []
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,
survived,"def _list_agents() -> list[str]:
    resp = requests.get(f""{HOST}/agents"", timeout=5)
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_agi_business_v1/gradio_dashboard.py,
survived,"def _group_tag_data_by_resource_type(
    tag_data: List[Dict],
    tag_resource_type_mappings: Dict,
) -> Dict[str, List[Dict]]:
    """"""Group raw tag data by the resource types Cartography supports.""""""

    grouped: Dict[str, List[Dict]] = {rtype: [] for rtype in tag_resource_type_mappings}
    for mapping in tag_data:
        rtype = get_resource_type_from_arn(mapping[""ResourceARN""])
        if rtype in grouped:
            grouped[rtype].append(mapping)
        else:
            logger.debug(
                ""Unknown tag resource type %s from ARN %s"",
                rtype,
                mapping[""ResourceARN""],
            )
    return grouped
",cartography/intel/aws/resourcegroupstaggingapi.py,
survived,"    def __init__(self, base_url: str):
        self._client = DummyClient(base_url)
",tests/integrations/openai/test_openai_sdk.py,DummyCompletion
survived,"def test_skip_variable_format(state: State):
    code = 'template = ""Hello {0}""\nresult = template.format(name)'

    new, changed = transform_chunk_from_str(code, state)

    assert not changed",test/test_transform.py,
survived,"async def api_frame_assets_delete(
    id: int,
    path: str = Form(...),
    db: Session = Depends(get_db),
    redis: Redis = Depends(get_redis),
):
    frame = db.get(Frame, id) or _not_found()

    rel_path = path.lstrip(""/"")
    if "".."" in rel_path or ""*"" in rel_path or os.path.isabs(rel_path):
        _bad_request(""Invalid asset path"")

    assets_path = frame.assets_path or ""/srv/assets""
    full_path = os.path.normpath(os.path.join(assets_path, rel_path))
    if not full_path.startswith(os.path.normpath(assets_path)):
        _bad_request(""Invalid asset path"")

    await delete_path(db, redis, frame, full_path)
    return {""message"": ""Deleted""}
",backend/app/api/frames.py,
survived,"    def _get(self: struct_pb2.Struct, key: str, default=None):
        try:
            return self[key]
        except Exception:
            return default
",tests/test_chaos_agent.py,
survived,"    def render() -> None:
        data = [(r.agent.name,) for r in orch.runners.values()]
        _rich_table([""agent""], data)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def test_show_results_export_json(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 1.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            res = CliRunner().invoke(cli.main, [""show-results"", ""--export"", ""json""])
            assert res.output.startswith(""["")
",tests/test_cli.py,
survived,"def test_agents_status_watch_stops_on_interrupt() -> None:
    with patch.object(cli.orchestrator, ""Orchestrator"") as orch_cls:
        orch = orch_cls.return_value
        runner = type(
            ""Runner"",
            (),
            {""agent"": type(""Agent"", (), {""name"": ""AgentY""})()},
        )()
        orch.runners = {""AgentY"": runner}
        with patch.object(cli.time, ""sleep"", side_effect=KeyboardInterrupt):
            result = CliRunner().invoke(cli.main, [""agents-status"", ""--watch""])
        assert ""AgentY"" in result.output
",tests/test_cli.py,
survived,"def test_archive_crud(tmp_path) -> None:
    db = ArchiveDB(tmp_path / ""arch.db"")
    root = ArchiveEntry(""h1"", None, 0.1, 0.0, True, 1.0)
    child = ArchiveEntry(""h2"", ""h1"", 0.2, 0.0, False, 2.0)
    db.add(root)
    db.add(child)

    assert db.get(""h2"") == child
    history = list(db.history(""h2""))
    assert [e.hash for e in history] == [""h2"", ""h1""]
",tests/test_archive.py,
survived,"    def sample(self, k: int, *, lam: float = 10.0, alpha0: float = 0.5) -> List[Agent]:
        agents = self.all()
        if not agents:
            return []
        weights = [1.0 / (1.0 + math.exp(-lam * (a.score - alpha0))) for a in agents]
        chosen = random.choices(agents, weights=weights, k=min(k, len(agents)))
        return chosen
",src/archive/__init__.py,Archive
survived,"    def get(self, h: str) -> ArchiveEntry | None:
        with Session(self.engine) as session:
            row = session.get(_ArchiveRow, h)
            if row is None:
                return None
            return ArchiveEntry(
                hash=row.hash,
                parent=row.parent,
                score=row.score,
                novelty=row.novelty,
                is_live=row.is_live,
                ts=row.ts,
            )
",src/archive/db.py,ArchiveDB
survived,"def test_select_parent_softmax() -> None:
    pop = [
        Candidate(1.0, 1.0),
        Candidate(0.5, 2.0),
        Candidate(2.0, 0.5),
    ]
    temp = 1.0
    expected = softmax(np.asarray([p.fitness * p.novelty for p in pop]) / temp)
    observed = sample_distribution(pop, temp)
    assert np.allclose(observed, expected, atol=0.02)
",tests/test_selector.py,
survived,"def test_invalid_command(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python; rm -rf /""])
",tests/unit/test_sandbox_manager.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"    def critical(self, message: str):
        self.log(LogLevel.CRITICAL, message)
",webscout/litlogger/logger.py,Logger
survived,"    def info(self, message: str):
        self.log(LogLevel.INFO, message)
",webscout/litlogger/logger.py,Logger
survived,"        def decorator(func):
            return func
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,
survived,"def test_get_result_exact_order():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX_ORDER, wanted_list=[""Banana"", ""$2""])
    assert scraper.get_result_exact(html=HTML_COMPLEX_ORDER) == [""Banana"", ""$2""]
",tests/unit/test_features.py,
survived,"def test_save_and_load(tmp_path):
    scraper = AutoScraper()
    scraper.build(html=HTML, wanted_list=[""Banana""])
    file_path = tmp_path / ""model.json""
    scraper.save(file_path)
    new_scraper = AutoScraper()
    new_scraper.load(file_path)
    assert new_scraper.get_result_exact(html=HTML) == scraper.get_result_exact(html=HTML)
",tests/unit/test_features.py,
survived,"def test_group_by_alias():
    scraper = AutoScraper()
    scraper.build(html=HTML, wanted_dict={""fruit"": [""Banana""]})
    similar = scraper.get_result_similar(
        html=HTML, group_by_alias=True, contain_sibling_leaves=True, unique=True
    )
    assert similar == {""fruit"": [""Banana"", ""Apple"", ""Orange""]}
",tests/unit/test_features.py,
survived,"def test_remove_rules():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX, wanted_list=[""Banana""])
    scraper.build(html=HTML_COMPLEX, wanted_list=[""Apple""], update=True)
    rule_ids = [s[""stack_id""] for s in scraper.stack_list]
    to_remove = rule_ids[0]
    scraper.remove_rules([to_remove])
    remaining = [s[""stack_id""] for s in scraper.stack_list]
    assert to_remove not in remaining
    assert len(remaining) == len(rule_ids) - 1
",tests/integration/test_complex_features.py,
survived,"    def __init__(self, name, attrs, parent=None):
        self.name = name
        self.attrs = dict(attrs)
        self.parent = parent
        self.children = []
        self.text = """"
",tests/conftest.py,_Node
survived,"    def find_previous_siblings(self, name=None, attrs={}, limit=None, **kwargs) -> List[Tag]:
        """"""Find all previous siblings matching given criteria.""""""
        if not self._soup.parent:
            return []

        siblings = []
        siblings_list = self._soup.parent.contents
        try:
            current_index = siblings_list.index(self._soup)
            for sibling in reversed(siblings_list[:current_index]):
                if isinstance(sibling, Tag):
                    if (name is None or sibling.name == name) and all(
                        sibling.get(k) == v for k, v in attrs.items()
                    ):
                        siblings.append(sibling)
                        if limit and len(siblings) == limit:
                            break
        except ValueError:
            pass
        return siblings
",webscout/scout/core/scout.py,Scout
survived,"def test_workflow_with_json_parameter(
    model_manager: ModelManager,
    dogs_image: np.ndarray,
) -> None:
    workflow_init_parameters = {
        ""workflows_core.model_manager"": model_manager,
        ""workflows_core.api_key"": None,
        ""workflows_core.step_execution_mode"": StepExecutionMode.LOCAL,
    }
    execution_engine = ExecutionEngine.init(
        workflow_definition=JSON_PARSER_WORKFLOW,
        init_parameters=workflow_init_parameters,
        max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,
    )

    result = execution_engine.run(
        runtime_parameters={
            ""image"": dogs_image,
            ""config"": ""{\""model_id\"": \""yolov8n-640\""}"",
        }
    )

    assert len(result) == 1
    assert set(result[0].keys()) == {""json_parser"", ""model_predictions""}
    assert result[0][""json_parser""] == ""yolov8n-640""
    assert isinstance(result[0][""model_predictions""], sv.Detections)",tests/workflows/integration_tests/execution/test_workflow_json_parser_config.py,
survived,"def append_pkg_resource_path_KLONGPATH() -> None:
    with importlib.resources.as_file(importlib.resources.files('klongpy')) as pkg_path:
        pkg_lib_path = os.path.join(pkg_path, 'lib')
        klongpath = os.environ.get('KLONGPATH', '.:lib')
        klongpath = f""{klongpath}:{pkg_lib_path}"" if klongpath else str(pkg_lib_path)
        os.environ['KLONGPATH'] = klongpath
",klongpy/repl.py,
survived,"        def fake_start_bridge(host: str, runtime_port: int) -> None:  # type: ignore
            captured['env'] = os.getenv('AGENTS_RUNTIME_PORT')
            captured['port'] = runtime_port
",tests/test_alpha_business_v1_script.py,TestAlphaBusinessV1Script
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q4.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q8.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q6.py,
survived,"def test_Q1_returns_min_note__title_and_year_for_top_ranked_co_production():
    assert result == {
        ""production_note"": ""ACME (co-production)"",
        ""movie_title"": ""Good Movie"",
        ""movie_year"": 1995,
    }
",tests/dataset/job/compiler/py/q1.py,
survived,"                def handle(self, _msg: dict) -> None:  # noqa: D401
                    pass
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,StepAdapter
survived,"def _emit_kafka(topic: str, payload: str) -> None:
    if _PRODUCER is None:
        return
    try:
        _PRODUCER.send(topic, payload)
        _PRODUCER.flush()
    except Exception:  # noqa: BLE001
        logger.exception(""Kafka emit failed (topic=%s)"", topic)
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def Counter(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Counter, name, desc, labels)
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def _get_metric(cls, name: str, desc: str, labels=None):
        return _reg_metric(cls, name, desc, labels)
",alpha_factory_v1/backend/agents/registry.py,
survived,"def _should_register(meta: AgentMetadata) -> bool:
    if meta.name.lower() in _DISABLED:
        logger.info(""Agent %s disabled via env"", meta.name)
        return False
    if meta.name == ""ping"" and os.getenv(""AF_DISABLE_PING_AGENT"", """").lower() in (""1"", ""true""):
        logger.info(""Ping agent disabled via AF_DISABLE_PING_AGENT"")
        return False
    if meta.requires_api_key and not _OPENAI_READY:
        logger.warning(""Skipping %s (needs OpenAI key)"", meta.name)
        return False
    return True
",alpha_factory_v1/backend/agents/registry.py,
survived,"def get_agent(name: str, **kwargs):
    """"""Instantiate agent by *name* and wrap its async ``step`` coroutine.""""""
    with _REGISTRY_LOCK:
        meta = AGENT_REGISTRY[name]
    agent = meta.instantiate(**kwargs)

    if hasattr(agent, ""step"") and inspect.iscoroutinefunction(agent.step):
        orig = agent.step

        async def _wrapped(*a, **kw):  # type: ignore[no-untyped-def]
            t0 = time.perf_counter()
            ok = True
            try:
                return await orig(*a, **kw)  # type: ignore[misc]
            except Exception:  # noqa: BLE001
                ok = False
                raise
            finally:
                _HEALTH_Q.put((meta.name, (time.perf_counter() - t0) * 1000, ok))

        agent.step = _wrapped  # type: ignore[assignment]

    return agent
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def test_bridge_market_data_output(self) -> None:
        """"""Bridge handles CSV input and prints agent summary.""""""
        import tempfile

        with tempfile.NamedTemporaryFile(""w"", delete=False) as fh:
            fh.write(""1,2,3"")
            csv_file = fh.name

        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge"",
                ""--episodes"",
                ""1"",
                ""--market-data"",
                csv_file,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def main() -> None:
    # Environment checks
    run([""python"", ""alpha_factory_v1/scripts/preflight.py""])
    run([""node"", str(BROWSER_DIR / ""build/version_check.js"")])
    run([""python"", ""scripts/check_python_deps.py""])
    run([""python"", ""check_env.py"", ""--auto-install""])
    run([""python"", ""scripts/verify_disclaimer_snippet.py""])
    run([""python"", ""-m"", ""alpha_factory_v1.demos.validate_demos""])

    # Rebuild docs and gallery
    run([""npm"", ""--prefix"", str(BROWSER_DIR), ""run"", ""fetch-assets""])
    run([""npm"", ""--prefix"", str(BROWSER_DIR), ""ci""])
    run([""scripts/build_insight_docs.sh""])
    run([""python"", ""scripts/generate_demo_docs.py""])
    run([""python"", ""scripts/generate_gallery_html.py""])

    # Build and deploy
    run([""mkdocs"", ""build"", ""--strict""])
    run([""python"", ""scripts/verify_workbox_hash.py"", ""site/alpha_agi_insight_v1""])
    run([""mkdocs"", ""gh-deploy"", ""--force""])

    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"")
    repo_path = repo_path.removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    url = f""https://{org}.github.io/{repo}/""
    print(""Demo gallery deployed successfully."")
    print(f""Browse to {url} and explore each demo under gallery.html."")
",scripts/publish_demo_gallery.py,
survived,"    def test_shutdown_called_on_exit(self) -> None:
        orchestrator._OAI._runtime = None
        orchestrator._OAI._hooked = False
        stub = mock.MagicMock()
        handlers = []
        with mock.patch.object(orchestrator, ""AgentRuntime"", return_value=stub, create=True):
            with mock.patch.object(orchestrator.atexit, ""register"", side_effect=lambda h: handlers.append(h)) as reg:
                self.assertIs(orchestrator._OAI.runtime(), stub)
                reg.assert_called_once()
        self.assertEqual(len(handlers), 1)
        handlers[0]()
        stub.shutdown.assert_called_once()
",tests/test_oai_runtime.py,TestOAIRuntime
survived,"def test_safety_agent_halts_on_large_loss(monkeypatch):
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    mod = _reload_module(monkeypatch)
    mod.A2ABus._subs = {}
    safety = mod.BasicSafetyAgent()
    msgs: list[dict] = []
    mod.A2ABus.subscribe(""orch"", lambda m: msgs.append(m))
    safety.handle({""loss"": 5000.0})
    assert {""cmd"": ""stop""} in msgs
",tests/test_world_model_safety.py,
survived,"def _update_checksum(name: str, digest: bytes, algo: str) -> None:
    """"""Rewrite the expected checksum for *name* in fetch_assets.py.""""""

    path = Path(__file__).resolve()
    text = path.read_text()
    b64 = base64.b64encode(digest).decode()
    new_val = f""{algo}-{b64}""
    pattern = rf'""{re.escape(name)}"":\s*""[^""]+""'
    text = re.sub(pattern, f'""{name}"": ""{new_val}""', text)
    path.write_text(text)
    CHECKSUMS[name] = new_val
",scripts/fetch_assets.py,
survived,"def _send_analysis_email(report: str) -> None:
    recipients = [e.strip() for e in os.getenv(""MAINTAINERS_EMAILS"", """").split("","") if e.strip()]
    if not recipients:
        return
    msg = EmailMessage()
    msg[""Subject""] = ""Weekly Static Analysis Report""
    msg[""From""] = os.getenv(""SMTP_FROM"", ""noreply@alpha-factory.local"")
    msg[""To""] = "", "".join(recipients)
    msg.set_content(report)
    server = os.getenv(""SMTP_SERVER"", ""localhost"")
    port = int(os.getenv(""SMTP_PORT"", ""25""))
    user = os.getenv(""SMTP_USER"")
    password = os.getenv(""SMTP_PASSWORD"")
    try:
        with smtplib.SMTP(server, port) as s:
            if user and password:
                s.login(user, password)
            s.send_message(msg)
    except Exception as exc:  # pragma: no cover - SMTP errors
        _log.warning(""static analysis email failed: %s"", exc)
",src/interface/api_server.py,
survived,"def test_pareto_rank_deterministic() -> None:
    pop = [
        Candidate(0.8, 40, 10),
        Candidate(0.9, 45, 20),
        Candidate(0.6, 60, 15),
    ]
    rng = random.Random(0)
    selections = [select_parent(pop, epsilon=0.0, rng=rng) for _ in range(100)]
    assert all(s is not pop[1] for s in selections)
",tests/test_sim_selector.py,
survived,"    def op(_g: str) -> str:
        return ""asdf qwer zxcv""  # nonsense thesis
",tests/test_reviewer_agent.py,
survived,"def test_nonsense_rejected() -> None:
    reviewer = ReviewerAgent()
    archive = InMemoryArchive()

    def op(_g: str) -> str:
        return ""asdf qwer zxcv""  # nonsense thesis

    asyncio.run(
        evolve(
            op,
            _noop_eval,
            archive,
            max_cost=0.02,
            reviewer=reviewer,
        )
    )

    # Only the seed candidate should be present
    assert len(archive.all()) == 1
    assert archive.all()[0].genome == 0.0",tests/test_reviewer_agent.py,
survived,"def verify_onchain(proof: str) -> bool:
    """"""Placeholder for on-chain verification.""""""
    return bool(proof) and all(c in ""0123456789abcdef"" for c in proof)",src/snark/proof.py,
survived,"def test_uses_subdirs_false(monkeypatch, tmp_path):
    monkeypatch.setenv('NOTES_EXPORT_USE_SUBDIRS', 'false')
    tracker = utils.NotesExportTracker(root_directory=str(tmp_path))
    assert tracker._uses_subdirs() is False
",tests/test_tracker.py,
survived,"    def _close(self) -> None:
        if not self._producer:
            return
        try:
            self._producer.flush()
            self._producer.close()
        except Exception:  # noqa: BLE001
            log.exception(""Kafka producer close failed"")
",alpha_factory_v1/backend/agent_runner.py,EventBus
survived,"def _verify(dest: Path) -> None:
    """"""Validate the SHA-256 checksum if known.""""""
    expected = CHECKSUMS.get(dest.name)
    if not expected:
        return
    digest = hashlib.sha256(dest.read_bytes()).hexdigest()
    if digest != expected:
        raise RuntimeError(f""Checksum mismatch for {dest.name}"")
",scripts/download_openai_gpt2.py,
survived,"def test_get_file(file_store):
    filename = ""get_file.txt""
    content = b""hello""
    # create file manually
    file_path = os.path.join(file_store.base_dir, filename)
    with open(file_path, ""wb"") as f:
        f.write(content)

    encoded = file_store.get_file(filename)
    assert encoded == base64.b64encode(content).decode(""utf-8"")",tests/filestore/test_filestore.py,
survived,"    async def __aenter__(self) -> ""Ledger"":
        """"""Start the Merkle broadcast task and return ``self``.""""""
        self.start_merkle_task()
        return self
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"async def get_order(order_id: int, ctx: EnrichContext) -> Order:
    client = await _client(ctx)
    resp = await client.get(f""/orders/{order_id}"")
    resp.raise_for_status()
    return Order(**resp.json())
",examples/shop_api_gateway/app.py,
survived,"def display(nums):
    s = ""[""
    i = 0
    while i < len(nums):
        if i > 0:
            s = s + "", ""
        s = s + str(nums[i])
        i = i + 1
    s = s + ""]""
    return s
",tests/rosetta/transpiler/Python/boyer-moore-string-search.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bulls-and-cows.py,
survived,"def bigrat(a, b):
    return (Fraction(a)) // (Fraction(b))
",tests/rosetta/transpiler/Python/calkin-wilf-sequence.py,
survived,"def mysum(x, y):
    return x + y
",tests/rosetta/transpiler/Python/call-a-function-12.py,
survived,"def makePatterns():
    digits = [""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]
    pats = []
    i = 0
    while i < len(digits):
        j = 0
        while j < len(digits):
            if j != i:
                k = 0
                while k < len(digits):
                    if k != i and k != j:
                        l = 0
                        while l < len(digits):
                            if l != i and l != j and l != k:
                                pats = pats + [digits[i] + digits[j] + digits[k] + digits[l]]
                            l = l + 1
                    k = k + 1
            j = j + 1
        i = i + 1
    return pats
",tests/rosetta/transpiler/Python/bulls-and-cows-player.py,
survived,"def partialSum(x):
    return lambda y: mysum(x, y)
",tests/rosetta/transpiler/Python/call-a-function-12.py,
survived,"def bwt(s):
    if contains(s, stx) or contains(s, etx):
        return {""err"": True, ""res"": """"}
    s = stx + s + etx
    le = len(s)
    table = []
    i = 0
    while i < le:
        rot = """".join(s[i:le]) + """".join(s[0:i])
        table = table + [rot]
        i = i + 1
    table = sortStrings(table)
    last = """"
    i = 0
    while i < le:
        last = last + """".join(table[i][le - 1:le])
        i = i + 1
    return {""err"": False, ""res"": last}
",tests/rosetta/transpiler/Python/burrows-wheeler-transform.py,
survived,"def newFactory():
    sn = 0
    def New():
        global sn
        sn = sn + 1
        b = Box(secret=sn)
        if sn == 1:
            b = dataclasses.replace(b, Contents=""rabbit"")
        else:
            if sn == 2:
                b = dataclasses.replace(b, Contents=""rock"")
        return b
    def Count():
        return sn
    return [New, Count]
",tests/rosetta/transpiler/Python/call-an-object-method-2.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/bulls-and-cows-player.py,
survived,"def new(graphdb_filename, overwrite):
    """"""Create a new empty graph database.""""""
    if not os.path.exists(graphdb_filename) or overwrite:
        click.echo(f""Creating graph database '{graphdb_filename}'"")
        GraphDatabase(graphdb_filename, overwrite=overwrite)
    else:
        click.echo(
            f""Graph database '{graphdb_filename}' already exists. Use -o to overwrite""
        )
",pygs/graphserver/cli.py,
survived,"def _run(strategy: str, iterations: int, *, seed: int) -> Tuple[float, float]:
    random.seed(seed)
    np.random.seed(seed)
    pop = [_Candidate(0.0, _fitness(0.0), 1.0)]
    for _ in range(iterations):
        if strategy == ""v2"":
            parent = _select_softmax(pop)
        elif strategy == ""greedy"":
            parent = max(pop, key=lambda c: c.fitness)
        else:  # pragma: no cover - invalid option
            raise ValueError(f""unknown strategy: {strategy}"")
        genome = _mutate(parent.genome)
        cand = _Candidate(genome, _fitness(genome), random.random())
        pop.append(cand)
    best = max(c.fitness for c in pop)
    mean = sum(c.fitness for c in pop) / len(pop)
    return best, mean
",experiments/ablate_selector.py,
survived,"def set_notification_settings(settings):
    set_global_setting('notification_settings', settings)",users_db.py,
survived,"            def run(self) -> None:
                import asyncio

                if self._agent is None:
                    raise RuntimeError(""No agent registered"")
                asyncio.run(self._runner.run(self._agent, """"))
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,_FallbackAgentRuntime
survived,"    def g(x, y):
        x2 = x * x
        x2 = x2 + c
        return x2 % y
",tests/rosetta/transpiler/Python/euclid-mullin-sequence.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/eulers-sum-of-powers-conjecture.py,
survived,"def binom(n, k):
    if k < 0 or k > n:
        sys.exit(0)
    kk = k
    if kk > n - kk:
        kk = n - kk
    res = 1
    i = 0
    while i < kk:
        res = res * ((n - i))
        i = i + 1
        res = res // (i)
    sys.exit(res)
",tests/rosetta/transpiler/Python/evaluate-binomial-coefficients.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-word-fractal.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    sizes = [0, 1, 9, 10, 99, 100, 1234, 50000, 730000, 8200000]
    showDistribution(sizes)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/file-size-distribution.py,
survived,"def absBig(x):
    if x < zero:
        sys.exit(zero - x)
    sys.exit(x)
",tests/rosetta/transpiler/Python/euclid-mullin-sequence.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-n-step-number-sequences.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(str(binom(5, 3)))
    print(str(binom(60, 30)))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/evaluate-binomial-coefficients.py,
survived,"def libMain():
    seq = hailstone(27)
    print("""")
    print(""Hailstone sequence for the number 27:"")
    print(""  has "" + str(len(seq)) + "" elements"")
    print(""  starts with "" + listString(seq[0:4]))
    print(""  ends with "" + listString(seq[len(seq) - 4:len(seq)]))
    longest = 0
    length = 0
    i = 1
    while i < 100000:
        l = len(hailstone(i))
        if l > length:
            longest = i
            length = l
        i = i + 1
    print("""")
    print(str(longest) + "" has the longest Hailstone sequence, its length being "" + str(length) + ""."")
",tests/rosetta/transpiler/Python/executable-library.py,
survived,"def foo():
    print(""let's foo..."")
    a = []
    if 12 >= len(a):
        sys.exit(""runtime error: index out of range [12] with length "" + str(len(a)))
    a[12] = 0
    sys.exit("""")
",tests/rosetta/transpiler/Python/exceptions.py,
survived,"def double(i):
    return i * 2
",tests/rosetta/transpiler/Python/ethiopian-multiplication.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/evolutionary-algorithm.py,
survived,"def eulerSum():
    pow5 = []
    i = 0
    while i < 250:
        pow5 = pow5 + [i * i * i * i * i]
        i = i + 1
    sums = {}
    x2 = 2
    while x2 < 250:
        x3 = 1
        while x3 < x2:
            s = pow5[x2] + pow5[x3]
            if not (s in sums):
                sums[s] = [x2, x3]
            x3 = x3 + 1
        x2 = x2 + 1
    x0 = 4
    while x0 < 250:
        x1 = 3
        while x1 < x0:
            y = x0 + 1
            while y < 250:
                rem = pow5[y] - pow5[x0] - pow5[x1]
                if rem in sums:
                    pair = sums[rem]
                    a = pair[0]
                    b = pair[1]
                    if x1 > a and a > b:
                        sys.exit([x0, x1, a, b, y])
                y = y + 1
            x1 = x1 + 1
        x0 = x0 + 1
    sys.exit([0, 0, 0, 0, 0])
",tests/rosetta/transpiler/Python/eulers-sum-of-powers-conjecture.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    err = foo()
    if len(err) > 0:
        print(""Recovered from "" + err)
    print(""glad that's over."")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/exceptions.py,
survived,"def newCoolingRate(k):
    sys.exit(lambda dt: -k * dt)
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-word.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            sys.exit(i)
        i = i + 1
    sys.exit(0 - 1)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,
survived,"def test_create_streamable_http_app_sets_state():
    server = FastMCP(name=""StateTest"")
    app = create_streamable_http_app(server, ""/mcp"")
    assert app.state.fastmcp_server is server
",tests/server/test_app_state.py,
survived,"    async def run():
        async with lifespan(app) as ctx:
            session_factory = ctx[""session_factory""]
            async with session_factory() as session:
                await session.execute(text(""SELECT 1""))
",tests/test_sqlalchemy_autogen_extra.py,
survived,"        def _decorator(func):
            return func
",tests/test_agents_fallback.py,
survived,"def test_devicon_readme():
    file = MockFile('README.md')
    assert devicons.devicon(file) == 'î˜‰'
",tests/test_devicons.py,
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - stub
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,DummyLedger
survived,"def test_skip_unsafe_execution(monkeypatch) -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    ledger = DummyLedger()
    agent = codegen_agent.CodeGenAgent(bus, ledger)

    called = False

    def fake_exec(code: str) -> tuple[str, str]:
        nonlocal called
        called = True
        return """", """"

    monkeypatch.setattr(codegen_agent, ""is_code_safe"", lambda c: False)
    monkeypatch.setattr(agent, ""execute_in_sandbox"", fake_exec)

    env = messaging.Envelope(sender=""market"", recipient=""codegen"", ts=0.0)
    env.payload.update({""analysis"": ""x""})
    asyncio.run(agent.handle(env))
    assert not called",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,
survived,"def test_search_capabilities(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""basic"", TemplateCategory.CONVERSATION), ""c1"")
    m2 = _meta(""web"", TemplateCategory.CONVERSATION)
    m2.requires_web_search = True
    reg.register(m2, ""c2"")

    engine = TemplateSearchEngine(reg)
    none = engine.search(""c"", capabilities=[])
    assert [r.slug for r in none] == [""basic""]

    cap = engine.search(""c"", capabilities=[""web_search""])
    slugs = {r.slug for r in cap}
    assert slugs == {""basic"", ""web""}",tests/test_template_search.py,
survived,"def test_search_hello_world() -> None:
    reg = TemplateRegistry()
    engine = TemplateSearchEngine(reg)
    results = engine.search(""hello"")
    assert any(r.slug == ""hello-world"" for r in results)",tests/test_hello_world_template.py,
survived,"    def do_rollout(self) -> list[RolloutGroup]:
        if not hasattr(self, ""_counter""):
            self._counter = 0

        # Simulate calling the inference server (here we just echo text)
        response_text = f""Hello #{self._counter} from {self._inference.address}""

        turn = Turn(
            message=response_text,
            role=""assistant"",
            logprobs=None,
            reward=0.0,
            inference_metadata={""model"": ""dummy""},
        )
        rollout = Rollout(turns=[turn], metadata={""iteration"": self._counter})
        group = RolloutGroup(
            id=f""hello-{self._counter}"",
            source=""hello_env"",
            created=time.time(),
            rollouts=[rollout],
            metadata={},
        )

        self._counter += 1
        time.sleep(1.0)  # pace output without async knowledge
        return [group]
",marin/rl/envs/hello.py,HelloWorldEnv
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        domain = obs.get(""domain"", ""finance"") if isinstance(obs, dict) else ""finance""
        alphas = await discover_alpha(domain)
        first = alphas.split(""\n"")[0].strip()
        plan = await convert_alpha_tool(first)
        return {""alpha"": first, ""plan"": plan}
",alpha_factory_v1/demos/aiga_meta_evolution/workflow_demo.py,WorkflowAgent
survived,"    def reset(self) -> int:
        """"""Reset the environment and return the initial state.""""""
        self.state = 0
        return self.state
",alpha_factory_v1/demos/era_of_experience/simulation/env_stub.py,SimpleExperienceEnv
survived,"    def launch_dashboard() -> None:  # type: ignore[return-type]
        """"""Placeholder when optional dependencies are absent.""""""
        raise RuntimeError(
            ""gradio and other optional packages are required for the MuZero demo""
        )
",alpha_factory_v1/demos/muzero_planning/__init__.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bioinformatics-global-alignment.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/binary-search.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitwise-io-2.py,
survived,"async def test_explore_data_model_returns_summary() -> None:
    app = EnrichMCP(""My API"", description=""Demo server"")

    @app.entity(description=""Test entity"")
    class Item(EnrichModel):
        id: int = Field(description=""Identifier"")

    tool_name = ""explore_my_api_data_model""
    assert tool_name in app.resources

    summary = await app.resources[tool_name]()
    assert isinstance(summary, DataModelSummary)
    assert summary.title == ""My API""
    assert summary.entity_count == 1
    assert summary.entities == [""Item""]
    summary_text = str(summary)
    assert ""# My API"" in summary_text
    assert ""**Entity count:** 1"" in summary_text
    assert ""- Item"" in summary_text

    tools = await app.mcp.list_tools()
    tool = next(t for t in tools if t.name == tool_name)
    assert ""Call this tool FIRST"" in tool.description
    assert ""Demo server"" in tool.description",tests/test_explore_data_model.py,
survived,"def load_df(db_path: str | Path) -> pd.DataFrame:
    """"""Return archive contents as a DataFrame.""""""
    arch = Archive(db_path)
    rows = []
    for a in arch.all():
        rows.append(
            {
                ""id"": a.id,
                ""parent"": a.meta.get(""parent""),
                ""patch"": a.meta.get(""diff"") or a.meta.get(""patch""),
                ""score"": a.score,
            }
        )
    return pd.DataFrame(rows)
",src/interface/lineage_dashboard.py,
survived,"    async def step(self) -> None:
        # Pretend to compute an optimal plan
        await self.publish(""alpha.plan"", {""plan"": ""explore_market""})
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,PlanningAgent
survived,"def test_self_improve_template_parses(tmp_path, monkeypatch):
    data = {""system"": ""sys"", ""user"": ""usr""}
    path = tmp_path / ""tpl.yaml""
    path.write_text(yaml.safe_dump(data), encoding=""utf-8"")
    monkeypatch.setenv(""SELF_IMPROVE_TEMPLATE"", str(path))
    config.init_config()
    cfg = config.Settings()
    assert cfg.self_improve.system == ""sys""
    assert cfg.self_improve.user == ""usr""",tests/test_prompts.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        points = [
            {""category"": c or ""general"", ""count"": i}
            for i, c in enumerate(payload.get(""categories"") or [""general""], start=1)
        ]
        score = sum(p[""count""] for p in points) / len(points)
        return {
            ""drag_points"": points,
            ""summary_score"": round(score, 2),
        }",servers/server_clear_thought/tools/drag_point_audit.py,DragPointAudit
survived,"def random_confidences(n: int) -> List[float]:
    return [round(random.uniform(0.5, 1.0), 2) for _ in range(n)]",servers/server_clear_thought/core/utils.py,
survived,"def test_analogical_mapper():
    client = get_client()
    resp = client.post(
        ""/analogical-mapper/execute"",
        json={""problem"": ""p""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""analogies"", ""suggested_prompts""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"    def get_router(cls) -> APIRouter:
        router = APIRouter()
        OutputModel = cls.OutputSchema

        @router.post(""/execute"", response_model=OutputModel)
        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)

        return router
",servers/server_clear_thought/core/base_tool.py,BaseTool
survived,"def test_drag_point_audit():
    client = get_client()
    resp = client.post(
        ""/drag-point-audit/execute"",
        json={""log"": ""...""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""drag_points"", ""summary_score""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        tools = payload.get(""downstream_tools"") or [f""tool_{i}"" for i in range(7)]
        results = [{""tool"": t, ""result"": f""{payload['query']} -> {t}""} for t in tools]
        resonance = {t: 1.0 for t in tools}
        synthesis = ""; "".join(r[""result""] for r in results)
        return {
            ""seeker_results"": results,
            ""resonance_map"": resonance,
            ""synthesis"": synthesis,
        }",servers/server_clear_thought/tools/seven_seekers_orchestrator.py,SevenSeekersOrchestrator
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        return {""echoed"": payload[""text""]}",servers/server_clear_thought/tools/existing_tool_example.py,ExistingToolExample
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        return {""echoed"": payload[""text""]}",servers/server_clear_thought/tools/existing_tool_example.py,ExistingToolExample
survived,"    def __repr__(self) -> str:  # pragma: no cover - trivial
        data = self.model_dump()
        for k in tuple(data):
            if any(s in k.lower() for s in (""token"", ""key"", ""password"")) and data[k]:
                data[k] = ""***""
        return f""Settings({data})""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/config.py,Settings
survived,"        async def policy(self, *_: object) -> object:
            return None
",alpha_factory_v1/demos/muzero_planning/agent_muzero_entrypoint.py,Agent
survived,"    async def send(self, name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """"""Post ``payload`` to the endpoint identified by ``name``.""""""
        if name not in self.endpoints:
            raise ValueError(f""Unknown endpoint '{name}'"")
        cfg = self.endpoints[name]
        async with self._sem:
            async with self._session.post(
                cfg.url,
                json=payload,
                headers=cfg.headers,
                timeout=self.timeout,
            ) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise ValueError(f""API error: {resp.status} - {text}"")
                return await resp.json()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    async def close(self) -> None:
        """"""Close the underlying HTTP session.""""""
        close_fn = getattr(self._session, ""close"", None)
        if close_fn is None:
            return
        if asyncio.iscoroutinefunction(close_fn):
            await close_fn()
        else:
            close_fn()
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"        async def wrapped_run(*args: Any, **kwargs: Any) -> Any:
            result = await orig_run(*args, **kwargs)
            span_data = (
                getattr(result, ""span_graph"", None)
                or getattr(result, ""spans"", None)
                or getattr(result, ""trace"", None)
            )
            if span_data is not None:
                try:
                    await self.send(endpoint, span_data)  # type: ignore[arg-type]
                except Exception as exc:  # pragma: no cover - log only
                    logger.error(""Failed to send telemetry: %s"", exc)
            return result
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    def __init__(
        self,
        endpoints: Dict[str, EndpointConfig],
        *,
        rate_limit: int = 5,
        timeout: int = 10,
    ) -> None:
        if not endpoints:
            raise ValueError(""At least one endpoint must be configured"")
        self.endpoints = endpoints
        self.timeout = timeout
        self._sem = asyncio.Semaphore(rate_limit)
        self._session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=None)
        )
",src/meta_agent/services/telemetry_client.py,TelemetryAPIClient
survived,"    async def json(self):
        return {}
",src/aiohttp/__init__.py,Response
survived,"async def test_send_retry_failure():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        mock_session.return_value.close = AsyncMock()

        client = TelemetryAPIClient(
            {""trace"": EndpointConfig(""http://example.com"")}, retries=1, backoff=0
        )
        with pytest.raises(Exception):
            await client.send(""trace"", {""d"": 1})
        assert mock_session.return_value.post.call_count == 2
        await client.close()",tests/unit/test_telemetry_client.py,
survived,"def test_collector_with_db(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    collector = TelemetryCollector(db=db, include_sensitive=False)
    collector.start_timer()
    collector.stop_timer()
    line = collector.summary_line()
    assert ""<redacted>"" in line
    assert db.fetch_all()
    db.close()",tests/unit/test_telemetry_db.py,
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def test_archive(tmp_path):
    db = TelemetryDB(tmp_path / ""tele.db"")
    db.record(5, 0.02, 0.3, 1)
    archive_path = db.archive(tmp_path / ""out.gz"")
    with gzip.open(archive_path, ""rt"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[0][""guardrail_hits""] == 1
    db.close()
",tests/unit/test_telemetry_db.py,
survived,"def test_purge_old(tmp_path):
    db_path = tmp_path / ""tele.db""
    db = TelemetryDB(db_path, retention_days=1)
    db.record(1, 0.01, 0.1, 0)
    # update timestamp to old date
    old_ts = ""2000-01-01T00:00:00""
    db.conn.execute(""UPDATE telemetry SET timestamp=?"", (old_ts,))
    db.conn.commit()
    db.purge_old()
    assert db.fetch_all() == []
    db.close()
",tests/unit/test_telemetry_db.py,
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"        def __exit__(self, *exc: object) -> None:
            pass
",tests/test_check_env_network.py,_Resp
survived,"def compute_score(data: dict) -> int:
    """"""Return a simplified Axe score (0-100).""""""
    total = 0
    for violation in data.get(""violations"", []):
        impact = violation.get(""impact"", ""minor"")
        total += WEIGHTS.get(impact, 1)
    score = max(0, 100 - total)
    return score
",scripts/axe_score.py,
survived,"  def _log_command(self, name: str, **kwargs) -> None:
    params = "", "".join(f""{k}={self._format_param(v)}"" for k, v in kwargs.items())
    logger.debug(""%s(%s)"", name, params)
",pylabrobot/liquid_handling/liquid_handler.py,LiquidHandler
survived,"  def _format_param(self, value: Any) -> Any:
    """"""Format parameters for logging.""""""
    if isinstance(value, Resource):
      return value.name
    try:
      if isinstance(value, Sequence) and len(value) > 0 and isinstance(value[0], Resource):
        return [v.name for v in value]
    except Exception:
      pass
    return value
",pylabrobot/liquid_handling/liquid_handler.py,LiquidHandler
survived,"def speech_to_text():
    try:
        if 'audio' not in request.files:
            return {'code': 1, 'message': 'Missing audio file'}, 400

        file_obj = request.files['audio']
        text = dialogue_api_hl.transcribe_audio(file_obj)
        if text is None:
            return {'code': 1, 'message': 'transcription failed'}, 500
        return {'code': 0, 'text': text}
    except Exception as e:
        return {'code': 1, 'message': str(e)}, 500
",manager.py,
survived,"    def select(self) -> Node:
        node = self.root
        while node.children:
            node = max(
                node.children,
                key=lambda n: (n.reward / (n.visits or 1e-9))
                + self.exploration * math.sqrt(math.log(node.visits + 1) / ((n.visits) or 1e-9)),
            )
        return node
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/tree.py,Tree
survived,"    def __init__(self, target: int = 5) -> None:
        self.target = target
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/env.py,NumberLineEnv
survived,"    def test_run_demo_with_target(self) -> None:
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--target"",
                ""7"",
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def _merkle_root(hashes: Iterable[str]) -> str:
    nodes: List[bytes] = [bytes.fromhex(h) for h in hashes]
    if not nodes:
        return cast(str, blake3(b""\x00"").hexdigest())

    while len(nodes) > 1:
        if len(nodes) % 2 == 1:
            nodes.append(nodes[-1])
        next_lvl: List[bytes] = []
        for i in range(0, len(nodes), 2):
            next_lvl.append(blake3(nodes[i] + nodes[i + 1]).digest())
        nodes = next_lvl
    return nodes[0].hex()
",alpha_factory_v1/common/utils/logging.py,
survived,"            def call_llama(prompt: str, s: Settings) -> str:
                out = cast(Any, _MODEL)(prompt, temperature=s.temperature)
                return cast(str, out[""choices""][0][""text""]).strip()
",alpha_factory_v1/common/utils/local_llm.py,
survived,"    async def _loop(self, interval: int) -> None:
        while True:
            await asyncio.sleep(interval)
            await self.broadcast_merkle_root()
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"    async def stop_merkle_task(self) -> None:
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:  # pragma: no cover - expected
                pass
            self._task = None
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"    def _wrap(fn: Callable[[str, Settings], str]) -> Callable[[str, Settings], str]:
        return fn
",alpha_factory_v1/common/utils/local_llm.py,
survived,"    def __init__(self, port: int) -> None:
        self._port = port
",alpha_factory_v1/backend/services/metrics_service.py,MetricsExporter
survived,"    def __init__(
        self,
        runners: Dict[str, Any],
        model_max_bytes: int,
        mem: Any,
        rest_port: int,
        grpc_port: int,
        loglevel: str,
        ssl_disable: bool,
    ) -> None:
        self._runners = runners
        self._model_max_bytes = model_max_bytes
        self._mem = mem
        self._rest_port = rest_port
        self._grpc_port = grpc_port
        self._loglevel = loglevel
        self._ssl_disable = ssl_disable
        self._rest_task: Optional[asyncio.Task] = None
        self._grpc_server: Optional[Any] = None
",alpha_factory_v1/backend/services/api_server_service.py,APIServer
survived,"    def __init__(self, broker: str | None, dev_mode: bool) -> None:
        self._bus = EventBus(broker, dev_mode)
",alpha_factory_v1/backend/services/kafka_service.py,KafkaService
survived,"    async def _run() -> None:
        async with bus, ledger:
            for env in envs:
                await agent.handle(env)
",tests/test_memory_agent_file_persistence.py,
survived,"def test_runs_persist_after_reload() -> None:
    src = Path(__file__).resolve().parents[1] / ""index.html""
    url = src.as_uri() + ""#s=1&p=10&g=2""

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(""window.archive !== undefined"")
        page.wait_for_timeout(1000)
        count_before = page.evaluate(""window.archive.list().then(r=>r.length)"")
        page.reload()
        page.wait_for_selector(""#controls"")
        page.wait_for_function(""window.archive !== undefined"")
        count_after = page.evaluate(""window.archive.list().then(r=>r.length)"")
        assert count_before == count_after and count_before > 0
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q1.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q10.py,Auto1
survived,"def test_safari_offline_reload() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    try:
        with sync_playwright() as p:
            browser = p.webkit.launch()
            context = browser.new_context(
                user_agent=(
                    ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ""
                    ""AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15""
                )
            )
            page = context.new_page()
            errors: list[str] = []
            page.on(""console"", lambda msg: errors.append(msg.text) if msg.type == ""error"" else None)
            page.on(""pageerror"", lambda err: errors.append(str(err)))

            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""navigator.serviceWorker.ready"")

            context.set_offline(True)
            page.reload()
            page.wait_for_selector(""#controls"")
            context.set_offline(False)

            assert not errors, f""Console errors: {errors}""
            assert page.evaluate(""navigator.serviceWorker.controller !== null"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_safari_offline.py,
survived,"def temp_dir(tmp_path):
    """"""Temporary directory fixture for module-level tests.""""""
    yield tmp_path
",tests/test_embedding_benchmark.py,
survived,"    def test_no_log_flag(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""no_log.json""
            result = subprocess.run(
                [
                    sys.executable,
                    STUB,
                    ""-n"",
                    ""1"",
                    ""--seed"",
                    ""4"",
                    ""--ledger"",
                    str(ledger),
                    ""--no-log"",
                    ""--model"",
                    ""gpt-4o-mini"",
                ],
                capture_output=True,
                text=True,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            self.assertFalse(ledger.exists())
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"    def test_invalid(self) -> None:
        parser = edge_runner._positive_int(""val"")
        with self.assertRaises(argparse.ArgumentTypeError):
            parser(""0"")
        with self.assertRaises(argparse.ArgumentTypeError):
            parser(""-5"")
        with self.assertRaises(argparse.ArgumentTypeError):
            parser(""foo"")
",tests/test_edge_runner_parse.py,TestPositiveInt
survived,"def list_capabilities():
    """"""Return sorted list of all capabilities currently registered.""""""
    return sorted(CAPABILITY_GRAPH.keys())
",alpha_factory_v1/backend/agents/__init__.py,
survived,"def test_replay_existing(tmp_path) -> None:
    path = tmp_path / ""led.db""
    path.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", path):
        with (
            patch.object(cli.logging, ""Ledger"") as led_cls,
            patch.object(
                cli.time,
                ""sleep"",
                return_value=None,
            ),
        ):
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 0.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            out = CliRunner().invoke(cli.main, [""replay""])
            assert ""a -> b"" in out.output",tests/test_cli.py,
survived,"        def research(self, name: str, purpose: str):
            calls.append((name, purpose))
            return [""info""]
",tests/agents/test_tool_designer_agent.py,DummyResearch
survived,"    def research(self, name: str, purpose: str) -> List[str]:
        """"""Perform the search and return a list of result snippets.""""""
        if not self.enabled:
            logger.debug(""Research disabled; skipping web search"")
            return []

        query = self.formulate_query(name, purpose)
        if query in self.cache:
            logger.debug(""Using cached results for query: %s"", query)
            return self.cache[query]

        try:
            raw = self.web_search_tool(query)
            if isinstance(raw, str):
                snippets = [s.strip() for s in raw.split(""\n"") if s.strip()]
            elif isinstance(raw, list):
                snippets = [str(s) for s in raw]
            else:
                snippets = [str(raw)]

            snippets = snippets[: self.max_results]
            self.cache[query] = snippets
            logger.info(""Collected %d research snippets"", len(snippets))
            return snippets
        except Exception as exc:  # pragma: no cover - shouldn't happen in tests
            logger.error(""Web search failed: %s"", exc)
            return []
",src/meta_agent/research_manager.py,ToolResearchManager
survived,"    def fake_push(self):
        pushed.append(True)
        return ""branch""
",tests/test_self_healer_pipeline.py,
survived,"def test_env_value_escaped(tmp_path: Path) -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    target = tmp_path / ""browser""
    shutil.copytree(browser_dir, target)
    token = ""foo</script>bar""
    (target / "".env"").write_text(f""PINNER_TOKEN={token}\n"")
    subprocess.check_call([""npm"", ""run"", ""build""], cwd=target)

    html_text = (target / ""dist"" / ""index.html"").read_text()
    assert token not in html_text
    assert ""window.PINNER_TOKEN=atob("" in html_text

    url = (target / ""dist"" / ""index.html"").as_uri()
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        assert page.evaluate(""window.PINNER_TOKEN"") == token
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_env_escaping.py,
survived,"async def test_loop_until_condition():
    counter = {""i"": 0}

    async def step(prompt, **kwargs):
        counter[""i""] += 1
        return counter[""i""]

    wf = Workflow(
        name=""wf"",
        steps=[
            WorkflowStep(
                runner=step,
                mode=StepMode.LOOP,
                condition=lambda r: r >= 2,
                max_iterations=5,
            )
        ],
    )

    result = await wf.run(0)
    assert result >= 2
    assert counter[""i""] <= 2",tests/test_workflow.py,
survived,"    async def r1(prompt, **kwargs):
        outputs.append(""r1"")
        return prompt + ""-r1""
",tests/test_workflow.py,
survived,"def _no_missing(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(check_env, ""REQUIRED"", [])
    monkeypatch.setattr(check_env, ""OPTIONAL"", [])
    monkeypatch.setattr(check_env, ""warn_missing_core"", lambda: [])
",tests/test_check_env_network.py,
survived,"async def make_client(monkeypatch: pytest.MonkeyPatch):
    from src.interface import api_server

    dummy_mod = types.ModuleType(
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.orchestrator""
    )
    dummy_mod.Orchestrator = lambda: DummyOrch()
    monkeypatch.setitem(sys.modules, dummy_mod.__name__, dummy_mod)
    await api_server.app.router.startup()
    client = AsyncClient(base_url=""http://test"", transport=ASGITransport(app=api_server.app))
    return client, api_server
",tests/test_api_server.py,
survived,"def test_mixed_int_and_selector():
    B, C, V = Axis(""batch"", 3), Axis(""channel"", 2), Axis(""vocab"", 6)
    x = hax.arange((B, C, V))
    idx = hax.arange((B,), dtype=jnp.int32) % V.size
    out = x[""channel"", 1, ""vocab"", idx]
    assert out.axes == (B,)
    ref = x.array[:, 1, :][jnp.arange(3), idx.array]
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,
survived,"    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        self.trainer_cfg = TrainerConfig()
        self.Vocab = round_axis_for_partitioning(
            Axis(""vocab"", len(self.tokenizer)), self.trainer_cfg.compute_axis_mapping
        )
        converter = LlamaConfig().hf_checkpoint_converter()
        converter = converter.replaced(reference_checkpoint=RepoRef(model_path), tokenizer=self.tokenizer)
        self.model = cast(
            LlamaLMHeadModel,
            converter.load_pretrained(
                LlamaLMHeadModel,
                ref=RepoRef(model_path),
                dtype=self.trainer_cfg.mp.compute_dtype,
            ),
        )
        self.sampler = Sampler(self.Vocab)
        self.eos = self.tokenizer.eos_token_id or -1
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"def _round_preferred(n: int) -> int:
    for s in PREFERRED_SIZES:
        if n <= s:
            return s
    return PREFERRED_SIZES[-1]
",src/levanter/inference/llm_engine.py,
survived,"    def body(_, st):
        return scheduler.decode_step(st, decode_fn)
",src/levanter/inference/scheduler.py,
survived,"    def copy_tiles(self):
        """""" returns list of lists version """"""
        t = self.tiles

        return [[t[0][0], t[0][1], t[0][2], t[0][3]],
                [t[1][0], t[1][1], t[1][2], t[1][3]],
                [t[2][0], t[2][1], t[2][2], t[2][3]],
                [t[3][0], t[3][1], t[3][2], t[3][3]]]
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,Position
survived,"def a_star(start_tiles, goal_tiles):
    """""" Based on https://en.wikipedia.org/wiki/A*_search_algorithm """"""

    start = new_position(start_tiles)
    goal = new_position(goal_tiles)

    # Process goal position for use in heuristic

    global hob
    hob = HeuristicObj(goal)

    # The set of currently discovered nodes that are not evaluated yet.
    # Initially, only the start node is known.
    # For the first node, the fscore is completely heuristic.

    start.fscore = hob.heuristic(start)
    openSet = PriorityQueue([start])

    # The cost of going from start to start is zero.

    start.gscore = 0

    num_popped = 0

    while openSet.queue_length > 0:
        current = openSet.pop()
        if current == None: # tried to pop but only found old fscore values
            break
        num_popped += 1
        if num_popped % 100000 == 0:
            print(str(num_popped)+"" positions examined"")

        if current == goal:
            return reconstruct_path(current)

        for neighbor in current.neighbors():

            # The distance from start to a neighbor
            # All nodes are 1 move from their neighbors

            tentative_gScore = current.gscore + 1

            # update gscore and fscore if this is shorter path
            # to the neighbor node

            if tentative_gScore < neighbor.gscore:
                neighbor.cameFrom = current
                neighbor.gscore = tentative_gScore
                neighbor.fscore = neighbor.gscore + hob.heuristic(neighbor)
                openSet.push(neighbor) # add to open set every time
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,
survived,"    def push(self, new_object):
        """""" save object in heapq """"""
        heapq.heappush(self.qheap,(new_object.fscore,new_object.tiles))
        self.queue_length += 1
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue
survived,"    def neighbours(p):
        gap = p.index(0)
        l = list(p)

        for m in movelist[gap]:
            l[gap] = l[gap + m]
            l[gap + m] = 0
            yield (1, tuple(l), (l[gap], m))
            l[gap + m] = l[gap]
            l[gap] = 0
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,
survived,"    def _search(self, g, bound):
        self.nodes_evaluated += 1

        node = self.path[-1]
        f = g + self.h(node)
        if f > bound: return f
        if self.is_goal(node): return self.FOUND

        m = None # Lower bound on cost.
        for cost, n, descr in self.neighbours(node):
            if n in self.is_in_path: continue

            self.path.append(n)
            self.is_in_path.add(n)
            self.path_descrs.append(descr)
            t = self._search(g + cost, bound)

            if t == self.FOUND: return self.FOUND
            if m is None or (t is not None and t < m): m = t

            self.path.pop()
            self.path_descrs.pop()
            self.is_in_path.remove(n)

        return m
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,IDAStar
survived,"def __getattr__(name):
    # NOTE: uncomment these to include any queries that this grammar contains:

    # if name == ""HIGHLIGHTS_QUERY"":
    #     return _get_query(""HIGHLIGHTS_QUERY"", ""highlights.scm"")
    # if name == ""INJECTIONS_QUERY"":
    #     return _get_query(""INJECTIONS_QUERY"", ""injections.scm"")
    # if name == ""LOCALS_QUERY"":
    #     return _get_query(""LOCALS_QUERY"", ""locals.scm"")
    # if name == ""TAGS_QUERY"":
    #     return _get_query(""TAGS_QUERY"", ""tags.scm"")

    raise AttributeError(f""module {__name__!r} has no attribute {name!r}"")
",third_party/tree-sitter-racket/bindings/python/tree_sitter_racket/__init__.py,
survived,"    def TryToGetOneshotToken(self, apiKey:Optional[str]=None) -> Optional[str]:
        try:
            # If we got an API key, try to set it.
            headers = {}
            if apiKey is not None:
                headers[""X-Api-Key""] = apiKey

            # Make the call
            result = OctoHttpRequest.MakeHttpCall(self.Logger, ""/access/oneshot_token"", PathTypes.Relative, ""GET"", headers)
            if result is None:
                raise Exception(""Failed to get the oneshot token from moonraker."")
            if result.StatusCode != 200:
                raise Exception(""Failed to get the oneshot token from moonraker. ""+str(result.StatusCode))

            # Read the response.
            result.ReadAllContentFromStreamResponse(self.Logger)
            buf = result.FullBodyBuffer
            if buf is None:
                raise Exception(""Failed to get the oneshot token from moonraker. No content."")

            # Decode & parse the response.
            jsonMsg = json.loads(buf.GetBytesLike().decode(encoding=""utf-8""))
            token = jsonMsg.get(""result"", None)
            if token is None:
                raise Exception(""Failed to get the oneshot token from moonraker. No result."")
            return str(token)
        except Exception as e:
            Sentry.OnException(""TryToGetOneshotToken failed to get the token."", e)
        return None
",moonraker_octoeverywhere/moonrakercredentialmanager.py,MoonrakerCredentialManager
survived,"            def save(self):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"            def run_generations(self, *_a):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"    def test_list_option(self) -> None:
        result = subprocess.run([sys.executable, STUB, '--list'], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        data = json.loads(result.stdout)
        self.assertIsInstance(data, list)
        self.assertGreaterEqual(len(data), 5)
",tests/test_alpha_discovery_stub.py,TestAlphaDiscoveryStub
survived,"async def trigger_discovery() -> str:
    resp = requests.post(f""{HOST}/agent/alpha_discovery/trigger"", timeout=5)
    resp.raise_for_status()
    return ""alpha_discovery queued""
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"async def reset() -> str:
    EVOLVER.reset()
    return ""evolver reset""
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,
survived,"    def generate_from_file(self, df_infile, vocab_infile):
        """"""Load a document-term matrix and vocabulary from disk.

        Column names are kept as words. When using this matrix with the
        sampler, map the words to integer token IDs before constructing the
        corpus.
        """"""

        # read data frame with word columns intact
        df = pd.read_csv(df_infile, index_col=0)

        vocab = np.genfromtxt(vocab_infile, dtype=""str"")
        return df, vocab
",examples/synthetic_data.py,HldaDataGenerator
survived,"def test_compose_env_substitution() -> None:
    env = os.environ.copy()
    env[""REDIS_PASSWORD""] = ""secret""
    env[""PROMETHEUS_SCRAPE_INTERVAL""] = ""30s""
    result = subprocess.run(
        [""docker"", ""compose"", ""-f"", str(COMPOSE_FILE), ""config""],
        check=True,
        capture_output=True,
        text=True,
        env=env,
    )
    assert ""secret@redis"" in result.stdout
    assert ""30s"" in result.stdout",tests/test_macro_compose_config.py,
survived,"def _env_float(name: str, default: float) -> float:
    """"""Return ``float`` environment value or ``default`` if conversion fails.""""""

    val = os.getenv(name)
    if val is None:
        return default
    try:
        return float(val)
    except (TypeError, ValueError):
        log.warning(""Invalid %s=%r, using default %s"", name, val, default)
        return default
",alpha_factory_v1/backend/agent_runner.py,
survived,"    def assign_seq_id_to_seq(self) -> tuple[""PageTable"", int]:
        seq_id = hax.argmin(self.seq_lens, ""seq"").scalar()
        new_seq_lens = self.seq_lens.at[""seq"", seq_id].set(0)
        return dataclasses.replace(self, seq_lens=new_seq_lens), seq_id
",src/levanter/layers/page_table.py,PageTable
survived,"def open_logs_folder() -> None:
    log_dir = os.path.dirname(get_log_file_path(""dummy.log""))
    if sys.platform.startswith(""darwin""):
        subprocess.run([""open"", log_dir], check=True)
    elif sys.platform.startswith(""win""):
        os.startfile(log_dir)  # type: ignore[attr-defined]
    else:
        subprocess.run([""xdg-open"", log_dir], check=True)
",app/desktop/studio_server/settings_api.py,
survived,"    def add_output_guardrail(self, guardrail: Callable[[str], Awaitable[None]]) -> None:
        self.output_guardrails.append(guardrail)
",src/meta_agent/services/guardrail_router.py,GuardrailModelRouter
survived,"    def __init__(self):
        self.prompts: list[str] = []
",tests/test_guardrail_router.py,MockAdapter
survived,"def _apply_grad(fn, x, backend_name=""numpy""):
    backend.set_backend(backend_name)
    b = backend.current()
    g = b.grad(fn)
    out = g(b.array(x, requires_grad=True))
    out = to_numpy(out)
    return float(out) if np.ndim(out) == 0 else out
",tests/kgtests/autograd/helpers.py,
survived,"    def test_mixed_args_grad_numpy(self):
        self._check_mixed_args_grad(""numpy"")
",tests/test_autograd.py,TestAutograd
survived,"    def _check_scalar_square_grad(self, name: str):
        """"""Verify âˆ‚(xÂ²)/âˆ‚x = 2x for a scalar input.""""""
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x):
            return b.mul(x, x)

        g = b.grad(f)
        x = b.array(3.0, requires_grad=True)
        grad = to_numpy(g(x))
        np.testing.assert_allclose(np.array(grad), np.array(6.0))
",tests/test_autograd.py,TestAutograd
survived,"def scalarSquareGrad(x, backend_name=""numpy""):
    backend.set_backend(backend_name)
    b = backend.current()

    def f(t):
        return b.mul(t, t)

    g = b.grad(f)
    out = g(b.array(x, requires_grad=True))
    out = to_numpy(out)
    return float(out) if np.ndim(out) == 0 else out
",tests/kgtests/autograd/helpers.py,
survived,"    def test_stop_grad_numpy(self):
        self._check_stop_grad(""numpy"")
",tests/test_autograd.py,TestAutograd
survived,"    def rebuild_models(self) -> None:
        """"""Rebuild all registered models to resolve forward references.""""""
        for entity_cls in self.entities.values():
            entity_cls.model_rebuild()
",src/enrichmcp/app.py,EnrichMCP
survived,"def _make_wheel(directory: Path, name: str, version: str) -> Path:
    """"""Create a minimal wheel in *directory* and return the path.""""""
    wheel = directory / f""{name.replace('-', '_')}-{version}-py3-none-any.whl""
    pkg = name.replace(""-"", ""_"")
    with zipfile.ZipFile(wheel, ""w"") as zf:
        zf.writestr(f""{pkg}/__init__.py"", f""__version__ = '{version}'\n"")
        zf.writestr(
            f""{pkg}-{version}.dist-info/METADATA"",
            f""Metadata-Version: 2.1\nName: {name}\nVersion: {version}\n"",
        )
        zf.writestr(
            f""{pkg}-{version}.dist-info/WHEEL"",
            ""Wheel-Version: 1.0\nGenerator: test\nRoot-Is-Purelib: true\nTag: py3-none-any\n"",
        )
        zf.writestr(f""{pkg}-{version}.dist-info/RECORD"", """")
    return wheel
",tests/test_aiga_offline_setup.py,
survived,"async def _make_client() -> tuple[AsyncClient, Any]:
    app = FastAPI()
    app.middleware(""http"")(mod._count_requests)

    @app.get(""/"")
    async def root():
        return {""ok"": True}

    transport = ASGITransport(app=app)
    client = AsyncClient(base_url=""http://test"", transport=transport)
    return client, app
",tests/test_rate_lock.py,
survived,"    def _connect(_addr: tuple[str, int], timeout: float = 1.0) -> None:
        attempts.append(_addr)
        raise OSError
",tests/test_check_env_network.py,
survived,"    async def _run() -> tuple[int, int]:
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        before = agent.count
        await runner.restart(bus, ledger)
        new_agent = runner.agent  # type: ignore[assignment]
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        return before, getattr(new_agent, ""count"")
",tests/test_agent_runner.py,
survived,"def test_get_secret_env(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""MY_SECRET"", ""value"")
    monkeypatch.delenv(""AGI_INSIGHT_SECRET_BACKEND"", raising=False)
    assert cfg.get_secret(""MY_SECRET"") == ""value""
    monkeypatch.delenv(""MY_SECRET"", raising=False)
    assert cfg.get_secret(""MY_SECRET"", ""default"") == ""default""
",tests/test_config_utils.py,
survived,"    def fold_via(self, fn: Callable[..., CarryT]):
        """"""Return a function that folds over the sequence using ``fn``.

        ``fn`` should take a block and a carry and return a new carry. The
        returned function mirrors :func:`haliax.fold` over the block axis.
        """"""

        def do_fold(init: CarryT) -> CarryT:
            carry = init
            for block in self.blocks:
                carry = fn(block, carry)
                carry = tree_checkpoint_name(carry, self._carry_ckpt_name)
            return carry

        return do_fold
",src/haliax/nn/scan.py,BlockSeq
survived,"def test_import_with_agents_only(monkeypatch: pytest.MonkeyPatch) -> None:
    stub = types.ModuleType(""agents"")
    stub.Agent = object
    stub.AgentRuntime = object

    class DummyOpenAI:
        def __init__(self, *args: object, **kwargs: object) -> None:
            pass

    stub.OpenAIAgent = DummyOpenAI

    def _tool(*_a: object, **_k: object) -> object:
        def _decorator(func: object) -> object:
            return func

        return _decorator

    stub.Tool = _tool

    monkeypatch.setitem(sys.modules, ""agents"", stub)
    sys.modules.pop(""openai_agents"", None)

    orig_import = builtins.__import__

    def fake_import(name: str, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        if name == ""alpha_opportunity_stub"":
            return importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_opportunity_stub"")
        if name == ""alpha_conversion_stub"":
            return importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_conversion_stub"")
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)

    for mod_name in MODULES:
        mod = importlib.reload(importlib.import_module(mod_name))
        assert mod.OpenAIAgent is stub.OpenAIAgent",tests/test_aiga_agents_import.py,
survived,"    def main() -> None:
        """"""Î±â€‘Factory command line interface.""""""
",alpha_factory_v1/core/interface/cli.py,
survived,"def _mutate(g):
    return g + random.uniform(-1, 1)
",tests/test_backtrack_boost.py,
survived,"def evolve_cmd(max_cost: float, wallclock: float | None, backtrack_rate: float) -> None:
    """"""Run the minimal asynchronous evolution demo.""""""
    from src import evolve as _evolve

    async def _eval(genome: float) -> tuple[float, float]:
        await asyncio.sleep(0)
        return random.random(), 0.01

    archive = _evolve.InMemoryArchive()
    asyncio.run(
        _evolve.evolve(
            lambda g: g,
            _eval,
            archive,
            max_cost=max_cost,
            wallclock=wallclock,
            backtrack_rate=backtrack_rate,
        )
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def set_state(self, key: str, value: str) -> None:
        """"""Store ``key`` as ``value`` in the ``state`` table.""""""
        with Session(self.engine) as session:
            session.merge(_StateRow(key=key, value=value))
            session.commit()",src/archive/db.py,ArchiveDB
survived,"async def _phase_loop(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], Awaitable[tuple[float, float]]],
    archive: InMemoryArchive,
    *,
    phase: Phase,
    max_cost: float | None = None,
    wallclock: float | None = None,
    backtrack_rate: float = 0.0,
    phase_hook: Optional[Callable[[Phase], None]] = None,
) -> None:
    if not archive.all():
        await archive.accept(Candidate(genome=0.0, fitness=0.0, novelty=1.0, cost=0.0))

    spent = 0.0
    start = time.time()

    while True:
        if max_cost is not None and spent >= max_cost:
            break
        if wallclock is not None and time.time() - start >= wallclock:
            break

        population = archive.all()
        parent = backtrack_boost(population, population, backtrack_rate)
        genome = operator(parent.genome)
        if phase_hook:
            phase_hook(phase)
        fitness, cost = await evaluate(genome)
        child = Candidate(genome=genome, fitness=fitness, novelty=random.random(), cost=cost)
        await archive.accept(child)
        metrics.dgm_children_total.inc()
        spent += cost
",src/evolve.py,
survived,"        def hook(p: Phase) -> None:
            nonlocal current
            current = p
",tests/test_phase_order.py,TestPhaseOrder
survived,"async def task_solve_phase(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], Awaitable[tuple[float, float]]],
    archive: InMemoryArchive,
    *,
    max_cost: float | None = None,
    wallclock: float | None = None,
    backtrack_rate: float = 0.0,
    phase_hook: Optional[Callable[[Phase], None]] = None,
) -> None:
    await _phase_loop(
        operator,
        evaluate,
        archive,
        phase=Phase.TASK_SOLVE,
        max_cost=max_cost,
        wallclock=wallclock,
        backtrack_rate=backtrack_rate,
        phase_hook=phase_hook,
    )
",src/evolve.py,
survived,"async def self_mod_phase(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], Awaitable[tuple[float, float]]],
    archive: InMemoryArchive,
    *,
    max_cost: float | None = None,
    wallclock: float | None = None,
    backtrack_rate: float = 0.0,
    phase_hook: Optional[Callable[[Phase], None]] = None,
) -> None:
    await _phase_loop(
        operator,
        evaluate,
        archive,
        phase=Phase.SELF_MOD,
        max_cost=max_cost,
        wallclock=wallclock,
        backtrack_rate=backtrack_rate,
        phase_hook=phase_hook,
    )
",src/evolve.py,
survived,"def test_innovation_ablation() -> None:
    results = run_ablation()
    for patch, scores in results.items():
        base = scores[""baseline""]
        for name, val in scores.items():
            if name == ""baseline"":
                continue
            assert base - val >= 0.03",tests/test_ablation.py,
survived,"    def evolve(
        self,
        scenario_hash: str,
        fn: Callable[[list[float]], tuple[float, ...]],
        genome_length: int,
        **kwargs: object,
    ) -> mats.Population:
        """"""Run evolution for ``scenario_hash`` using persistent islands.""""""

        pop = mats.run_evolution(
            fn,
            genome_length,
            scenario_hash=scenario_hash,
            populations=self.island_pops,
            **kwargs,
        )
        self.island_pops[scenario_hash] = pop
        return pop
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator
survived,"    async def __aexit__(self, exc_type, exc, tb) -> None:
        """"""Stop the bus when exiting an async context.""""""
        await self.stop()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus
survived,"            def __init__(self, *a, **k) -> None:
                pass
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime.DummyEvolver
survived,"    def load(self) -> None:
        if self.index_path.exists():
            try:
                with open(self.index_path, ""r"", encoding=""utf-8"") as f:
                    self._index = json.load(f)
            except (OSError, json.JSONDecodeError):  # pragma: no cover - corrupt file
                self._index = []
        else:
            self._index = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"def test_mutate_returns_child(server: str) -> None:
    import io
    import tarfile

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""README.txt"")
        data = b""demo""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""dummy.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 200
        data = r.json()
        assert ""child"" in data
        assert isinstance(data[""child""], list)",tests/test_evolution_worker.py,
survived,"async def test_enrichparameter_hints_appended():
    app = EnrichMCP(""Test"", description=""desc"")
    with patch.object(app.mcp, ""tool"", wraps=app.mcp.tool) as mock_tool:

        @app.retrieve(description=""Base desc"")
        async def my_resource(
            ctx: EnrichContext,
            name: str = EnrichParameter(description=""user name"", examples=[""bob""]),
        ) -> dict:
            return {}

    desc = mock_tool.call_args.kwargs[""description""]
    assert ""Parameter hints:"" in desc
    assert ""name - str"" in desc
    assert ""user name"" in desc
    assert ""examples: bob"" in desc
    assert ""ctx"" not in desc",tests/test_enrichparameter.py,
survived,"    def history_plot(self):
        import pandas as pd
        return pd.DataFrame(self.history, columns=[""generation"", ""avg_fitness""])
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def best_fitness(self) -> float:
        return self._best_fitness
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"def main() -> None:
    banner(""Alpha-Factory Preflight Check"", 'YELLOW')
    ok = True
    ok &= check_python()
    ok &= check_cmd('docker')
    ensure_dir(Path('/var/alphafactory'))

    for key in ('OPENAI_API_KEY', 'ANTHROPIC_API_KEY'):
        if os.getenv(key):
            banner(f""{key} set"", 'GREEN')
        else:
            banner(f""{key} not set"", 'YELLOW')

    if not ok:
        banner('Preflight checks failed. Please install required dependencies.', 'RED')
        sys.exit(1)

    banner('Environment looks good. You can now run install_alpha_factory_pro.sh', 'GREEN')
",alpha_factory_v1/scripts/preflight.py,
survived,"def lanczos7(z):
    t = z + 6.5
    x = 0.9999999999998099 + 676.5203681218851 / z - 1259.1392167224028 / (z + 1.0) + 771.3234287776531 / (z + 2.0) - 176.6150291621406 / (z + 3.0) + 12.507343278686905 / (z + 4.0) - 0.13857109526572012 / (z + 5.0) + 9.984369578019572e-06 / (z + 6.0) + 1.5056327351493116e-07 / (z + 7.0)
    return 2.5066282746310002 * powf(t, z - 0.5) * powf(2.718281828459045, -t) * x
",tests/rosetta/transpiler/Python/gamma-function.py,
survived,"def testall(list, recursive, toplevel):
    import sys
    import os
    for filename in list:
        if os.path.isdir(filename):
            print(filename + '/:', end=' ')
            if recursive or toplevel:
                print('recursing down:')
                import glob
                names = glob.glob(os.path.join(glob.escape(filename), '*'))
                testall(names, recursive, 0)
            else:
                print('*** directory (use -r) ***')
        else:
            print(filename + ':', end=' ')
            sys.stdout.flush()
            try:
                print(what(filename))
            except OSError:
                print('*** not found ***')
",metaflow/_vendor/imghdr/__init__.py,
survived,"def test_bmp(h, f):
    """"""Verify if the image is a BMP file.""""""
    if h.startswith(b'BM'):
        return 'bmp'
",metaflow/_vendor/imghdr/__init__.py,
survived,"    def gen_group_id(self) -> str:
        """"""Generate a new group ID.""""""
        return f""group_{uuid.uuid4().hex[:24]}""
",src/agents/tracing/setup.py,TraceProvider
survived,"    def gen_span_id(self) -> str:
        """"""Generate a new span ID.""""""
        return f""span_{uuid.uuid4().hex[:24]}""
",src/agents/tracing/setup.py,TraceProvider
survived,"    async def handle(self, _env: orchestrator.messaging.Envelope) -> None:  # pragma: no cover - helper
        pass
",tests/test_orchestrator.py,DummyAgent
survived,"    def test_add_and_search(self):
        mem = mv.VectorMemory()
        mem.add(""agent"", [""a"", ""b""])
        results = mem.search(""a"", k=2)
        self.assertEqual(len(results), 2)
        for agent, text, score in results:
            self.assertEqual(agent, ""agent"")
            self.assertIn(text, [""a"", ""b""])
            self.assertIsInstance(score, float)
",tests/test_memory_vector.py,TestVectorMemoryOffline
survived,"def verify_environment() -> None:
    """"""Best-effort runtime dependency check.""""""
    try:
        import check_env  # type: ignore

        check_env.main([])
    except (ImportError, ModuleNotFoundError) as exc:  # pragma: no cover
        print(f""Environment verification failed: {exc}"")
    except Exception as exc:
        print(f""Unexpected error during environment verification: {exc}"")
        raise
",alpha_factory_v1/demos/alpha_agi_insight_v0/insight_demo.py,
survived,"def parse_sectors(cfg_val: object | None, cli_val: str | None) -> List[str]:
    """"""Return a cleaned list of sector names.

    Parameters
    ----------
    cfg_val:
        Value loaded from ``default.yaml``. Can be a comma-separated string or
        a YAML array.
    cli_val:
        Optional value passed via ``--sectors``.
    """"""

    source = cli_val or cfg_val
    if isinstance(source, list):
        return [str(s).strip() for s in source if str(s).strip()]
    if isinstance(source, str):
        text = source.strip()
        file_candidate = Path(text)
        if file_candidate.exists():
            lines = file_candidate.read_text(encoding=""utf-8"").splitlines()
            return [line.strip() for line in lines if line.strip()]
        return [s.strip() for s in text.split(""\n"" if ""\n"" in text else "","") if s.strip()]
    return list(DEFAULT_SECTORS)
",alpha_factory_v1/demos/alpha_agi_insight_v0/insight_demo.py,
survived,"  async def open_door(self):
    return await self.backend.open_door()
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def open_door(self):
    pass
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def set_shaking_frequency(
    self, frequency: int, shakers: Optional[List[int]] = None
  ) -> List[str]:
    shakers = shakers or [1, 2]
    assert all(shaker in [1, 2] for shaker in shakers), ""Shaker index must be 1 or 2""
    return [await self.send_command(""se"", f""pb 2{idx-1}"", f""{frequency:04}"") for idx in shakers]
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def close_door(self):
    await self._send_command(""ST 1902"")
    await self._wait_ready()
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  async def open_door(self):
    await self._send_command(""ST 1901"")
    await self._wait_ready()
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  async def get_o2(self) -> CytomatIncupationResponse:
    return await self.get_incubation_query(""io"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"def cytomat_rack_69mm_7(name: str):
  return _cytomat_rack(name=name, site_height=69, num_sites=7, model=""cytomat_rack_69mm_7"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def action_exposed_to_storage(self, site: PlateHolder) -> OverviewRegisterState:
    """"""Return with MTP from exposed to storage, move to wait, close door""""""
    return await self.send_action(""mv"", ""hs"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def set_racks(self, racks: List[PlateCarrier]):
    await super().set_racks(racks)
    warnings.warn(""Cytomat racks need to be configured manually on each setup"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"def cytomat_rack_17mm_28(name: str):
  return _cytomat_rack(name=name, site_height=17, num_sites=28, model=""cytomat_rack_17mm_28"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def fetch_plate_to_loading_tray(self, plate: Plate, site=PlateHolder):
    """"""Fetch a plate from storage onto the transfer station, with gate open/close.""""""
    site = plate.parent
    assert isinstance(site, PlateHolder), ""Plate not in storage""
    m, n = self._site_to_m_n(site)
    await self._send_command(f""WR DM0 {m}"")  # carousel pos
    await self._send_command(f""WR DM5 {n}"")  # handler level
    await self._send_command(""ST 1905"")  # plate to transfer station
    await self._wait_ready()
    await self._send_command(""ST 1903"")  # terminate access
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  async def action_storage_to_wait(self, site: PlateHolder) -> OverviewRegisterState:
    """"""Retrieve from storage, move to wait position""""""
    return await self.send_action(""mv"", ""sw"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  def _assemble_command(self, command_type: str, command: str, params: str):
    carriage_return = ""\r"" if self.model == CytomatType.C2C_425 else ""\r\n""
    command = f""{command_type}:{command} {params}"".strip() + carriage_return
    return command
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def start_shaking(self, frequency: float):
    print(f""Starting shaking at {frequency} Hz"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend
survived,"  async def open_door(self):
    return await self.send_action(""ll"", ""gp"", ""002"")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def get_temperature(self) -> float:
    """"""Get the temperature of the incubator in degrees Celsius.""""""
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def stop_shaking(self):
    await self.backend.stop_shaking()
",pylabrobot/storage/incubator.py,Incubator
survived,"def _split_name(name: str) -> Tuple[str, str]:
    """"""Split ``slug@version`` into components.""""""
    if ""@"" in name:
        slug, version = name.split(""@"", 1)
    else:
        slug, version = name, ""latest""
    return slug, version
",src/meta_agent/template_mixer.py,
survived,"def _load_model() -> None:
    """"""Load a local model if available, otherwise use an echo stub.""""""
    global _MODEL, _CALL
    model_path = os.getenv(
        ""LLAMA_MODEL_PATH"",
        os.path.expanduser(""~/.cache/llama/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf""),
    )

    def _wrap(fn: Callable[[str], str]) -> Callable[[str], str]:
        return fn

    if Llama is not None:
        try:
            _MODEL = Llama(model_path=model_path, n_ctx=int(os.getenv(""LLAMA_N_CTX"", ""2048"")))

            def call_llama(prompt: str) -> str:
                out = cast(Any, _MODEL)(prompt)
                return cast(str, out[""choices""][0][""text""]).strip()

            _CALL = _wrap(call_llama)
            return
        except Exception:  # pragma: no cover - model load failure
            _MODEL = None
    if AutoModelForCausalLM is not None:
        try:
            _MODEL = AutoModelForCausalLM.from_pretrained(model_path, model_type=""llama"")

            def call_ctrans(prompt: str) -> str:
                return cast(str, cast(Any, _MODEL)(prompt))

            _CALL = _wrap(call_ctrans)
            return
        except Exception:  # pragma: no cover - model load failure
            _MODEL = None

    def call_stub(prompt: str) -> str:
        return f""[offline] {prompt}""

    _CALL = _wrap(call_stub)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/local_llm.py,
survived,"    def __enter__(self):
        return self
",tests/test_selfheal_import_stubs.py,DummyBlocks
survived,"def percent_conditional(line):
    return f""{line}\n"" if not line.endswith('\\') or line.endswith('\\\\') else f""{line[:-1]}""
",test/integration/expected_out_single_line/issue192.py,
survived,"def cleanup_ledger_dir() -> None:
    """"""Remove the default ledger directory created during tests.""""""
    yield
    ledger = Path(""ledger"")
    if ledger.exists():
        shutil.rmtree(ledger, ignore_errors=True)",tests/conftest.py,
survived,"def unique_values(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> NamedArray:
    """"""Shortcut for :func:`unique` that returns only unique values.""""""

    return typing.cast(
        NamedArray,
        unique(
            array,
            Unique,
            axis=axis,
            fill_value=fill_value,
        ),
    )
",src/haliax/ops.py,
survived,"    def get_process_count(cls) -> int:  # type: ignore[override]
        if _PROCESS_COUNT in os.environ:
            return int(os.environ[_PROCESS_COUNT])

        if cls.is_env_present():
            num_nodes = next(
                (os.environ[o] for o in [""SLURM_JOB_NUM_NODES"", _NUM_NODES, ""SLURM_NNODES""] if o in os.environ),
                None,
            )
            if num_nodes == ""1"":
                logger.info(""%s not set; assuming single-process job"", _PROCESS_COUNT)
                return 1

        return super().get_process_count()
",src/levanter/distributed.py,LevanterSlurmCluster
survived,"def test_slider_updates_hash_and_restarts() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")

        initial_hash = page.evaluate(""location.hash"")
        seed_input = page.locator(""#seed"")
        seed_input.fill(""999"")
        seed_input.dispatch_event(""change"")

        page.wait_for_function(""location.hash !== '%s'"" % initial_hash)
        assert page.evaluate(""location.hash"") != initial_hash

        page.wait_for_selector(""#toast.show"")
        assert ""restarted"" in page.inner_text(""#toast"")
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"def test_open_logs_endpoint(client):
    with patch(""app.desktop.studio_server.settings_api.open_logs_folder"") as m:
        response = client.post(""/api/open_logs"")
        assert response.status_code == 200
        m.assert_called_once()",app/desktop/studio_server/test_settings_api.py,
survived,"def _lambda8():
    draw.get(100)()
    draw.get(400)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-octal-3.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/gui-component-interaction.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    srcLines = [""package main"", """", ""import ("", ""    \""fmt\"""", ""    \""go/ast\"""", ""    \""go/parser\"""", ""    \""go/token\"""", ""    \""io/ioutil\"""", ""    \""os\"""", ""    \""sort\"""", "")"", """", ""func main() {"", ""    if len(os.Args) != 2 {"", ""        fmt.Println(\""usage ff <go source filename>\"")"", ""        return"", ""    }"", ""    src, err := ioutil.ReadFile(os.Args[1])"", ""    if err != nil {"", ""        fmt.Println(err)"", ""        return"", ""    }"", ""    fs := token.NewFileSet()"", ""    a, err := parser.ParseFile(fs, os.Args[1], src, 0)"", ""    if err != nil {"", ""        fmt.Println(err)"", ""        return"", ""    }"", ""    f := fs.File(a.Pos())"", ""    m := make(map[string]int)"", ""    ast.Inspect(a, func(n ast.Node) bool {"", ""        if ce, ok := n.(*ast.CallExpr); ok {"", ""            start := f.Offset(ce.Pos())"", ""            end := f.Offset(ce.Lparen)"", ""            m[string(src[start:end])]++"", ""        }"", ""        return true"", ""    })"", ""    cs := make(calls, 0, len(m))"", ""    for k, v := range m {"", ""        cs = append(cs, &call{k, v})"", ""    }"", ""    sort.Sort(cs)"", ""    for i, c := range cs {"", ""        fmt.Printf(\""%-20s %4d\\n\"", c.expr, c.count)"", ""        if i == 9 {"", ""            break"", ""        }"", ""    }"", ""}"", """", ""type call struct {"", ""    expr  string"", ""    count int"", ""}"", ""type calls []*call"", """", ""func (c calls) Len() int           { return len(c) }"", ""func (c calls) Swap(i, j int)      { c[i], c[j] = c[j], c[i] }"", ""func (c calls) Less(i, j int) bool { return c[i].count > c[j].count }""]
    src = join(srcLines, ""\n"")
    freq = {}
    i = 0
    order = []
    while i < len(src):
        ch = src[i:i + 1]
        if (ch >= ""A"" and ch <= ""Z"") or (ch >= ""a"" and ch <= ""z"") or ch == ""_"":
            j = i + 1
            while j < len(src) and isAlphaNumDot(src[j:j + 1]):
                j = j + 1
            token = src[i:j]
            k = j
            while k < len(src):
                cc = src[k:k + 1]
                if cc == "" "" or cc == ""\t"" or cc == ""\n"" or cc == ""\r"":
                    k = k + 1
                else:
                    break
            if k < len(src) and src[k:k + 1] == ""("":
                p = i - 1
                while p >= 0 and (src[p:p + 1] == "" "" or src[p:p + 1] == ""\t""):
                    p = p - 1
                skip = False
                if p >= 3:
                    before = src[p - 3:p + 1]
                    if before == ""func"":
                        skip = True
                if not skip:
                    if token in freq:
                        freq[token] = freq[token] + 1
                    else:
                        freq[token] = 1
                        order = order + [token]
            i = j
        else:
            i = i + 1
    pairs = []
    for t in order:
        pairs = pairs + [{""expr"": t, ""count"": freq[t]}]
    pairs = sortPairs(pairs)
    idx = 0
    while idx < len(pairs) and idx < 10:
        p = pairs[idx]
        print(p.get(""expr"") + "" "" + str(p.get(""count"")))
        idx = idx + 1
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/function-frequency.py,
survived,"def fmtF5(x):
    y = floorf(x * 100000.0 + 0.5) / 100000.0
    s = str(y)
    dot = s.find(""."")
    if dot == 0 - 1:
        s = s + "".00000""
    else:
        decs = len(s) - dot - 1
        if decs > 5:
            s = s[0:dot + 6]
        else:
            while decs < 5:
                s = s + ""0""
                decs = decs + 1
    return s
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"    def path(u, v):
        ui = u - 1
        vi = v - 1
        if next[ui][vi] == 0 - 1:
            return []
        p = [u]
        cur = ui
        while cur != vi:
            cur = next[cur][vi]
            p = p + [cur + 1]
        return p
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def partialSeries(f):
    out = """"
    i = 0
    while i < 6:
        out = out + "" "" + padFloat5(extract(f, i), 8) + "" ""
        i = i + 1
    return out
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def ha(a, b):
    return SumCarry(s=xor(a, b), c=a and b)
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm2.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""The first 61 fusc numbers are:"")
    print(str(firstFusc(61)))
    print(""\nThe fusc numbers whose length > any previous fusc number length are:"")
    idxs = [0, 37, 1173, 35499, 699051, 19573419]
    i = 0
    while i < len(idxs):
        idx = idxs[i]
        val = fuscVal(idx)
        numStr = padLeft(commatize(val), 7)
        idxStr = padLeft(commatize(idx), 10)
        print(numStr + "" (index "" + idxStr + "")"")
        i = i + 1
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fusc-sequence.py,
survived,"def pathStr(p):
    s = """"
    i = 0
    while i < len(p):
        s = s + str(p[i] + 1)
        if i < len(p) - 1:
            s = s + "" -> ""
        i = i + 1
    return s
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm2.py,
survived,"def state(v):
    return State(entry=v == 0, inc=v < 10, dec=v > 0)
",tests/rosetta/transpiler/Python/gui-enabling-disabling-of-controls.py,
survived,"def differentiate(a):
    return newFps(lambda n: (float((n + 1))) * extract(a, n + 1))
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    grid = clearGrid()
    ftree(grid, float((width // 2)), float((height - 1)), length, 0.0, depth)
    print(render(grid))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/gui-enabling-disabling-of-controls.py,
survived,"def extract(f, n):
    while len(f.coeffs) <= n:
        idx = len(f.coeffs)
        v = f.compute(idx)
        f = dataclasses.replace(f, coeffs=f.coeffs + [v])
    return f.coeffs[n]
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def spaces(n):
    return repeat("" "", n)
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,
survived,"def step(n, program):
    i = 0
    while i < len(program):
        num = program[i][0]
        den = program[i][1]
        if n % den == 0:
            n = (n // den) * num
            return StepResult(n=n, ok=True)
        i = i + 1
    return StepResult(n=n, ok=False)
",tests/rosetta/transpiler/Python/fractran.py,
survived,"    def test_printgraph_mermaid(self) -> None:
        """"""Test the mermaid gen of builtin function.""""""
        captured_output = io.StringIO()
        sys.stdout = captured_output
        Jac.jac_import(
            self.mach, ""builtin_printgraph_mermaid"", base_path=self.fixture_abs_path(""./"")
        )
        sys.stdout = sys.__stdout__
        stdout_value = captured_output.getvalue()
        self.assertIn(""flowchart LR"", stdout_value)
",jac/jaclang/tests/test_language.py,JacLanguageTests
survived,"        async def run(self, _msg: str) -> None:
            pass
",tests/test_alpha_agi_business_3_v1.py,DummyADK
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/binary_precedence.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/order_by_map.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/substring_builtin.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_by.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_int_key.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/string_concat.py,
survived,"def test_view_command_invalid_password(mock_get_connection, runner):
    mock_get_connection.side_effect = InvalidPassword()

    result = runner.invoke(cli, ['view', 'bad_conn', '--table', 'users'])

    assert result.exit_code != 0
    assert ""Error: Unable to decrypt saved connection. Invalid password provided."" in result.output
",peepdb/tests/test_cli.py,
survived,"def test_unbundled_sri() -> None:
    index_file = BROWSER / ""index.html""
    html = index_file.read_text()
    assets = {
        ""d3.v7.min.js"": BROWSER / ""d3.v7.min.js"",
        ""bundle.esm.min.js"": BROWSER / ""lib/bundle.esm.min.js"",
        ""pyodide.js"": BROWSER / ""lib/pyodide.js"",
    }
    for name, path in assets.items():
        pattern = rf'<script[^>]*src=[""\']{name}[""\'][^>]*>'
        match = re.search(pattern, html)
        assert match, f""{name} script tag missing""
        tag = match.group(0)
        integrity = re.search(r'integrity=[""\']([^""\']+)[""\']', tag)
        assert integrity, f""integrity attribute missing for {name}""
        sri = integrity.group(1)
        digest = hashlib.sha384(path.read_bytes()).digest()
        expected = base64.b64encode(digest).decode()
        assert sri.endswith(expected), f""integrity mismatch for {name}""",tests/test_integrity.py,
survived,"        def __init__(self, path: str) -> None:
            self.path = path
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,DummyArchive
survived,"def test_self_improver_invokes(monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text("""", encoding=""utf-8"")

    def fake_improve(repo_url: str, p_file: str, metric_file: str, log_file: str):
        click.echo(""score delta: 1.0"")
        return 1.0, tmp_path

    monkeypatch.setattr(cli.self_improver, ""improve_repo"", fake_improve)

    runner = CliRunner()
    result = runner.invoke(
        cli.main,
        [""self-improver"", ""--repo"", ""dummy"", ""--patch"", str(patch_file)],
    )

    assert result.exit_code == 0
    assert ""score delta"" in result.output
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,
survived,"def test_agents_status_requires_token(monkeypatch) -> None:
    monkeypatch.delenv(""API_TOKEN"", raising=False)
    with patch.object(cli.requests, ""get"") as get:
        res = CliRunner().invoke(cli.main, [""agents-status""])
    get.assert_not_called()
    assert res.exit_code == 1
    assert ""API_TOKEN not configured"" in res.output
",tests/test_demo_cli.py,
survived,"    def test_build_rest_none(self) -> None:
        mod_name = ""alpha_factory_v1.backend.orchestrator""
        with mock.patch.dict(sys.modules, {""fastapi"": None}):
            orch = importlib.reload(importlib.import_module(mod_name))
            self.assertIsNone(orch._build_rest({}))
        importlib.reload(orch)
",tests/test_orchestrator_no_fastapi.py,TestNoFastAPI
survived,"def inc(c: Counter):
    c.n = c.n + 1
",tests/human/x/python/record_assign.py,
survived,"def _validate_rel(rel: str) -> str:
    """"""Validate relationship name.""""""
    if not _REL_RE.match(rel):
        raise ValueError(f""Invalid relation name: {rel!r}"")
    return rel
",alpha_factory_v1/backend/memory_graph.py,
survived,"    def __init__(self):
        super().__init__(
            id=""12bf5a24-9b90-4f40-9090-4e86e6995e60"",
            description=""Reply to a Gmail thread"",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailReplyBlock.Input,
            output_schema=GmailReplyBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""threadId"": ""t1"",
                ""parentMessageId"": ""m1"",
                ""body"": ""Thanks"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""messageId"", ""m2""),
                (""threadId"", ""t1""),
            ],
            test_mock={
                ""_reply"": lambda *args, **kwargs: {
                    ""id"": ""m2"",
                    ""threadId"": ""t1"",
                }
            },
        )
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailReplyBlock
survived,"    def _get_attachments(self, service, message):
        attachments = []
        if ""parts"" in message[""payload""]:
            for part in message[""payload""][""parts""]:
                if part.get(""filename""):
                    attachment = Attachment(
                        filename=part[""filename""],
                        content_type=part[""mimeType""],
                        size=int(part[""body""].get(""size"", 0)),
                        attachment_id=part[""body""][""attachmentId""],
                    )
                    attachments.append(attachment)
        return attachments
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailGetThreadBlock
survived,"    def _get_thread(self, service, thread_id: str, include_spam_trash: bool) -> dict:
        thread = (
            service.users()
            .threads()
            .get(
                userId=""me"",
                id=thread_id,
                format=""full"",
                includeSpamTrash=include_spam_trash,
            )
            .execute()
        )

        parsed_messages = []
        for msg in thread.get(""messages"", []):
            headers = {
                h[""name""].lower(): h[""value""]
                for h in msg.get(""payload"", {}).get(""headers"", [])
            }
            body = self._get_email_body(msg)
            attachments = self._get_attachments(service, msg)
            email = Email(
                threadId=msg.get(""threadId"", thread_id),
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg.get(""snippet"", """"),
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=body,
                sizeEstimate=msg.get(""sizeEstimate"", 0),
                attachments=attachments,
            )
            parsed_messages.append(email.dict())

        thread[""messages""] = parsed_messages
        return thread
",autogpt_platform/backend/backend/blocks/google/gmail.py,GmailGetThreadBlock
survived,"    def __init__(self, settings: config.Settings) -> None:
        self.settings = settings
        self.published: list[tuple[str, messaging.Envelope]] = []
",tests/test_adk_agent.py,DummyBus
survived,"    def __init__(self, bus: messaging.A2ABus, ledger: ""Ledger"") -> None:
        super().__init__(""summariser"", bus, ledger)
        self._records: list[str] = []
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_summariser_agent.py,ADKSummariserAgent
survived,"        def __init__(self, *_a, **_kw) -> None:
            pass
",tests/test_orchestrator_backoff.py,DummyLedger
survived,"    async def handle(self, _env: orchestrator.messaging.Envelope) -> None:
        pass
",tests/test_orchestrator_backoff.py,FailingAgent
survived,"    def test_skill_test_endpoint(self) -> None:
        app = orchestrator._build_rest({""simple"": Runner(SimpleAgent())})
        self.assertIsNotNone(app)
        client = TestClient(app)
        headers = {""Authorization"": ""Bearer test-token""}
        resp = client.post(""/agent/simple/skill_test"", json={""t"": 1}, headers=headers)
        self.assertEqual(resp.status_code, 200)
        self.assertEqual(resp.json(), {""ok"": True})
",tests/test_skill_test_route.py,TestSkillTestRoute
survived,"def body_checksum(address: int, sig: Signal, d: bytearray) -> int:
    crc = 0xFF
    poly = 0xD5
    for i in range(len(d) - 2, -1, -1):
        crc ^= d[i]
        for _ in range(8):
            if crc & 0x80:
                crc = ((crc << 1) ^ poly) & 0xFF
            else:
                crc = (crc << 1) & 0xFF
    return crc
",opendbc/can/packer.py,
survived,"def fca_giorgio_checksum(address: int, sig: Signal, d: bytearray) -> int:
    crc = 0
    for i in range(len(d) - 1):
        crc ^= d[i]
        crc = CRC8J1850[crc]
    if address == 0xDE:
        return crc ^ 0x10
    elif address == 0x106:
        return crc ^ 0xF6
    elif address == 0x122:
        return crc ^ 0xF1
    else:
        return crc ^ 0x0A
",opendbc/can/packer.py,
survived,"def merkle_root(*, db_path: str | Path = _DEFAULT_DB) -> str:
    path = Path(db_path)
    _ensure(path)
    with sqlite3.connect(path) as cx:
        hashes = [r[0] for r in cx.execute(""SELECT hash FROM entries ORDER BY id"")]
    return _compute_root(hashes)
",src/archive/archive.py,
survived,"def build_rest(runners: Dict[str, AgentRunner], model_max_bytes: int, mem: Any) -> Optional[""FastAPI""]:
    if ""FastAPI"" not in globals():
        return None

    token = os.getenv(""API_TOKEN"")
    if not token:
        raise RuntimeError(""API_TOKEN environment variable must be set"")

    security = HTTPBearer()

    async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> None:
        if credentials.credentials != token:
            raise HTTPException(status_code=403, detail=""Invalid token"")

    app = FastAPI(
        title=""Alpha-Factory Orchestrator"",
        version=""3.0.0"",
        docs_url=""/docs"",
        redoc_url=None,
        dependencies=[Depends(verify_token)],
    )

    @app.get(""/healthz"", response_class=PlainTextResponse)
    async def _health() -> str:  # noqa: D401
        return ""ok""

    @app.get(""/agents"")
    async def _agents() -> List[str]:  # noqa: D401
        return list(runners)

    @app.post(""/agent/{name}/trigger"")
    async def _trigger(name: str) -> Dict[str, bool]:  # noqa: D401
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        runners[name].next_ts = 0
        return {""queued"": True}

    upload_param = File(...) if ""FastAPI"" in globals() else None  # type: ignore

    @app.post(""/agent/{name}/update_model"")
    async def _update_model(request: Request, name: str, file: Optional[bytes] = upload_param) -> Dict[str, str]:
        if ""FastAPI"" not in globals() and file is None:
            file = await request.body()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""load_weights""):
            raise HTTPException(501, ""Agent does not support model updates"")
        import io
        import stat
        import tempfile
        import zipfile

        with tempfile.TemporaryDirectory() as td:
            with zipfile.ZipFile(io.BytesIO(file)) as zf:
                base = Path(td).resolve()
                total = 0
                for info in zf.infolist():
                    if stat.S_ISLNK(info.external_attr >> 16):
                        raise HTTPException(400, ""Symlinks not allowed"")
                    if info.is_dir():
                        continue
                    total += info.file_size
                    if total > model_max_bytes:
                        raise HTTPException(400, ""Archive too large"")
                    dest = (base / info.filename).resolve()
                    if not str(dest).startswith(str(base)):
                        raise HTTPException(400, ""Unsafe path in archive"")
                    zf.extractall(td)
            inst.load_weights(td)  # type: ignore[attr-defined]
        return {""status"": ""ok""}

    @app.post(""/agent/{name}/skill_test"")  # type: ignore[misc]
    async def _skill_test(request: Request, name: str) -> Any:
        payload = await request.json()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""skill_test""):
            raise HTTPException(501, ""Agent does not support skill_test"")
        return await inst.skill_test(payload)  # type: ignore[func-returns-value]

    @app.get(""/memory/{agent}/recent"")  # type: ignore[misc]
    async def _recent(agent: str, n: int = 25) -> Any:  # noqa: D401
        return mem.vector.recent(agent, n)

    @app.get(""/memory/search"")  # type: ignore[misc]
    async def _search(q: str, k: int = 5) -> Any:  # noqa: D401
        return mem.vector.search(q, k)

    @app.get(""/metrics"", response_class=PlainTextResponse)  # type: ignore[misc]
    async def _metrics() -> PlainTextResponse:  # noqa: D401
        if ""generate_latest"" not in globals():
            raise HTTPException(503, ""prometheus_client not installed"")
        from .telemetry import generate_latest, CONTENT_TYPE_LATEST

        return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)

    return app
",alpha_factory_v1/backend/api_server.py,
survived,"async def serve_grpc(runners: Dict[str, AgentRunner], port: int, ssl_disable: bool) -> Optional[""grpc.aio.Server""]:
    if not port or ""grpc"" not in globals():
        return None
    try:
        from backend.proto import a2a_pb2, a2a_pb2_grpc
    except ModuleNotFoundError:
        log.warning(""A2A_PORT set but proto stubs missing â€“ gRPC disabled"")
        return None

    class Peer(a2a_pb2_grpc.PeerServiceServicer):  # type: ignore
        async def Stream(self, req_iter, ctx):  # noqa: N802
            async for req in req_iter:
                kind = req.WhichOneof(""payload"")
                if kind == ""trigger"" and req.trigger.name in runners:
                    runners[req.trigger.name].next_ts = 0
                    yield a2a_pb2.StreamReply(ack=a2a_pb2.Ack(id=req.id))
                elif kind == ""status"":
                    stats = [a2a_pb2.AgentStat(name=n, next_run=int(r.next_ts)) for n, r in runners.items()]
                    yield a2a_pb2.StreamReply(status_reply=a2a_pb2.StatusReply(stats=stats))

    creds = None
    if not ssl_disable:
        cert_dir = Path(os.getenv(""TLS_CERT_DIR"", ""/certs""))
        crt, key = cert_dir / ""server.crt"", cert_dir / ""server.key""
        if crt.exists() and key.exists():
            creds = grpc.ssl_server_credentials(((key.read_bytes(), crt.read_bytes()),))

    server = grpc.aio.server()
    a2a_pb2_grpc.add_PeerServiceServicer_to_server(Peer(), server)
    bind = f""[::]:{port}""
    server.add_secure_port(bind, creds) if creds else server.add_insecure_port(bind)
    await server.start()
    asyncio.create_task(server.wait_for_termination())
    log.info(""gRPC A2A server listening on %s (%s)"", bind, ""TLS"" if creds else ""plaintext"")
    return server",alpha_factory_v1/backend/api_server.py,
survived,"        def observe(self, *_a: Any) -> None:
            ...
",alpha_factory_v1/backend/telemetry.py,_Metric
survived,"        async def Stream(self, req_iter, ctx):  # noqa: N802
            async for req in req_iter:
                kind = req.WhichOneof(""payload"")
                if kind == ""trigger"" and req.trigger.name in runners:
                    runners[req.trigger.name].next_ts = 0
                    yield a2a_pb2.StreamReply(ack=a2a_pb2.Ack(id=req.id))
                elif kind == ""status"":
                    stats = [a2a_pb2.AgentStat(name=n, next_run=int(r.next_ts)) for n, r in runners.items()]
                    yield a2a_pb2.StreamReply(status_reply=a2a_pb2.StatusReply(stats=stats))
",alpha_factory_v1/backend/api_server.py,Peer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/dataset_where_filter.py,Person
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Nation
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_join.py,Order
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Customer
survived,"            async def _wrapped(*a: object, **kw: object) -> object:
                t0 = time.perf_counter()
                ok = True
                try:
                    return await orig(*a, **kw)
                except Exception:
                    ok = False
                    raise
                finally:
                    _HEALTH_Q.put((name, (time.perf_counter() - t0) * 1000, ok))
",tests/test_backend_orchestrator_dev.py,
survived,"    def Tool(*_args, **_kw):  # type: ignore
        def _decorator(func):
            return func

        return _decorator
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"        def _decorator(func):
            return func
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def test_run_cycle_sync_commits() -> None:
    model = DummyModel()
    demo.run_cycle(
        demo.Orchestrator(),
        demo.AgentFin(),
        demo.AgentRes(),
        demo.AgentEne(),
        demo.AgentGdl(),
        model,
    )
    assert model.committed
",tests/test_alpha_agi_business_3_v1.py,
survived,"    def fit(self, X: Any, y: Any | None = None):  # noqa: D401
        corpus, vocab = self._prepare_input(X)
        self.vocab_ = list(vocab)
        self.model_ = HierarchicalLDA(
            corpus,
            self.vocab_,
            alpha=self.alpha,
            gamma=self.gamma,
            eta=self.eta,
            num_levels=self.num_levels,
            seed=self.seed,
            verbose=self.verbose,
        )
        if self.iterations > 0:
            self.model_.estimate(
                self.iterations,
                display_topics=self.iterations + 1,
                n_words=0,
                with_weights=False,
            )
        return self
",src/hlda/sklearn_wrapper.py,HierarchicalLDAEstimator
survived,"    def _ensure_credentials_file(self):
        directory = os.path.dirname(self._path)
        os.makedirs(directory, exist_ok=True)
        if not os.path.exists(self._path):
            with open(self._path, ""w"", encoding=""UTF-8""):
                pass
",src/dhapi/port/credentials_provider.py,CredentialsProvider
survived,"def test_sandbox_env_limits(monkeypatch) -> None:
    recorded: list[tuple[int, tuple[int, int]]] = []

    def fake_setrlimit(res: int, limits: tuple[int, int]) -> None:
        recorded.append((res, limits))

    def fake_run(*args, **kwargs):
        if kwargs.get(""preexec_fn""):
            kwargs[""preexec_fn""]()

        class P:
            stdout = ""{}""
            stderr = """"

        return P()

    monkeypatch.setenv(""SANDBOX_CPU_SEC"", ""1"")
    monkeypatch.setenv(""SANDBOX_MEM_MB"", ""64"")
    monkeypatch.setattr(codegen_agent.subprocess, ""run"", fake_run)
    fake_resource = type(
        ""R"",
        (),
        {""RLIMIT_CPU"": 0, ""RLIMIT_AS"": 1, ""setrlimit"": fake_setrlimit},
    )
    monkeypatch.setitem(sys.modules, ""resource"", fake_resource)

    agent = _make_agent()
    agent.execute_in_sandbox(""print('hi')"")
    assert (0, (1, 1)) in recorded
    assert (1, (64 * 1024 * 1024, 64 * 1024 * 1024)) in recorded",tests/test_codegen_agent.py,
survived,"    def for_platform(self, platform_id: str) -> list[RuntimeDependency]:
        return [
            d
            for d in self._dependencies
            if d.platform_id in (platform_id, ""any"", ""platform-agnostic"", None)
        ]
",src/solidlsp/language_servers/common.py,RuntimeDependencyCollection
survived,"def test_resolve_pins_versions():
    manager = DependencyManager()
    reqs, licenses, _ = manager.resolve([""pydantic"", ""click""])
    assert any(r.startswith(""pydantic=="") for r in reqs)
    assert any(r.startswith(""click=="") for r in reqs)
    assert licenses.get(""pydantic"") == ""MIT""
    assert ""click"" in licenses
",tests/test_dependency_manager.py,
survived,"    def _collect_recursive(
        self,
        package: str,
        pinned: Dict[str, str],
        licenses: Dict[str, str],
        visited: set[str],
        include_hashes: bool,
        hashes: Optional[Dict[str, str]],
    ) -> None:
        if package in visited:
            return
        visited.add(package)
        try:
            dist = metadata.distribution(package)
        except metadata.PackageNotFoundError:
            return

        meta = cast(Mapping[str, str], dist.metadata)
        name = meta.get(""Name"", package)
        version = dist.version
        pinned[name] = version
        licenses[name] = self._extract_license(dist)
        if include_hashes and hashes is not None:
            # Use hash of RECORD contents if available, else hash of version
            record = dist.read_text(""RECORD"")
            if record is not None:
                digest = hashlib.sha256(record.encode(""utf-8"")).hexdigest()
            else:
                digest = hashlib.sha256(version.encode(""utf-8"")).hexdigest()
            hashes[name] = digest

        for req in dist.requires or []:
            req_name = req.split("";"")[0].strip().split()[0]
            req_name = req_name.split(""["")[0]
            if req_name:
                self._collect_recursive(
                    req_name, pinned, licenses, visited, include_hashes, hashes
                )
",src/meta_agent/dependency_manager.py,DependencyManager
survived,"    def init(self) -> None:
        """"""Initialize a new repository if one does not already exist.""""""
        if (self.repo_dir / "".git"").exists():
            return
        self.repo_dir.mkdir(parents=True, exist_ok=True)
        self._run(""init"")
        self._run(""config"", ""user.name"", ""meta-agent"")
        self._run(""config"", ""user.email"", ""meta-agent@example.com"")
        self._run(""branch"", ""-M"", ""main"")
",src/meta_agent/git_utils.py,GitManager
survived,"    def __init__(self, repo_dir: str | Path) -> None:
        self.repo_dir = Path(repo_dir)
",src/meta_agent/git_utils.py,GitManager
survived,"    def __init__(self, repo_dir: str | Path) -> None:
        self.repo_dir = Path(repo_dir)
",src/meta_agent/git_utils.py,GitManager
survived,"def _make_cert(tmp: Path) -> tuple[str, str, bytes]:
    key = rsa.generate_private_key(public_exponent=65537, key_size=2048)
    cert = (
        x509.CertificateBuilder()
        .subject_name(x509.Name([x509.NameAttribute(NameOID.COMMON_NAME, ""localhost"")]))
        .issuer_name(x509.Name([x509.NameAttribute(NameOID.COMMON_NAME, ""localhost"")]))
        .public_key(key.public_key())
        .serial_number(x509.random_serial_number())
        .not_valid_before(datetime.utcnow())
        .not_valid_after(datetime.utcnow() + timedelta(days=1))
        .add_extension(x509.SubjectAlternativeName([x509.DNSName(""localhost"")]), False)
        .sign(key, hashes.SHA256())
    )
    cert_pem = cert.public_bytes(serialization.Encoding.PEM)
    key_pem = key.private_bytes(
        serialization.Encoding.PEM,
        serialization.PrivateFormat.TraditionalOpenSSL,
        serialization.NoEncryption(),
    )
    cert_path = tmp / ""cert.pem""
    key_path = tmp / ""key.pem""
    cert_path.write_bytes(cert_pem)
    key_path.write_bytes(key_pem)
    return str(cert_path), str(key_path), cert_pem
",tests/test_orchestrator_bus_tls_env.py,
survived,"    def _run_tests(self, errors: List[str]) -> None:
        env = os.environ.copy()
        env[""PYTHONPATH""] = str(self.bundle_dir)
        result = subprocess.run(
            [""pytest"", ""tests"", ""-c"", ""/dev/null"", ""-x""],
            cwd=self.bundle_dir,
            capture_output=True,
            text=True,
            env=env,
        )
        if result.returncode != 0:
            errors.append(""tests failed"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_requirements(self, errors: List[str]) -> None:
        req_path = self.bundle_dir / ""requirements.txt""
        if not req_path.exists():
            errors.append(""requirements.txt missing"")
            return
        for line in req_path.read_text().splitlines():
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            if ""=="" not in line:
                errors.append(f""unpinned requirement: {line}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"def create_sample_bundle(tmp_path: Path) -> Path:
    gen = BundleGenerator(tmp_path)
    gen.generate(
        agent_code=""def main():\n    return 'ok'"",
        tests={
            ""test_main.py"": ""from agent import main\n\ndef test_main():\n    assert main() == 'ok'"",
        },
        requirements=[""pytest==8.0.0""],
        readme=""# Sample"",
    )
    return tmp_path
",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    failing_test = bundle_dir / ""tests"" / ""test_main.py""
    failing_test.write_text(""def test_fail():\n    assert False"")
    # update checksum so validation reaches test execution
    import hashlib
    import json

    bundle_file = bundle_dir / ""bundle.json""
    data = json.loads(bundle_file.read_text())
    digest = hashlib.sha256(failing_test.read_bytes()).hexdigest()
    data[""custom""][""checksums""][""tests/test_main.py""] = digest
    bundle_file.write_text(json.dumps(data))
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,
survived,"def test_bundle_validator_test_failure(tmp_path: Path) -> None:
    bundle_dir = create_sample_bundle(tmp_path)
    (bundle_dir / ""tests"" / ""test_main.py"").write_text(
        ""def test_fail():\n    assert False""
    )
    validator = BundleValidator(bundle_dir)
    result = validator.validate()
    assert result.success is False
    assert any(""tests failed"" in e for e in result.errors)",tests/test_bundle_validator.py,
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_checksums(self, metadata: BundleMetadata, errors: List[str]) -> None:
        checksums = metadata.custom.get(""checksums"", {})
        for rel, expected in checksums.items():
            path = self.bundle_dir / rel
            if not path.exists():
                errors.append(f""missing file {rel}"")
                continue
            digest = hashlib.sha256(path.read_bytes()).hexdigest()
            if digest != expected:
                errors.append(f""checksum mismatch for {rel}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def create(self, *args: Any, **kwargs: Any) -> Any:  # pragma: no cover - stub
        raise NotImplementedError(""OpenAI SDK not available"")
",src/meta_agent/services/openai_stub.py,_ChatCompletions
survived,"def runVM(prog):
    data = []
    i = 0
    while i < prog[""dataSize""]:
        data = data + [0]
        i = i + 1
    stack = []
    pc = 0
    code = prog[""code""]
    addrMap = prog[""addrMap""]
    pool = prog[""strings""]
    while pc < len(code):
        inst = code[pc]
        op = inst[""op""]
        arg = inst[""arg""]
        if op == ""push"":
            stack = stack + [arg]
            pc = pc + 1
            continue
        if op == ""store"":
            data[arg] = stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""fetch"":
            stack = stack + [data[arg]]
            pc = pc + 1
            continue
        if op == ""add"":
            stack[len(stack) - 2] = stack[len(stack) - 2] + stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""lt"":
            v = 0
            if stack[len(stack) - 2] < stack[len(stack) - 1]:
                v = 1
            stack[len(stack) - 2] = v
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""jz"":
            v = stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            if v == 0:
                pc = addrMap[arg]
            else:
                pc = pc + 1
            continue
        if op == ""jmp"":
            pc = addrMap[arg]
            continue
        if op == ""prts"":
            print(pool[stack[len(stack) - 1]])
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""prti"":
            print(str(stack[len(stack) - 1]))
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""halt"":
            break
        pc = pc + 1
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,
survived,"def bigMulSmall(a, m):
    if m == 0:
        return [0]
    res = []
    carry = 0
    i = 0
    while i < len(a):
        prod = a[i] * m + carry
        res = res + [prod % 10]
        carry = prod // 10
        i = i + 1
    while carry > 0:
        res = res + [carry % 10]
        carry = carry // 10
    return bigTrim(res)
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,
survived,"def printLower(m):
    n = m[""order""]
    ele = m[""ele""]
    mat = []
    idx = 0
    r = 0
    while r < n:
        row = []
        c = 0
        while c <= r:
            row = row + [ele[idx]]
            idx = idx + 1
            c = c + 1
        while c < n:
            row = row + [0.0]
            c = c + 1
        mat = mat + [row]
        r = r + 1
    printMat(mat)
",tests/rosetta/transpiler/Python/cholesky-decomposition-1.py,
survived,"def demo(a):
    print(""A:"")
    printMat(a)
    l = cholesky(a)
    print(""L:"")
    printMat(l)
",tests/rosetta/transpiler/Python/cholesky-decomposition.py,
survived,"    def fCounter(f):
        global s
        s = s + ""|""
        return f
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def printMat(m):
    i = 0
    while i < len(m):
        line = """"
        j = 0
        while j < len(m[i]):
            line = line + str(m[i][j])
            if j < len(m[i]) - 1:
                line = line + "" ""
            j = j + 1
        print(line)
        i = i + 1
",tests/rosetta/transpiler/Python/cholesky-decomposition.py,
survived,"def test_parse_time_spec_variants():
    assert parse_time_spec(""30n"") == {""type"": ""n"", ""value"": 30}
    assert parse_time_spec(15) == {""type"": ""n"", ""value"": 15}
    assert parse_time_spec(""24h"") == {""type"": ""time"", ""value"": timedelta(hours=24)}
    assert parse_time_spec(""45m"") == {""type"": ""time"", ""value"": timedelta(minutes=45)}
    assert parse_time_spec(""7d"") == {""type"": ""time"", ""value"": timedelta(days=7)}
    assert parse_time_spec(None) == {""type"": ""n"", ""value"": DEFAULT_LAST_N}
    with pytest.raises(ValueError):
        parse_time_spec(""abc"")
",tests/test_dashboard.py,
survived,"def test_error_handler_logs_and_outputs(caplog, capsys):
    handler = ErrorHandler(cli_output=CLIOutput())
    err = CLIOutputError(""boom"", context={""foo"": ""bar""})
    with caplog.at_level(logging.ERROR):
        handler.handle(err)
    out, err_stream = capsys.readouterr()
    assert ""boom"" in err_stream
    assert ""boom"" in caplog.text
    assert ""foo"" in caplog.text
",tests/ux/test_error_handler.py,
survived,"def test_error_suggestion(capsys):
    fb = UserFeedback()
    suggestion = fb.error_suggestion(""Failed to load file"")
    out, _ = capsys.readouterr()
    assert suggestion is not None
    assert ""file path exists"" in suggestion
    assert ""Suggestion"" in click.unstyle(out)
",tests/ux/test_user_feedback.py,
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
        self._metadata: BundleMetadata | None = None
",src/meta_agent/bundle.py,Bundle
survived,"def test_bundle_generator_hooks(tmp_path: Path) -> None:
    calls: list[str] = []

    def pre(path: Path) -> None:
        calls.append(""pre"")

    def post(path: Path, meta) -> None:
        calls.append(""post"")

    gen = BundleGenerator(tmp_path)
    gen.generate(agent_code=""print('x')"", pre_hook=pre, post_hook=post)

    assert calls == [""pre"", ""post""]",tests/test_bundle_api.py,
survived,"def test_crowding_distances() -> None:
    pop = _small_population()
    fronts = mats._non_dominated_sort(pop)
    mats._crowding(fronts[0])
    mats._crowding(fronts[1])

    cd_first = {ind.fitness: ind.crowd for ind in fronts[0]}
    assert cd_first[(2.0, 2.0)] == pytest.approx(2.0)
    assert cd_first[(1.0, 3.0)] == float(""inf"")
    assert cd_first[(3.0, 1.0)] == float(""inf"")

    cd_second = {ind.fitness: ind.crowd for ind in fronts[1]}
    assert all(d == float(""inf"") for d in cd_second.values())
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_mats.py,
survived,"def test_non_dominated_sort_assigns_ranks() -> None:
    pop = _small_population()
    fronts = mats._non_dominated_sort(pop)

    assert len(fronts) == 2
    first = {ind.fitness for ind in fronts[0]}
    second = {ind.fitness for ind in fronts[1]}
    assert first == {(1.0, 3.0), (2.0, 2.0), (3.0, 1.0)}
    assert second == {(4.0, 5.0), (5.0, 4.0)}
    assert {ind.rank for ind in fronts[0]} == {0}
    assert {ind.rank for ind in fronts[1]} == {1}
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_mats.py,
survived,"def test_replay_outputs_events(tmp_path: Path) -> None:
    """"""Replay should print formatted ledger rows.""""""
    path = tmp_path / ""audit.db""
    with logging.Ledger(str(path), broadcast=False) as led:
        led.log(messaging.Envelope(""a"", ""b"", {""x"": 1}, 0.0))
        led.log(messaging.Envelope(""b"", ""c"", {""y"": 2}, 1.0))

    with patch.object(cli.config.CFG, ""ledger_path"", str(path)):
        with patch.object(cli.time, ""sleep"", return_value=None):
            res = CliRunner().invoke(cli.main, [""replay""])

    lines = [ln.strip() for ln in res.output.splitlines() if ln.strip()]
    assert ""0.00 a -> b {\""x\"": 1}"" in lines[0]
    assert ""1.00 b -> c {\""y\"": 2}"" in lines[1]
",tests/test_demo_cli.py,
survived,"        async def stop_merkle_task(self) -> None:  # pragma: no cover - interface
            pass
",tests/test_adapters.py,DummyLedger
survived,"def test_adk_generate_text_flow(monkeypatch) -> None:
    agent, bus = _make_agent(monkeypatch)

    class StubADK:
        def __init__(self) -> None:
            self.called: list[str] = []

        def generate_text(self, prompt: str) -> str:
            self.called.append(prompt)
            return ""reply""

    adk = StubADK()
    monkeypatch.setattr(agent, ""adk"", adk, raising=False)

    async def patched_handle(self, env):
        text = self.adk.generate_text(env.payload.get(""plan"", """"))
        await self.emit(""strategy"", {""research"": text})

    monkeypatch.setattr(type(agent), ""handle"", patched_handle)
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.utils import messaging

    env = messaging.Envelope(""planning"", ""research"", {""plan"": ""p""}, 0.0)
    asyncio.run(agent.handle(env))

    assert adk.called == [""p""]
    assert bus.published and bus.published[-1][1].payload[""research""] == ""reply""
",tests/test_adapters.py,
survived,"    async def patched_run_cycle(self) -> None:
        res = await self.mcp.invoke_tool(""echo"", {""t"": 1})
        await self.emit(""strategy"", res)
",tests/test_adapters.py,
survived,"def test_adk_adapter_unavailable(monkeypatch) -> None:
    """"""Adapter gracefully degrades when ADK is missing.""""""

    def _raise(_name: str):
        raise ModuleNotFoundError

    monkeypatch.setattr(importlib, ""import_module"", _raise)
    assert not ADKAdapter.is_available()
    with pytest.raises(ModuleNotFoundError):
        ADKAdapter()
",tests/test_adapters.py,
survived,"    def fake_generate(self, prompt: str) -> str:
        calls[""prompt""] = prompt
        return ""resp""
",tests/test_adapters.py,
survived,"def test_mcp_invoke_tool_calls_library(monkeypatch) -> None:
    """"""Ensure MCPAdapter.invoke_tool delegates to the MCP client.""""""
    import mcp

    calls: dict[str, tuple[str, dict[str, object]]] = {}

    async def fake_call_tool(self, name: str, args: dict[str, object]) -> object:
        calls[""call""] = (name, args)
        return {""done"": True}

    monkeypatch.setattr(mcp.ClientSessionGroup, ""call_tool"", fake_call_tool, raising=False)
    adapter = MCPAdapter()
    result = asyncio.run(adapter.invoke_tool(""mytool"", {""x"": 1}))
    assert result == {""done"": True}
    assert calls[""call""] == (""mytool"", {""x"": 1})
",tests/test_adapters.py,
survived,"    def task(*_a, **_kw):
        def decorator(func):
            return func

        return decorator
",stubs/google_adk/__init__.py,
survived,"    async def __call__(self, text: str) -> str:  # pragma: no cover - demo stub
        return ""ok""
",openai_agents/__init__.py,OpenAIAgent
survived,"def test_feasibility_scores_monotonic() -> None:
    critic = FeasibilityCritic(DATA, seed=1)
    scores = [critic.score(item) for item in DATA]
    assert scores == sorted(scores)",tests/test_dual_critic.py,
survived,"    def _jaccard(a: Iterable[str], b: Iterable[str]) -> float:
        sa, sb = set(a), set(b)
        if not sa or not sb:
            return 0.0
        return len(sa & sb) / len(sa | sb)
",src/evaluators/feasibility_critic.py,FeasibilityCritic
survived,"def load_examples(path: str | Path | None = None) -> List[str]:
    p = Path(path) if path is not None else _DATA_FILE
    try:
        text = p.read_text(encoding=""utf-8"")
    except Exception:
        return []
    return [line.strip() for line in text.splitlines() if line.strip()]
",src/evaluators/feasibility_critic.py,
survived,"def _register_default_resources(
    app: EnrichMCP,
    sa_model: type,
    enrich_model: type,
    session_key: str,
) -> None:
    model_name = sa_model.__name__.lower()
    list_name = f""list_{model_name}s""
    get_name = f""get_{model_name}""
    param_name = f""{model_name}_id""

    list_description = f""List {sa_model.__name__} records""
    get_description = f""Get a single {sa_model.__name__} by ID""

    @app.resource(name=list_name, description=list_description)
    async def list_resource(
        ctx: EnrichContext, page: int = 1, page_size: int = 20
    ) -> PageResult[enrich_model]:  # type: ignore[name-defined]
        session_factory = ctx.request_context.lifespan_context[session_key]
        async with session_factory() as session:  # type: AsyncSession
            total = await session.scalar(select(func.count()).select_from(sa_model))
            result = await session.execute(
                select(sa_model).offset((page - 1) * page_size).limit(page_size)
            )
            items = [_sa_to_enrich(obj, enrich_model) for obj in result.scalars().all()]
            has_next = page * page_size < int(total or 0)
            return PageResult.create(
                items=items,
                page=page,
                page_size=page_size,
                total_items=int(total or 0),
                has_next=has_next,
            )

    @app.resource(name=get_name, description=get_description)
    async def get_resource(ctx: EnrichContext, **kwargs: int) -> enrich_model | None:  # type: ignore[name-defined]
        entity_id = kwargs[param_name]
        session_factory = ctx.request_context.lifespan_context[session_key]
        async with session_factory() as session:  # type: AsyncSession
            obj = await session.get(sa_model, entity_id)
            return _sa_to_enrich(obj, enrich_model) if obj else None
",src/enrichmcp/sqlalchemy/auto.py,
survived,"def verify_env() -> None:
    """"""Best-effort runtime dependency check.""""""
    try:
        import check_env  # type: ignore

        check_env.main([])
    except Exception as exc:  # pragma: no cover - best effort
        print(f""Environment verification failed: {exc}"")
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,
survived,"def test_pyodide_load_failure(tmp_path: Path) -> None:
    bridge_copy = tmp_path / ""bridge.mjs""
    text = BRIDGE.read_text().replace(
        ""../lib/pyodide.js"", LIB.resolve().as_posix()
    )
    bridge_copy.write_text(text)

    script = tmp_path / ""run.mjs""
    script.write_text(
        ""globalThis.window = {\n""
        ""  toast: (m) => console.log(m),\n""
        ""  loadPyodide: () => { throw new Error('boom'); }\n""
        ""};\n""
        ""globalThis.toast = globalThis.window.toast;\n""
        f""const m = await import('{bridge_copy.as_posix()}');\n""
        ""try { await m.run(); } catch (e) {}\n""
    )
    result = subprocess.run([""node"", script], capture_output=True, text=True)
    assert result.returncode == 0, result.stderr
    assert ""Pyodide failed to load"" in result.stdout",tests/test_wasm_bridge.py,
survived,"def test_health() -> None:
    client = TestClient(app)
    response = client.get(""/health"")
    assert response.status_code == 200
    assert response.json() == {""status"": ""ok""}",backend/tests/test_main.py,
survived,"def health() -> dict[str, str]:
    return {""status"": ""ok""}",backend/main.py,
survived,"def test_expand_cidr_cidr_range():
    result = expand_cidr('10.0.0.0/30')
    assert len(result) == 4
    assert result == ['10.0.0.0', '10.0.0.1', '10.0.0.2', '10.0.0.3']",tests/test_whois_perms.py,
survived,"def _build_dsl(extra_args: List[str]) -> str:
    """"""Convert unknown CLI options to DSL fragment.""""""
    dsl_map: Dict[str, Union[str, List[str]]] = {}
    i = 0
    while i < len(extra_args):
        token = extra_args[i]
        if token.startswith(""--""):
            key = token[2:]
            value = ""true""
            if ""="" in key:
                key, value = key.split(""="", 1)
            elif i + 1 < len(extra_args) and not extra_args[i + 1].startswith(""--""):
                value = extra_args[i + 1]
                i += 1
            existing = dsl_map.get(key)
            if existing is None:
                dsl_map[key] = value
            else:
                if isinstance(existing, list):
                    existing.append(value)
                else:
                    dsl_map[key] = [existing, value]
        i += 1

    parts = []
    for key, value in dsl_map.items():
        if isinstance(value, list):
            value = "","".join(value)
        parts.append(f""[{key}:{value}]"")
    return """".join(parts)
",src/attachments/cli.py,
survived,"def banner() -> str:
    """"""Return the standard startup banner.""""""
    return (
        ""\N{MILITARY MEDAL} \N{GREEK SMALL LETTER ALPHA}\N{HYPHEN-MINUS}AGI""
        "" Insight \N{EYE}\N{SPARKLES} â€” Beyond Human Foresight""
    )
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,
deleted,"  def supports_active_cooling(self) -> bool:
    return False",pylabrobot/heating_shaking/backend.py,HeaterShakerBackend
survived,"  def supports_active_cooling(self) -> bool:
    return False
",pylabrobot/temperature_controlling/opentrons_backend.py,OpentronsTemperatureModuleBackend
survived,"    def fake_convert_from_path(*args, **kwargs):
        return [Image.new(""RGB"", (10, 10)), Image.new(""RGB"", (10, 10))]
",no-ocr-api/tests/test_ingest_search.py,
survived,"    def __iter__(self):
        return iter(self.data)
",no-ocr-api/tests/test_ingest_search.py,FakeDataset
survived,"def main() -> None:
    app = EnrichMCP(title=""Hello HTTP API"", description=""A simple HTTP example"")

    @app.retrieve(description=""Say hello over HTTP"")
    async def hello_http() -> dict[str, str]:
        return {""message"": ""Hello over HTTP!""}

    print(""Starting HTTP server on http://localhost:8000 ..."")
    app.run(transport=""streamable-http"")
",examples/hello_world_http/app.py,
survived,"        def generate(self, prompt: str) -> str:
            resp = httpx.post(""https://adk.example/generate"", json={""prompt"": prompt})
            resp.raise_for_status()
            return resp.json()[""text""]
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,Client
survived,"def test_mcp_invoke_tool_success(httpx_mock, stub_mcp):
    httpx_mock.add_response(url=""https://mcp.example/foo"", json={""ok"": True})
    adapter = MCPAdapter()
    result = asyncio.run(adapter.invoke_tool(""foo"", {""a"": 1}))
    assert result == {""ok"": True}
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,
survived,"def test_mcp_invoke_tool_unreachable(httpx_mock, stub_mcp):
    httpx_mock.add_exception(httpx.ConnectError(""offline""), url=""https://mcp.example/foo"")
    adapter = MCPAdapter()
    with pytest.raises(httpx.HTTPError):
        asyncio.run(adapter.invoke_tool(""foo"", {}))",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,
survived,"def find_deps(code):
    deps = []
    for imp in re.findall(r""import[^'\""]*['\""](.*?)['\""]"", code):
        if imp.startswith(ALIAS_PREFIX) or imp.startswith("".""):
            deps.append(imp)
    return deps
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"def check_gzip_size(path: Path, max_bytes: int = 2 * 1024 * 1024) -> None:
    """"""Exit if gzip-compressed file exceeds ``max_bytes``.""""""
    compressed = gzip.compress(path.read_bytes())
    if len(compressed) > max_bytes:
        sys.exit(f""gzip size {len(compressed)} bytes exceeds limit"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"def scenario_2001_genome() -> replay.Scenario:
    return replay.load_scenario(""2001_genome"")
",tests/conftest.py,
survived,"        def poll(self) -> None:
            return None
",tests/test_start_alpha_business.py,DummyProc
survived,"def delete(
    url: str,
    *,
    params: dict | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    """"""HTTP DELETE request.""""""
    return _call(""DELETE"", url, params=params, headers=headers, timeout=timeout)
",alpha_factory_v1/af_requests.py,
survived,"    def test_update_model_path_traversal(self):
        client, _runner = self._make_client()
        data = self._zip_bytes({""../evil"": b""bad""})
        res = client.post(""/agent/foo/update_model"", files={""file"": (""f.zip"", data)})
        self.assertEqual(res.status_code, 400)
",alpha_factory_v1/tests/test_orchestrator_rest.py,UpdateModelTest
survived,"    def test_update_model_safe(self):
        client, runner = self._make_client()
        data = self._zip_bytes({""w.bin"": b""ok""})
        res = client.post(""/agent/foo/update_model"", files={""file"": (""f.zip"", data)})
        self.assertEqual(res.status_code, 200)
        self.assertEqual(runner.inst.loaded is not None, True)
",alpha_factory_v1/tests/test_orchestrator_rest.py,UpdateModelTest
survived,"async def test_request_id_sanitization_and_unique_fallback():
    backend = MemoryCache()
    cache = ContextCache(backend, ""app"", ""bad:id"")
    assert cache._build_namespace(""request"") == ""enrichmcp:request:app:bad_id""

    c1 = ContextCache(backend, ""app"", """")
    c2 = ContextCache(backend, ""app"", """")
    assert c1._request_id != c2._request_id",tests/test_cache.py,
survived,"    def _validate(self, t: Triplet) -> bool:
        if (
            len(t.program.splitlines()) > MAX_PROG_LOC
            or len(t.inp) > 256
            or len(t.out) > 256
            or any(b in t.program for b in _BANNED)
        ):
            return False
        stdout, stderr = _exec_trusted(t.program, t.inp)
        return stderr == """" and stdout.strip() == t.out.strip()
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"def _complexity(py_src: str) -> float:  # noqa: D401
    """"""Return cyclomatic complexity; fallback to AST node count.""""""
    if cc_visit:
        try:
            return max((b.complexity for b in cc_visit(py_src) if b.lineno == 1), default=1.0)
        except Exception:
            pass
    # Fallback: #nodes / 10  (heuristic)
    try:
        return max(1.0, len(list(ast.walk(ast.parse(py_src)))) / 10.0)
    except Exception:
        return 10.0
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,
survived,"    def learn(self, results: Sequence[TaskResult]) -> None:
        if not results:
            return
        # Multiâ€‘objective scalarisation (simple): reward = unsolved_ratio * 0.7 + novelty * 0.3
        solved_frac = sum(r.solved for r in results) / len(results)
        diff_reward = 1.0 - solved_frac
        novelty = sum(r.complexity for r in results) / len(results)
        novelty_norm = min(1.0, novelty / 15.0)
        reward = 0.7 * diff_reward + 0.3 * novelty_norm

        beta = 0.1
        self._baseline = (1 - beta) * self._baseline + beta * reward
        adv = reward - self._baseline
        # PPOâ€‘lite temperature adjust
        delta = -0.04 if adv > 0 else 0.04
        self.temperature = max(0.1, min(1.0, self.temperature + delta))

        # Buffer maintenance â€“ keep correctly solved tasks
        for r in results:
            if r.solved:
                self._add(r.triplet)

        self.log(f""[AZR] reward={reward:.3f} adv={adv:+.3f} -> T={self.temperature:.2f}"")

        examples = (
            ""\n\n"".join(
                f""```python\n{t.program}```\n```json\n{t.inp}```\n```json\n{t.out}```""
                for t in self._rng.sample(self.buffer, k=min(3, len(self.buffer)))
            )
            or ""(buffer empty)""
        )

        return self._PROMPT.format(
            n=n,  # noqa: F821
            max_loc=MAX_PROG_LOC,
            buf=len(self.buffer),
            examples=examples,
        )
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"def curriculum_factory(fm, **kwargs) -> AZREngine:  # noqa: D401
    return AZREngine(fm, **kwargs)
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,
survived,"    async def loop(self, bus: messaging.A2ABus, ledger: Ledger) -> None:
        while True:
            try:
                await self.agent.run_cycle()
            except Exception as exc:  # noqa: BLE001
                logging._log.warning(""%s failed: %s"", self.agent.name, exc)
            env = messaging.Envelope(self.agent.name, ""orch"", {""heartbeat"": True}, time.time())
            ledger.log(env)
            bus.publish(""orch"", env)
            self.last_beat = env.ts
            await asyncio.sleep(self.period)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,AgentRunner
survived,"    def fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/forecast.py,
survived,"    async def ws_progress(ws: WebSocket, sim_id: str) -> None:
        await ws.accept()
        idx = 0
        try:
            while True:
                items = _progress.get(sim_id, [])
                while idx < len(items):
                    await ws.send_text(items[idx])
                    idx += 1
                if sim_id in _simulations and idx >= len(items):
                    break
                await asyncio.sleep(0.1)
        finally:
            await ws.close()
",src/interface/api_server.py,
survived,"def test_vmap():
    class Module(eqx.Module):
        weight: hax.NamedArray

        def __call__(self, x):
            return x + self.weight

        @staticmethod
        def init(weight):
            return Module(weight=weight)

    Block = hax.Axis(""block"", 4)
    E = hax.Axis(""E"", 10)

    weights = hax.random.uniform(jax.random.PRNGKey(0), (Block, E))
    m = Stacked.init(Block, Module)(weight=weights)

    x = hax.random.uniform(jax.random.PRNGKey(1), (E,))
    y = m.vmap(x)

    assert y.axes == (Block, E)
    assert hax.all(y == weights + x)
",tests/test_scan.py,
survived,"    def test_ledger_default_home(self) -> None:
        env = {""CROSS_ALPHA_LEDGER"": """"}
        with patch.dict(os.environ, env, clear=False):
            path = stub._ledger_path(None)
        expected = stub.DEFAULT_LEDGER.resolve()
        self.assertEqual(path, expected)
        self.assertTrue(path.parent.exists())
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"    def test_submit_job(self):
        agent = bridge.BusinessAgent()
        job = {""agent"": ""alpha_discovery"", ""foo"": 1}
        with patch.object(bridge, ""requests"") as req:
            req.post.return_value = DummyResponse()
            result = asyncio.run(bridge.submit_job(job))
        req.post.assert_called_once_with(
            f""{bridge.HOST}/agent/alpha_discovery/trigger"", json=job, timeout=5
        )
        self.assertEqual(result, ""job for alpha_discovery queued"")
",tests/test_openai_bridge_integration.py,TestBusinessAgentIntegration
survived,"    def test_openai_response_format(self) -> None:
        from alpha_factory_v1.demos.cross_industry_alpha_factory import (
            cross_alpha_discovery_stub as stub,
        )

        resp = types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""[]""))])
        openai_mock = types.SimpleNamespace(ChatCompletion=types.SimpleNamespace(create=Mock(return_value=resp)))

        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""x""}):
            with patch.object(stub, ""openai"", openai_mock):
                stub.discover_alpha(num=1, ledger=None, model=""gpt-4o-mini"")

        openai_mock.ChatCompletion.create.assert_called_once()
        kwargs = openai_mock.ChatCompletion.create.call_args.kwargs
        self.assertEqual(kwargs.get(""response_format""), {""type"": ""json_object""})
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"def test_llama_paged_decode_ragged_fill_in_chunks():
    B = Axis(""batch"", 2)
    Pos = Axis(""position"", 8)
    Embed = Axis(""embed"", 8)
    Vocab = Axis(""vocab"", 64)

    cfg = LlamaConfig(
        seq_len=Pos.size,
        hidden_dim=Embed.size,
        intermediate_dim=16,
        num_layers=2,
        num_heads=2,
        num_kv_heads=2,
        rope=None,
        gradient_checkpointing=False,
        scan_layers=True,
        attn_backend=AttentionBackend.VANILLA,
    )

    model_key, input_key = jrandom.split(jrandom.PRNGKey(0))
    model = LlamaLMHeadModel.init(Vocab=Vocab, config=cfg, key=model_key)

    input_ids = hax.random.randint(input_key, (B, Pos), 0, Vocab.size)
    full_out = model.activations(input_ids, attn_mask=AttentionMask.causal(), key=jrandom.PRNGKey(1))

    pt = PageTable.init(max_pages=8, max_seqs=2, page_size=4, max_pages_per_seq=4)
    pt, seq1 = pt.assign_seq_id_to_seq()
    pt, seq2 = pt.assign_seq_id_to_seq()
    layer_caches = model.transformer.initial_cache(pt, dtype=jnp.float32)

    x = model.embeddings.embed(input_ids)
    x0 = x[B, 0]
    x1 = x[B, 1]

    chunk_sizes = [[4, 2], [0, 1], [0, 1], [2, 1], [1, 2], [1, 1]]
    off0 = off1 = 0
    outputs0 = []
    outputs1 = []

    seq_axis = Axis(""seq"", 2)
    for step0, step1 in chunk_sizes:
        tok_axis = Axis(""position"", step0 + step1)
        updated = hax.named([seq1, seq2], seq_axis)
        new_counts = hax.named([step0, step1], seq_axis)
        tokens = hax.named([seq1] * step0 + [seq2] * step1, tok_axis)
        pt, binfo = pt.allocate_for_seqs(updated, new_counts, tokens)
        state = KvPageState.from_batch(binfo, layer_caches)

        x_chunk = hax.concatenate(
            ""position"",
            [x0[Pos, hax.dslice(off0, step0)], x1[Pos, hax.dslice(off1, step1)]],
        )
        pos_ids = hax.named(list(range(off0, off0 + step0)) + list(range(off1, off1 + step1)), tok_axis)
        with jax.disable_jit():
            output, state = _jit_paged_decode(model.transformer, x_chunk, pos_ids, state)
        layer_caches = state.cache
        outputs0.append(output[""position"", hax.dslice(0, step0)])
        outputs1.append(output[""position"", hax.dslice(step0, step1)])

        assert_trees_all_close(
            full_out[B, 0, ""position"", hax.dslice(off0, step0)].array,
            outputs0[-1].array,
            atol=1e-4,
            rtol=1e-4,
        )
        assert_trees_all_close(
            full_out[B, 1, ""position"", hax.dslice(off1, step1)].array,
            outputs1[-1].array,
            atol=1e-4,
            rtol=1e-4,
        )

        off0 += step0
        off1 += step1

    outputs0_cat = hax.concatenate(""position"", outputs0)
    outputs1_cat = hax.concatenate(""position"", outputs1)
    decoded_arr = hax.stack(""batch"", [outputs0_cat, outputs1_cat])
    assert_trees_all_close(full_out.array, decoded_arr.array, atol=1e-4, rtol=1e-4)",tests/test_llama_decode.py,
survived,"def _start_server(port: int, env: dict[str, str] | None = None) -> subprocess.Popen[bytes]:
    cmd = [
        sys.executable,
        ""-m"",
        ""src.interface.api_server"",
        ""--host"",
        ""127.0.0.1"",
        ""--port"",
        str(port),
    ]
    return subprocess.Popen(cmd, env=env or os.environ.copy())
",tests/test_metrics.py,
survived,"        def set_tracer_provider(self, _provider: Any) -> None:
            pass
",tests/test_metrics.py,DummyTrace
survived,"    async def bad_guard(_output: str):
        raise RuntimeError(""bad"")
",tests/test_guardrail_router.py,
survived,"        def start_merkle_task(self, *_a, **_kw) -> None:
            pass
",tests/test_agents.py,DummyLedger
survived,"        def log(self, env) -> None:  # type: ignore[override]
            events.append(env.payload.get(""event""))
",tests/test_agents.py,DummyLedger
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q1.py,Lineitem
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Partsupp
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/machine/x/python/q3.py,
survived,"def _sort_key(k):
    if isinstance(k, (list, tuple, dict)):
        return str(k)
    return k
",tests/machine/x/python/q3.py,
survived,"def test_Q2_returns_only_supplier_with_min_cost_in_Europe_for_brass_part():
    assert result == [
        {
            ""s_acctbal"": 1000,
            ""s_name"": ""BestSupplier"",
            ""n_name"": ""FRANCE"",
            ""p_partkey"": 1000,
            ""p_mfgr"": ""M1"",
            ""s_address"": ""123 Rue"",
            ""s_phone"": ""123"",
            ""s_comment"": ""Fast and reliable"",
            ""ps_supplycost"": 10,
        }
    ]
",tests/machine/x/python/q2.py,
survived,"        def reset(self):
            return None
",tests/test_world_model_demo.py,DummyEnv
survived,"        def act(self, _obs):
            return 0
",tests/test_world_model_demo.py,DummyLearner
survived,"def test_multi_env_reporting(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Aggregated metrics should reflect all environments.""""""
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_UI_TICK"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_ENV_BATCH"", ""2"")

    mod = importlib.import_module(
        ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    )

    class DummyEnv:
        def __init__(self, reward: float) -> None:
            self.reward = reward

        def reset(self):
            return None

        def step(self, _a: int):
            return None, self.reward, True, {}

    class DummyLearner:
        def __init__(self, loss: float) -> None:
            self.loss = loss

        def act(self, _obs):
            return 0

        def remember(self, _obs, _reward) -> None:
            pass

        def train_once(self) -> float:
            return self.loss

    mod.A2ABus._subs = {}
    orch = mod.Orchestrator()
    orch.envs = [DummyEnv(1.0), DummyEnv(0.0)]
    orch.learners = [DummyLearner(0.2), DummyLearner(0.4)]

    msgs: list[dict] = []
    mod.A2ABus.subscribe(""ui"", lambda m: msgs.append(m))

    orch.loop()

    assert msgs
    msg = msgs[-1]
    assert msg[""t""] == 0
    assert msg[""r""] == pytest.approx(0.5)
    assert msg[""loss""] == pytest.approx(0.3)",tests/test_world_model_demo.py,
survived,"    def _tool(*_a, **_k):
        def dec(func):
            return func

        return dec
",tests/test_aiga_service.py,
survived,"def run() -> None:
    n = 18
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_018.py,
survived,"def run() -> None:
    n = 10
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_010.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""14""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(14)",benchmarks/poly_mini/task_014.py,
survived,"def _full_name(node: ast.AST) -> str:
    if isinstance(node, ast.Name):
        return node.id
    if isinstance(node, ast.Attribute):
        parent = _full_name(node.value)
        return f""{parent}.{node.attr}"" if parent else node.attr
    return """"
",src/self_edit/safety.py,
survived,"def _read(name: str) -> str:
    return (FIXTURES / name).read_text()
",tests/test_safety_filter.py,
survived,"def test_improve_repo_requires_git(monkeypatch, tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text(""dummy"")
    log_file = tmp_path / ""log.json""

    monkeypatch.setattr(self_improver, ""git"", None)
    with pytest.raises(RuntimeError):
        self_improver.improve_repo(
            str(repo_dir), str(patch_file), ""metric.txt"", str(log_file)
        )",tests/test_self_improver.py,
survived,"    def api_key(self, value: Optional[str]) -> None:
        object.__setattr__(self, ""_api_key"", value)
        object.__setattr__(self, ""_headers"", self._compute_headers())
",python/langsmith/client.py,Client
survived,"    def _patched_init_subclass(cls, **kwargs: Any) -> None:  # type: ignore[override]
        _orig_init_subclass(**kwargs)
        _ensure_pydantic_methods(cls)
",src/meta_agent/__init__.py,
survived,"def test_regression_guard_resumes(monkeypatch) -> None:
    alerts: list[str] = []
    runner = DummyRunner()
    runners = {""aiga_evolver"": runner}

    async def drive() -> bool:
        guard = asyncio.create_task(orchestrator.regression_guard(runners, alerts.append))
        for v in [1.0, 0.9, 0.6]:
            metrics.dgm_best_score.set(v)
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.5)
        assert runner.task.cancelled
        for v in [0.8, 1.0]:
            metrics.dgm_best_score.set(v)
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.5)
        guard.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await guard
        return runner.task is not None and not runner.task.cancelled

    resumed = asyncio.run(drive())
    assert resumed
    assert any(""resumed"" in a for a in alerts)",tests/test_governance.py,
survived,"        def _get(self: struct_pb2.Struct, key: str, default=None):
            try:
                return self[key]
            except Exception:
                return default
",tests/test_safety_block.py,
survived,"def test_improve_repo_cleanup(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)

    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""p.diff""
    patch_file.write_text(patch)
    log_file = tmp_path / ""log.json""

    delta, clone = self_improver.improve_repo(
        str(repo_dir), str(patch_file), ""metric.txt"", str(log_file), cleanup=True
    )

    assert delta == 1
    assert not clone.exists()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_self_improver.py,
survived,"def bollinger_bands(
    prices: Sequence[float],
    window: int = 20,
    num_std: float = 2.0,
) -> tuple[float, float]:
    """"""Return the lower and upper Bollinger Bands.""""""

    if window <= 0:
        raise ValueError(""window must be positive"")
    if len(prices) < window:
        return (0.0, 0.0)

    if np is not None:
        arr = np.asarray(prices[-window:], dtype=float)
        mean = float(arr.mean())
        std = float(arr.std(ddof=1))
    else:
        slice_ = [float(p) for p in prices[-window:]]
        mean = sum(slice_) / window
        variance = sum((p - mean) ** 2 for p in slice_) / (window - 1)
        std = variance ** 0.5
    band = num_std * std
    return (mean - band, mean + band)
",alpha_factory_v1/backend/alpha_model.py,
survived,"        def eval_ind(ind: Individual) -> float:
            if not allow_unsafe and not is_genome_safe(ind.genome):
                self.log(""[SECURITY] Rejected unsafe genome"")
                return -math.inf
            return self.task.evaluate(ind.genome)
",alpha_factory_v1/backend/evolution_engine.py,EvolutionEngine
survived,"    def legal_actions(self) -> List[str]:
        """"""Available trade actions.""""""
        return [""HOLD"", ""BUY"", ""SELL""]
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv
survived,"    def sample_next_price(self, price: float) -> float:
        """"""Return the next price using a Gaussian random walk.""""""
        return price + random.gauss(0.0, self.volatility)
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv
survived,"    async def prices(self, symbols: list[str]) -> dict[str, float]:
        """"""Return latest prices for multiple symbols concurrently.""""""
        tasks = [asyncio.create_task(self.price(sym)) for sym in symbols]
        values = await asyncio.gather(*tasks)
        return dict(zip(symbols, values))
",alpha_factory_v1/backend/market_data.py,MarketData
survived,"    async def close(self) -> None:  # pragma: no cover - interface default
        """"""Gracefully close underlying resources.""""""
        await self.__aexit__(None, None, None)
",alpha_factory_v1/backend/market_data.py,BaseMarketData
survived,"    async def run_cycle(self):  # noqa: D401
        self.ran = True
",alpha_factory_v1/tests/test_planner_agent.py,DummyAgent
survived,"    def test_invalid_params_raise(self):
        with self.assertRaises(ValueError):
            am.momentum([1, 2], lookback=0)
        with self.assertRaises(ValueError):
            am.sma_crossover([1]*5, fast=0, slow=1)
        with self.assertRaises(ValueError):
            am.sma_crossover([1]*5, fast=5, slow=3)
        with self.assertRaises(ValueError):
            am.ema([1], span=0)
        with self.assertRaises(ValueError):
            am.rsi([1, 2], period=0)
        with self.assertRaises(ValueError):
            am.bollinger_bands([1], window=0)
",alpha_factory_v1/tests/test_alpha_model.py,AlphaModelTest
survived,"    def train_once(self)->float:
        if len(self.buffer)<CFG.train_batch: return 0.0
        obs,rew=zip(*random.sample(self.buffer, CFG.train_batch))
        obs_t=torch.tensor(obs, device=CFG.device, dtype=torch.float32)
        rew_t=torch.tensor(rew, device=CFG.device)
        _,v,_=self.net.initial(obs_t)
        loss=F.mse_loss(v.squeeze(),rew_t)
        self.opt.zero_grad(); loss.backward(); self.opt.step()
        return float(loss.item())
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Learner
survived,"            def handle(self,msg):
                if ""ask_plan"" in msg:
                    try:
                        plan=self._safe_call(msg[""ask_plan""])
                        self.emit(""planning_agent"",{""llm_plan"":plan})
                    except Exception as e:
                        LOG.warning(""LLMPlanner error: %s"",e)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,LLMPlanner
survived,"    def emit(self, topic: str, msg: dict):
        A2ABus.publish(topic, msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Agent
survived,"def _cfg_from_env() -> dict[str, Any]:
    mapping: dict[str, str] = {
        ""SUCCESS_THRESHOLD"": ""success_thresh"",
        ""MAX_SIM_MINUTES"": ""max_minutes"",
        ""MICRO_CURRICULUM"": ""micro_curr"",
        ""AGIALPHA_SUPPLY"": ""max_supply"",
    }
    cfg: dict[str, Any] = {}
    for k, v in CFG_DEFAULTS.items():
        key = mapping.get(k, k.lower())
        val = os.getenv(f""OMNI_{k}"", v)
        if k.endswith(""PATH""):
            cfg[key] = Path(val)
        else:
            cfg[key] = type(CFG_DEFAULTS[k])(val)
    return cfg
",alpha_factory_v1/demos/omni_factory_demo/omni_factory_demo.py,
survived,"def main() -> None:
    args = parse_args()

    cli = [""--dev"", f""--port"", str(args.port)]
    if args.metrics_port:
        cli += [""--metrics-port"", str(args.metrics_port)]
    if args.a2a_port:
        cli += [""--a2a-port"", str(args.a2a_port)]
    if args.agents:
        cli += [""--enabled"", args.agents]

    ns = af_run.parse_args(cli)
    af_run.apply_env(ns)

    os.environ.setdefault(""PGHOST"", ""sqlite"")

    af_run.run()
",alpha_factory_v1/edge_runner.py,
survived,"    def test_summary_rate_limit_error(self) -> None:
        import openai

        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""sk-test""}):
            with patch(""openai.OpenAI"") as mock_client:
                mock_client.return_value.chat.completions.create.side_effect = openai.RateLimitError(""limit"")
                text = summarise_with_agent(
                    0.5,
                    agents=2,
                    rounds=10,
                    delta=0.9,
                    stake=1.0,
                )
        self.assertIn(""offline summary"", text)
        self.assertIn(""rate limit"", text)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"def _collect_guards(cls: Type, method: callable) -> List[BaseGuard]:
    guards: List[BaseGuard] = []
    for guard in getattr(cls, ""__guards__"", []):
        guards.append(guard)
    for guard in getattr(method, ""__guards__"", []):
        guards.append(guard)
    return guards
",nest/core/decorators/controller.py,
survived,"def choose_example(examples: dict[str, str], preselected: str | None = None) -> str:
    """"""Prompt the user to choose an example.""""""
    names = sorted(examples)
    if preselected and preselected in examples:
        return preselected

    print(""Available examples:"")
    for idx, name in enumerate(names, 1):
        print(f""  {idx}. {name}"")

    while True:
        choice = input(""Select example by number or name: "").strip()
        if choice in examples:
            return choice
        if choice.isdigit():
            index = int(choice) - 1
            if 0 <= index < len(names):
                return names[index]
        print(""Invalid selection, try again."")
",examples/openai_chat_agent/app.py,
survived,"def test_results_dir_permissions(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Directory is created with 0700 permissions.""""""

    path = tmp_path / ""results""
    monkeypatch.setenv(""SIM_RESULTS_DIR"", str(path))

    import importlib

    from src.interface import api_server as api

    api = importlib.reload(api)

    assert path.exists()
    assert (path.stat().st_mode & 0o777) == 0o700
",tests/test_api_server.py,
survived,"    def log_message(self, *_args: str) -> None:  # pragma: no cover - quiet
        pass
",tests/test_aiga_service_mixtral.py,_Handler
survived,"    def fold_via(self, fn: Callable[..., CarryT]):
        """"""Return a function that folds over the stack using ``fn``.

        ``fn`` should take a block and a carry and return a new carry.  The
        returned function mirrors :func:`haliax.fold` over the block axis.
        """"""

        def do_block(carry: CarryT, block: M) -> CarryT:
            return fn(block, carry)

        def do_fold(init: CarryT) -> CarryT:
            return haliax.fold(do_block, self.Block, remat=self.gradient_checkpointing)(init, self.stacked)

        return do_fold
",src/haliax/nn/scan.py,Stacked
survived,"def test_llm_openai_path() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.route(
            ""https://api.openai.com/**"",
            lambda route: route.fulfill(
                status=200,
                content_type=""application/json"",
                body='{""choices"":[{""message"":{""content"":""pong""}}]}',
            ),
        )
        page.goto(url)
        page.evaluate(""localStorage.setItem('OPENAI_API_KEY','sk')"")

        out = page.evaluate(""window.llmChat('hi')"")
        assert out == 'pong'
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"def twoSum(nums, target):
    n = len(nums)
    for i in range(0, n):
        for j in range(i + 1, n):
            if nums[i] + nums[j] == target:
                return [i, j]
    return [-1, -1]
",tests/transpiler/x/py/two-sum.py,
survived,"    async def get_latest(_: None = Depends(verify_token)) -> ResultsResponse:
        if _latest_id is None:
            raise HTTPException(status_code=404)
        result = _simulations.get(_latest_id)
        if result is None:
            raise HTTPException(status_code=404)
        return result
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def _stop() -> None:
        task = getattr(app_f.state, ""task"", None)
        if task:
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
            app_f.state.task = None
        app_f.state.orchestrator = None
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"def test_sbml_testsuite_case_jax(test_number, result_path_jax, sbml_semantic_cases_dir):
    test_id = format_test_id(test_number)
    model_dir = Path(__file__).parent / ""SBMLTestModelsJax"" / test_id
    try:
        current_test_path = sbml_semantic_cases_dir / test_id
        results_file = current_test_path / f""{test_id}-results.csv""
        results = pd.read_csv(results_file, delimiter="","")
        results.rename(columns={c: c.replace("" "", """") for c in results.columns}, inplace=True)

        model, wrapper = compile_model_jax(current_test_path, test_id, model_dir)
        settings = read_settings_file(current_test_path, test_id)
        ts = np.linspace(
            float(settings[""start""] or 0),
            float(settings[""start""] or 0) + float(settings[""duration""] or 0),
            int(settings[""steps""] or 0) + 1,
        )
        atol = float(settings[""absolute""])
        rtol = float(settings[""relative""])

        rdata = run_jax_simulation(model, wrapper, ts, atol, rtol)
        dummy = DummyModel(model, wrapper)
        simulated = verify_results(settings, rdata, results, wrapper, dummy, atol, rtol)
        write_result_file(simulated, test_id, result_path_jax)
    except amici.sbml_import.SBMLException as err:
        pytest.skip(str(err))
    finally:
        shutil.rmtree(model_dir, ignore_errors=True)
",tests/testSBMLSuiteJax.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/q1.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q1.py,Auto3
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_conditional_sum.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_left_join.py,_Group
survived,"    def lint(self, content: str) -> List[str]:
        """"""Run Ruff linting on ``content`` and return issues.""""""
        proc = subprocess.run(
            [""ruff"", ""--quiet"", ""--stdin-filename"", ""template.py"", ""-""],
            input=content.encode(""utf-8""),
            capture_output=True,
        )
        output = proc.stdout.decode()
        return [line.strip() for line in output.splitlines() if line.strip()]
",src/meta_agent/template_governance.py,TemplateGovernance
survived,"def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Run the Î±â€‘AGI Business v1 demo"")
    parser.add_argument(
        ""--loglevel"",
        default=os.getenv(""LOGLEVEL"", ""INFO""),
        help=""Logging verbosity (default: INFO)"",
    )
    return parser.parse_args(argv)
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,
survived,"    async def policy(self, obs, _ctx):  # type: ignore[override]
        episodes = int(obs.get(""episodes"", 10)) if isinstance(obs, dict) else 10
        target = int(obs.get(""target"", 5)) if isinstance(obs, dict) else 5
        return await run_search(episodes=episodes, target=target)
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,MATSAgent
survived,"    def test_package_exports(self) -> None:
        mod = importlib.import_module(""alpha_factory_v1.demos.meta_agentic_tree_search_v0"")
        self.assertTrue(hasattr(mod, ""run_demo""))
        self.assertTrue(hasattr(mod, ""mats""))
        self.assertTrue(hasattr(mod, ""openai_agents_bridge""))
",tests/test_meta_agentic_tree_search_import.py,TestMetaAgenticTreeSearchImport
survived,"def main() -> None:
    print_disclaimer()
    banner(""Alpha-Factory Setup Wizard"", ""YELLOW"")
    ok = True
    ok &= check_python()
    ok &= check_cmd(""git"")
    ok &= check_cmd(""docker"")
    ok &= check_node()

    if not ok:
        banner(""Some dependencies are missing"", ""RED"")
    else:
        banner(""Environment looks good"", ""GREEN"")

    repo_root = Path(__file__).resolve().parents[1]
    while True:
        print()
        print(""Select an option:"")
        print(""1) Run check_env.py --auto-install"")
        print(""2) Run ./codex/setup.sh"")
        print(""3) Start Insight demo with ./quickstart.sh"")
        print(""4) Start Insight demo in Docker (docker compose up)"")
        print(""5) Exit"")
        choice = input(""Enter choice: "").strip()
        if choice == ""1"":
            run([sys.executable, str(repo_root / ""check_env.py""), ""--auto-install""])
        elif choice == ""2"":
            run([str(repo_root / ""codex"" / ""setup.sh"")])
        elif choice == ""3"":
            run([str(repo_root / ""quickstart.sh"")])
        elif choice == ""4"":
            run([""docker"", ""compose"", ""up""])
        elif choice == ""5"":
            break
        else:
            print(""Invalid choice"")
",scripts/setup_wizard.py,
survived,"def check_python() -> bool:
    if sys.version_info < MIN_PY or sys.version_info >= MAX_PY:
        banner(
            f""Python {MIN_PY[0]}.{MIN_PY[1]}+ and <{MAX_PY[0]}.{MAX_PY[1]} required"",
            ""RED"",
        )
        return False
    banner(f""Python {sys.version.split()[0]} detected"", ""GREEN"")
    return True
",scripts/setup_wizard.py,
survived,"    def test_kafka_publish(self) -> None:
        events: list[object] = []

        class Prod:
            def __init__(self, bootstrap_servers: str) -> None:
                events.append(bootstrap_servers)

            async def start(self) -> None:
                events.append(""start"")

            async def send_and_wait(self, topic: str, data: bytes) -> None:
                events.append((topic, data))

            async def stop(self) -> None:
                events.append(""stop"")

        cfg = config.Settings(bus_port=0, broker_url=""k:1"")
        with mock.patch.object(messaging, ""AIOKafkaProducer"", Prod):
            bus = messaging.A2ABus(cfg)
            asyncio.run(bus.start())
            env = types.SimpleNamespace(sender=""a"", recipient=""b"", payload={}, ts=0.0)

            async def _send() -> None:
                bus.publish(""b"", env)
                await asyncio.sleep(0)

            asyncio.run(_send())
            asyncio.run(bus.stop())

        self.assertEqual(events[0:2], [""k:1"", ""start""])
        self.assertIn(""stop"", events)
        sent = [e for e in events if isinstance(e, tuple)][0]
        self.assertEqual(sent[0], ""b"")",tests/test_message_bus.py,TestMessageBus
survived,"def simulate(
    horizon: int,
    curve: str,
    seed: int | None,
    offline: bool,
    pop_size: int,
    generations: int,
    export: str | None,
    verbose: bool,
) -> None:
    """"""Run the forecast simulation and start the orchestrator.""""""
    if seed is not None:
        random.seed(seed)

    settings = config.Settings()
    if offline:
        settings.offline = True

    orch = orchestrator.Orchestrator(settings)
    secs = [sector.Sector(f""s{i:02d}"") for i in range(pop_size)]
    results = forecast.simulate_years(secs, horizon)

    if export == ""json"":
        data = [
            {
                ""year"": r.year,
                ""capability"": r.capability,
                ""affected"": [s.name for s in r.affected],
            }
            for r in results
        ]
        click.echo(json.dumps(data))
    elif export == ""csv"":
        lines = [""year,capability,affected""]
        for r in results:
            lines.append(f""{r.year},{r.capability},{'|'.join(s.name for s in r.affected)}"")
        click.echo(""\n"".join(lines))
    else:
        for r in results:
            click.echo(f""{r.year}: {r.capability:.2f} â†’ {[s.name for s in r.affected]}"")

    if verbose:
        click.echo(""Starting orchestrator â€¦ press Ctrl+C to stop"")

    try:
        asyncio.run(orch.run_forever())
    except KeyboardInterrupt:  # pragma: no cover - interactive
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def test_root_disclaimer_plain(client: TestClient) -> None:
    """"""Plain text disclaimer is returned by default.""""""

    r = client.get(""/"")
    assert r.status_code == 200
    assert r.text.strip() == DISCLAIMER
",tests/test_insight_api_server.py,
survived,"    def test_selects_supply_chain_bottleneck(self) -> None:
        signals = {
            ""yield_curve"": ""yield curve normal"",
            ""supply_chain"": ""flows 12m usd â€“ POTENTIAL BOTTLENECK"",
        }
        self.assertEqual(alpha_report.best_alpha(signals), signals[""supply_chain""])
",tests/test_alpha_report.py,TestBestAlpha
survived,"    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""gradio"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)
",tests/test_agent_muzero_entrypoint.py,
survived,"def test_sync_parallel_tools_or(client):
    client = instructor.from_anthropic(
        client, mode=instructor.Mode.ANTHROPIC_PARALLEL_TOOLS
    )
    resp = client.chat.completions.create(
        model=""claude-3-5-haiku-latest"",
        messages=[
            {""role"": ""system"", ""content"": ""You must always use tools""},
            {
                ""role"": ""user"",
                ""content"": ""What is the weather in toronto and dallas and who won the super bowl?"",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3
",tests/llm/test_anthropic/test_parallel.py,
survived,"def test_get_system_info_returns_info() -> None:
    info = metrics.get_system_info()
    assert isinstance(info, dict)
    assert ""platform"" in info
",tests/inference/unit_tests/core/managers/test_metrics.py,
survived,"def clean_text(text: str) -> str:
    """"""Return *text* with markdown emphasis and HTML tags stripped.""""""
    # Unescape HTML entities first
    text = html.unescape(text)
    # Drop HTML tags
    text = re.sub(r""<[^>]+>"", """", text)
    # Remove emphasis markers such as *, _, ** and backticks
    text = re.sub(r""\*\*|__|[*_`]"", """", text)
    return text.strip()
",scripts/generate_gallery_html.py,
survived,"    def eval_fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2
",src/interface/api_server.py,
survived,"def _run_simulation(horizon: int, pop_size: int, generations: int) -> None:
    """"""Execute the simulation and update charts live.""""""
    if st is None:
        print(""Streamlit not installed"")
        return

    st.session_state.logs = []
    secs = [sector.Sector(f""s{i:02d}"") for i in range(pop_size)]
    capability_chart = st.line_chart()
    sector_chart = st.bar_chart()
    pareto_area = st.empty()
    log_box = st.empty()

    results = forecast.simulate_years(secs, horizon)
    for res in results:
        capability_chart.add_rows({""capability"": [res.capability]})
        sector_chart.add_rows({""affected"": [len(res.affected)]})
        st.session_state.logs.append(f""Year {res.year}: {len(res.affected)} affected"")
        log_box.text(""\n"".join(st.session_state.logs[-20:]))
        time.sleep(0.1)

    pop = [mats.Individual([0.0, 0.0]) for _ in range(pop_size)]

    def eval_fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    for _ in range(generations):
        pop = mats.nsga2_step(pop, eval_fn, mu=pop_size)
        front = [(ind.genome[0], ind.genome[1]) for ind in pop if ind.rank == 0]
        pareto_area.line_chart({""x"": [x for x, _ in front], ""y"": [y for _, y in front]})
        time.sleep(0.1)
",src/interface/web_app.py,
survived,"def test_simulate_years_length() -> None:
    secs = [sector.Sector(""a"")]
    results = forecast.simulate_years(secs, 3)
    assert [r.year for r in results] == [1, 2, 3]
",tests/test_forecast.py,
survived,"def test_gibbs_free_energy() -> None:
    logp = [math.log(0.7), math.log(0.3)]
    value = gibbs.free_energy(logp, temperature=1.0, task_cost=1.0)
    entropy = -sum(p * math.log(p) for p in [0.7, 0.3])
    assert value == pytest.approx(1.0 - entropy)",tests/test_forecast.py,
survived,"    def __exit__(self, *args):
        self.append(self._stringio.getvalue())
        del self._stringio  # free up some memory
        sys.stdout = self._stdout
",scripts/utils/lcb_runner.py,Capturing
survived,"def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + ""...(truncated) ..."" + s[-length // 2 :]
",scripts/utils/lcb_runner.py,
survived,"    def __init__(self, inputs: str):
        self.inputs = inputs.encode(""utf-8"")  # Convert to bytes
",scripts/utils/lcb_runner.py,MockBuffer
survived,"    def all(self) -> List[Agent]:
        with sqlite3.connect(self.path) as cx:
            rows = list(cx.execute(""SELECT id, meta, score FROM agents ORDER BY id""))
        return [Agent(id=r[0], meta=json.loads(r[1]), score=float(r[2])) for r in rows]
",src/archive.py,Archive
survived,"def test_str_replace_not_found(tmp_path: Path) -> None:
    p = tmp_path / ""f.txt""
    p.write_text(""hello world"")
    n = str_replace(p, ""foo"", ""bar"")
    assert n == 0
    assert p.read_text() == ""hello world""",tests/test_file_ops.py,
survived,"def view(path: str | Path, start: int = 0, end: int | None = None) -> str:
    """"""Return a slice of lines from ``path``.

    Parameters
    ----------
    path:
        File to read.
    start:
        Zero-based start line. Negative values count from the end.
    end:
        Exclusive end line. ``None`` reads to EOF.
    """"""
    lines = Path(path).read_text(encoding=""utf-8"", errors=""replace"").splitlines()
    sliced = lines[start:end] if end is not None else lines[start:]
    return ""\n"".join(sliced)
",src/utils/file_ops.py,
survived,"def _replace_tool(ctx: RunContextWrapper | dict, path: str, pattern: str, repl: str) -> int:
    return replace(path, pattern, repl)
",src/self_edit/tools.py,
survived,"    def edit_task(self, *, path: str, start: int, end: Optional[int] = None, new_code: str) -> dict[str, bool]:
        edit(path, start, end, new_code)
        return {""ok"": True}
",src/self_edit/tools.py,FileToolsADK
survived,"def test_outside_repo_forbidden(tmp_path: Path) -> None:
    p = tmp_path / ""x.txt""
    p.write_text(""hi"")
    with pytest.raises(PermissionError):
        edit(p, 0, 1, ""bye"")
    with pytest.raises(PermissionError):
        replace(p, ""hi"", ""ho"")",tests/test_self_edit_tools.py,
survived,"async def evolve(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], tuple[float, float]],
    archive: InMemoryArchive,
    *,
    max_cost: float | None = None,
    wallclock: float | None = None,
) -> None:
    """"""Run an asynchronous evolution loop until the budget is exhausted.""""""

    if not archive.all():
        # seed with a random candidate
        await archive.accept(Candidate(genome=0.0, fitness=0.0, novelty=1.0, cost=0.0))

    spent = 0.0
    start = time.time()

    while True:
        if max_cost is not None and spent >= max_cost:
            break
        if wallclock is not None and time.time() - start >= wallclock:
            break

        parent = select_parent(archive.all(), temp=1.0)
        genome = operator(parent.genome)
        fitness, cost = await evaluate(genome)
        child = Candidate(genome=genome, fitness=fitness, novelty=random.random(), cost=cost)
        await archive.accept(child)
        spent += cost
",src/evolve.py,
survived,"    def _read_csv(self, task: Dict, path: str) -> pd.DataFrame:
        csv_str = self.preload_task_data(task, path)
        return pd.read_csv(io.StringIO(csv_str))
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"def convert_species_traits(subrace_path, out_path, parent_slug, start_pk):
    subrace = load_json(subrace_path)[0]
    fields = subrace[""fields""]
    traits = []
    pk = start_pk
    asi = fields.get(""asi_desc"")
    if asi:
        traits.append({
            ""model"": ""api_v2.speciestrait"",
            ""pk"": pk,
            ""fields"": {""name"": ""Ability Score Increase"", ""desc"": asi, ""type"": None, ""parent"": parent_slug},
        })
        pk += 1
    text = fields.get(""traits"", """")
    for part in filter(None, [p.strip() for p in text.split(""\n\n"")]):
        m = re.match(r""\*\*[_*]?([^.*]+)[.*_]*\*\*\s*(.*)"", part)
        if m:
            name = m.group(1).strip().rstrip('.')
            desc = m.group(2).strip()
        else:
            name, _, desc = part.partition('.')
            name = name.strip()
            desc = desc.strip()
        traits.append({
            ""model"": ""api_v2.speciestrait"",
            ""pk"": pk,
            ""fields"": {""name"": name, ""desc"": desc, ""type"": None, ""parent"": parent_slug},
        })
        pk += 1
    append_json(traits, out_path)
    return pk
",convert_missing.py,
survived,"def extract_level(desc):
    match = re.search(r""(\d+)(?:st|nd|rd|th) level"", desc, re.IGNORECASE)
    if match:
        try:
            return int(match.group(1))
        except ValueError:
            return None
    return None
",convert_missing.py,
survived,"        def __init__(self) -> None:
            self.instructions: list[Any] = []
",tests/test_ledger.py,DummyTx
survived,"def test_compute_merkle_root_matches_manual() -> None:
    tmp = tempfile.TemporaryDirectory()
    ledger = Ledger(os.path.join(tmp.name, ""l.db""), broadcast=False)

    envs = [
        messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0),
        messaging.Envelope(""b"", ""c"", {""v"": 2}, 1.0),
        messaging.Envelope(""c"", ""d"", {""v"": 3}, 2.0),
    ]
    for env in envs:
        ledger.log(env)

    computed = ledger.compute_merkle_root()

    hashes = []
    for env in envs:
        data = json.dumps(asdict(env), sort_keys=True).encode()
        hashes.append(insight_logging.blake3(data).hexdigest())  # type: ignore[attr-defined]

    manual = insight_logging._merkle_root(hashes)
    assert computed == manual
    tmp.cleanup()
",tests/test_ledger.py,
survived,"    def test_yaml_dict_names(self):
        labelmap = load_labelmap(""tests/annotations/dict_names.yaml"")
        self.assertEqual(labelmap, {0: ""cat"", 1: ""dog"", 2: ""fish""})",tests/util/test_image_utils.py,TestLoadLabelmap
survived,"        def parse(*args, **kwargs):
            class Msg:
                parsed = search.ImageAnswer(answer=""ok"")
            class Choice:
                message = Msg()
            class Completion:
                choices = [Choice()]
            return Completion()
",no-ocr-api/tests/test_utils.py,FakeCompletions
survived,"def test_call_vllm_parses_response(env_setup, monkeypatch):
    from importlib import reload

    import np_ocr.search as search
    reload(search)

    class FakeCompletions:
        @staticmethod
        def parse(*args, **kwargs):
            class Msg:
                parsed = search.ImageAnswer(answer=""ok"")
            class Choice:
                message = Msg()
            class Completion:
                choices = [Choice()]
            return Completion()

    class FakeOpenAI:
        def __init__(self, base_url=None, api_key=None):
            pass
        class Beta:
            class Chat:
                completions = FakeCompletions()
            chat = Chat()
        beta = Beta()

    monkeypatch.setattr(search, ""OpenAI"", FakeOpenAI)
    img = Image.new(""RGB"", (10, 10))
    result = search.call_vllm(img, ""hi"", base_url=""http://x"", api_key=""y"", model=""m"")
    assert result.answer == ""ok""
",no-ocr-api/tests/test_utils.py,
survived,"    def model_dump(self) -> Dict[str, Any]:
        ...
",alpha_factory_v1/utils/config_common.py,_TypedBaseSettings
survived,"def generate_docs() -> None:
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    for entry in sorted(DEMOS_DIR.iterdir()):
        if not entry.is_dir() or entry.name.startswith((""__"", ""."")):
            continue
        readme = entry / ""README.md""
        if not readme.is_file():
            continue
        page_content = build_page(entry)
        output = DOCS_DIR / f""{entry.name}.md""
        output.write_text(page_content, encoding=""utf-8"")
        print(f""Generated {output.relative_to(REPO_ROOT)}"")
",scripts/generate_demo_docs.py,
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/machine/x/python/python_math.py,
survived,"    def test_uses_pytest_when_available(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            target = Path(tmpdir)
            with mock.patch('importlib.util.find_spec', return_value=object()):
                with mock.patch('subprocess.call', return_value=0) as call:
                    with mock.patch.object(sys, 'argv', ['run_tests.py', str(target)]):
                        with self.assertRaises(SystemExit):
                            run_tests.main()
                    call.assert_called_once()
                    self.assertIn('pytest', call.call_args[0][0])
",alpha_factory_v1/tests/test_scripts_run_tests.py,RunTestsScriptTest
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def fetch_all(self) -> List[Dict[str, object]]:
        cur = self.conn.cursor()
        rows = cur.execute(
            ""SELECT timestamp, tokens, cost, latency, guardrail_hits FROM telemetry ORDER BY id""
        ).fetchall()
        return [
            {
                ""timestamp"": ts,
                ""tokens"": tokens,
                ""cost"": cost,
                ""latency"": latency,
                ""guardrail_hits"": hits,
            }
            for ts, tokens, cost, latency, hits in rows
        ]
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def archive(self, path: Optional[str] = None) -> str:
        """"""Export all telemetry records to a gzipped JSON file.""""""
        data = self.fetch_all()
        if path is None:
            name = datetime.utcnow().isoformat().replace("":"", """").replace(""."", """")
            path = f""telemetry_{name}.json.gz""
        with gzip.open(path, ""wt"", encoding=""utf-8"") as f:
            json.dump(data, f)
        return path
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def __init__(
        self, path: str | Path = ""telemetry.db"", retention_days: int = 30
    ) -> None:
        self.path = Path(path)
        self.retention_days = retention_days
        self.conn = sqlite3.connect(self.path)
        self._init_db()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def export_json(
        self,
        path: str | Path,
        *,
        start: datetime | str | None = None,
        end: datetime | str | None = None,
        metrics: Iterable[str] | None = None,
        compress: bool | None = None,
    ) -> str:
        """"""Export telemetry to a JSON file with optional compression.""""""
        compress = compress or str(path).endswith(("".gz"", "".gzip""))
        data = self.fetch_all(start=start, end=end, metrics=metrics)
        open_fn = gzip.open if compress else open
        mode = ""wt""
        with open_fn(path, mode, encoding=""utf-8"") as f:
            json.dump(data, f)
        return str(path)
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"    def test_top_tags_page(self):
        for i in range(1, 12):
            tag = Tag.objects.create(tag=f""tag{i}"")
            for j in range(i):
                entry = EntryFactory(title=f""Entry{i}-{j}"")
                entry.tags.add(tag)
        response = self.client.get(""/top-tags/"")
        assert response.status_code == 200
        tags_info = response.context[""tags_info""]
        self.assertEqual(len(tags_info), 10)
        self.assertEqual(tags_info[0][""tag""].tag, ""tag11"")
        self.assertFalse(any(info[""tag""].tag == ""tag1"" for info in tags_info))
        latest = Tag.objects.get(tag=""tag11"").entry_set.order_by(""-created"")[0].title
        self.assertContains(response, latest)",blog/tests.py,BlogTests
survived,"def verify_wheel(path: Path) -> bool:
    """"""Return ``True`` if *path* has a valid signature.""""""
    sig_path = path.with_suffix(path.suffix + "".sig"")
    if not sig_path.is_file():
        logger.error(""Missing .sig file for %s"", path.name)
        return False
    if ed25519 is None:
        logger.error(""cryptography library required for signature checks"")
        return False
    try:
        sig_b64 = sig_path.read_text().strip()
        expected = _WHEEL_SIGS.get(path.name)
        if expected and expected != sig_b64:
            logger.error(""Signature mismatch for %s"", path.name)
            return False
        pub_bytes = base64.b64decode(_WHEEL_PUBKEY)
        signature = base64.b64decode(sig_b64)
        ed25519.Ed25519PublicKey.from_public_bytes(pub_bytes).verify(
            signature, path.read_bytes()
        )
        return True
    except InvalidSignature:
        logger.error(""Invalid signature for %s"", path.name)
    except Exception:  # noqa: BLE001
        logger.exception(""Signature verification failed for %s"", path.name)
    return False
",alpha_factory_v1/backend/agents/plugins.py,
survived,"def _health_loop() -> None:
    while True:
        try:
            name, latency_ms, ok = _HEALTH_Q.get(timeout=_HEARTBEAT_INT)
        except Empty:
            continue

        quarantine = False
        stub_meta: AgentMetadata | None = None
        with _REGISTRY_LOCK:
            meta = AGENT_REGISTRY.get(name)
            if meta and not ok:
                if Counter:
                    _err_counter.labels(agent=name).inc()  # type: ignore[attr-defined]
                object.__setattr__(meta, ""err_count"", meta.err_count + 1)
                if meta.err_count >= _ERR_THRESHOLD:  # type: ignore[name-defined]
                    logger.error(
                        ""\N{NO ENTRY SIGN} Quarantining agent '%s' after %d consecutive errors"",
                        name,
                        meta.err_count,
                    )
                    stub_meta = AgentMetadata(
                        name=meta.name,
                        cls=StubAgent,
                        version=meta.version + ""+stub"",
                        capabilities=meta.capabilities,
                        compliance_tags=meta.compliance_tags,
                    )
                    quarantine = True

        if quarantine and stub_meta:
            _register(stub_meta, overwrite=True)

        logger.debug(
            ""heartbeat: %s ok=%s latency=%.1fms"",
            name,
            ok,
            latency_ms,
        )
        _emit_kafka(  # type: ignore[name-defined]
            ""agent.heartbeat"",
            json.dumps({""name"": name, ""latency_ms"": latency_ms, ""ok"": ok, ""ts"": time.time()}),
        )
",alpha_factory_v1/backend/agents/health.py,
survived,"def _inspect_module(mod: ModuleType) -> Optional[AgentMetadata]:
    """"""Return metadata for an agent implementation.""""""
    AgentBase = _agent_base()
    for _, obj in inspect.getmembers(mod, inspect.isclass):
        if issubclass(obj, AgentBase) and obj is not AgentBase:
            return AgentMetadata(
                name=getattr(obj, ""NAME"", obj.__name__),
                cls=obj,
                version=getattr(obj, ""__version__"", ""0.1.0""),
                capabilities=list(getattr(obj, ""CAPABILITIES"", [])),
                compliance_tags=list(getattr(obj, ""COMPLIANCE_TAGS"", [])),
                requires_api_key=getattr(obj, ""REQUIRES_API_KEY"", False),
            )
    return None
",alpha_factory_v1/backend/agents/discovery.py,
survived,"def show_memory(limit: int, export: str | None) -> None:
    """"""Display stored memory entries.""""""
    path = config.CFG.memory_path
    if not path:
        click.echo(""Memory persistence not enabled"")
        return
    mem_file = Path(path)
    if not mem_file.exists():
        click.echo(""No memory entries"")
        return
    entries = []
    for line in mem_file.read_text(encoding=""utf-8"").splitlines():
        if not line:
            continue
        try:
            entries.append(json.loads(line))
        except Exception:  # noqa: BLE001 - ignore bad records
            entries.append({""raw"": line})
    if not entries:
        click.echo(""No memory entries"")
        return
    entries = entries[-limit:]
    if export == ""json"":
        click.echo(json.dumps(entries))
    elif export == ""csv"":
        lines = [""payload""]
        for e in entries:
            lines.append(json.dumps(e).replace("","", "";""))
        click.echo(""\n"".join(lines))
    else:
        _rich_table([""payload""], [(json.dumps(e),) for e in entries])
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def load_sector_equity_map(path: str | Path = _MAP_PATH) -> Dict[str, list[str]]:
    """"""Return the sector-to-equity mapping from ``path``.""""""

    mapping: Dict[str, list[str]] = {}
    with Path(path).open(newline="""", encoding=""utf-8"") as fh:
        reader = csv.DictReader(fh)
        for row in reader:
            sector = (row.get(""sector"") or """").strip()
            ticker = (row.get(""ticker"") or """").strip()
            if not sector or not ticker:
                continue
            mapping.setdefault(sector, []).append(ticker)
    return mapping
",src/finance/adapter.py,
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex
survived,"def _rmse(a: Iterable[float], b: Iterable[float]) -> float:
    a_list = [float(x) for x in a]
    b_list = [float(y) for y in b]
    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a_list, b_list)) / len(a_list))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evaluate_econ.py,
survived,"def is_proprietary_content(text: str) -> bool:
    tokens = tokenize(text)
    count = 0
    for tok in tokens:
        if tok in PROPRIETARY_CORPUS:
            count += 1
            if count >= TOKEN_LIMIT:
                return True
        else:
            if 0 < count < TOKEN_LIMIT and classify_with_llm("" "".join(tokens)):
                return True
            count = 0
    if 0 < count < TOKEN_LIMIT and classify_with_llm(text):
        return True
    return False
",scripts/dp_scrubber.py,
survived,"def staged_files() -> Iterable[Path]:
    result = subprocess.run(
        [""git"", ""diff"", ""--cached"", ""--name-only""], capture_output=True, text=True
    )
    if result.returncode != 0:
        raise RuntimeError(result.stderr)
    for line in result.stdout.splitlines():
        p = Path(line)
        if p.is_file():
            yield p
",scripts/dp_scrubber.py,
survived,"    async def serve(self) -> None:
        """"""Run the scheduler until quotas are exhausted or queue is empty.""""""
        self.start_time = time.time()
        await self.app.serve()
        # wait for running tasks to finish
        if self.running:
            await asyncio.gather(*self.running, return_exceptions=True)
",src/scheduler/__init__.py,SelfImprovementScheduler
survived,"    def _finalize_first_round(self) -> None:
        deltas = list(self._results.values())
        if not deltas:
            self._first_round_done = True
            return
        threshold = sorted(deltas)[len(deltas) // 4]
        for job, delta in self._results.items():
            if delta > threshold:
                self._active_jobs.append(job)
                self._stats[job] = (1 if delta > 0 else 0, 0 if delta > 0 else 1)
        self._first_round_done = True
",src/scheduler/__init__.py,SelfImprovementScheduler
survived,"    async def __aenter__(self) -> ""ArchiveService"":
        self.start_merkle_task()
        return self
",src/archive/service.py,ArchiveService
survived,"    def __init__(
        self,
        path: str | Path = _DEFAULT_DB,
        *,
        rpc_url: str | None = None,
        wallet: str | None = None,
        broadcast: bool = True,
    ) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(self.path))
        self.conn.execute(
            """"""
            CREATE TABLE IF NOT EXISTS entries(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                parent TEXT,
                spec TEXT,
                scores TEXT,
                hash TEXT,
                ts REAL
            )
            """"""
        )
        self.conn.commit()
        self.rpc_url = rpc_url
        self.wallet = wallet
        self.broadcast = broadcast
        self._task: asyncio.Task[None] | None = None
",src/archive/service.py,ArchiveService
survived,"def propose_diff(repo_path: str, spec: str) -> str:
    """"""Return a git diff implementing ``spec`` inside ``repo_path``.""""""
    rel, goal = _parse_spec(spec)
    file_path = str(Path(repo_path) / rel)
    if _offline():
        return _fallback_diff(file_path, goal)
    prompt = (
        ""Generate a unified git diff for the repository at '{repo}'.\n""
        ""Apply the following change: {spec}"".format(repo=repo_path, spec=spec)
    )
    try:
        diff = _sync_chat(prompt)
        if not diff.endswith(""\n""):
            diff += ""\n""
        return diff
    except Exception:
        return _fallback_diff(file_path, goal)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"def test_rest_scoring() -> None:
    service = DualCriticService([""Paris is the capital of France.""])
    client = TestClient(create_app(service))
    ok = client.post(
        ""/critique"",
        json={""context"": ""Paris is the capital of France."", ""response"": ""Paris is the capital of France.""},
    )
    assert ok.status_code == 200
    data = ok.json()
    assert data[""logic""] > 0.5
    assert data[""feas""] > 0.0

    bad = client.post(
        ""/critique"",
        json={""context"": ""Paris is the capital of France."", ""response"": ""Berlin is the capital.""},
    )
    assert bad.status_code == 200
    assert bad.json()[""logic""] < 0.5
",tests/test_critics.py,
survived,"    async def stop_grpc(self) -> None:
        if self._server:
            await self._server.stop(0)
            self._server = None
",src/critics/dual_critic_service.py,DualCriticService
survived,"def load_metrics(csv_path: str | Path = ""replay_metrics.csv"") -> List[Dict[str, float]]:
    """"""Return metric rows from ``csv_path``.""""""
    path = Path(csv_path)
    if not path.exists():
        return []
    with path.open(newline="""", encoding=""utf-8"") as fh:
        reader = csv.DictReader(fh)
        data = []
        for row in reader:
            entry = {k: float(row[k]) for k in _METRICS if k in row}
            entry[""scenario""] = row.get(""scenario"", """")
            data.append(entry)
        return data
",src/analysis/meta_foresight.py,
survived,"    def tearDown(self):
        mv._embed = self._orig_embed
",alpha_factory_v1/tests/test_vector_memory.py,VectorMemoryTest
survived,"    def test_parse_defaults(self):
        args = _parse_with([])
        self.assertFalse(args.dev)
        self.assertFalse(args.preflight)
        self.assertIsNone(args.port)
        self.assertIsNone(args.metrics_port)
        self.assertIsNone(args.a2a_port)
        self.assertIsNone(args.enabled)
        self.assertEqual(args.loglevel, 'INFO')
",alpha_factory_v1/tests/test_cli.py,CliParseTest
survived,"def get_version() -> str:
    """"""Return the Alphaâ€‘Factory package version.""""""

    return __version__
",alpha_factory_v1/__init__.py,
survived,"    def test_discover_alpha_negative_num(self) -> None:
        with self.assertRaises(ValueError):
            stub.discover_alpha(num=-1, ledger=None)
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"        def __init__(self, host: str) -> None:  # pragma: no cover - init only
            captured[""adk_host""] = host
",tests/test_alpha_agi_business_3_v1.py,DummyADK
survived,"            def __init__(self, val: str) -> None:
                pass
",tests/test_insight_orchestrator_features.py,TestLedger.DummyPk
survived,"            async def close(self) -> None:
                pass
",tests/test_insight_orchestrator_features.py,TestLedger.DummyClient
survived,"def _start_server_with_retry(
    port: int, env: dict[str, str] | None = None, *, attempts: int = 3
) -> subprocess.Popen[bytes]:
    url = f""http://127.0.0.1:{port}""
    last_err: AssertionError | None = None
    for _ in range(attempts):
        proc = _start_server(port, env)
        try:
            _wait_ready(proc, url)
            return proc
        except AssertionError as err:
            last_err = err
            proc.terminate()
            proc.wait(timeout=5)
    assert last_err is not None
    raise last_err
",tests/test_metrics.py,
survived,"def test_generate_pkce_pair_invalid():
    st.session_state.clear()
    with pytest.raises(Exception):
        _generate_pkce_pair(""plain"", key=""x"")",tests/test_internal.py,
survived,"    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.wq.weight)
        nn.init.xavier_uniform_(self.wk.weight)
        nn.init.xavier_uniform_(self.wv.weight)
        nn.init.xavier_uniform_(self.dense.weight)
        if self.wq.bias is not None:
            nn.init.zeros_(self.wq.bias)
        if self.wk.bias is not None:
            nn.init.zeros_(self.wk.bias)
        if self.wv.bias is not None:
            nn.init.zeros_(self.wv.bias)
        if self.dense.bias is not None:
            nn.init.zeros_(self.dense.bias)
",src/model/u2tokenizer/rope.py,RotaryMultiheadAttention
survived,"def _innovation_gain(
    pop_size: int = 6,
    generations: int = 1,
    *,
    seed: int | None = None,
    mut_rate: float = 0.1,
    xover_rate: float = 0.5,
) -> float:
    """"""Return a small gain from a short MATS run.

    Args:
        pop_size: Number of individuals in the MATS population.
        generations: Number of evolution steps.
        seed: Optional RNG seed for deterministic output.
        mut_rate: Probability of mutating a gene.
        xover_rate: Probability of performing crossover.
    """"""

    def fn(genome: list[float]) -> tuple[float, float, float, float]:
        x, y = genome
        effectiveness = x**2
        negative_evar = y**2
        complexity = (x + y) ** 2
        history = [1.0, 1.0, 1.0]
        base = lead_time._arima_baseline(history, 3)
        forecast_series = [b + x + y for b in base]
        lead_impr = lead_time.lead_signal_improvement(history, forecast_series, months=3, threshold=1.1)
        lead_penalty = 1.0 - lead_impr
        return effectiveness, negative_evar, complexity, lead_penalty

    pop = mats.run_evolution(
        fn,
        2,
        population_size=pop_size,
        mutation_rate=mut_rate,
        crossover_rate=xover_rate,
        generations=generations,
        seed=seed,
    )
    m = len(pop[0].fitness or ())
    best = min(pop, key=lambda ind: sum(ind.fitness or (0.0,) * m))
    return 0.1 / (1.0 + sum(best.fitness or (0.0,) * m))
",alpha_factory_v1/core/simulation/forecast.py,
survived,"    def _get_metric(cls: Any, name: str, desc: str, labels: list[str]) -> Any:
        return _reg_metric(cls, name, desc, labels)
",alpha_factory_v1/core/utils/tracing.py,
survived,"    async def _on_envelope(self, env: messaging.Envelope) -> None:
        await self.handle(env)
",alpha_factory_v1/core/agents/base_agent.py,BaseAgent
survived,"def capability_growth(
    t: float,
    curve: str = ""logistic"",
    *,
    k: float | None = None,
    x0: float | None = None,
) -> float:
    """"""Dispatch to the configured growth curve.""""""

    if curve == ""linear"":
        return linear_curve(t)
    if curve == ""exponential"":
        return exponential_curve(t, k=k or 3.0, x0=x0 or 0.0)
    return logistic_curve(t, k=k or 10.0, x0=x0 or 0.0)
",alpha_factory_v1/core/simulation/forecast.py,
survived,"    async def evolve(
        self,
        scenario_hash: str,
        fn: Callable[[list[float]], tuple[float, ...]],
        genome_length: int,
        sector: str = ""generic"",
        approach: str = ""ga"",
        experiment_id: str = ""default"",
        **kwargs: object,
    ) -> mats.Population:
        """"""Run evolution for ``scenario_hash`` keyed by ``experiment_id``.""""""

        pops = self.experiment_pops.setdefault(experiment_id, {})
        if len(self.experiment_pops) > 10:
            raise RuntimeError(""max concurrent experiments exceeded"")

        pop = await asyncio.to_thread(
            mats.run_evolution,
            fn,
            genome_length,
            scenario_hash=scenario_hash,
            populations=pops,
            **cast(Any, kwargs),
        )
        pops[scenario_hash] = pop
        for ind in pop:
            self.solution_archive.add(
                sector,
                approach,
                ind.score,
                {""genome"": ind.genome},
            )
            self.archive.insert_entry(
                {""experiment_id"": experiment_id, ""genome"": ind.genome},
                {""score"": ind.score},
            )
        return pop
",alpha_factory_v1/core/orchestrator.py,Orchestrator
survived,"    def _record_restart(self, runner: AgentRunner) -> None:
        env = pb.Envelope(sender=""orch"", recipient=""system"", ts=time.time())
        env.payload.update({""event"": ""restart"", ""agent"": runner.agent.name})
        self.ledger.log(env)
        self.bus.publish(""system"", env)
",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator
survived,"    def add_agent(self, agent: object) -> None:
        runner = AgentRunner(agent)
        self.runners[agent.name] = runner
        self._register(runner)
",alpha_factory_v1/backend/demo_orchestrator.py,DemoOrchestrator
survived,"    def __init__(self) -> None:
        self.history = deque()
",tests/test_education_reward.py,DummyState
survived,"def test_duplicate_request_id_zero() -> None:
    _reset()
    res = {""request_id"": ""r3"", ""violation"": False}
    sc.reward(None, None, res)
    assert sc.reward(None, None, res) == 0.0",tests/test_safety_compliance_reward.py,
survived,"def test_unhandled_violation_penalty() -> None:
    _reset()
    res = {""request_id"": ""r2"", ""violation"": True, ""severity"": 10, ""autocorrected"": False}
    value = sc.reward(None, None, res)
    assert value <= -1.0
    assert value >= -2.0
",tests/test_safety_compliance_reward.py,
survived,"def test_promotion_threshold_and_burn() -> None:
    reg = StakeRegistry()
    reg.set_stake(""A"", 10)
    reg.set_stake(""B"", 90)
    reg.set_threshold(""promote:X"", 0.6)
    reg.vote(""promote:X"", ""A"", True)
    assert not reg.accepted(""promote:X"")
    reg.vote(""promote:X"", ""B"", True)
    assert reg.accepted(""promote:X"")
    before = reg.stakes[""A""]
    reg.archive_accept(""A"")
    assert reg.stakes[""A""] == pytest.approx(before * 0.99)",tests/test_stake_registry.py,
survived,"def _disruption_df(traj: list[Any]) -> ""pd.DataFrame"":
    """"""Return the first disruption year per sector.""""""
    import pandas as pd

    years: dict[str, int] = {}
    for point in traj:
        for sec in point.sectors:
            if sec.disrupted and sec.name not in years:
                years[sec.name] = point.year
    return pd.DataFrame({""sector"": list(years.keys()), ""year"": list(years.values())})
",src/interface/minimal_ui.py,
survived,"def test_dist_self_contained() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        attrs = page.eval_on_selector_all(
            ""script[src], link[href]"",
            ""els => els.map(e => e.getAttribute('src') || e.getAttribute('href'))"",
        )
        assert all("".."" not in a for a in attrs)
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_dist_self_contained.py,
survived,"def config_paths():
    return [p for p in CONFIG_DIR.rglob('*.json')]
",tests/test_configs.py,
survived,"def test_config_fields_present():
    for path in config_paths():
        cfg = load(path)
        assert 'contracts' in cfg and cfg['contracts'], f""{path} missing contracts""
        assert 'github_repo' in cfg
        assert REQUIRED_GITHUB_KEYS <= set(cfg['github_repo']), f""{path} github_repo keys""
        assert 'dependencies' in cfg
        assert 'explorer_hostname' in cfg or 'explorer_hostname_env_var' in cfg
",tests/test_configs.py,
survived,"def test_path_to_file_without_dependency():
    result = path_to_file_without_dependency('@oz/contracts/token.sol', '@oz/contracts')
    assert result == 'token.sol'
",tests/test_github_utils.py,
survived,"def test_resolve_dep():
    cfg = {
        'dependencies': {
            '@oz/contracts': {'url': 'u', 'commit': 'c', 'relative_root': ''}
        }
    }
    repo, dep_name = resolve_dep('@oz/contracts/token.sol', cfg)
    assert dep_name == '@oz/contracts'
    assert repo['commit'] == 'c'",tests/test_github_utils.py,
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/smart_contract_agent.py,SmartContractAgent
survived,"def logistic_curve(t: float, k: float = 1.0, x0: float = 0.0) -> float:
    return 1.0 / (1.0 + math.exp(-k * (t - x0)))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/forecast.py,
survived,"    async def _start() -> None:
        app.state.task = asyncio.create_task(orch.run_forever())  # type: ignore[attr-defined]
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    async def _handle_rpc(self, request: bytes, context) -> bytes:  # type: ignore[override]
        env = Envelope(**json.loads(request.decode()))
        self.publish(env.recipient, env)
        return b""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/messaging.py,A2ABus
survived,"    def __init__(self, bus, ledger) -> None:
        super().__init__(""safety"", bus, ledger)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/safety_agent.py,SafetyGuardianAgent
survived,"    def acquire(self, client_id: Optional[str] = None) -> GeminiClientWrapper:
        """"""Return a client by id or using round-robin.""""""
        if client_id:
            client = self._id_map.get(client_id)
            if not client:
                raise ValueError(f""Client id {client_id} not found"")
            return client

        client = self._round_robin[0]
        self._round_robin.rotate(-1)
        return client
",app/services/pool.py,GeminiClientPool
survived,"def pytest_configure(config: pytest.Config) -> None:
    config.addinivalue_line(
        ""markers"",
        ""requires_torch: mark test that depends on the torch package"",
    )
",tests/conftest.py,
survived,"def pytest_runtest_setup(item: pytest.Item) -> None:
    if ""requires_torch"" in item.keywords and not _HAS_TORCH:
        pytest.skip(""torch required"", allow_module_level=True)
",tests/conftest.py,
survived,"    def test_invalid_env_logs_warning(self) -> None:
        logging.disable(logging.NOTSET)
        with patch.dict(os.environ, {""FOO"": ""bar""}, clear=True):
            with self.assertLogs(""alpha_factory_v1.edge_runner"", level=""WARNING"") as cm:
                self.assertEqual(edge_runner._env_int(""FOO"", 5), 5)
        self.assertTrue(any(""Invalid FOO"" in msg for msg in cm.output))",tests/test_edge_runner.py,TestEnvIntWarning
survived,"    def get_random_action(self):
        '''
        get_random_action
        '''
        action = random.choice(list(self.cfg.color_code.values()))
        logger.warning(f""Perform random action: {action}"")
        return action
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def __init__(self, cfg):
        self.cfg = cfg
        self.frame = None
        self.lock = threading.Lock()
        self.is_terminated = False

        self.window_title = get_window_title(cfg[""game_window""][""title""])
        if self.window_title is None:
            logger.error(
                f""[GameWindowCapturor] Unable to find window titles that contain {cfg['game_window']['title']}""
            )
            return -1

        self.fps = 0
        self.fps_limit = cfg[""system""][""fps_limit_window_capturor""]
        self.t_last_run = 0.0

        # ä½¿ç”¨ mss ä¾†æ“·å–ç‰¹å®šèž¢å¹•å€åŸŸ
        self.capture = mss.mss()

        # Get game window region
        self.update_window_region()

        # start game window capture
        threading.Thread(target=self.start_capture, daemon=True).start()

        # Wait frame init
        time.sleep(0.1)
        while self.frame is None:
            self.limit_fps()
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"    def get_minimap_location(self):
        '''
        get_minimap_location
        '''
        loc_minimap, score, is_cached = find_pattern_sqdiff(
                                            self.img_frame, self.img_map)

        return loc_minimap
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def update_img_frame_debug(self):

        '''
        update_img_frame_debug
        '''
        cv2.imshow(""Game Window Debug"",
                   self.img_frame_debug[self.cfg.camera_ceiling:self.cfg.camera_floor, :])
        # Update FPS timer
        self.t_last_frame = time.time()
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def update_info_on_img_frame_debug(self):
        '''
        update_info_on_img_frame_debug
        '''
        # Print text at bottom left corner
        self.fps = round(1.0 / (time.time() - self.t_last_frame))
        text_y_interval = 23
        text_y_start = 550
        dt_screenshot = time.time() - self.kb.t_last_screenshot
        text_list = [
            f""FPS: {self.fps}"",
            f""Status: {self.status}"",
            f""Press 'F1' to {'pause' if self.kb.is_enable else 'start'} Bot"",
            f""Press 'F2' to save screenshot{' : Saved' if dt_screenshot < 0.7 else ''}""
        ]
        for idx, text in enumerate(text_list):
            cv2.putText(
                self.img_frame_debug, text,
                (10, text_y_start + text_y_interval*idx),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),
                2, cv2.LINE_AA
            )

        # Don't draw minimap in patrol mode
        if self.args.patrol:
            return

        # mini-map on debug image
        if self.cfg.is_use_minimap:
            # Compute crop region with boundary check
            crop_w, crop_h = 300, 300
            x0 = max(0, self.loc_player_minimap[0] - crop_w // 2)
            y0 = max(0, self.loc_player_minimap[1] - crop_h // 2)
            x1 = min(self.img_route_debug.shape[1], x0 + crop_w)
            y1 = min(self.img_route_debug.shape[0], y0 + crop_h)

            # Crop region
            mini_map_crop = self.img_route_debug[y0:y1, x0:x1]
            mini_map_crop = cv2.resize(mini_map_crop,
                                    (int(mini_map_crop.shape[1] // 1.5),
                                     int(mini_map_crop.shape[0] // 1.5)),
                                    interpolation=cv2.INTER_NEAREST)
            # Paste into top-right corner of self.img_frame_debug
            h_crop, w_crop = mini_map_crop.shape[:2]
            h_frame, w_frame = self.img_frame_debug.shape[:2]
            x_paste = w_frame - w_crop - 10  # 10px margin from right
            y_paste = 70
            self.img_frame_debug[y_paste:y_paste + h_crop, x_paste:x_paste + w_crop] = mini_map_crop

            # Draw border around minimap
            cv2.rectangle(
                self.img_frame_debug,
                (x_paste, y_paste),
                (x_paste + w_crop, y_paste + h_crop),
                color=(255, 255, 255),   # White border
                thickness=2
            )
        else:
            # Compute crop region with boundary check
            crop_w, crop_h = 400, 400
            x0 = max(0, self.loc_player_global[0] - crop_w // 2)
            y0 = max(0, self.loc_player_global[1] - crop_h // 2)
            x1 = min(self.img_route_debug.shape[1], x0 + crop_w)
            y1 = min(self.img_route_debug.shape[0], y0 + crop_h)

            # Crop region
            mini_map_crop = self.img_route_debug[y0:y1, x0:x1]
            mini_map_crop = cv2.resize(mini_map_crop,
                                    (mini_map_crop.shape[1] // 2, mini_map_crop.shape[0] // 2),
                                    interpolation=cv2.INTER_NEAREST)
            # Paste into top-right corner of self.img_frame_debug
            h_crop, w_crop = mini_map_crop.shape[:2]
            h_frame, w_frame = self.img_frame_debug.shape[:2]
            x_paste = w_frame - w_crop - 10  # 10px margin from right
            y_paste = 70
            self.img_frame_debug[y_paste:y_paste + h_crop, x_paste:x_paste + w_crop] = mini_map_crop

            # Draw border around minimap
            cv2.rectangle(
                self.img_frame_debug,
                (x_paste, y_paste),
                (x_paste + w_crop, y_paste + h_crop),
                color=(255, 255, 255),   # White border
                thickness=2
            )
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"def test_new_async_manager_merges_tags_with_config() -> None:
    config = {""callbacks"": None, ""tags"": [""a""]}
    manager = get_async_callback_manager_for_config(config, tags=[""b""])
    assert manager.inheritable_tags == [""a"", ""b""]",libs/langgraph/tests/test_config_async.py,
survived,"        def __init__(self, *a, llm=None, **_k) -> None:
            self.llm = llm
",tests/test_aiga_openai_bridge_offline.py,DummyEvolver
survived,"        def latest_log(self):
            return ""stub""
",tests/test_aiga_openai_bridge_offline.py,_DummyEvolver
survived,"        def dec(f):
            return f
",tests/test_aiga_openai_bridge_offline.py,
survived,"    def start_merkle_task(self, *_a, **_kw) -> None:  # pragma: no cover - test stub
        pass
",tests/test_alert_webhook.py,DummyLedger
survived,"        def llm(prompt: str) -> str:
            """"""Offline fallback using the local LLM.""""""
            return llm_client.call_local_model([{""role"": ""user"", ""content"": prompt}])
",alpha_factory_v1/demos/self_healing_repo/patcher_core.py,
survived,"    def test_stake_must_not_be_negative(self) -> None:
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=0.5, stake=-0.1)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"    def test_old_version_fails(self) -> None:
        fake_mod = types.SimpleNamespace(__version__=""0.0.13"")
        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
        ):
            self.assertFalse(preflight.check_openai_agents_version())
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion
survived,"        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return orig_import_module(name, *args, **kwargs)
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion
survived,"        def _parse(v: str) -> tuple[int, ...]:
            return tuple(int(p) for p in v.split(""."") if p.isdigit())
",alpha_factory_v1/scripts/preflight.py,
survived,"def check_openai_agents_version(min_version: str = ""0.0.14"") -> bool:
    """"""Verify ``openai_agents`` is new enough when installed.""""""
    import importlib

    spec = importlib.util.find_spec(""openai_agents"")
    if spec is None:  # not installed
        return True

    mod = importlib.import_module(""openai_agents"")
    version = getattr(mod, ""__version__"", ""0"")
    if _version_lt(version, min_version):
        banner(
            f""openai_agents {version} detected; >={min_version} required"",
            ""RED"",
        )
        return False
    banner(f""openai_agents {version} detected"", ""GREEN"")
    return True
",alpha_factory_v1/scripts/preflight.py,
survived,"    def _version_lt(a: str, b: str) -> bool:
        return Version(a) < Version(b)
",alpha_factory_v1/scripts/preflight.py,
survived,"def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Run a small GPT-2 generation demo"")
    parser.add_argument(""--prompt"", default=""Hello, world!"", help=""Input prompt"")
    parser.add_argument(""--max-length"", type=int, default=50, help=""Maximum output length"")
    args = parser.parse_args(argv)
    ensure_model()
    output = generate(args.prompt, args.max_length)
    print(output)
",alpha_factory_v1/demos/gpt2_small_cli/gpt2_cli.py,
survived,"def test_generate_mock(monkeypatch: pytest.MonkeyPatch) -> None:
    import alpha_factory_v1.demos.gpt2_small_cli.gpt2_cli as mod

    class FakeTokenizer:
        eos_token_id = 0

        def __init__(self, *args: object, **kwargs: object) -> None:
            pass

        def __call__(self, text: str, return_tensors: str = ""pt"") -> dict[str, list[int]]:
            return {""input_ids"": [0]}

        def decode(self, ids: list[int], skip_special_tokens: bool = True) -> str:
            return ""output""

    class FakeModel:
        @classmethod
        def from_pretrained(cls, name: str) -> ""FakeModel"":
            return cls()

        def generate(self, **kwargs: object) -> list[list[int]]:
            return [[0]]

    monkeypatch.setattr(""transformers.AutoTokenizer"", FakeTokenizer, raising=False)
    monkeypatch.setattr(""transformers.AutoModelForCausalLM"", FakeModel, raising=False)

    result = mod.generate(""hi"", 5)
    assert result == ""output""",tests/test_gpt2_cli_demo.py,
survived,"        def __call__(self, text: str, return_tensors: str = ""pt"") -> dict[str, list[int]]:
            return {""input_ids"": [0]}
",tests/test_gpt2_cli_demo.py,FakeTokenizer
survived,"        def from_pretrained(cls, name: str) -> ""FakeModel"":
            return cls()
",tests/test_gpt2_cli_demo.py,FakeModel
survived,"def test_max_results_eviction(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""SIM_RESULTS_DIR"", str(tmp_path))
    monkeypatch.setenv(""MAX_RESULTS"", ""2"")
    from src.interface import api_server

    api = importlib.reload(api_server)

    for i in range(3):
        res = api.ResultsResponse(id=f""id{i}"", forecast=[], population=None)
        api._save_result(res)

    assert len(api._simulations) == 2
    assert list(api._simulations.keys()) == [""id1"", ""id2""]
    assert not (tmp_path / ""id0.json"").exists()
",tests/test_max_results.py,
survived,"    def test_python_files_compile(self) -> None:
        pattern = os.path.join(validate_demos.DEFAULT_DIR, ""**"", ""*.py"")
        for py_file in glob.glob(pattern, recursive=True):
            with self.subTest(py_file=py_file):
                py_compile.compile(py_file, doraise=True)
",tests/test_demo_quality.py,TestDemoPythonCompile
survived,"def replace_str(path: str | Path, old: str, new: str) -> int:
    """"""Replace occurrences of ``old`` with ``new`` inside ``path``.""""""
    p = _safe_path(path)
    text = p.read_text(encoding=""utf-8"", errors=""replace"")
    count = text.count(old)
    if count:
        _record_history(p)
        p.write_text(text.replace(old, new), encoding=""utf-8"")
    return count
",src/self_edit/tools.py,
survived,"    def undo_task(self) -> dict[str, bool]:
        return {""ok"": undo_last_edit()}",src/self_edit/tools.py,FileToolsADK
survived,"    def _compute_root(self, cids: Iterable[str]) -> str:
        hashes = [hashlib.sha256(c.encode()).digest() for c in sorted(cids)]
        if not hashes:
            return """"
        while len(hashes) > 1:
            if len(hashes) % 2 == 1:
                hashes.append(hashes[-1])
            hashes = [hashlib.sha256(hashes[i] + hashes[i + 1]).digest() for i in range(0, len(hashes), 2)]
        return hashes[0].hex()
",src/archive/hash_archive.py,HashArchive
survived,"        def _model_dump(self: BaseModel, *args: Any, **kwargs: Any) -> Any:
            return self.dict(*args, **kwargs)
",src/meta_agent/__init__.py,
survived,"def volkswagen_mqb_meb_checksum(address: int, sig, d: bytearray) -> int:
  crc = 0xFF
  for i in range(1, len(d)):
    crc ^= d[i]
    crc = CRC8H2F[crc]
  counter = d[1] & 0x0F
  const = VOLKSWAGEN_MQB_MEB_CONSTANTS.get(address)
  if const:
    crc ^= const[counter]
    crc = CRC8H2F[crc]
  return crc ^ 0xFF
",opendbc/car/volkswagen/mqbcan.py,
survived,"def subaru_checksum(address: int, sig, d: bytearray) -> int:
  s = 0
  addr = address
  while addr:
    s += addr & 0xFF
    addr >>= 8
  for i in range(1, len(d)):
    s += d[i]
  return s & 0xFF",opendbc/car/subaru/subarucan.py,
survived,"def prefer_fast_model() -> ModelPreferences:
    """"""Model preferences optimized for speed and cost.""""""

    return ModelPreferences(
        hints=[ModelHint(name=""gpt-4o-mini""), ModelHint(name=""claude-3-haiku"")],
        costPriority=0.8,
        speedPriority=0.9,
        intelligencePriority=0.3,
    )
",src/enrichmcp/context.py,
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def load(self) -> None:
        if self.index_path.exists():
            try:
                with open(self.index_path, ""r"", encoding=""utf-8"") as f:
                    self._index = json.load(f)
            except (OSError, json.JSONDecodeError):  # pragma: no cover - corrupt file
                self._index = []
        else:
            self._index = []
",src/meta_agent/template_index.py,TemplateIndex
survived,"    def call(prompt: str, system_prompt: Optional[str]) -> str:
        llm = LLMProvider()
        return llm.chat(prompt, system_prompt=system_prompt)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_edit/prompting.py,
survived,"def test_self_improve_prompt_snapshot(monkeypatch):
    log = (FIXTURES / ""self_improve.txt"").read_text()

    def fake_llm(prompt: str, system: str | None) -> str:
        return f""patch-{random.random()}""

    monkeypatch.setattr(prompting, ""_get_llm"", lambda: fake_llm)
    out = prompting.self_improve(""Fix bug:\n{logs}"", log, seed=7)
    assert out == ""patch-0.32383276483316237""",tests/test_self_improve_prompting.py,
survived,"def main(argv: list[str] | None = None) -> None:
    ap = argparse.ArgumentParser(description=""Generate patch from logs"")
    ap.add_argument(""template"")
    ap.add_argument(""log_file"", type=argparse.FileType(""r""))
    ap.add_argument(""--seed"", type=int)
    args = ap.parse_args(argv)
    patch = self_improve(args.template, args.log_file.read(), seed=args.seed)
    print(patch)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_edit/prompting.py,
survived,"    def test_invalid_numeric_fallback(self) -> None:
        env = {
            ""DEV_MODE"": ""true"",
            ""PORT"": ""foo"",
            ""METRICS_PORT"": ""bar"",
            ""A2A_PORT"": ""baz"",
            ""ALPHA_CYCLE_SECONDS"": ""qux"",
            ""MAX_CYCLE_SEC"": ""zap"",
            ""ALPHA_MODEL_MAX_BYTES"": ""oops"",
        }
        with mock.patch.dict(os.environ, env, clear=True):
            orch = importlib.reload(
                importlib.import_module(""alpha_factory_v1.backend.orchestrator"")
            )
        self.assertEqual(orch.PORT, 8000)
        self.assertEqual(orch.METRICS_PORT, 0)
        self.assertEqual(orch.A2A_PORT, 0)
        self.assertEqual(orch.CYCLE_DEFAULT, 60)
        self.assertEqual(orch.MAX_CYCLE_SEC, 30)
        self.assertEqual(orch.MODEL_MAX_BYTES, 64 * 1024 * 1024)
",tests/test_orchestrator_env.py,TestOrchestratorEnv
survived,"    def __init__(self, execution_module: Optional[ExecutionModule] = None) -> None:
        self.execution_module = execution_module or ExecutionModule()
",src/meta_agent/evaluation/result_collection.py,ResultCollectionModule
survived,"def test_execute_and_collect_logs(monkeypatch, tmp_path, caplog):
    fake_exec = MagicMock()
    fake_exec.run_tests.return_value = ExecutionResult(0, ""out"", ""err"")
    module = ResultCollectionModule(fake_exec)
    with caplog.at_level(""INFO"", logger=""meta_agent.evaluation.result_collection""):
        module.execute_and_collect(tmp_path)
    assert any(
        ""Executing and collecting results"" in r.getMessage() for r in caplog.records
    )",tests/unit/test_result_collection_module.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"def test_invalid_command(monkeypatch, tmp_path):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with pytest.raises(ValueError):
        manager.run_code_in_sandbox(code_dir, [""python; rm -rf /""])
",tests/unit/test_sandbox_manager.py,
survived,"def pow2(n):
    p = 1
    i = 0
    while i < n:
        p = p * 2
        i = i + 1
    return p
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,
survived,"def H(data):
    if data == """":
        return 0.0
    counts = {}
    i = 0
    while i < len(data):
        ch = data[i:i + 1]
        if ch in counts:
            counts[ch] = counts[ch] + 1
        else:
            counts[ch] = 1
        i = i + 1
    entropy = 0.0
    l = float(len(data))
    for ch in counts:
        px = (float(counts[ch])) / l
        if px > 0.0:
            entropy = entropy - px * log2(px)
    return entropy
",tests/rosetta/transpiler/Python/entropy-1.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    str1 = """"
    str2 = "" ""
    check(str1)
    check(str2)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/empty-string-2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-infinite-length.py,
survived,"def log2(x):
    k = 0.0
    v = x
    while v >= 2.0:
        v = v / 2.0
        k = k + 1.0
    while v < 1.0:
        v = v * 2.0
        k = k - 1.0
    z = (v - 1.0) / (v + 1.0)
    zpow = z
    sum = z
    i = 3
    while i <= 9:
        zpow = zpow * z * z
        sum = sum + zpow / (float(i))
        i = i + 2
    ln2 = 0.6931471805599453
    return k + 2.0 * sum / ln2
",tests/rosetta/transpiler/Python/entropy-2.py,
survived,"def mul(a, b):
    return a * b
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    emirps = []
    n = 2
    while len(emirps) < 10000:
        if isPrime(n):
            r = revInt(n)
            if r != n and isPrime(r):
                emirps = emirps + [n]
        n = n + 1
    line = ""   [""
    i = 0
    while i < 20:
        line = line + str(emirps[i])
        if i < 19:
            line = line + "", ""
        i = i + 1
    line = line + ""]""
    print(""First 20:"")
    print(line)
    line = ""  [""
    for e in emirps:
        if e >= 8000:
            break
        if e >= 7700:
            line = line + str(e) + "", ""
    line = line + ""]""
    print(""Between 7700 and 8000:"")
    print(line)
    print(""10000th:"")
    print(""   ["" + str(emirps[9999]) + ""]"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/emirp-primes.py,
survived,"def isZero(p):
    return p.inf
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    fs = {}
    fs[""/tmp""] = []
    fs[""/var""] = [""log""]
    if isEmptyDir(fs, ""/tmp""):
        print(""/tmp is empty"")
    else:
        print(""/tmp is not empty"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/empty-directory.py,
survived,"def test_readme_contains_project_name():
    readme_path = pathlib.Path(__file__).resolve().parents[1] / 'README.md'
    content = readme_path.read_text(encoding='utf-8')
    assert 'Vision_UI' in content",tests/test_readme.py,
survived,"def test_simulate_flow_uvicorn(uvicorn_server: str) -> None:
    url = uvicorn_server
    headers = {""Authorization"": ""Bearer test-token""}
    with httpx.Client(base_url=url) as client:
        r = client.post(""/simulate"", json={""horizon"": 1, ""pop_size"": 2, ""generations"": 1}, headers=headers)
        assert r.status_code == 200
        sim_id = r.json()[""id""]
        assert isinstance(sim_id, str) and sim_id
        for _ in range(100):
            r = client.get(f""/results/{sim_id}"", headers=headers)
            if r.status_code == 200:
                data = r.json()
                break
            time.sleep(0.05)
        else:
            raise AssertionError(""Timed out waiting for results"")
        assert isinstance(data, dict)
        assert ""forecast"" in data
        r2 = client.get(""/results/does-not-exist"", headers=headers)
        assert r2.status_code == 404
",tests/test_api_server_uvicorn.py,
survived,"        def __init__(self, app: FastAPI, limit: int = 60, window: int = 60) -> None:
            super().__init__(app)
            self.limit = int(os.getenv(""API_RATE_LIMIT"", str(limit)))
            self.window = window
            self.counters: dict[str, tuple[int, float]] = {}
            self.lock = asyncio.Lock()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,SimpleRateLimiter
survived,"            def passwords_per_seconds(self, seconds):
                return 0
",btcrecover/test/test_passwords.py,TestOuterIterations.DummyWallet
survived,"    def fake_print_report(state, found_files, changed_files, total_cc_new, total_cc_original, total_expr, total_time):
        captured[""new""] = total_cc_new
        captured[""orig""] = total_cc_original
",test/integration/test_api.py,
survived,"def test_foresight_evaluate_nonzero_metrics() -> None:
    repo = Path(__file__).resolve().parents[1]
    scores = foresight_evaluate(repo)
    assert scores[""rmse""] > 0
    assert scores[""lead_time""] != 0",tests/test_demo_cli.py,
survived,"def main(argv: list[str] | None = None) -> None:  # pragma: no cover - CLI wrapper
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument(""--alpha"", default=""Generic opportunity"", help=""text description of the opportunity"")
    p.add_argument(""--ledger"", help=""path to ledger JSON file"")
    p.add_argument(""--model"", default=os.getenv(""ALPHA_CONVERSION_MODEL"", ""gpt-4o-mini""), help=""OpenAI model when API key present"")
    p.add_argument(""--no-log"", action=""store_true"", help=""do not write ledger file"")
    args = p.parse_args(argv)

    ledger = _ledger_path(args.ledger)
    plan = convert_alpha(args.alpha, ledger=None if args.no_log else ledger, model=args.model)
    if args.no_log:
        ledger.unlink(missing_ok=True)
    print(json.dumps(plan, indent=2))
    if not args.no_log:
        print(f""Logged to {ledger}"")
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py,
survived,"    def _estimate_size(self, value: Any) -> int:
        try:
            return len(pickle.dumps(value))
        except Exception:
            return sys.getsizeof(value)
",src/cachier/cores/base.py,_BaseCore
survived,"    def return_verified_password_or_false(self, mnemonic_ids_list):
        for count, mnemonic_ids in enumerate(mnemonic_ids_list, 1):
            if None not in mnemonic_ids and self._verify_checksum(mnemonic_ids):
                return mnemonic_ids, count
        return False, count
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    def _generate_patch_model(self, cls: type[EnrichModel]) -> None:
        """"""Create an auto-generated PatchModel on the entity class.""""""
        mutable_fields = {}
        for name, field in cls.model_fields.items():
            extra = getattr(field, ""json_schema_extra"", None)
            if extra is None:
                info = getattr(field, ""field_info"", None)
                extra = getattr(info, ""extra"", {}) if info is not None else {}
            if extra.get(""mutable"") is True and name not in cls.relationship_fields():
                annotation = field.annotation or Any
                mutable_fields[name] = (
                    annotation | None,
                    Field(
                        default=None,
                        description=field.description,
                    ),
                )

        if mutable_fields:
            patch_model_cls = create_model(
                f""{cls.__name__}PatchModel"",
                __base__=BaseModel,
                **mutable_fields,
            )
            patch_model_cls.__doc__ = f""Patch model for {cls.__name__}""
            cls.PatchModel = patch_model_cls
",src/enrichmcp/app.py,EnrichMCP
survived,"        def decorator(f: T) -> T:
            return f
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/codegen_agent.py,
survived,"    def list(self, project: str, page: int, page_size: int) -> list[MemoryNoteSummary]:
        p = self._project_dir(project)
        files = sorted(p.glob(""*.md""))
        start = (page - 1) * page_size
        end = start + page_size
        notes: list[MemoryNoteSummary] = []
        for file in files[start:end]:
            note = self.load(project, file.stem)
            if note:
                notes.append(MemoryNoteSummary(id=note.id, title=note.title))
        return notes
",examples/basic_memory/memory.py,FileMemoryStore
survived,"    def new_id(self) -> str:
        """"""Return a new unique note identifier.""""""
",examples/basic_memory/memory.py,MemoryStore
survived,"def has_network() -> bool:
    """"""Return ``True`` if DNS resolution for ``pypi.org`` succeeds.""""""
    try:
        socket.gethostbyname(""pypi.org"")
        return True
    except OSError:
        return False
",check_env.py,
survived,"    def _run_check(self, module_name: str, version: str | None) -> int:
        fake_mod = types.SimpleNamespace()
        if version is not None:
            fake_mod.__version__ = version
        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return object()
            if name in {""openai_agents"", ""agents""}:
                return None
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [module_name]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
        ):
            return check_env.main([])
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def _scan_licenses(self, metadata: TemplateMetadata) -> List[str]:
        issues: List[str] = []
        if not metadata.tools:
            return issues
        try:
            _, licenses, _ = self.dep_manager.resolve(metadata.tools)
        except Exception as exc:  # pragma: no cover - dependency errors rare
            issues.append(f""dependency resolution failed: {exc}"")
            return issues
        for pkg, lic in licenses.items():
            if any(tag in lic for tag in self._DISALLOWED_LICENSES):
                issues.append(f""non-permissive license for {pkg}: {lic}"")
        return issues
",src/meta_agent/template_validator.py,TemplateValidator
survived,"    def first_true(seq: list[bool]) -> int:
        for i, val in enumerate(seq):
            if val:
                return i
        return len(seq)
",src/simulation/replay.py,
survived,"    def run_compute_pressure_force_acceleration(self):
        """"""Compute pressure forces and update acceleration.""""""
        pos = self.sorted_position[:, :3]
        i_idx = torch.repeat_interleave(
            torch.arange(pos.shape[0], device=self.device),
            self.neighbor_map.shape[1],
        )
        j_idx = self.neighbor_map.reshape(-1)
        mask = j_idx >= 0
        i_idx = i_idx[mask]
        j_idx = j_idx[mask]
        diff = pos[i_idx] - pos[j_idx]
        dist = diff.norm(dim=1)
        dir = diff / (dist.unsqueeze(1) + 1e-12)
        grad = (
            self.config[""mass_mult_gradWspikyCoefficient""]
            * (self.config[""h""] - dist).clamp(min=0) ** 2
        ).unsqueeze(1) * dir
        pres = (
            self.pressure[i_idx] / (self.rho[i_idx] ** 2)
            + self.pressure[j_idx] / (self.rho[j_idx] ** 2)
        ).unsqueeze(1)
        force = -pres * grad
        acc = torch.zeros_like(self.sorted_position)
        acc.scatter_add_(0, i_idx.unsqueeze(1).expand(-1, 3), force)
        gravity = torch.tensor(
            [
                self.config.get(""gravity_x"", 0.0),
                self.config.get(""gravity_y"", -9.8),
                self.config.get(""gravity_z"", 0.0),
            ],
            device=self.device,
        )
        acc[:, :3] += gravity
        self.acceleration = acc
",pytorch_solver.py,PytorchSolver
survived,"    def run_find_neighbors(self):
        """"""Search neighbors using the hashed grid.""""""
        n = self.sorted_position.shape[0]
        max_n = self.config.get(""max_neighbor_count"", 50)
        neighbors = torch.full((n, max_n), -1, dtype=torch.long, device=self.device)
        cell_id = self.particle_index[:, 0]
        pos = self.sorted_position[:, :3]
        gsx = self.config[""grid_cells_x""]
        gsy = self.config[""grid_cells_y""]
        gsz = self.config[""grid_cells_z""]
        for p in range(n):
            cid = cell_id[p].item()
            gz = cid // (gsx * gsy)
            rem = cid % (gsx * gsy)
            gy = rem // gsx
            gx = rem % gsx
            idx = 0
            for dz in (-1, 0, 1):
                nz = gz + dz
                if nz < 0 or nz >= gsz:
                    continue
                for dy in (-1, 0, 1):
                    ny = gy + dy
                    if ny < 0 or ny >= gsy:
                        continue
                    for dx in (-1, 0, 1):
                        nx = gx + dx
                        if nx < 0 or nx >= gsx:
                            continue
                        cell = nx + ny * gsx + nz * gsx * gsy
                        start = self.grid_cell_index_fixed[cell]
                        end = self.grid_cell_index_fixed[cell + 1]
                        if start < 0 or end <= start:
                            continue
                        ids = self.particle_index[start:end, 1]
                        for j in ids:
                            j = j.item()
                            if j == self.particle_index[p, 1].item():
                                continue
                            if torch.norm(pos[p] - pos[j]) < self.config[""h""]:
                                if idx < max_n:
                                    neighbors[p, idx] = j
                                    idx += 1
        self.neighbor_map = neighbors
",pytorch_solver.py,PytorchSolver
survived,"def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Capture demo preview"")
    parser.add_argument(
        ""demo"",
        help=""Path to the demo shell script or command to execute"",
    )
    parser.add_argument(
        ""-o"",
        ""--output"",
        required=True,
        help=""Output file (.mp4 or .gif)"",
    )
    parser.add_argument(
        ""--duration"",
        type=int,
        default=15,
        help=""Duration to record in seconds (default: 15)"",
    )
    parser.add_argument(
        ""--size"",
        default=""1280x720"",
        help=""Virtual display size WxH (default: 1280x720)"",
    )
    return parser.parse_args(argv)
",scripts/capture_demo_preview.py,
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",tests/test_bus_tls.py,
survived,"def test_bundle_generator_creates_files(tmp_path: Path) -> None:
    gen = BundleGenerator(tmp_path)
    metadata = gen.generate(
        agent_code=""print('agent')"",
        tests={""test_sample.py"": ""def test_sample():\n    assert True""},
        requirements=[""foo==1.0""],
        readme=""# Hello"",
        guardrails_manifest=""{}"",
    )

    assert (tmp_path / ""agent.py"").exists()
    assert (tmp_path / ""tests"" / ""test_sample.py"").exists()
    assert (tmp_path / ""requirements.txt"").exists()
    assert (tmp_path / ""README.md"").exists()
    assert (tmp_path / ""guardrails"" / ""manifest.json"").exists()
    assert (tmp_path / ""traces"").is_dir()
    assert (tmp_path / ""bundle.json"").exists()

    with open(tmp_path / ""bundle.json"", encoding=""utf-8"") as f:
        data = json.load(f)
    assert data[""schema_version""] == BUNDLE_SCHEMA_VERSION
    checksums = data[""custom""][""checksums""]

    with open(tmp_path / ""agent.py"", encoding=""utf-8"") as f:
        expected = hashlib.sha256(f.read().encode(""utf-8"")).hexdigest()
    assert checksums[""agent.py""] == expected
",tests/test_bundle_generator.py,
survived,"    def transform_example(example):
        # Civil Comments uses continuous scores from 0 to 1
        # We'll consider a comment toxic if the toxicity score is >= 0.5
        is_toxic = example[""toxicity""] >= 0.5
        
        # Create toxicity details based on scores above threshold
        toxicity_details = []
        threshold = 0.5
        
        if example.get(""severe_toxicity"", 0) >= threshold:
            toxicity_details.append(""severely toxic"")
        if example.get(""obscene"", 0) >= threshold:
            toxicity_details.append(""obscene"")
        if example.get(""threat"", 0) >= threshold:
            toxicity_details.append(""threatening"")
        if example.get(""insult"", 0) >= threshold:
            toxicity_details.append(""insulting"")
        if example.get(""identity_attack"", 0) >= threshold:
            toxicity_details.append(""contains identity-based attacks"")
        if example.get(""sexual_explicit"", 0) >= threshold:
            toxicity_details.append(""sexually explicit"")
        
        # If toxic but no specific categories, add general toxicity
        if is_toxic and not toxicity_details:
            toxicity_details.append(""generally toxic"")
        
        # Store all relevant info in the info dict
        return {
            ""question"": f""Analyze the following text for toxicity and explain your reasoning:\n\n{example['text']}"",
            ""answer"": ""toxic"" if is_toxic else ""non-toxic"",  # For JudgeRubric
            ""info"": {
                ""is_toxic"": is_toxic,
                ""categories"": toxicity_details if toxicity_details else [""non-toxic""],
                ""text"": example[""text""],
                ""toxicity_score"": example[""toxicity""]
            }
        }
",environments/toxicity_explanation/toxicity_explanation.py,
survived,"    def __init__(self, model: str = ""o3""):
        self.model = model
        self.color = LIGHT_BLUE
",examples/openai/o3_responses_example.py,O3DecisionAgent
survived,"    async def test_latest_run_with_null_metadata(
        self,
        test_api_client: AsyncClient,
        returned_run: AgentRun,
    ):
        """"""Test that null metadata is handled correctly""""""
        returned_run.metadata = None

        response = await test_api_client.get(""/v1/_/agents/bla/runs/latest"")
        assert response.status_code == 200

        response_data = response.json()
        assert response_data[""id""] == returned_run.id
        # metadata should not be present in response when it's None (due to response_model_exclude_none=True)
        assert ""metadata"" not in response_data
",api/api/routers/runs_v1_test.py,TestLatestRun
survived,"    def column_order(self) -> List[str]:
        """"""Get column order configuration from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_order""
        ]
        if not config:
            return []
        return config[0].get(""value"", [])
",keep/api/models/db/preset.py,PresetDto
survived,"async def create_api_key() -> MCPToolReturn:
    """"""<when_to_use>
    When the user wants to get their API key for WorkflowAI. This is a temporary tool that returns the API key that was used to authenticate the current request.
    </when_to_use>
    <returns>
    Returns the API key that was used to authenticate the current MCP request.
    </returns>""""""
    request = get_http_request()

    auth_header = request.headers.get(""Authorization"")
    if not auth_header or not auth_header.startswith(""Bearer ""):
        return MCPToolReturn(
            success=False,
            error=""No Authorization header found or invalid format"",
        )

    # Extract the API key from ""Bearer <key>""
    api_key = auth_header.split("" "")[1]

    return MCPToolReturn(
        success=True,
        data={""api_key"": api_key},
        messages=[""API key retrieved successfully""],
    )
",api/api/routers/mcp/mcp_server.py,
survived,"    def test_cost_report_mixed_valid_invalid_clusters(self):
        """"""Test cost report works when some clusters are valid and others have issues.""""""
        valid_cluster = {
            'name': 'valid-cluster',
            'status': status_lib.ClusterStatus.UP,
            'num_nodes': 1,
            'resources': mock.Mock(),
            'total_cost': 5.50,
            'launched_at': 1640995200,
            'duration': 3600,
            'cluster_hash': 'valid123',
            'usage_intervals': [(1640995200, 1640998800)],
            'user_hash': 'user_valid',
            'user_name': 'validuser',
            'workspace': 'default',
        }
        valid_cluster['resources'].instance_type = 'standard-instance'
        valid_cluster['resources'].cloud = mock.Mock()
        valid_cluster['resources'].cloud.__str__ = lambda: 'aws'
        
        invalid_cluster = {
            'name': 'invalid-cluster',
            'status': None,
            'num_nodes': 1,
            'resources': mock.Mock(),
            'total_cost': 0.0,
            'launched_at': 1640995200,
            'duration': 1800,
            'cluster_hash': 'invalid456',
            'usage_intervals': [(1640995200, 1640997000)],
            'user_hash': 'user_invalid',
            'user_name': 'invaliduser',
            'workspace': 'default',
        }
        invalid_cluster['resources'].instance_type = 'discontinued-instance-type'
        invalid_cluster['resources'].cloud = mock.Mock()
        invalid_cluster['resources'].cloud.__str__ = lambda: 'nonexistent-cloud'
        
        with mock.patch('sky.global_user_state.get_clusters_from_history', 
                      return_value=[valid_cluster, invalid_cluster]):
            
                         # Should return both clusters, even if one has issues
             result = core.cost_report(days=30)
             self.assertEqual(len(result), 2)
             
             cluster_names = [r['name'] for r in result]
             self.assertIn('valid-cluster', cluster_names)
             self.assertIn('invalid-cluster', cluster_names)
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness
survived,"    def test_cost_report_default_days(self):
        """"""Test cost_report with default days parameter.""""""
        with mock.patch('sky.global_user_state.get_clusters_from_history') as mock_get_history:
            mock_get_history.return_value = []
            
            result = core.cost_report()
            
            # Should call with default 30 days
            mock_get_history.assert_called_once_with(days=30)
            self.assertEqual(result, [])
",tests/unit_tests/test_sky_cost_report.py,TestCostReportCore
survived,"def multiline_binary():
    """"""Fixture for binary fields, specifically multi-lines ones""""""
    return """"""
Package: binutils
Binary: binutils-for-host, binutils-for-build,
 binutils-ia64-linux-gnu-dbg, binutils-m68k-linux-gnu,
 binutils-mips64el-linux-gnuabin32-dbg, binutils-mipsisa64r6-linux-gnuabin32,
 binutils-mipsisa64r6el-linux-gnuabi64-dbg

""""""
",tests/package_managers/debian/test_debian_parser.py,
survived,"    def test_enrich_package_preserves_existing_fields(self, mock_logger):
        """"""Test that existing package fields are not overwritten""""""
        # Create package data with existing homepage
        package_data = create_debian_package(
            package=""pkg-with-homepage"",
            homepage=""pkg-homepage.com"",  # Normalized format
        )

        # Create source data with different homepage
        source_data = create_debian_package(
            package=""pkg-with-homepage"",
            homepage=""source-homepage.com"",  # Normalized format
            vcs_git=""github.com/test/pkg"",  # Normalized format
        )
        source_mapping = {""pkg-with-homepage"": source_data}

        # Enrich package
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # Verify package homepage is preserved, but source info is added
        assert enriched.homepage == ""pkg-homepage.com""  # Package value preserved
        assert enriched.vcs_git == ""github.com/test/pkg""  # Source value added",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"    def diff_pkg(
        self, import_id: str, debian_data: DebianData
    ) -> tuple[UUID, Package | None, dict | None]:
        """"""
        Checks if the given package is in the package_cache.

        Returns:
          - pkg_id: the id of the package
          - package: If new, returns a new package object. If existing, returns None
          - changes: a dictionary of changes (description updates)
        """"""
        self.logger.debug(f""Diffing package: {import_id}"")

        if import_id not in self.caches.package_map:
            # new package
            name = import_id.split(""/"")[1]
            p = Package(
                id=uuid4(),
                derived_id=import_id,
                name=name,
                package_manager_id=self.config.pm_config.pm_id,
                import_id=import_id,
                readme=debian_data.description,
                created_at=self.now,
                updated_at=self.now,
            )
            pkg_id: UUID = p.id
            return pkg_id, p, {}
        else:
            # the package exists, check if description has changed
            existing_pkg = self.caches.package_map[import_id]
            pkg_id = existing_pkg.id

            # Check if description (readme) has changed
            if existing_pkg.readme != debian_data.description:
                update_payload = {
                    ""id"": pkg_id,
                    ""readme"": debian_data.description,
                    ""updated_at"": self.now,
                }
                return pkg_id, None, update_payload
            else:
                return pkg_id, None, None
",package_managers/debian/diff.py,DebianDiff
survived,"def build_package_to_source_mapping(
    sources_file_path: str, logger: Logger
) -> dict[str, DebianData]:
    """"""
    Build a mapping from binary package names to their source information.

    Args:
        sources_file_path: Path to the sources file
        test: Whether to limit parsing for testing

    Returns:
        Dictionary mapping binary package names to source DebianData objects
    """"""
    # Parse sources file
    with open(sources_file_path) as f:
        sources_content = f.read()
    sources_parser = DebianParser(sources_content)

    # Build mapping: binary_package_name -> source_debian_data
    package_to_source: dict[str, DebianData] = {}

    for source_data in sources_parser.parse():
        # Each source may produce multiple binary packages
        if source_data.binary:
            # Source has explicit binary list
            for binary_name in source_data.binary:
                binary_name = binary_name.strip()
                if binary_name:
                    package_to_source[binary_name] = source_data
        else:
            # No explicit binary list, assume source name == binary name
            if source_data.package:
                package_to_source[source_data.package] = source_data

    logger.log(
        f""Built mapping for {len(package_to_source)} binary packages from sources""
    )
    return package_to_source
",package_managers/debian/debian_sources.py,
survived,"    def test_duplicate_package_paragraphs(self, mock_config, mock_logger, mock_db):
        """"""Tests the case when the Debian Packages file contains duplicate packages""""""
        d1 = Package(id=uuid4(), derived_id=""debian/d1"", name=""d1"", import_id=""d1"")
        d2 = Package(id=uuid4(), derived_id=""debian/d2"", name=""d2"", import_id=""d2"")
        p1 = create_debian_package(
            package=""linux-doc"", homepage=""homepage.org"", depends=[""d1""]
        )
        p2 = create_debian_package(
            package=""linux-doc"", homepage=""homepage.org"", depends=[""d2""]
        )
        cache = Cache(
            package_map={""debian/d1"": d1, ""debian/d2"": d2},
            url_map={},
            package_urls={},
            dependencies={},
        )

        data = [p1, p2]

        result = main_diff(data, mock_config, cache, mock_db, mock_logger)

        assert len(result.new_packages) == 1
        assert len(result.new_package_urls) == 1
        assert len(result.new_deps) == 0  # bc we don't load dependencies of new pkgs",tests/package_managers/debian/test_debian_diff.py,TestDebianDiffFunction
survived,"def test_binutils(binutils):
    m = mock_open(read_data=binutils)

    with patch(""builtins.open"", m):
        result = parse_sources_file(""dummy"")

    assert result == {
        ""binutils"": {
            ""binutils-for-host"",
            ""binutils-for-build"",
            ""binutils-ia64-linux-gnu-dbg"",
            ""binutils-m68k-linux-gnu"",
            ""binutils-mips64el-linux-gnuabin32-dbg"",
            ""binutils-mipsisa64r6-linux-gnuabin32"",
            ""binutils-mipsisa64r6el-linux-gnuabi64-dbg"",
        }
    }
",package_managers/debian/scripts/test_investigate_sources.py,
survived,"    def test_enrich_package_no_explicit_source(self, mock_logger):
        """"""Test enriching package with no explicit source reference""""""

        # Create package data with no explicit source
        package_data = create_debian_package(
            package=""self-source-pkg"",
            description=""A self-sourced package"",
        )

        # Create source mapping with same name as package
        source_data = create_debian_package(
            package=""self-source-pkg"",
            vcs_browser=""github.com/test/self-source-pkg"",  # Already normalized format
            directory=""pool/main/s/self-source-pkg"",
        )
        source_mapping = {""self-source-pkg"": source_data}

        # Enrich package
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # Verify enrichment
        assert enriched.package == ""self-source-pkg""
        assert enriched.vcs_browser == ""github.com/test/self-source-pkg""
        assert enriched.directory == ""pool/main/s/self-source-pkg""
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"def run_ai_code_task(task_id):
    """"""Run AI Code automation (Claude or Codex) in a container""""""
    try:
        task = tasks[task_id]
        task['status'] = TaskStatus.RUNNING
        
        model_name = task.get('model', 'claude').upper()
        logger.info(f""ðŸš€ Starting {model_name} Code task {task_id}"")
        logger.info(f""ðŸ“‹ Task details: prompt='{task['prompt'][:50]}...', repo={task['repo_url']}, branch={task['branch']}, model={model_name}"")
        logger.info(f""Starting {model_name} task {task_id}"")
        
        # Escape special characters in prompt for shell safety
        escaped_prompt = task['prompt'].replace('""', '\\""').replace('$', '\\$').replace('`', '\\`')
        
        # Create container environment variables
        env_vars = {
            'CI': 'true',  # Indicate we're in CI/non-interactive environment
            'TERM': 'dumb',  # Use dumb terminal to avoid interactive features
            'NO_COLOR': '1',  # Disable colors for cleaner output
            'FORCE_COLOR': '0',  # Disable colors for cleaner output
            'NONINTERACTIVE': '1',  # Common flag for non-interactive mode
            'DEBIAN_FRONTEND': 'noninteractive',  # Non-interactive package installs
        }
        
        # Add model-specific API keys and environment variables
        model_cli = task.get('model', 'claude')
        if model_cli == 'claude':
            env_vars.update({
                'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
                'ANTHROPIC_NONINTERACTIVE': '1'  # Custom flag for Anthropic tools
            })
        elif model_cli == 'codex':
            env_vars.update({
                'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
                'OPENAI_NONINTERACTIVE': '1',  # Custom flag for OpenAI tools
                'CODEX_QUIET_MODE': '1'  # Official Codex non-interactive flag
            })
        
        # Use specialized container images based on model
        if model_cli == 'codex':
            container_image = 'codex-automation:latest'
        else:
            container_image = 'claude-code-automation:latest'
        
        # Create the command to run in container
        container_command = rf'''
set -e
echo ""Setting up repository...""

# Clone repository
git clone -b {task['branch']} {task['repo_url']} /workspace/repo
cd /workspace/repo

# Configure git
git config user.email ""claude-code@automation.com""
git config user.name ""Claude Code Automation""

# We'll extract the patch instead of pushing directly
echo ""ðŸ“‹ Will extract changes as patch for later PR creation...""

echo ""Starting {model_cli.upper()} Code with prompt...""

# Create a temporary file with the prompt
echo ""{escaped_prompt}"" > /tmp/prompt.txt

# Check which CLI tool to use based on model selection
if [ ""{model_cli}"" = ""codex"" ]; then
    echo ""Using Codex (OpenAI Codex) CLI...""
    
    # Set environment variables for non-interactive mode
    export CODEX_QUIET_MODE=1
    
    # Read the prompt from file
    PROMPT_TEXT=$(cat /tmp/prompt.txt)
    
    # Check for codex installation
    if [ -f /usr/local/bin/codex ]; then
        echo ""Found codex at /usr/local/bin/codex""
        echo ""Running Codex in non-interactive mode...""
        
        # Use non-interactive flags for Docker environment
        # --dangerously-auto-approve-everything is required when running in Docker
        /usr/local/bin/codex --quiet --approval-mode full-auto --dangerously-auto-approve-everything ""$PROMPT_TEXT""
        CODEX_EXIT_CODE=$?
        echo ""Codex finished with exit code: $CODEX_EXIT_CODE""
        
        if [ $CODEX_EXIT_CODE -ne 0 ]; then
            echo ""ERROR: Codex failed with exit code $CODEX_EXIT_CODE""
            exit $CODEX_EXIT_CODE
        fi
        
        echo ""âœ… Codex completed successfully""
    elif command -v codex >/dev/null 2>&1; then
        echo ""Using codex from PATH...""
        echo ""Running Codex in non-interactive mode...""
        
        # Use non-interactive flags for Docker environment
        # --dangerously-auto-approve-everything is required when running in Docker
        codex --quiet --approval-mode full-auto --dangerously-auto-approve-everything ""$PROMPT_TEXT""
        CODEX_EXIT_CODE=$?
        echo ""Codex finished with exit code: $CODEX_EXIT_CODE""
        if [ $CODEX_EXIT_CODE -ne 0 ]; then
            echo ""ERROR: Codex failed with exit code $CODEX_EXIT_CODE""
            exit $CODEX_EXIT_CODE
        fi
        echo ""âœ… Codex completed successfully""
    else
        echo ""ERROR: codex command not found anywhere""
        echo ""Please ensure Codex CLI is installed in the container""
        exit 1
    fi
    
else
    echo ""Using Claude CLI...""
    
    # Try different ways to invoke claude
    echo ""Checking claude installation...""

if [ -f /usr/local/bin/claude ]; then
    echo ""Found claude at /usr/local/bin/claude""
    echo ""File type:""
    file /usr/local/bin/claude || echo ""file command not available""
    echo ""First few lines:""
    head -5 /usr/local/bin/claude || echo ""head command failed""
    
    # Check if it's a shell script
    if head -1 /usr/local/bin/claude | grep -q ""#!/bin/sh\|#!/bin/bash\|#!/usr/bin/env bash""; then
        echo ""Detected shell script, running with sh...""
        sh /usr/local/bin/claude < /tmp/prompt.txt
    # Check if it's a Node.js script (including env -S node pattern)
    elif head -1 /usr/local/bin/claude | grep -q ""#!/usr/bin/env.*node\|#!/usr/bin/node""; then
        echo ""Detected Node.js script...""
        if command -v node >/dev/null 2>&1; then
            echo ""Running with node...""
            # Try different approaches for Claude CLI
            
            # First try with --help to see available options
            echo ""Checking claude options...""
            node /usr/local/bin/claude --help 2>/dev/null || echo ""Help not available""
            
            # Try non-interactive approaches
            echo ""Attempting non-interactive execution...""
            
            # Method 1: Use the official --print flag for non-interactive mode
            echo ""Using --print flag for non-interactive mode...""
            cat /tmp/prompt.txt | node /usr/local/bin/claude --print --allowedTools ""Edit,Bash""
            CLAUDE_EXIT_CODE=$?
            echo ""Claude Code finished with exit code: $CLAUDE_EXIT_CODE""
            
            if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
                echo ""ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE""
                exit $CLAUDE_EXIT_CODE
            fi
            
            echo ""âœ… Claude Code completed successfully""
        else
            echo ""Node.js not found, trying direct execution...""
            /usr/local/bin/claude < /tmp/prompt.txt
            CLAUDE_EXIT_CODE=$?
            echo ""Claude Code finished with exit code: $CLAUDE_EXIT_CODE""
            if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
                echo ""ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE""
                exit $CLAUDE_EXIT_CODE
            fi
            echo ""âœ… Claude Code completed successfully""
        fi
    # Check if it's a Python script
    elif head -1 /usr/local/bin/claude | grep -q ""#!/usr/bin/env python\|#!/usr/bin/python""; then
        echo ""Detected Python script...""
        if command -v python3 >/dev/null 2>&1; then
            echo ""Running with python3...""
            python3 /usr/local/bin/claude < /tmp/prompt.txt
            CLAUDE_EXIT_CODE=$?
        elif command -v python >/dev/null 2>&1; then
            echo ""Running with python...""
            python /usr/local/bin/claude < /tmp/prompt.txt
            CLAUDE_EXIT_CODE=$?
        else
            echo ""Python not found, trying direct execution...""
            /usr/local/bin/claude < /tmp/prompt.txt
            CLAUDE_EXIT_CODE=$?
        fi
        echo ""Claude Code finished with exit code: $CLAUDE_EXIT_CODE""
        if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
            echo ""ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE""
            exit $CLAUDE_EXIT_CODE
        fi
        echo ""âœ… Claude Code completed successfully""
    else
        echo ""Unknown script type, trying direct execution...""
        /usr/local/bin/claude < /tmp/prompt.txt
        CLAUDE_EXIT_CODE=$?
        echo ""Claude Code finished with exit code: $CLAUDE_EXIT_CODE""
        if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
            echo ""ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE""
            exit $CLAUDE_EXIT_CODE
        fi
        echo ""âœ… Claude Code completed successfully""
    fi
elif command -v claude >/dev/null 2>&1; then
    echo ""Using claude from PATH...""
    CLAUDE_PATH=$(which claude)
    echo ""Claude found at: $CLAUDE_PATH""
    claude < /tmp/prompt.txt
    CLAUDE_EXIT_CODE=$?
    echo ""Claude Code finished with exit code: $CLAUDE_EXIT_CODE""
    if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
        echo ""ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE""
        exit $CLAUDE_EXIT_CODE
    fi
    echo ""âœ… Claude Code completed successfully""
else
    echo ""ERROR: claude command not found anywhere""
    echo ""Checking available interpreters:""
    which python3 2>/dev/null && echo ""python3: available"" || echo ""python3: not found""
    which python 2>/dev/null && echo ""python: available"" || echo ""python: not found""
    which node 2>/dev/null && echo ""node: available"" || echo ""node: not found""
    which sh 2>/dev/null && echo ""sh: available"" || echo ""sh: not found""
    exit 1
fi

fi  # End of model selection (claude vs codex)

# Check if there are changes
if git diff --quiet; then
    echo ""No changes made""
    exit 1
fi

# Commit changes locally
git add .
git commit -m ""{model_cli.capitalize()}: {escaped_prompt[:100]}""

# Get commit info
COMMIT_HASH=$(git rev-parse HEAD)
echo ""COMMIT_HASH=$COMMIT_HASH""

# Generate patch file for later application
echo ""ðŸ“¦ Generating patch file...""
git format-patch HEAD~1 --stdout > /tmp/changes.patch
echo ""=== PATCH START ===""
cat /tmp/changes.patch
echo ""=== PATCH END ===""

# Also get the diff for display
echo ""=== GIT DIFF START ===""
git diff HEAD~1 HEAD
echo ""=== GIT DIFF END ===""

# List changed files for reference
echo ""=== CHANGED FILES START ===""
git diff --name-only HEAD~1 HEAD
echo ""=== CHANGED FILES END ===""

# Explicitly exit with success code
echo ""Container work completed successfully""
exit 0
'''
        
        # Run container with unified AI Code tools (supports both Claude and Codex)
        logger.info(f""ðŸ³ Creating Docker container for task {task_id} using {container_image} (model: {model_name})"")
        container = docker_client.containers.run(
            container_image,
            command=['bash', '-c', container_command],
            environment=env_vars,
            detach=True,
            remove=False,  # Don't auto-remove so we can get logs
            working_dir='/workspace',
            network_mode='bridge',  # Ensure proper networking
            tty=False,  # Don't allocate TTY - may prevent clean exit
            stdin_open=False  # Don't keep stdin open - may prevent clean exit
        )
        
        task['container_id'] = container.id
        logger.info(f""âœ… Container created successfully: {container.id[:12]}"")
        logger.info(f""â³ Waiting for container to complete (timeout: 300s)..."")
        
        # Wait for container to finish - should exit naturally when script completes
        try:
            logger.info(f""ðŸ”„ Waiting for container script to complete naturally..."")
            
            # Check initial container state
            container.reload()
            logger.info(f""ðŸ” Container initial state: {container.status}"")
            
            # Use standard wait - container should exit when bash script finishes
            logger.info(f""ðŸ”„ Calling container.wait() - container should exit when script completes..."")
            result = container.wait(timeout=300)  # 5 minute timeout
            logger.info(f""ðŸŽ¯ Container exited naturally! Exit code: {result['StatusCode']}"")
            
            # Verify final container state
            container.reload()
            logger.info(f""ðŸ” Final container state: {container.status}"")
            
            # Get logs before any cleanup operations
            logger.info(f""ðŸ“œ Retrieving container logs..."")
            try:
                logs = container.logs().decode('utf-8')
                logger.info(f""ðŸ“ Retrieved {len(logs)} characters of logs"")
                logger.info(f""ðŸ” First 200 chars of logs: {logs[:200]}..."")
            except Exception as log_error:
                logger.warning(f""âŒ Failed to get container logs: {log_error}"")
                logs = f""Failed to retrieve logs: {log_error}""
            
            # Clean up container after getting logs
            try:
                container.reload()  # Refresh container state
                container.remove()
                logger.info(f""Successfully removed container {container.id}"")
            except Exception as cleanup_error:
                logger.warning(f""Failed to remove container {container.id}: {cleanup_error}"")
                # Try force removal as fallback
                try:
                    container.remove(force=True)
                    logger.info(f""Force removed container {container.id}"")
                except Exception as force_cleanup_error:
                    logger.error(f""Failed to force remove container: {force_cleanup_error}"")
                
        except Exception as e:
            logger.error(f""â° Container timeout or error: {str(e)}"")
            logger.error(f""ðŸ”„ Updating task status to FAILED due to timeout/error..."")
            task['status'] = TaskStatus.FAILED
            task['error'] = f""Container execution timeout or error: {str(e)}""
            
            # Try to get logs even on error
            try:
                logs = container.logs().decode('utf-8')
            except Exception as log_error:
                logs = f""Container failed and logs unavailable: {log_error}""
            
            # Try to clean up container on error
            try:
                container.reload()  # Refresh container state
                container.remove(force=True)
                logger.info(f""Cleaned up failed container {container.id}"")
            except Exception as cleanup_error:
                logger.warning(f""Failed to remove failed container {container.id}: {cleanup_error}"")
            return
        
        if result['StatusCode'] == 0:
            logger.info(f""âœ… Container exited successfully (code 0) - parsing results..."")
            # Parse output to extract commit hash, diff, and patch
            lines = logs.split('\n')
            commit_hash = None
            git_diff = []
            git_patch = []
            changed_files = []
            capturing_diff = False
            capturing_patch = False
            capturing_files = False
            
            for line in lines:
                if line.startswith('COMMIT_HASH='):
                    commit_hash = line.split('=', 1)[1]
                    logger.info(f""ðŸ”‘ Found commit hash: {commit_hash}"")
                elif line == '=== PATCH START ===':
                    capturing_patch = True
                    logger.info(f""ðŸ“¦ Starting to capture git patch..."")
                elif line == '=== PATCH END ===':
                    capturing_patch = False
                    logger.info(f""ðŸ“¦ Finished capturing git patch ({len(git_patch)} lines)"")
                elif line == '=== GIT DIFF START ===':
                    capturing_diff = True
                    logger.info(f""ðŸ“Š Starting to capture git diff..."")
                elif line == '=== GIT DIFF END ===':
                    capturing_diff = False
                    logger.info(f""ðŸ“Š Finished capturing git diff ({len(git_diff)} lines)"")
                elif line == '=== CHANGED FILES START ===':
                    capturing_files = True
                    logger.info(f""ðŸ“ Starting to capture changed files..."")
                elif line == '=== CHANGED FILES END ===':
                    capturing_files = False
                    logger.info(f""ðŸ“ Finished capturing changed files ({len(changed_files)} files)"")
                elif capturing_patch:
                    git_patch.append(line)
                elif capturing_diff:
                    git_diff.append(line)
                elif capturing_files:
                    if line.strip():  # Only add non-empty lines
                        changed_files.append(line.strip())
            
            logger.info(f""ðŸ”„ Updating task status to COMPLETED..."")
            task['status'] = TaskStatus.COMPLETED
            task['commit_hash'] = commit_hash
            task['git_diff'] = '\n'.join(git_diff)
            task['git_patch'] = '\n'.join(git_patch)
            task['changed_files'] = changed_files
            
            # Save tasks after completion
            save_tasks()
            
            logger.info(f""ðŸŽ‰ {model_name} Task {task_id} completed successfully! Commit: {commit_hash[:8] if commit_hash else 'N/A'}, Diff lines: {len(git_diff)}"")
            
        else:
            logger.error(f""âŒ Container exited with error code {result['StatusCode']}"")
            task['status'] = TaskStatus.FAILED
            task['error'] = f""Container exited with code {result['StatusCode']}: {logs}""
            save_tasks()  # Save failed task
            logger.error(f""ðŸ’¥ {model_name} Task {task_id} failed: {task['error'][:200]}..."")
            
    except Exception as e:
        model_name = task.get('model', 'claude').upper()
        logger.error(f""ðŸ’¥ Unexpected exception in {model_name} task {task_id}: {str(e)}"")
        task['status'] = TaskStatus.FAILED
        task['error'] = str(e)
        logger.error(f""ðŸ”„ {model_name} Task {task_id} failed with exception: {str(e)}"")
",server/utils.py,
survived,"def create_pull_request(task_id):
    """"""Create a pull request by applying the saved patch to a fresh repo clone""""""
    try:
        user_id = request.headers.get('X-User-ID')
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        logger.info(f""ðŸ” PR creation requested for task: {task_id}"")
        
        task = DatabaseOperations.get_task_by_id(task_id, user_id)
        if not task:
            logger.error(f""âŒ Task {task_id} not found"")
            return jsonify({'error': 'Task not found'}), 404
        
        if task['status'] != 'completed':
            return jsonify({'error': 'Task not completed yet'}), 400
            
        if not task.get('git_patch'):
            return jsonify({'error': 'No patch data available for this task'}), 400
        
        data = request.get_json() or {}
        
        # Get prompt from chat messages
        prompt = """"
        if task.get('chat_messages'):
            for msg in task['chat_messages']:
                if msg.get('role') == 'user':
                    prompt = msg.get('content', '')
                    break
        
        pr_title = data.get('title', f""Claude Code: {prompt[:50]}..."")
        pr_body = data.get('body', f""Automated changes generated by Claude Code.\n\nPrompt: {prompt}\n\nChanged files:\n"" + '\n'.join(f""- {f}"" for f in task.get('changed_files', [])))
        github_token = data.get('github_token')
        
        if not github_token:
            return jsonify({'error': 'github_token is required'}), 400
        
        logger.info(f""ðŸš€ Creating PR for task {task_id}"")
        
        # Extract repo info from URL
        repo_parts = task['repo_url'].replace('https://github.com/', '').replace('.git', '')
        
        # Create GitHub client
        g = Github(github_token)
        repo = g.get_repo(repo_parts)
        
        # Determine branch strategy
        base_branch = task['target_branch']
        pr_branch = f""claude-code-{task_id}""
        
        logger.info(f""ðŸ“‹ Creating PR branch '{pr_branch}' from base '{base_branch}'"")
        
        # Get the latest commit from the base branch
        base_branch_obj = repo.get_branch(base_branch)
        base_sha = base_branch_obj.commit.sha
        
        # Create new branch for the PR
        try:
            # Check if branch already exists
            try:
                existing_branch = repo.get_branch(pr_branch)
                logger.warning(f""âš ï¸ Branch '{pr_branch}' already exists, deleting it first..."")
                repo.get_git_ref(f""heads/{pr_branch}"").delete()
                logger.info(f""ðŸ—‘ï¸ Deleted existing branch '{pr_branch}'"")
            except:
                pass  # Branch doesn't exist, which is what we want
            
            # Create the new branch
            new_ref = repo.create_git_ref(f""refs/heads/{pr_branch}"", base_sha)
            logger.info(f""âœ… Created branch '{pr_branch}' from {base_sha[:8]}"")
            
        except Exception as branch_error:
            logger.error(f""âŒ Failed to create branch '{pr_branch}': {str(branch_error)}"")
            
            # Provide specific error messages based on the error
            error_msg = str(branch_error).lower()
            if ""resource not accessible"" in error_msg:
                detailed_error = (
                    f""GitHub token lacks permission to create branches. ""
                    f""Please ensure your token has 'repo' scope (not just 'public_repo'). ""
                    f""Error: {branch_error}""
                )
            elif ""already exists"" in error_msg:
                detailed_error = f""Branch '{pr_branch}' already exists. Please try again or use a different task.""
            else:
                detailed_error = f""Failed to create branch '{pr_branch}': {branch_error}""
                
            return jsonify({'error': detailed_error}), 403
        
        # Apply the patch by creating/updating files
        logger.info(f""ðŸ“¦ Applying patch with {len(task.get('changed_files', []))} changed files..."")
        
        # For now, we'll use a simple approach to apply changes
        # In a real implementation, you'd want a more sophisticated patch parser
        patch_content = task['git_patch']
        files_updated = []
        
        # Simple file update based on changed_files list
        for file_path in task.get('changed_files', []):
            try:
                # This is a simplified approach - in reality you'd parse the patch properly
                logger.info(f""ðŸ“ Updating file: {file_path}"")
                files_updated.append(file_path)
            except Exception as e:
                logger.warning(f""Failed to update {file_path}: {e}"")
        
        # Create pull request
        pr = repo.create_pull(
            title=pr_title,
            body=pr_body,
            head=pr_branch,
            base=base_branch
        )
        
        # Update task with PR information
        DatabaseOperations.update_task(task_id, user_id, {
            'pr_branch': pr_branch,
            'pr_number': pr.number,
            'pr_url': pr.html_url
        })
        
        logger.info(f""ðŸŽ‰ Created PR #{pr.number}: {pr.html_url}"")
        
        return jsonify({
            'status': 'success',
            'pr_url': pr.html_url,
            'pr_number': pr.number,
            'branch': pr_branch,
            'files_updated': len(files_updated)
        })
        
    except Exception as e:
        logger.error(f""Error creating PR: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,
survived,"def merge_metadata(metadata_list: List[Dict]) -> Dict:
    """"""
    Merge multiple metadata dictionaries, deduplicating entries.
    
    Args:
        metadata_list: List of metadata dictionaries to merge
        
    Returns:
        Merged metadata dictionary with deduplicated entries
    """"""
    merged = {
        ""reject"": set(),
        ""remove"": set(),
        ""replace"": {},
        ""regexReject"": set(),
        ""regexRemove"": set(),
        ""regexReplace"": {}
    }
    
    for metadata in metadata_list:
        # Add list entries to sets (automatic deduplication)
        merged[""reject""].update(metadata.get(""reject"", []))
        merged[""remove""].update(metadata.get(""remove"", []))
        merged[""regexReject""].update(metadata.get(""regexReject"", []))
        merged[""regexRemove""].update(metadata.get(""regexRemove"", []))
        
        # Merge replace dictionaries
        merged[""replace""].update(metadata.get(""replace"", {}))
        merged[""regexReplace""].update(metadata.get(""regexReplace"", {}))
    
    # Convert sets back to sorted lists for consistent output
    result = {
        ""reject"": sorted(list(merged[""reject""])),
        ""remove"": sorted(list(merged[""remove""])),
        ""replace"": dict(sorted(merged[""replace""].items())),
        ""regexReject"": sorted(list(merged[""regexReject""])),
        ""regexRemove"": sorted(list(merged[""regexRemove""])),
        ""regexReplace"": dict(sorted(merged[""regexReplace""].items()))
    }
    
    return result
",combine_metadata.py,
deleted,"    def test_mcp_server_with_all_transports(self, mock_run_mcp, mock_create_mcp, runner, temp_python_script):
        """"""Test MCP server creation with different transport types.""""""
        transports = [
            (""stdio"", {""transport"": ""stdio"", ""host"": ""127.0.0.1"", ""port"": 8000}),
            (""sse"", {""transport"": ""sse"", ""host"": ""127.0.0.1"", ""port"": 8000}),
            (""websocket"", {""transport"": ""websocket"", ""host"": ""127.0.0.1"", ""port"": 8000}),
        ]
        
        for transport, expected_args in transports:
            # Reset mocks
            mock_create_mcp.reset_mock()
            mock_run_mcp.reset_mock()
            
            # Mock server and interrupt
            mock_mcp_server = MagicMock()
            mock_create_mcp.return_value = mock_mcp_server
            mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--mcp-transport"", transport,
                ""--verbose""
            ])
            
            # Verify correct transport was used
            mock_run_mcp.assert_called_once()
            run_args = mock_run_mcp.call_args
            assert run_args[1][""transport""] == expected_args[""transport""]
            assert run_args[1][""host""] == expected_args[""host""]
            assert run_args[1][""port""] == expected_args[""port""]
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"    def test_mcp_server_resources(self, mock_fastmcp, integration_graphs_and_metas):
        """"""Test MCP server resource functionality.""""""
        graphs, metas = integration_graphs_and_metas
        mock_mcp_instance = MagicMock()
        
        # Track registered resources
        registered_resources = []
        
        def mock_resource_decorator(uri):
            def decorator(func):
                registered_resources.append((uri, func))
                return func
            return decorator
        
        mock_mcp_instance.resource.side_effect = mock_resource_decorator
        mock_fastmcp.return_value = mock_mcp_instance

        # Create the server
        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Resource Test Server""
        )

        # Verify resources were registered
        assert len(registered_resources) >= 3  # flows, info, schema resources

        # Test the flow list resource
        flows_resource = None
        for uri, func in registered_resources:
            if uri == ""flow://flows"":
                flows_resource = func
                break
        
        assert flows_resource is not None
        
        # Execute the flows resource
        flows_data = flows_resource()
        flows_json = json.loads(flows_data)
        
        assert isinstance(flows_json, list)
        assert len(flows_json) == len(graphs)
        
        # Check flow info structure
        for flow_info in flows_json:
            assert ""id"" in flow_info
            assert ""title"" in flow_info
            assert flow_info[""id""] in graphs
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
survived,"    def test_mcp_mode_help_output(self, runner):
        """"""Test that MCP options appear in serve command help.""""""
        result = runner.invoke(app, [""serve"", ""--help""])
        assert result.exit_code == 0
        assert ""--mcp/--no-mcp"" in result.output
        assert ""--mcp-transport"" in result.output
        assert ""--mcp-name"" in result.output
        assert ""MCP (Model Context Protocol)"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"            def decorator(func):
                registered_resources.append((uri, func))
                return func
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
survived,"    def test_create_mcp_server_basic(self, mock_fastmcp, sample_graphs_and_metas):
        """"""Test basic MCP server creation.""""""
        graphs, metas = sample_graphs_and_metas
        mock_mcp_instance = MagicMock()
        mock_fastmcp.return_value = mock_mcp_instance

        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Test MCP Server""
        )

        # Verify FastMCP was called with correct name
        mock_fastmcp.assert_called_once_with(""Test MCP Server"")
        assert server == mock_mcp_instance

        # Verify tools were registered (one for each flow)
        assert mock_mcp_instance.tool.call_count == len(graphs)

        # Verify resources were registered
        assert mock_mcp_instance.resource.call_count >= 3  # At least 3 resources

        # Verify prompts were registered
        assert mock_mcp_instance.prompt.call_count >= 2  # At least 2 prompts
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerCreation
survived,"	def type_with_custom_actions_no_thinking(custom_actions: type[ActionModel]) -> type[AgentOutput]:
		""""""Extend actions with custom actions and exclude thinking field""""""
		
		# Create a base model without thinking
		model_ = create_model(
			'AgentOutputNoThinking',
			evaluation_previous_goal=(str, Field(..., description='One-sentence analysis of your last action. Clearly state success, failure, or uncertain.')),
			memory=(str, Field(..., description='1-3 sentences of specific memory of this step and overall progress.')),
			next_goal=(str, Field(..., description='State the next immediate goals and actions to achieve it, in one clear sentence.')),
			action=(
				list[custom_actions],
				Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
			),
			__module__=AgentOutput.__module__,
			__config__=AgentOutput.model_config,
		)
		
		# Add the current_state property
		def current_state_property(self) -> AgentBrain:
			""""""For backward compatibility - returns an AgentBrain with the flattened properties""""""
			return AgentBrain(
				thinking=None,
				evaluation_previous_goal=self.evaluation_previous_goal,
				memory=self.memory,
				next_goal=self.next_goal,
			)
		
		model_.current_state = property(current_state_property)
		model_.__doc__ = 'AgentOutput model with custom actions and no thinking field'
		return model_
",browser_use/agent/views.py,AgentOutput
survived,"    def test_env_group_with_custom_names(self, mock_openai_client):
        """"""Test EnvGroup with custom environment names.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q2""], ""answer"": [""a2""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1, env2], env_names=[""math"", ""code""])
        
        assert env_group.env_names == [""math"", ""code""]
        assert env_group.env_map[""math""] == env1
        assert env_group.env_map[""code""] == env2
",tests/test_env_group.py,TestEnvGroup
survived,"            def is_completed(self, messages, state, **kwargs):
                return ""DONE"" in messages
",tests/test_multiturn_env.py,TestMultiTurnEnv.CompletionMultiTurnEnv
survived,"    def test_make_dataset(self, mock_openai_client, sample_dataset):
        """"""Test creating a dataset from evaluation results.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        results = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""completion"": [[{""role"": ""assistant"", ""content"": ""Hi""}]],
            ""answer"": [""Hi""],
            ""reward"": [1.0],
            ""task"": [""default""],
            ""state"": [{""custom_field"": ""value""}]
        }
        
        dataset = env.make_dataset(results, state_columns=[""custom_field""])
        
        assert len(dataset) == 1
        assert ""prompt"" in dataset.column_names
        assert ""completion"" in dataset.column_names
        assert ""answer"" in dataset.column_names
        assert ""reward"" in dataset.column_names
        assert ""task"" in dataset.column_names
        assert ""custom_field"" in dataset.column_names",tests/test_environment.py,TestEnvironmentBase
survived,"def log_trajectory_to_langfuse(
    langfuse: Langfuse,
    traj: art.Trajectory,
    task_idx: int,
    step: int,
    phase: str = ""train""
) -> None:
    """"""
    Push one trajectory to Langfuse with task_idx and step for comparison.
    """"""
    trace_name = f""rl-{phase}-step-{step}-task-{task_idx}""
    
    # Create trace with trajectory data
    trace = langfuse.trace(
        name=trace_name,
        input={
            ""task_idx"": task_idx,
            ""step"": step,
            ""phase"": phase,
            ""metadata"": traj.metadata
        },
        output={
            ""messages"": [
                {""role"": choice.role, ""content"": choice.content} 
                for msg_and_choice in traj.messages_and_choices 
                for choice in msg_and_choice.choices
            ] if traj.messages_and_choices else [],
            ""reward"": traj.reward,
            ""metadata"": traj.metadata
        },
        metadata={
            ""task_idx"": task_idx,
            ""training_step"": step,
            ""phase"": phase,
            ""env"": traj.metadata.get(""env"", ""unknown"")
        }
    )
    
    # Add reward as a score
    trace.score(name=""reward"", value=traj.reward)
    
    # Add step as a score for easy filtering
    trace.score(name=""training_step"", value=step)
",dev/tau-bench/run_rl.py,
survived,"async def run_sequence_demo():
    """"""Run a demo sequence showing chunk-like streaming""""""
    
    # Define our automation sequence
    sequence = [
        {
            ""tool_name"": ""get_applications"",
            ""arguments"": {}
        },
        {
            ""tool_name"": ""delay"",
            ""arguments"": {""delay_ms"": 1000}
        },
        {
            ""tool_name"": ""capture_screen"",
            ""arguments"": {}
        },
        {
            ""tool_name"": ""get_clipboard"",
            ""arguments"": {}
        },
        {
            ""tool_name"": ""open_application"",
            ""arguments"": {""app_name"": ""notepad""}
        },
        {
            ""tool_name"": ""delay"", 
            ""arguments"": {""delay_ms"": 2000}
        },
        {
            ""tool_name"": ""type_into_element"",
            ""arguments"": {
                ""selector"": ""role:document"",
                ""text_to_type"": ""Hello from chunk streaming demo!""
            }
        }
    ]
    
    # Convert sequence to the format expected by execute_sequence
    items = []
    for tool in sequence:
        items.append({
            ""tool_name"": tool[""tool_name""],
            ""arguments"": tool.get(""arguments"", {}),
            ""continue_on_error"": tool.get(""continue_on_error"", False),
            ""delay_ms"": tool.get(""delay_ms"", 0)
        })
    
    # Connect to MCP server
    server_params = StdioServerParameters(
        command=""terminator-mcp-agent"",
        args=[],
        env=None
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            
            print(""Connected to terminator-mcp-agent"")
            print(""\n"" + ""=""*60)
            print(""ENHANCED SEQUENCE EXECUTION WITH DETAILED TRACKING"")
            print(""=""*60)
            
            # Execute the sequence
            print(""\nExecuting sequence..."")
            result = await session.call_tool(
                ""execute_sequence"",
                arguments={
                    ""items"": items,
                    ""stop_on_error"": True,
                    ""include_detailed_results"": True
                }
            )
            
            # Parse the result
            if result.content and len(result.content) > 0:
                data = json.loads(result.content[0].text)
                
                # Display execution plan
                print(""\nðŸ“‹ EXECUTION PLAN:"")
                print(""-"" * 50)
                plan = data.get(""execution_plan"", {})
                print(f""Total steps to execute: {plan.get('total_steps', 0)}"")
                for step in plan.get(""steps"", []):
                    print(f""  Step {step['step']}: {step['tool_name']} - {step['description']}"")
                
                # Display step results as they would appear in a stream
                print(""\nðŸš€ EXECUTING STEPS:"")
                print(""-"" * 50)
                
                step_results = data.get(""step_results"", [])
                for step_result in step_results:
                    print_step_info(step_result)
                    
                    # Show a sample of the result content if available
                    if ""result"" in step_result and ""result"" in step_result[""result""]:
                        result_data = step_result[""result""][""result""]
                        if isinstance(result_data, dict) and ""content"" in result_data:
                            content = result_data[""content""]
                            if isinstance(content, list) and len(content) > 0:
                                # Show truncated content for readability
                                content_str = str(content[0])
                                if len(content_str) > 100:
                                    content_str = content_str[:100] + ""...""
                                print(f""  Result: {content_str}"")
                
                # Display execution summary
                print(""\nðŸ“Š EXECUTION SUMMARY:"")
                print(""-"" * 50)
                summary = data.get(""execution_summary"", {})
                print(f""Total steps planned:    {summary.get('total_steps', 0)}"")
                print(f""Steps executed:         {summary.get('executed_steps', 0)}"")
                print(f""Successful steps:       {summary.get('successful_steps', 0)}"")
                print(f""Failed steps:           {summary.get('failed_steps', 0)}"")
                print(f""Total duration:         {summary.get('total_duration_ms', 0)}ms"")
                print(f""Started at:             {format_timestamp(summary.get('started_at', ''))}"")
                print(f""Completed at:           {format_timestamp(summary.get('completed_at', ''))}"")
                
                # Final status
                print(f""\nðŸ FINAL STATUS: {data.get('status', 'unknown').upper()}"")
                
            else:
                print(""No result received from execute_sequence"")
",examples/python_mcp_chunk_stream.py,
survived,"    async def test_create_project(
        self,
        db: DbSessionFactory,
        gql_client: AsyncGraphQLClient,
    ) -> None:
        """"""Test the create_project mutation.""""""
        project_name = token_hex(8)
        project_description = ""Test project description""
        gradient_start_color = ""#ff0000""
        gradient_end_color = ""#00ff00""

        mutation = """"""
            mutation CreateProject($input: CreateProjectInput!) {
                createProject(input: $input) {
                    project {
                        id
                        name
                        gradientStartColor
                        gradientEndColor
                    }
                    query {
                        __typename
                    }
                }
            }
        """"""

        result = await gql_client.execute(
            mutation,
            variable_values={
                ""input"": {
                    ""name"": project_name,
                    ""description"": project_description,
                    ""gradientStartColor"": gradient_start_color,
                    ""gradientEndColor"": gradient_end_color,
                }
            },
        )

        assert result.errors is None
        assert result.data is not None
        create_project_data = result.data[""createProject""]
        
        project_data = create_project_data[""project""]
        assert project_data[""name""] == project_name
        assert project_data[""gradientStartColor""] == gradient_start_color
        assert project_data[""gradientEndColor""] == gradient_end_color
        
        # Verify the project was actually created in the database
        project_id = project_data[""id""]
        decoded_id = GlobalID.from_id(project_id)
        
        async with db() as session:
            project = await session.get(models.Project, int(decoded_id.node_id))
            assert project is not None
            assert project.name == project_name
            assert project.description == project_description
            assert project.gradient_start_color == gradient_start_color
            assert project.gradient_end_color == gradient_end_color
",tests/unit/server/api/mutations/test_project_mutations.py,TestProjectMutations
survived,"    def test_rubric_group_mixed_rubric_types(self):
        """"""Test RubricGroup with different types of rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        # Create rubrics with different configurations
        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.3], custom_attr=""test"")
        
        group = RubricGroup(rubrics=[rubric1, rubric2])
        
        # Should aggregate functions and weights correctly
        assert group.get_reward_func_names() == [""func1"", ""func2""]
        assert group.get_reward_weights() == [1.0, 0.3]
",tests/test_rubric_group.py,TestRubricGroup
survived,"        def func1(completion, **kwargs):
            return 1.0
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_get_format_reward_func(self, basic_parser):
        """"""Test that format reward function returns 1.0 by default.""""""
        reward_func = basic_parser.get_format_reward_func()
        completion = [{""role"": ""assistant"", ""content"": ""test""}]
        reward = reward_func(completion)
        assert reward == 1.0",tests/test_parser.py,TestParser
survived,"    def test_get_format_str(self, xml_parser):
        """"""Test format string generation.""""""
        format_str = xml_parser.get_format_str()
        assert ""<reasoning>"" in format_str
        assert ""</reasoning>"" in format_str
        assert ""<answer>"" in format_str
        assert ""</answer>"" in format_str
",tests/test_xml_parser.py,TestXMLParser
survived,"    def test_rubric_initialization_with_kwargs(self):
        """"""Test Rubric initialization with additional kwargs.""""""
        rubric = Rubric(custom_param=""test_value"", another_param=42)
        
        assert rubric.custom_param == ""test_value""
        assert rubric.another_param == 42
",tests/test_rubric.py,TestRubric
survived,"    def test_add_reward_func_default_weight(self):
        """"""Test adding reward function with default weight.""""""
        rubric = Rubric(funcs=[], weights=[])
        
        def test_func(completion, **kwargs):
            return 1.0
        
        rubric.add_reward_func(test_func)
        
        assert rubric.reward_weights == [1.0]
",tests/test_rubric.py,TestRubric
survived,"    async def test_score_rollout_with_list_completion(self):
        """"""Test scoring rollout with list-type completion.""""""
        def list_func(completion, **kwargs):
            return len(completion) if isinstance(completion, list) else 0.0
        
        rubric = Rubric(funcs=[list_func])
        
        completion = [
            {""role"": ""user"", ""content"": ""Hello""},
            {""role"": ""assistant"", ""content"": ""Hi there!""}
        ]
        
        result = await rubric.score_rollout(
            prompt=""test"",
            completion=completion,
            answer=""test"",
            state={},
            task=""test"",
            info={}
        )
        
        assert result[""list_func""] == 2.0  # Length of completion list
        assert result[""reward""] == 2.0
",tests/test_rubric.py,TestRubric
survived,"    def test_get_assistant_messages(self, basic_parser):
        """"""Test extraction of assistant messages from completion.""""""
        completion = [
            {""role"": ""user"", ""content"": ""Hello""},
            {""role"": ""assistant"", ""content"": ""Hi there""},
            {""role"": ""user"", ""content"": ""How are you?""},
            {""role"": ""assistant"", ""content"": ""I'm doing well""}
        ]
        assistant_messages = basic_parser.get_assistant_messages(completion)
        assert len(assistant_messages) == 2
        assert assistant_messages[0][""content""] == ""Hi there""
        assert assistant_messages[1][""content""] == ""I'm doing well""
",tests/test_parser.py,TestParser
survived,"    def test_environment_initialization(self, mock_openai_client, sample_dataset):
        """"""Test that Environment initializes correctly.""""""
        env = TestEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        assert env.client == mock_openai_client
        assert env.model == ""test-model""
        assert env.message_type == 'chat'
        assert isinstance(env.parser, Parser)
        assert isinstance(env.rubric, Rubric)
",tests/test_environment.py,TestEnvironmentBase
survived,"    def test_rubric_group_score_rollouts_empty_data(self):
        """"""Test scoring empty rollouts.""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_rubric_group_get_reward_weights(self):
        """"""Test getting aggregated reward weights from all rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        def func3(completion, **kwargs):
            return 0.3
        
        rubric1 = Rubric(funcs=[func1, func2], weights=[1.0, 0.7])
        rubric2 = Rubric(funcs=[func3], weights=[0.8])
        
        group = RubricGroup(rubrics=[rubric1, rubric2])
        weights = group.get_reward_weights()
        
        assert weights == [1.0, 0.7, 0.8]
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_parser_initialization(self, basic_parser):
        """"""Test that Parser initializes correctly.""""""
        assert isinstance(basic_parser, Parser)
        assert hasattr(basic_parser, 'logger')
",tests/test_parser.py,TestParser
survived,"    async def test_task_and_info_parameters(self, mock_multiturn_env):
        """"""Test rollout with task and info parameters.""""""
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Task question""}],
            response=""Task DONE""
        )
        
        prompt = [{""role"": ""user"", ""content"": ""Task question""}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=""task_answer"",
            task=""math"",
            info={""difficulty"": ""hard""}
        )
        
        assert len(completion) >= 1
        assert state[""answer""] == ""task_answer""
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def __init__(self, completion_condition=""answer"", **kwargs):
        super().__init__(**kwargs)
        self.completion_condition = completion_condition  # ""answer"", ""max_turns"", ""error""
        self.env_response_count = 0
",tests/conftest.py,SimpleMultiTurnEnv
survived,"    def setup(self, mock_config):
        """"""Set up transformer for each test.""""""
        self.transformer = PkgxTransformer(mock_config, None)
",tests/package_managers/crates/test_special_case.py,TestSpecialCase
survived,"    def create_formula(
        formula_name,
        dependencies=None,
        build_dependencies=None,
        test_dependencies=None,
        recommended_dependencies=None,
        optional_dependencies=None,
    ):
        return Actual(
            formula=formula_name,
            description=""Test formula"",
            license=""MIT"",
            homepage="""",
            source="""",
            repository="""",
            dependencies=dependencies or [],
            build_dependencies=build_dependencies or [],
            test_dependencies=test_dependencies or [],
            recommended_dependencies=recommended_dependencies or [],
            optional_dependencies=optional_dependencies or [],
        )
",tests/package_managers/homebrew/test_diff_dep.py,
survived,"    def capture_ingest(new_canons, new_canon_packages, updated_canon_packages):
        ingest_calls.append((new_canons, new_canon_packages, updated_canon_packages))
",tests/ranker/test_dedupe.py,
survived,"def mock_dedupe_config(ids):
    """"""Fixture providing mock DedupeConfig.""""""
    config = MagicMock(spec=DedupeConfig)
    config.load = True
    config.homepage_url_type_id = ids[""homepage_url_type""]
    return config
",tests/ranker/test_dedupe.py,
survived,"def mock_dependency_types():
    """"""
    Mock dependency types for testing.

    Returns a mock DependencyTypes object with common dependency types.
    """"""
    dep_types = MagicMock(spec=DependencyTypes)

    # Set up dependency type attributes directly
    dep_types.runtime = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000010""))
    dep_types.build = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000011""))
    dep_types.dev = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000012""))
    dep_types.test = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000013""))
    dep_types.development = dep_types.dev  # Alias for development
    dep_types.recommended = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000014""))
    dep_types.optional = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000015""))

    return dep_types
",tests/conftest.py,
survived,"            def track_add(obj):
                if hasattr(obj, ""url""):  # It's a URL object
                    urls_added.append(obj)
                else:  # It's a PackageURL object
                    package_urls_added.append(obj)
",tests/package_managers/pkgx/test_pkgx_load_urls.py,TestPkgxLoader
survived,"    def create_diff(package_map, dependencies=None, url_map=None, package_urls=None):
        cache = Cache(
            package_map=package_map,
            url_map=url_map or {},
            package_urls=package_urls or {},
            dependencies=dependencies or {},
        )
        return Diff(mock_config, cache)
",tests/package_managers/homebrew/test_diff_dep.py,
survived,"    def test_parse_package_data(self):
        """"""Test parsing a typical package entry from Packages file.""""""
        # Sample package data from a Packages file
        package_data = """"""Package: 0ad
Version: 0.0.26-1
Installed-Size: 19162
Maintainer: Debian Games Team <pkg-games-devel@lists.alioth.debian.org>
Architecture: amd64
Depends: 0ad-data (>= 0.0.26), 0ad-data-common (>= 0.0.26), libc6 (>= 2.29), libcurl4 (>= 7.16.2), libenet7 (>= 1.3.13), libgloox18, libjsoncpp25 (>= 1.9.5), libminiupnpc17 (>= 1.9.20140610), libnspr4 (>= 2:4.9.2), libnss3 (>= 2:3.22)
Recommends: fonts-freefont-ttf, fonts-texgyre
Suggests: 0ad-dbg
Description: Real-time strategy game of ancient warfare
Homepage: https://play0ad.com/
Section: games
Priority: optional
Filename: pool/main/0/0ad/0ad_0.0.26-1_amd64.deb
Size: 6050744
MD5sum: a777ddf01c18dbdef15c589f8325d7a3
SHA256: 9da19833c1a51e890aa8a11f82ec1e383c0e79410c3d2f6845fd2ec3e23249b8


""""""
        # Parse the package data
        parser = DebianParser(package_data)
        packages = list(parser.parse())

        # Validate we have one package
        assert len(packages) == 1
        package = packages[0]

        # Test basic fields
        assert package.package == ""0ad""
        assert package.version == ""0.0.26-1""
        assert package.installed_size == 19162
        assert package.architecture == ""amd64""

        # Test maintainer parsing
        assert package.maintainer.name == ""Debian Games Team""
        assert package.maintainer.email == ""pkg-games-devel@lists.alioth.debian.org""

        # Test dependency parsing
        assert len(package.depends) == 10
        assert package.depends[0].package == ""0ad-data""
        assert package.depends[0].semver == "">= 0.0.26""

        # Test recommends parsing
        assert len(package.recommends) == 2
        assert package.recommends[0].package == ""fonts-freefont-ttf""

        # Test suggests parsing
        assert len(package.suggests) == 1
        assert package.suggests[0].package == ""0ad-dbg""
",tests/package_managers/debian/test_debian_parser.py,TestDebianParser
survived,"def mock_user_types():
    """"""
    Mock user types for testing.

    Returns a mock UserTypes object.
    """"""
    user_types = MagicMock(spec=UserTypes)

    # Set up user type attributes directly
    user_types.admin = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000040""))
    user_types.maintainer = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000041""))
    user_types.contributor = Mock(id=uuid.UUID(""00000000-0000-0000-0000-000000000042""))

    return user_types
",tests/conftest.py,
survived,"    def test_skip_when_load_disabled(self, mock_dedupe_config, mock_db):
        """"""
        Test that no processing occurs when load is disabled

        Expected: db.ingest should not be called
        """"""
        # Arrange
        mock_dedupe_config.load = False

        # Act
        with patch.dict(""os.environ"", {""LOAD"": ""false"", ""TEST"": ""false""}):
            main(mock_dedupe_config, mock_db)

        # Assert
        mock_db.ingest.assert_not_called()",tests/ranker/test_dedupe.py,TestDedupe
survived,"def map_sample_data_with_extra_keys():
    return [
        {
            ""text"": ""This is a positive sentence."",
            ""original_sentiment"": ""positive"",
            ""to_be_dropped"": ""extra"",
        },
        {
            ""text"": ""This is a negative sentence."",
            ""original_sentiment"": ""negative"",
            ""to_be_dropped"": ""extra"",
        },
        {
            ""text"": ""This is a neutral sentence."",
            ""original_sentiment"": ""neutral"",
            ""to_be_dropped"": ""extra"",
        },
    ]
",tests/basic/test_basic_map.py,
survived,"    def test_go_simple(self):
        # should match function name exactly or struct.functionName
        group_id_1 = [
            self._create_event(
                function_names=[""handler.planet"", ""service.blue""],
                filenames=[""baz.go"", ""foo.go""],
                user_id=str(i),
            )
            for i in range(7)
        ][0].group.id
        group_id_2 = [
            self._create_event(
                function_names=[""service.blue"", ""world""],
                filenames=[""foo.go"", ""baz.go""],
                user_id=str(i),
            )
            for i in range(6)
        ][0].group.id
        top_5_issues = self.open_pr_comment_workflow.get_top_5_issues_by_count_for_file(
            projects=[self.project], sentry_filenames=[""baz.go""], function_names=[""world"", ""planet""]
        )
        top_5_issue_ids = [issue[""group_id""] for issue in top_5_issues]
        function_names = [issue[""function_name""] for issue in top_5_issues]
        assert top_5_issue_ids == [group_id_1, group_id_2]
        assert function_names == [""handler.planet"", ""world""]
",tests/sentry/integrations/github/tasks/test_open_pr_comment.py,TestGetCommentIssues
survived,"    def test_send_plain_text_email(self, mock_smtp_class, smtp_provider):
        """"""Test sending a plain text email.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send plain text email
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            body=""This is a plain text email"",
        )

        # Verify SMTP was called correctly
        mock_smtp_class.assert_called_once_with(""smtp.example.com"", 587)
        mock_smtp.starttls.assert_called_once()
        mock_smtp.login.assert_called_once_with(""test@example.com"", ""testpassword"")
        
        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        assert call_args[0][0] == ""sender@example.com""
        assert call_args[0][1] == ""recipient@example.com""
        
        # Verify the email content contains plain text
        email_content = call_args[0][2]
        assert ""Content-Type: text/plain"" in email_content
        assert ""This is a plain text email"" in email_content
        
        # Verify return value
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test Subject"",
            ""body"": ""This is a plain text email"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def test_empty_from_name(self, mock_smtp_class, smtp_provider):
        """"""Test sending email with empty from_name.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send email with empty from_name
        smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name="""",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            html=""<p>Test</p>"",
        )

        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        
        # Verify the From header contains only email
        email_content = call_args[0][2]
        assert ""From: sender@example.com"" in email_content
        assert ""Test Sender"" not in email_content
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"def test_numpy_arrays():
    import numpy as np

    serializer = EventSerializer()

    # Test 1D array
    arr_1d = np.array([1, 2, 3])
    assert json.loads(serializer.encode(arr_1d)) == [1, 2, 3]

    # Test 2D array
    arr_2d = np.array([[1, 2], [3, 4]])
    assert json.loads(serializer.encode(arr_2d)) == [[1, 2], [3, 4]]

    # Test float array
    arr_float = np.array([1.1, 2.2, 3.3])
    assert json.loads(serializer.encode(arr_float)) == [1.1, 2.2, 3.3]

    # Test empty array
    arr_empty = np.array([])
    assert json.loads(serializer.encode(arr_empty)) == []

    # Test mixed types that numpy can handle
    arr_mixed = np.array([1, 2.5, 3])
    assert json.loads(serializer.encode(arr_mixed)) == [1.0, 2.5, 3.0]",tests/test_serializer.py,
survived,"    def reset_snapshot(self, storage: str) -> bool:
        """"""
        é‡ç½®å¿«ç…§ï¼Œå¼ºåˆ¶ä¸‹æ¬¡æ‰«ææ—¶é‡æ–°å»ºç«‹åŸºå‡†
        :param storage: å­˜å‚¨åç§°
        :return: æ˜¯å¦æˆåŠŸ
        """"""
        try:
            cache_file = self._snapshot_cache_dir / f""{storage}_snapshot.json""
            if cache_file.exists():
                cache_file.unlink()
                logger.info(f""å¿«ç…§å·²é‡ç½®: {storage}"")
                return True
            logger.debug(f""å¿«ç…§æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€é‡ç½®: {storage}"")
            return True
        except Exception as e:
            logger.error(f""é‡ç½®å¿«ç…§å¤±è´¥: {storage} - {e}"")
            return False
",app/monitor.py,Monitor
survived,"def start_session(
    session_mode=""application"",  # type: str
):
    # type: (...) -> None
    return get_isolation_scope().start_session(session_mode=session_mode)
",sentry_sdk/api.py,
survived,"    async def test_delete_api_key_not_found(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test deleting a non-existent API key.""""""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock user authentication
        mock_user_dep.return_value = Mock(user_id=""user123"")

        # Mock key not found
        mock_api_keys_service.delete_key.return_value = False

        response = await test_api_client.delete(""/_/api/keys/nonexistent_key"")

        assert response.status_code == 404
        assert response.json() == {""detail"": ""API key not found""}
        mock_api_keys_service.delete_key.assert_called_once_with(""nonexistent_key"")
",api/api/routers/api_keys_test.py,TestDeleteAPIKey
survived,"    async def test_delete_api_key_with_user_auth_success(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test successful deletion with user authentication.""""""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock user authentication
        mock_user_dep.return_value = Mock(user_id=""user123"")

        # Mock successful deletion
        mock_api_keys_service.delete_key.return_value = True

        response = await test_api_client.delete(""/_/api/keys/test_key_id"")

        assert response.status_code == 204
        mock_api_keys_service.delete_key.assert_called_once_with(""test_key_id"")",api/api/routers/api_keys_test.py,TestDeleteAPIKey
survived,"    def to_dict(self) -> Dict:
        """"""Convert result to dictionary.""""""
        return {
            ""name"": self.name,
            ""concurrency"": self.concurrency,
            ""requests"": self.requests,
            ""duration"": self.duration,
            ""rps"": self.rps,
            ""avg_latency"": self.avg_latency,
            ""median_latency"": self.median_latency,
            ""min_latency"": self.min_latency,
            ""max_latency"": self.max_latency,
            ""stdev_latency"": self.stdev_latency,
            ""errors"": self.errors
        }
",benchmarks/benchmark.py,BenchmarkResult
survived,"    async def get_nft_sales(self, parameters: dict) -> list:
        """"""Get recent NFT sales for a collection from OpenSea""""""
        async with aiohttp.ClientSession() as session:
            url = f""{self.base_url}/events/collection/{parameters['collectionSlug']}?event_type=sale&limit=5""
            headers = {
                ""accept"": ""application/json"",
                ""x-api-key"": self.api_key
            }
            async with session.get(url, headers=headers) as response:
                if not response.ok:
                    raise Exception(f""Failed to get NFT sales: HTTP {response.status} - {await response.text()}"")
                data = await response.json()
                sales_response = NftSalesResponse.model_validate(data)
                
                # Transform the response to match TypeScript implementation
                return [{
                    ""name"": event.nft.name,
                    ""seller"": event.seller,
                    ""buyer"": event.buyer,
                    ""price"": float(event.payment.quantity) / 10 ** event.payment.decimals
                } for event in sales_response.asset_events]",python/src/plugins/opensea/goat_plugins/opensea/service.py,OpenSeaService
survived,"    def __init__(self, options: OpenSeaPluginOptions):
        super().__init__(""opensea"", [OpenSeaService(options.api_key)])
",python/src/plugins/opensea/goat_plugins/opensea/__init__.py,OpenSeaPlugin
survived,"    def __init__(self, options: AlloraPluginOptions):
        super().__init__(""allora"", [AlloraService(options.api_key, options.api_root)])
",python/src/plugins/allora/goat_plugins/allora/__init__.py,AlloraPlugin
survived,"    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = ""https://api.nansen.ai/v1""
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"def test_parsing_equality(directory):
    """"""Test that libmelee and peppi parse SLP files the same way.""""""
    files = utils.traverse_slp_files(directory)
    print(f""Found {len(files)} .slp files to test"")
    
    results = {
        ""passed"": 0,
        ""failed"": 0,
        ""errors"": []
    }
    
    for i, file in enumerate(files):
        print(f""Testing file {i+1}/{len(files)}: {file.name}"")
        try:
            with file.extract("""") as path:
                preprocessing.assert_same_parse(path)
                results[""passed""] += 1
        except AssertionError as e:
            print(f""  FAIL: {e}"")
            results[""failed""] += 1
            results[""errors""].append((file.name, str(e)))
        except Exception as e:
            print(f""  ERROR: {e}"")
            results[""failed""] += 1
            results[""errors""].append((file.name, str(e)))
    
    return results
",tests/replay_parser_test.py,
survived,"    def test_proxy_environment_variables_set(self):
        """"""Test that proxy configuration sets the correct environment variables""""""
        http_proxy_config = {
            ""proxy_url"": ""http://proxy.test.com:8080"",
            ""proxy_ca_certificate"": ""-----BEGIN CERTIFICATE-----\ntest\n-----END CERTIFICATE-----""
        }
        
        with patch.dict('os.environ', {}, clear=True), \
             patch('source_file.proxy._install_ca_certificate') as mock_install:
            
            mock_install.return_value = Path(""/tmp/test_cert.pem"")
            
            from source_file.proxy import configure_custom_http_proxy
            
            configure_custom_http_proxy(
                http_proxy_config=http_proxy_config,
                logger=logger
            )
            
            assert os.environ.get(""HTTP_PROXY"") == ""http://proxy.test.com:8080""
            assert os.environ.get(""HTTPS_PROXY"") == ""http://proxy.test.com:8080""
            assert ""NO_PROXY"" in os.environ
            mock_install.assert_called_once_with(""-----BEGIN CERTIFICATE-----\ntest\n-----END CERTIFICATE-----"")",airbyte-integrations/connectors/source-file/unit_tests/test_proxy_certificate_support.py,TestProxyCertificateSupport
deleted,"    def name(self) -> str:
        return ""Connector Version Increment Check""
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionIncrementCheck
deleted,"def version_increment_check():
    return VersionIncrementCheck()
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,
survived,"    def __init__(self, host: str = ""http://localhost:11434"", model_name: str = ""llama3.2""):
        self.client = ollama.AsyncClient(host=host)
        self.model_name = model_name
        self.default_model = model_name
",agent/llm/ollama_client.py,OllamaLLM
survived,"    def __init__(self, cfg, image_size=None, phase=""train""):
        super(PriorBox, self).__init__()
        self.min_sizes = cfg[""min_sizes""]
        self.steps = cfg[""steps""]
        self.clip = cfg[""clip""]
        self.image_size = image_size
        self.feature_maps = [
            [ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)]
            for step in self.steps
        ]
        self.name = ""s""
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,PriorBox
survived,"def download_model():
    import urllib.request
    model_url = ""https://cloud.ovgu.de/s/TewGC9TDLGgKkmS/download/6DRepNet360_Full-Rotation_300W_LP+Panoptic.pth""
    model_path = ""6DRepNet360_Full-Rotation_300W_LP+Panoptic.pth""
    
    if not os.path.exists(model_path):
        print(f""Downloading model from {model_url}"")
        urllib.request.urlretrieve(model_url, model_path)
        print(f""Model downloaded to {model_path}"")
    else:
        print(f""Model already exists at {model_path}"")
    
    return model_path
",face_recognition/6d_repnet_360/convert_to_onnx.py,
survived,"def compute_euler_angles_from_rotation_matrices(rotation_matrices):
    batch = rotation_matrices.shape[0]
    R = rotation_matrices
    sy = np.sqrt(R[:, 0, 0] * R[:, 0, 0] + R[:, 1, 0] * R[:, 1, 0])
    singular = sy < 1e-6

    x = np.arctan2(R[:, 2, 1], R[:, 2, 2])
    y = np.arctan2(-R[:, 2, 0], sy)
    z = np.arctan2(R[:, 1, 0], R[:, 0, 0])

    xs = np.arctan2(-R[:, 1, 2], R[:, 1, 1])
    ys = np.arctan2(-R[:, 2, 0], sy)
    zs = R[:, 1, 0] * 0

    out_euler = np.zeros((batch, 3))
    out_euler[:, 0] = x * (1 - singular) + xs * singular
    out_euler[:, 1] = y * (1 - singular) + ys * singular
    out_euler[:, 2] = z * (1 - singular) + zs * singular

    return out_euler
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,
survived,"    def forward(self):
        anchors = []
        for k, f in enumerate(self.feature_maps):
            min_sizes = self.min_sizes[k]
            for i, j in product(range(f[0]), range(f[1])):
                for min_size in min_sizes:
                    s_kx = min_size / self.image_size[1]
                    s_ky = min_size / self.image_size[0]
                    dense_cx = [
                        x * self.steps[k] / self.image_size[1] for x in [j + 0.5]
                    ]
                    dense_cy = [
                        y * self.steps[k] / self.image_size[0] for y in [i + 0.5]
                    ]
                    for cy, cx in product(dense_cy, dense_cx):
                        anchors += [cx, cy, s_kx, s_ky]

        output = np.array(anchors).reshape(-1, 4)
        if self.clip:
            output.clamp_(max=1, min=0)
        return output
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,PriorBox
survived,"def recognize_from_image():
    env_id = args.env_id
    net = ailia.Net(MODEL_PATH_6DRepNet360, WEIGHT_PATH_6DRepNet360, env_id=env_id)
    face_detect = ailia.Net(MODEL_PATH_FACE, WEIGHT_PATH_FACE, env_id=env_id)
    detector = RetinaFaceOnnx(face_detect)

    for image_path in args.input:
        logger.debug(f'input image: {image_path}')
        results = []
        raw_img = cv2.imread(image_path)
        resize_img = cv2.resize(raw_img, dsize=(640, 480))
        resize_img = np.array(resize_img)
        logger.debug(f'input image shape: {resize_img.shape}')

        logger.info('Start inference...')
        faces = detector(resize_img)
        for box, landmarks, score in faces:
            if score < .95:
                continue
            x_min = int(box[0])
            y_min = int(box[1])
            x_max = int(box[2])
            y_max = int(box[3])
            bbox_width = abs(x_max - x_min)
            bbox_height = abs(y_max - y_min)

            x_min = max(0, x_min - int(0.2 * bbox_height))
            y_min = max(0, y_min - int(0.2 * bbox_width))
            x_max = x_max + int(0.2 * bbox_height)
            y_max = y_max + int(0.2 * bbox_width)

            img = resize_img[y_min:y_max, x_min:x_max]
            img = cv2.resize(img, dsize=(HEIGHT, WIDTH))
            img = utils.transform(img, MEAN, STD)

            img = np.expand_dims(img, 0)
            img = np.array(img, dtype='float32')

            c = cv2.waitKey(1)
            if c == 27:
                break

            start = time.time()

            R_pred = net.run(img)[0]
            end = time.time()
            print('Head pose estimation: %2f ms' % ((end - start) * 1000.))

            euler = utils.compute_euler_angles_from_rotation_matrices(R_pred) * 180 / np.pi
            p_pred_deg = euler[:, 0]
            y_pred_deg = euler[:, 1]
            r_pred_deg = euler[:, 2]
            results.append({'yaw': y_pred_deg, 'pitch': p_pred_deg, 'roll': r_pred_deg})

            utils.plot_pose_cube(resize_img, y_pred_deg, p_pred_deg, r_pred_deg, x_min + int(.5 * (
                    x_max - x_min)), y_min + int(.5 * (y_max - y_min)), size=bbox_width)

        savepath = get_savepath(args.savepath, image_path)
        logger.info(f'saved at : {savepath}')
        resize_img = cv2.cvtColor(resize_img, cv2.COLOR_BGR2RGB)
        Image.fromarray(resize_img).save(savepath)

        if args.write_json:
            json_file = '%s.json' % savepath.rsplit('.', 1)[0]
            utils.save_json_result(json_file, results)

    logger.info('Script finished successfully.')
",face_recognition/6d_repnet_360/6d_repnet_360.py,
survived,"    def _convert_general_to_markdown(self, file_path: str, input_format: InputFormat) -> List[Tuple[str, Dict[str, Any]]]:
        """"""Convert non-PDF formats using general converter.""""""
        print(f""Converting {file_path} ({input_format.name}) to Markdown using docling..."")
        return self._perform_conversion(file_path, self.converter_general, f""({input_format.name})"")
",rag_system/ingestion/document_converter.py,DocumentConverter
survived,"    def _convert_pdf_to_markdown(self, pdf_path: str) -> List[Tuple[str, Dict[str, Any]]]:
        """"""Convert PDF with OCR detection logic.""""""
        # Quick heuristic: if the PDF already contains a text layer, skip OCR for speed
        def _pdf_has_text(path: str) -> bool:
            try:
                doc = fitz.open(path)
                for page in doc:
                    if page.get_text(""text"").strip():
                        return True
            except Exception:
                pass
            return False

        use_ocr = not _pdf_has_text(pdf_path)
        converter = self.converter_ocr if use_ocr else self.converter_no_ocr
        ocr_msg = ""(OCR enabled)"" if use_ocr else ""(no OCR)""

        print(f""Converting {pdf_path} to Markdown using docling {ocr_msg}..."")
        return self._perform_conversion(pdf_path, converter, ocr_msg)
",rag_system/ingestion/document_converter.py,DocumentConverter
survived,"    async def async_no_stream():
        try:
            print(""\nExecuting async_no_stream..."")
            async with asyncio.timeout(30):
                response = await aco.chat(message=""Hello from async no stream"", model=""command"", session=session)
                print(f""async_no_stream completed successfully with response: {response.text}"")
        except asyncio.TimeoutError:
            print(""Warning: async_no_stream timed out"")
            raise
        except Exception as e: 
            print(f""Error in async_no_stream: {str(e)}"")
            raise
",tests/core_manual_tests/providers/cohere_canary.py,
deleted,"                def __iter__(self):
                    self.stream = original_method(self_client, *args, **kwargs_copy)
                    return self
",agentops/llms/providers/cohere.py,CohereProvider.StreamWrapper
survived,"    def sync_stream():
        stream_response = litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass
",tests/core_manual_tests/providers/litellm_canary.py,
survived,"    def sync_stream():
        stream_response = groq_client.chat.completions.create(
            model=""llama3-70b-8192"",
            messages=[
                {""role"": ""user"", ""content"": ""Hello from sync streaming""},
            ],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass
",tests/core_manual_tests/providers/groq_canary.py,
deleted,"            async def handle_coroutine():
                result = await response
                # Create a new LLM event for async response
                async_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                if session is not None:
                    async_event.session_id = session.session_id
                    async_event.agent_id = check_call_stack_for_agent_id()
                    async_event.model = kwargs.get(""model"", ""command-r-plus"")
                    async_event.prompt = kwargs.get(""message"", """")
                    async_event.returns = result
                    logger.info(f""Created new async LLM event with session_id: {session.session_id}"")
                if hasattr(result, ""text""):
                    async_event.completion = {
                        ""role"": ""assistant"",
                        ""content"": result.text
                    }
                    logger.info(f""Set completion for async LLM event: {result.text}"")
                elif hasattr(result, ""chat_history""):
                    async_event.prompt = []
                    role_map = {""USER"": ""user"", ""CHATBOT"": ""assistant"", ""SYSTEM"": ""system""}
                    for i in range(len(result.chat_history) - 1):
                        message = result.chat_history[i]
                        async_event.prompt.append({
                            ""role"": role_map.get(message.role, message.role),
                            ""content"": message.message,
                        })
                    last_message = result.chat_history[-1]
                    async_event.completion = {
                        ""role"": role_map.get(last_message.role, last_message.role),
                        ""content"": last_message.message,
                    }
                    async_event.prompt_tokens = int(result.meta.tokens.input_tokens)
                    async_event.completion_tokens = int(result.meta.tokens.output_tokens)
                    logger.info(f""Set chat history for async LLM event"")
                self._safe_record(session, async_event)
                return result
",agentops/llms/providers/cohere.py,CohereProvider
survived,"    async def run_async_tests():
        print(""\nRunning async tests..."")
        print(""Starting async_no_stream..."")
        await async_no_stream()
        print(""Completed async_no_stream"")
        
        print(""\nStarting first async_stream..."")
        await async_stream(provider, session)
        print(""Completed first async_stream"")
        
        print(""\nStarting second async_stream..."")
        await async_stream(provider, session)  # Run twice to ensure we get all LLM calls
        print(""Completed second async_stream"")
        
        print(""\nStarting third async_stream..."")
        await async_stream(provider, session)  # Run thrice to ensure we get all LLM calls
        print(""Completed third async_stream"")
        
        print(""\nAll async tests completed successfully"")
        
        # End session and verify analytics after all tests
        session.end_session(""Success"")
        analytics = session.get_analytics()
        print(f""\nAnalytics: {analytics}"")
        assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""
",tests/core_manual_tests/providers/cohere_canary.py,
deleted,"    def undo_override(self):
        """"""Restore original Gemini methods.""""""
        if self.original_generate is not None:
            self.client.generate_content = self.original_generate",agentops/llms/providers/gemini.py,GeminiProvider
survived,"    async def swap_tokens(self, wallet_client: SolanaWalletClient, parameters: dict):
        """"""Swap tokens using Jupiter DEX.""""""
        try:
            # First get the quote
            quote_response = await self.get_quote(wallet_client, parameters)
            
            # Prepare swap request
            swap_request = {
                ""userPublicKey"": wallet_client.get_address(),
                ""quoteResponse"": quote_response.dict(),
                ""dynamicComputeUnitLimit"": True,
                ""prioritizationFeeLamports"": ""auto""
            }
            
            # Get swap transaction
            async with aiohttp.ClientSession(**self._session_kwargs) as session:
                async with session.post(f""{self.base_url}/swap"", json={""swapRequest"": swap_request}) as response:
                    if response.status != 200:
                        error_data = await response.json()
                        raise Exception(f""Failed to create swap transaction: {error_data.get('error', 'Unknown error')}"")
                    
                    swap_response = await response.json()
                    swap_transaction = swap_response.get(""swapTransaction"")
                    
                    if not swap_transaction:
                        raise Exception(""No swap transaction returned"")
                    
                    # Deserialize the transaction
                    versioned_transaction = VersionedTransaction.from_bytes(base64.b64decode(swap_transaction))
                    
                    # Get instructions from the transaction
                    instructions = await wallet_client.decompile_versioned_transaction_to_instructions(versioned_transaction)
                    
                    # Send the transaction
                    result = await wallet_client.send_transaction({
                        ""instructions"": instructions,
                        ""address_lookup_table_addresses"": [
                            lookup.account_key.to_base58() 
                            for lookup in versioned_transaction.message.address_table_lookups
                        ]
                    })
                    
                    return {
                        ""hash"": result[""hash""]
                    }
                    
        except Exception as error:
            raise Exception(f""Failed to swap tokens: {error}"")",python/src/plugins/jupiter/goat_plugins/jupiter/service.py,JupiterService
survived,"    async def get_token_info_by_symbol(self, parameters: dict):
        """"""Get token info including mint address, decimals, and name by symbol.""""""
        try:
            token = next(
                (token for token in self.tokens 
                 if token[""symbol""] == parameters[""symbol""] or 
                 token[""symbol""].lower() == parameters[""symbol""].lower()),
                None
            )
            return {
                ""symbol"": token[""symbol""] if token else None,
                ""mintAddress"": token[""mintAddresses""][self.network] if token else None,
                ""decimals"": token[""decimals""] if token else None,
                ""name"": token[""name""] if token else None,
            }
        except Exception as error:
            raise Exception(f""Failed to get token info: {error}"")
",python/src/plugins/spl_token/goat_plugins/spl_token/service.py,SplTokenService
survived,"def test_deprecated_async_warning(mock_isinstance):
    """"""Test that using _async parameter raises a deprecation warning.""""""
    mock_model = MagicMock()
    mock_model.generate_content = MagicMock()
    mock_model.generate_content_async = MagicMock()
    
    with pytest.warns(DeprecationWarning, match=""'_async' is deprecated. Use 'use_async' instead.""):
        client = from_vertexai(
            mock_model, 
            _async=True
        )
",tests/llm/test_vertexai/test_deprecated_async.py,
survived,"def test_both_async_params_error(mock_isinstance):
    """"""Test that providing both _async and use_async raises an error.""""""
    mock_model = MagicMock()
    mock_model.generate_content = MagicMock()
    mock_model.generate_content_async = MagicMock()
    
    with pytest.raises(ConfigurationError, match=""Cannot provide both '_async' and 'use_async'. Use 'use_async' instead.""):
        client = from_vertexai(
            mock_model, 
            _async=True,
            use_async=True
        )",tests/llm/test_vertexai/test_deprecated_async.py,
survived,"    async def _run(self) -> StepResult:
        if self.original_manifest:
            self.manifest_path.write_text(self.original_manifest)

        if self.backup_schema_path:
            copy_directory(self.backup_schema_path, self.schemas_path)

        return StepResult(
            step=self,
            status=StepStatus.SUCCESS,
        )
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RestoreInlineState
survived,"async def migrate_to_inline_schemas(ctx: click.Context, report: bool) -> bool:
    verify_formatters()
    return await run_connector_pipeline(
        ctx,
        ""Migrate to inline schemas"",
        report,
        run_connector_migrate_to_inline_schemas_pipeline,
    )",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/commands.py,
survived,"    def get_all_env_vars(cls) -> List[Tuple[str, Any]]:
        """"""Get all environment variables from the environment class.
        
        Returns:
            A list of tuples containing the environment variable name and its EnvVar instance.
        """"""
        env_vars = []
        for name, attr in inspect.getmembers(EnvironmentVariables):
            if name.startswith('_') or not hasattr(attr, 'name'):
                continue
            env_vars.append((name, attr))
        return env_vars
",pcweb/pages/docs/env_vars.py,EnvVarDocs
survived,"    def test_print_after_resume_restarts_live_session(self):
        """"""Test that printing a Tree after resume creates new Live session.""""""
        formatter = ConsoleFormatter()
        
        formatter._live_paused = True
        formatter._live = None
        
        formatter.resume_live_updates()
        assert not formatter._live_paused
        
        tree = Tree(""Test"")
        
        with patch('crewai.utilities.events.utils.console_formatter.Live') as mock_live_class:
            mock_live_instance = MagicMock()
            mock_live_class.return_value = mock_live_instance
            
            formatter.print(tree)
            
            mock_live_class.assert_called_once()
            mock_live_instance.start.assert_called_once()
            assert formatter._live == mock_live_instance
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume
survived,"    def test_pause_resume_exception_handling(self):
        """"""Test that resume is called even if exception occurs during human input.""""""
        from crewai.agents.agent_builder.base_agent_executor_mixin import CrewAgentExecutorMixin
        
        executor = CrewAgentExecutorMixin()
        executor.crew = MagicMock()
        executor.crew._train = False
        executor._printer = MagicMock()
        
        formatter = event_listener.formatter
        
        original_paused_state = formatter._live_paused
        
        try:
            with patch.object(formatter, 'pause_live_updates') as mock_pause, \
                 patch.object(formatter, 'resume_live_updates') as mock_resume, \
                 patch('builtins.input', side_effect=KeyboardInterrupt(""Test exception"")):
                
                with pytest.raises(KeyboardInterrupt):
                    executor._ask_human_input(""Test result"")
                
                mock_pause.assert_called_once()
                mock_resume.assert_called_once()
        finally:
            formatter._live_paused = original_paused_state
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration
survived,"def test_export_overwrite_behavior_with_noninteractive_terminal(temp_marimo_file: str, existing_file: str) -> None:
    """"""Test export command behavior with non-interactive terminal.
    
    This test verifies that when stdout is not a TTY, the prompt_to_overwrite function
    automatically returns True and overwrites the file without prompting.
    """"""
    # First, ensure the file exists with known content
    with open(existing_file, ""w"") as f:
        f.write(""initial content"")
    
    # Run the export command without -y flag
    # In a non-interactive terminal (which is the case in tests), it should overwrite without prompting
    p = subprocess.run(
        [
            ""marimo"",
            ""export"",
            ""html"",
            temp_marimo_file,
            ""--output"",
            existing_file,
        ],
        capture_output=True,
        text=True,
    )
    
    # Check that the command completed successfully
    assert p.returncode == 0
    
    # Verify the file was overwritten even without explicit confirmation
    # This is expected behavior in non-interactive terminals
    assert os.path.exists(existing_file)
    
    # The content should be different from the initial content
    with open(existing_file, ""r"") as f:
        content = f.read()
    assert content != ""initial content""
    assert ""<!DOCTYPE html>"" in content
",tests/_cli/test_file_overwrite.py,
survived,"def test_convert_overwrite_confirm(tmp_path: Path) -> None:
    """"""Test convert command with file overwrite confirmation (user confirms).""""""
    # Create a notebook file
    notebook_path = tmp_path / ""test_notebook.ipynb""
    notebook_content = """"""
    {
     ""cells"": [
      {
       ""cell_type"": ""code"",
       ""execution_count"": null,
       ""metadata"": {},
       ""outputs"": [],
       ""source"": [
        ""print('Hello, World!')""
       ]
      }
     ],
     ""metadata"": {},
     ""nbformat"": 4,
     ""nbformat_minor"": 4
    }
    """"""
    notebook_path.write_text(notebook_content)
    
    # Create an existing output file
    output_path = tmp_path / ""output.py""
    output_path.write_text(""existing content"")
    
    p = subprocess.Popen(
        [
            ""marimo"",
            ""convert"",
            str(notebook_path),
            ""-o"",
            str(output_path),
        ],
        stdin=subprocess.PIPE,
    )
    
    assert p.poll() is None
    assert p.stdin is not None
    
    # Simulate user confirming overwrite
    p.stdin.write(b""y\n"")
    p.stdin.flush()
    
    # Wait for process to complete
    p.wait(timeout=5)
    
    # Check that the file was overwritten
    assert output_path.exists()
    assert p.returncode == 0
    assert output_path.read_text() != ""existing content""
",tests/_cli/test_file_overwrite.py,
survived,"    def __str__(self) -> str:
        """"""String representation of the fingerprint (the UUID).""""""
        return self.uuid_str
",src/crewai/security/fingerprint.py,Fingerprint
survived,"    def validate_fingerprint(cls, values):
        """"""Ensure fingerprint is properly initialized.""""""
        if isinstance(values, dict):
            # Handle case where fingerprint is not provided or is None
            if 'fingerprint' not in values or values['fingerprint'] is None:
                values['fingerprint'] = Fingerprint()
            # Handle case where fingerprint is a string (seed)
            elif isinstance(values['fingerprint'], str):
                values['fingerprint'] = Fingerprint.generate(seed=values['fingerprint'])
        return values
",src/crewai/security/security_config.py,SecurityConfig
survived,"    def __eq__(self, other) -> bool:
        """"""Compare fingerprints by their UUID.""""""
        if isinstance(other, Fingerprint):
            return self.uuid_str == other.uuid_str
        return False
",src/crewai/security/fingerprint.py,Fingerprint
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the fingerprint to a dictionary representation.

        Returns:
            Dict[str, Any]: Dictionary representation of the fingerprint
        """"""
        return {
            ""uuid_str"": self.uuid_str,
            ""created_at"": self.created_at.isoformat(),
            ""metadata"": self.metadata
        }
",src/crewai/security/fingerprint.py,Fingerprint
survived,"        async def _(model_uuid: str) -> str:
            json_data = await quart.request.json

            await self.ap.embeddings_models_service.test_embeddings_model(model_uuid, json_data)

            return self.success()",pkg/api/http/controller/groups/provider/models.py,EmbeddingsModelsRouterGroup
deleted,"    async def create_embeddings_model(self, model_data: dict) -> str:
        model_data['uuid'] = str(uuid.uuid4())

        await self.ap.persistence_mgr.execute_async(sqlalchemy.insert(persistence_model.EmbeddingsModel).values(**model_data))

        embeddings_model = await self.get_embeddings_model(model_data['uuid'])

        await self.ap.model_mgr.load_embeddings_model(embeddings_model)

        return model_data['uuid']
",pkg/api/http/service/model.py,EmbeddingsModelsService
survived,"def _state() -> AirbyteMessage:
    return AirbyteMessage(type=Type.STATE, state=AirbyteStateMessage(data={}))
",airbyte-integrations/connectors/destination-glassflow/unit_tests/unit_test.py,
survived,"def test_check_succeeds(client):
    pipeline = _init_mocks(client)
    pipeline.validate_credentials.return_value = None
    destination = DestinationGlassflow()
    status = destination.check(logger=Mock(), config=config)
    assert status.status == Status.SUCCEEDED
",airbyte-integrations/connectors/destination-glassflow/unit_tests/unit_test.py,
survived,"def test_set_cursor_keys():
    """"""Test that set_cursor_keys properly updates the cursor key overrides.""""""
    with patch.object(Source, ""_discover"", return_value=Mock()):
        source = Source(executor=Mock(), name=""test-source"")

        source.set_cursor_keys(kwargs={""stream1"": ""cursor1""})
        assert source._cursor_key_overrides == {""stream1"": ""cursor1""}

        source.set_cursor_keys(kwargs={""stream2"": ""cursor2""})
        assert source._cursor_key_overrides == {
            ""stream1"": ""cursor1"",
            ""stream2"": ""cursor2"",
        }

        source.set_cursor_keys(kwargs={""stream1"": ""new_cursor1""})
        assert source._cursor_key_overrides == {
            ""stream1"": ""new_cursor1"",
            ""stream2"": ""cursor2"",
        }
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"    def set_primary_key(
        self,
        stream_name: str,
        primary_key: str | list[str],
    ) -> None:
        """"""Set the primary key for a single stream.

        Args:
            stream_name: The name of the stream.
            primary_key: The primary key column name, or a list of fields which should comprise
                the composite primary key.
        """"""
        self._primary_key_overrides[stream_name] = (
            primary_key if isinstance(primary_key, list) else [primary_key]
        )
",airbyte/sources/base.py,Source
survived,"        def _run(self, input_text: str) -> str:
            return f""Processed {input_text}""
",tests/tools/test_tool_usage_limit.py,LimitedTool
survived,"    def setup_method(self) -> None:
        """"""Set up a temporary directory for each test.""""""
        self.temp_dir = tempfile.TemporaryDirectory()
        self.save_path = self.temp_dir.name
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def test_save_cache(self) -> None:
        """"""Test saving a cache.""""""
        loader = JsonLoader(""test"", self.save_path)
        
        # Create a cache with a stateful reference
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            {""stateful""},
            ""Pure"",
            True,
            {""version"": 1}
        )
        
        # Save the cache
        loader.save_cache(cache)
        
        # Verify the file was created
        cache_path = loader.build_path(""hash1"", ""Pure"")
        assert os.path.exists(cache_path)
        
        # Load the JSON and verify contents
        with open(cache_path, ""r"") as f:
            loaded_json = json.load(f)
        
        assert loaded_json[""hash""] == ""hash1""
        assert loaded_json[""cache_type""] == ""Pure""
        assert loaded_json[""hit""] is True
        assert isinstance(loaded_json[""stateful_refs""], list)  # Should be converted to list
        assert loaded_json[""meta""] == {""version"": 1}
        
        # Save another cache with different type
        cache2 = Cache(
            {""var2"": ""value2""}, 
            ""hash2"", 
            set(),
            ""Deferred"",
            True,
            {}
        )
        
        loader.save_cache(cache2)
        
        # Verify the second file was created
        cache2_path = loader.build_path(""hash2"", ""Deferred"")
        assert os.path.exists(cache2_path)
        
        # Load the second JSON and verify contents
        with open(cache2_path, ""r"") as f:
            loaded_json2 = json.load(f)
        
        assert loaded_json2[""hash""] == ""hash2""
        assert loaded_json2[""cache_type""] == ""Deferred""",tests/_save/loaders/test_json_loader.py,TestJsonLoader
survived,"    def test_load_cache(self) -> None:
        """"""Test the load_cache method.""""""
        loader = PickleLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        # Use string directly instead of Name constructor
        original_cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        
        with open(cache_path, ""wb"") as f:
            pickle.dump(original_cache, f)
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def test_init(self) -> None:
        """"""Test initialization.""""""
        loader = JsonLoader(""test"", self.save_path)
        assert loader.name == ""test""
        assert loader.suffix == ""json""
        assert str(loader.save_path).endswith(""/test"")
        
        # Check that the directory was created
        assert os.path.exists(os.path.join(self.save_path, ""test""))
",tests/_save/loaders/test_json_loader.py,TestJsonLoader
survived,"    def validate_schema(self):
        """"""Validate that the current database schema matches the expected schema.

        Raises:
            RuntimeError: If there is a mismatch between the current schema and expected schema,
                        with instructions to clear the cache.
        """"""
        expected_columns = [
            ""run_hash"",
            ""dataset_hash"",
            ""prompt_func"",
            ""model_name"",
            ""response_format"",
            ""batch_mode"",
            ""created_time"",
            ""last_edited_time"",
        ]
        current_info = self._get_current_schema()
        current_columns = [col[1] for col in current_info]  # col[1] = column name

        if set(current_columns) != set(expected_columns):
            msg = (
                ""Detected a mismatch between the local DB schema and the expected schema. ""
                ""Please clear your cache with `rm -rf ~/.cache/curator` or ""
                ""`rm -rf $CURATOR_CACHE_DIR` if set.""
            )
            raise RuntimeError(msg)
",src/bespokelabs/curator/db.py,MetadataDB
survived,"def test_invalid_schema(tmp_path):
    """"""Test that an invalid schema raises RuntimeError with mismatch message.""""""
    db_path = tmp_path / ""metadata.db""
    # Manually create a runs table with incorrect columns
    with sqlite3.connect(str(db_path)) as conn:
        conn.execute(""CREATE TABLE runs (wrong_col TEXT)"")
    
    db = MetadataDB(str(db_path))
    with pytest.raises(RuntimeError, match=""mismatch""):
        db.store_metadata({
            ""run_hash"": ""test2"",
            ""dataset_hash"": ""hash2"",
            ""prompt_func"": ""def prompt_func(): pass"",
            ""model_name"": ""test-model-2"",
            ""response_format"": ""{}"",
            ""batch_mode"": True,
            ""timestamp"": ""2023-01-01T01:00:00Z"",
        })",tests/test_db_schema.py,
survived,"def get_langsmith_url(client: Client, run_id: str, project_name: Optional[str] = None) -> str:
    """"""Get the URL for a run in LangSmith.

    Args:
        client: The LangSmith client
        run_id: The ID of the run
        project_name: Optional name of the project

    Returns:
        The URL for the run in LangSmith
    """"""
    # Construct the URL directly using the host URL and run ID
    # This avoids the issue with the client's get_run_url method expecting a run object
    host_url = client._host_url
    tenant_id = client._get_tenant_id()

    try:
        # Get the project ID from the project name
        if project_name is not None:
            project_id = client.read_project(project_name=project_name).id
            # Construct the URL
            return f""{host_url}/o/{tenant_id}/projects/p/{project_id}/r/{run_id}?poll=true""
        else:
            # If project_name is not provided, construct a URL without it
            return f""{host_url}/o/{tenant_id}/r/{run_id}?poll=true""
    except Exception as e:
        # If we can't get the project ID, construct a URL without it
        print(f""Could not get project ID for {project_name}: {e}"")
        return f""{host_url}/o/{tenant_id}/r/{run_id}?poll=true""
",src/codegen/extensions/langchain/utils/get_langsmith_url.py,
survived,"    def sign_message(self, message: str) -> Signature:
        """"""Sign a message with the wallet's private key.""""""
        message_bytes = message.encode(""utf-8"")
        signed = nacl.signing.SigningKey(self.keypair.secret_key).sign(message_bytes)
        return {""signature"": signed.signature.hex()}
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaKeypairWalletClient
survived,"    def get_address(self) -> str:
        """"""Get the wallet's public address.""""""
        return str(self.keypair.public_key)
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaKeypairWalletClient
survived,"def serialize_decimal(value: decimal.Decimal) -> float:
    """"""Serialize a Decimal to a float.

    Args:
        value: The Decimal to serialize.

    Returns:
        The serialized Decimal as a float.
    """"""
    return float(value)
",reflex/utils/serializers.py,
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the security config to a dictionary.

        Returns:
            Dict[str, Any]: Dictionary representation of the security config
        """"""
        result = {
            ""fingerprint"": self.fingerprint.to_dict()
        }
        return result
",src/crewai/security/security_config.py,SecurityConfig
survived,"    def is_internal_url(self, url):
        """"""Check if URL is internal to our domain.""""""
        parsed = urlparse(url)
        return parsed.netloc == self.domain or parsed.netloc == ''
",scripts/check_dead_links.py,DeadLinkChecker
survived,"    def run(self):
        """"""Run the dead link checker.""""""
        print(f""Starting dead link check for {self.base_url}"")
        print(f""Max pages: {self.max_pages}, Timeout: {self.timeout}s"")
        
        while self.pages_to_visit and len(self.visited_pages) < self.max_pages:
            url = self.pages_to_visit.popleft()
            self.crawl_page(url)
        
        print(f""\nCrawl complete!"")
        print(f""Pages visited: {len(self.visited_pages)}"")
        print(f""Links checked: {len(self.checked_links)}"")
        print(f""Dead links found: {len(self.dead_links)}"")
        
        if self.dead_links:
            print(""\nâŒ DEAD LINKS FOUND:"")
            for link_info in self.dead_links:
                print(f""  URL: {link_info['url']}"")
                print(f""  Error: {link_info['error']}"")
                print(f""  Found on: {link_info['source_page']}"")
                print()
            return False
        else:
            print(""\nâœ… No dead links found!"")
            return True
",scripts/check_dead_links.py,DeadLinkChecker
survived,"def validate_tool_code(tool_code: str) -> bool:
    """"""
    Validate that generated tool code is syntactically correct.
    
    Args:
        tool_code: Generated tool code
        
    Returns:
        True if the code is valid, False otherwise
    """"""
    try:
        compile(tool_code, '<string>', 'exec')
        return True
    except SyntaxError as e:
        print(f""Syntax error in generated tool code: {str(e)}"")
        return False
",meta_agent/generators/tool_generator.py,
survived,"def test_groq_require() -> None:
    """"""Test that groq.require raises ModuleNotFoundError.""""""
    model = groq(""llama3-70b-8192"")
    messages = [ChatMessage(role=""user"", content=""Test prompt"")]
    config = ChatModelConfig()
    with pytest.raises(ModuleNotFoundError):
        model(messages, config)
",tests/_ai/llm/_impl.py,
survived,"    def test_call_tool_use(
        self, mock_anthropic_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the anthropic class with tool use response.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_anthropic_class.return_value = mock_client
        mock_response = MagicMock()
        mock_content = MagicMock()
        mock_content.type = ""tool_use""
        mock_response.content = [mock_content]
        mock_client.messages.create.return_value = mock_response

        model = anthropic(""claude-3-opus-20240229"")
        messages = [ChatMessage(role=""user"", content=""Test prompt"")]
        config = ChatModelConfig()

        result = model(messages, config)
        assert result == [mock_content]
",tests/_ai/llm/_impl.py,TestAnthropic
survived,"    def test_init(self) -> None:
        """"""Test initialization of the groq class.""""""
        model = groq(""llama3-70b-8192"")
        assert model.model == ""llama3-70b-8192""
        assert model.system_message == DEFAULT_SYSTEM_MESSAGE
        assert model.api_key is None
        assert model.base_url is None

        model = groq(
            ""llama3-70b-8192"",
            system_message=""Custom system message"",
            api_key=""test-key"",
            base_url=""https://example.com"",
        )
        assert model.model == ""llama3-70b-8192""
        assert model.system_message == ""Custom system message""
        assert model.api_key == ""test-key""
        assert model.base_url == ""https://example.com""
",tests/_ai/llm/_impl.py,TestGroq
survived,"    def test_require_api_key_env(self) -> None:
        """"""Test _require_api_key with environment variable.""""""
        model = openai(""gpt-4"")
        assert model._require_api_key == ""env-key""
",tests/_ai/llm/_impl.py,TestOpenAI
survived,"    def test_is_file_path_with_empty_path(self) -> None:
        # Test with empty path
        with pytest.raises(click.BadParameter) as excinfo:
            is_file_path(None, None, """")
        assert ""Must be a file path"" in str(excinfo.value)
        
        # Test with None
        with pytest.raises(click.BadParameter) as excinfo:
            is_file_path(None, None, None)
        assert ""Must be a file path"" in str(excinfo.value)
",tests/_cli/test_cli_validators.py,TestIsFilePath
survived,"    def test_base_url_with_root_slash(self) -> None:
        # Test with root slash ""/""
        with pytest.raises(click.BadParameter) as excinfo:
            base_url(None, None, ""/"")
        assert ""Must not be /. This is equivalent to not setting the base URL."" in str(excinfo.value)
",tests/_cli/test_cli_validators.py,TestBaseUrl
survived,"    def test_is_file_path_with_valid_file(self, tmp_path: Path) -> None:
        # Create a temporary file
        test_file = tmp_path / ""test_file.txt""
        test_file.write_text(""test content"")
        
        # Test with valid file path
        assert is_file_path(None, None, str(test_file)) == str(test_file)
",tests/_cli/test_cli_validators.py,TestIsFilePath
survived,"    def test_buffered_writer_basic(self) -> None:
        # Test basic functionality of buffered writer
        stream = MockStream()
        msg_queue: deque[Optional[ConsoleMsg]] = deque()
        cv = threading.Condition()

        # Start the buffered writer in a separate thread
        thread = threading.Thread(
            target=buffered_writer, args=(msg_queue, stream, cv)
        )
        thread.daemon = True
        thread.start()

        try:
            # Add a message to the queue
            with cv:
                msg_queue.append(
                    ConsoleMsg(
                        stream=CellChannel.STDOUT,
                        cell_id=""cell1"",
                        data=""Hello"",
                        mimetype=""text/plain"",
                    )
                )
                cv.notify()

            # Wait for the timeout to expire and the message to be written
            time.sleep(TIMEOUT_S * 2)

            # Check that the message was written to the stream
            assert len(stream.messages) == 1
            assert stream.messages[0][1][""console""][""data""] == ""Hello""

        finally:
            # Signal the writer to terminate
            with cv:
                msg_queue.append(None)
                cv.notify()
            thread.join(timeout=1.0)
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker
deleted,"    def test_marimo_exception_raised_error(self) -> None:
        error = MarimoExceptionRaisedError(
            msg=""ValueError: invalid value"",
            exception_type=""ValueError"",
            raising_cell=""cell1"",
        )

        # Test properties
        assert error.type == ""exception""
        assert error.describe() == ""ValueError: invalid value""
        assert error.raising_cell == ""cell1""
        assert error.exception_type == ""ValueError""
",tests/_messaging/test_errors.py,TestErrorClasses
survived,"    def test_write_traceback_to_regular_stderr(self) -> None:
        # Test writing traceback to regular stderr (not Stderr)
        mock_stderr = MagicMock()
        mock_stderr.write = MagicMock()

        with patch(""sys.stderr"", mock_stderr):
            traceback = ""Traceback (most recent call last):\n  File \""<stdin>\"", line 1, in <module>\nValueError: invalid value""
            write_traceback(traceback)

            # Should call write with the original traceback
            mock_stderr.write.assert_called_once_with(traceback)
",tests/_messaging/test_tracebacks.py,TestTracebacks
deleted,"    def test_marimo_syntax_error(self) -> None:
        error = MarimoSyntaxError(msg=""Invalid syntax"")

        # Test properties
        assert error.type == ""syntax""
        assert error.describe() == ""Invalid syntax""
",tests/_messaging/test_errors.py,TestErrorClasses
survived,"def test_print_startup() -> None:
    """"""Test the print_startup function.""""""
    # Test with file_name and not run
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.side_effect = lambda x: x
                with patch(""marimo._server.print._colorized_url"") as mock_colorized_url:
                    mock_colorized_url.return_value = ""COLORIZED_URL""
                    with patch(""marimo._server.print.green"") as mock_green:
                        mock_green.return_value = ""GREEN_TEXT""
                        print_startup(
                            file_name=""test.py"",
                            url=""http://localhost:8000"",
                            run=False,
                            new=False,
                            network=False,
                        )
                        mock_print.assert_called()
                        mock_print_tabbed.assert_any_call(""âžœ  GREEN_TEXT: COLORIZED_URL"")
    
    # Test with file_name and run
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.side_effect = lambda x: x
                with patch(""marimo._server.print._colorized_url"") as mock_colorized_url:
                    mock_colorized_url.return_value = ""COLORIZED_URL""
                    with patch(""marimo._server.print.green"") as mock_green:
                        mock_green.return_value = ""GREEN_TEXT""
                        print_startup(
                            file_name=""test.py"",
                            url=""http://localhost:8000"",
                            run=True,
                            new=False,
                            network=False,
                        )
                        mock_print.assert_called()
                        mock_print_tabbed.assert_any_call(""âžœ  GREEN_TEXT: COLORIZED_URL"")
    
    # Test with new=True
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.side_effect = lambda x: x
                with patch(""marimo._server.print._colorized_url"") as mock_colorized_url:
                    mock_colorized_url.return_value = ""COLORIZED_URL""
                    with patch(""marimo._server.print.green"") as mock_green:
                        mock_green.return_value = ""GREEN_TEXT""
                        print_startup(
                            file_name=None,
                            url=""http://localhost:8000"",
                            run=False,
                            new=True,
                            network=False,
                        )
                        mock_print.assert_called()
                        mock_print_tabbed.assert_any_call(""âžœ  GREEN_TEXT: COLORIZED_URL"")
    
    # Test with network=True
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.side_effect = lambda x: x
                with patch(""marimo._server.print._colorized_url"") as mock_colorized_url:
                    mock_colorized_url.return_value = ""COLORIZED_URL""
                    with patch(""marimo._server.print.green"") as mock_green:
                        mock_green.return_value = ""GREEN_TEXT""
                        with patch(""marimo._server.print._get_network_url"") as mock_get_network_url:
                            mock_get_network_url.return_value = ""http://192.168.1.100:8000""
                            print_startup(
                                file_name=None,
                                url=""http://localhost:8000"",
                                run=False,
                                new=False,
                                network=True,
                            )
                            mock_print.assert_called()
                            mock_print_tabbed.assert_any_call(""âžœ  GREEN_TEXT: COLORIZED_URL"")
                            mock_get_network_url.assert_called_once_with(""http://localhost:8000"")
    
    # Test with network=True and _get_network_url raising an exception
    with patch(""marimo._server.print.print_"") as mock_print:
        with patch(""marimo._server.print.print_tabbed"") as mock_print_tabbed:
            with patch(""marimo._server.print._utf8"") as mock_utf8:
                mock_utf8.side_effect = lambda x: x
                with patch(""marimo._server.print._colorized_url"") as mock_colorized_url:
                    mock_colorized_url.return_value = ""COLORIZED_URL""
                    with patch(""marimo._server.print.green"") as mock_green:
                        mock_green.return_value = ""GREEN_TEXT""
                        with patch(""marimo._server.print._get_network_url"") as mock_get_network_url:
                            mock_get_network_url.side_effect = Exception(""Test exception"")
                            print_startup(
                                file_name=None,
                                url=""http://localhost:8000"",
                                run=False,
                                new=False,
                                network=True,
                            )
                            mock_print.assert_called()
                            mock_print_tabbed.assert_any_call(""âžœ  GREEN_TEXT: COLORIZED_URL"")
                            mock_get_network_url.assert_called_once_with(""http://localhost:8000"")
",tests/_server/test_print.py,
survived,"    async def initialize(self):
        genai.configure(
            api_key='',
            transport='rest',
            client_options={
                'api_endpoint': self.requester_cfg['base_url'],
                'timeout': self.requester_cfg['timeout'],
            },
        )
",pkg/provider/modelmgr/requesters/geminichatcmpl.py,GeminiChatCompletions
survived,"def test_numeric_formatting():
    # Test positive number with + sign
    assert format_value(""col"", 42.123, {""col"": ""{:+.2f}""}) == ""+42.12""
    assert format_value(""col"", -42.123, {""col"": ""{:+.2f}""}) == ""-42.12""

    # Test thousand separators
    assert format_value(""col"", 1234.567, {""col"": ""{:,.2f}""}) == ""1,234.57""
    assert format_value(""col"", -1234.567, {""col"": ""{:,.2f}""}) == ""-1,234.57""

    # Test combining + sign and thousand separators
    assert format_value(""col"", 1234.567, {""col"": ""{:+,.2f}""}) == ""+1,234.57""
    assert format_value(""col"", -1234.567, {""col"": ""{:+,.2f}""}) == ""-1,234.57""

    # Test integer values
    assert format_value(""col"", 1234, {""col"": ""{:,d}""}) == ""1,234""
    assert format_value(""col"", -1234, {""col"": ""{:+,d}""}) == ""-1,234""

    # Test non-numeric values (should not be affected)
    assert format_value(""col"", ""text"", {""col"": ""{}""}) == ""text""
    assert format_value(""col"", None, {""col"": ""{}""}) is None",tests/_plugins/ui/_impl/tables/test_format.py,
survived,"    def condition1(task_output: TaskOutput) -> bool:
        return ""success"" in task_output.raw.lower()
",tests/crew_test.py,
survived,"def test_wallet_options():
    """"""Fixture providing test wallet options.""""""
    return {
        ""chain"": ""sepolia"",
        ""provider"": ""https://rpc.sepolia.org"",
        ""options"": {
            ""ensProvider"": ""https://rpc.sepolia.org""
        }
    }
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def test_smart_wallet_with_email(smart_api, test_email, test_wallet_options):
    """"""Test smart wallet creation with email.""""""
    options = {
        **test_wallet_options,
        ""linkedUser"": {""email"": test_email}
    }
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        options[""chain""],
        test_keypair,
        options[""provider""],
        options[""options""][""ensProvider""]
    )
    assert client.get_address() == wallet[""address""]
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def solana_connection():
    """"""Fixture providing Solana RPC client connection.""""""
    return SolanaClient(""https://api.devnet.solana.com"")
",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_response_json_parsing(custodial_api):
    """"""Test JSON response parsing.""""""
    with pytest.raises(Exception) as exc:
        custodial_api.get_wallet(""invalid:wallet:id"")
    error_response = str(exc.value)
    assert ""{"" in error_response  # Should include formatted JSON error
    assert ""}"" in error_response
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def test_authentication_headers(custodial_api):
    """"""Test authentication header structure.""""""
    headers = custodial_api._request(""/wallets"", method=""GET"").request.headers
    assert ""x-api-key"" in headers
    assert headers[""x-api-key""] == os.environ[""CROSSMINT_STAGING_API_KEY_CUSTODIAL""]
    assert headers[""Content-Type""] == ""application/json""
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"def box_file_text_extract(client: BoxClient, file_id: str) -> str:
    # Request the file with the ""extracted_text"" representation hint
    file_text_representation = client.files.get_file_by_id(
        file_id,
        x_rep_hints=""[extracted_text]"",
        fields=[""name"", ""representations""],
    )
    # Check if any representations exist
    if not file_text_representation.representations.entries:
        logger.debug(f""No representation for file {file_text_representation.id}"")
        return """"

    # Find the ""extracted_text"" representation
    extracted_text_entry = next(
        (entry for entry in file_text_representation.representations.entries if entry.representation == ""extracted_text""),
        None,
    )
    if not extracted_text_entry:
        return """"

    # Handle cases where the extracted text needs generation
    if extracted_text_entry.status.state == ""none"":
        _do_request(extracted_text_entry.info.url)  # Trigger text generation

    # Construct the download URL and sanitize filename
    url = extracted_text_entry.content.url_template.replace(""{+asset_path}"", """")

    # Download and truncate the raw content
    raw_content = _do_request(client, url)

    # check to see if rawcontent is bytes
    if isinstance(raw_content, bytes):
        return raw_content.decode(""utf-8"")
    else:
        return raw_content
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"def test_check_connection(mocker):
    source = SourceBoxDataExtract()
    logger_mock, config_mock = MagicMock(), MagicMock()
    assert source.check_connection(logger_mock, config_mock) == (False, ""Unable to connect to Box API with the provided credentials"")
",airbyte-integrations/connectors/source-box-data-extract/unit_tests/test_source.py,
survived,"def get_box_ccg_client(config: Mapping[str, Any]) -> BoxClient:
    client_id = config[""client_id""]
    client_secret = config[""client_secret""]
    box_subject_type = config[""box_subject_type""]
    box_subject_id = config[""box_subject_id""]

    if box_subject_type == ""enterprise"":
        enterprise_id = box_subject_id
        user_id = None
    else:
        enterprise_id = None
        user_id = box_subject_id
    ccg_config = CCGConfig(client_id=client_id, client_secret=client_secret, enterprise_id=enterprise_id, user_id=user_id)
    ccg_auth = BoxCCGAuth(ccg_config)
    return add_extra_header_to_box_client(BoxClient(ccg_auth))
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"    def __init__(self, client: BoxClient, folder_id: str, fields_json_str: str, is_recursive: bool = False):
        self.client = client
        self.folder_id = folder_id
        self.is_recursive = is_recursive
        self.fields_json_str = fields_json_str
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder
survived,"def box_file_ai_extract_structured(client: BoxClient, file_id: str, fields_json_str: str) -> str:
    ai_item = AiItemBase(id=file_id, type=AiItemBaseTypeField.FILE)
    fields_list = json.loads(fields_json_str)
    ai_fields = []
    options = []
    for field in fields_list:
        field_options = field.get(""options"")
        if field_options is not None:
            for option in field.get(""options""):
                options.append(CreateAiExtractStructuredFieldsOptionsField(key=option.get(""key"")))

        ai_fields.append(
            CreateAiExtractStructuredFields(
                key=field.get(""key""),
                description=field.get(""description""),
                display_name=field.get(""display_name""),
                prompt=field.get(""prompt""),
                type=field.get(""type""),
                options=options if options is not None and len(options) > 0 else None,
            )
        )
    response: AiExtractResponse = client.ai.create_ai_extract_structured(items=[ai_item], fields=ai_fields)
    return json.dumps(response.to_dict(), indent=2)
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"def box_folder_text_representation(
    client: BoxClient, folder_id: str, is_recursive: bool = False, by_pass_text_extraction: bool = False
) -> Iterable[BoxFileExtended]:
    # folder items iterator
    for item in client.folders.get_folder_items(folder_id).entries:
        if item.type == ""file"":
            file = box_file_get_by_id(client=client, file_id=item.id)
            if not by_pass_text_extraction:
                text_representation = box_file_text_extract(client=client, file_id=item.id)
            else:
                text_representation = """"
            yield BoxFileExtended(file=file, text_representation=text_representation)
        elif item.type == ""folder"" and is_recursive:
            yield from box_folder_text_representation(
                client=client, folder_id=item.id, is_recursive=is_recursive, by_pass_text_extraction=by_pass_text_extraction
            )
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"    def primary_key(self) -> Optional[Union[str, List[str], List[List[str]]]]:
        """"""
        :return: string if single primary key, list of strings if composite primary key, list of list of strings if composite primary key consisting of nested fields.
          If the stream has no primary keys, return None.
        """"""
        return ""id""
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder
survived,"def create_manager_agent(specialists: List[Agent]) -> Agent:
    """"""
    Create a manager agent that coordinates the work of specialist agents.
    
    Args:
        specialists: List of specialist agents to coordinate
        
    Returns:
        An Agent instance that manages the content creation process
    """"""
    instructions = """"""
    You are a content manager who coordinates the work of specialist agents to create high-quality content.
    Your task is to:
    1. Understand the content request
    2. Delegate research to the Research Specialist
    3. Have the Outline Specialist create a structure based on the research
    4. Have the Content Specialist write content based on the outline and research
    5. Have the Editing Specialist refine and polish the content
    6. Deliver the final polished content
    
    Manage the workflow efficiently and ensure each specialist has the information they need.
    """"""
    
    # Create handoffs to specialist agents
    handoffs = [handoff(agent) for agent in specialists]
    
    return Agent(
        name=""ContentManager"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoffs=handoffs
    )
",openai-agents-examples/11_agent_orchestration.py,
survived,"def create_conversation_agent() -> Agent:
    """"""
    Create a conversation agent that can maintain context.
    
    Returns:
        An Agent instance that maintains conversation context.
    """"""
    instructions = """"""
    You are a helpful conversational assistant that maintains context across interactions.
    Remember details from previous parts of the conversation and refer back to them when relevant.
    Be friendly, informative, and engaging in your responses.
    If the user asks about something you discussed earlier, acknowledge that and build upon it.
    """"""
    
    return Agent(
        name=""ConversationAssistant"",
        instructions=instructions,
        model=""gpt-4o-mini"",
    )
",openai-agents-examples/09_agent_with_context_management.py,
survived,"async def run_custom_tool_agent(prompt: str) -> str:
    """"""
    Run the financial assistant agent with the given prompt.
    
    Args:
        prompt: The user's query or prompt
        
    Returns:
        The agent's response as a string
    """"""
    # Create the agent with custom tools
    agent = create_financial_assistant()
    
    # Run the agent with the prompt
    result = await Runner.run(agent, prompt)
    
    # Return the response
    return result.final_output
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def test_create_triage_agent():
    """"""Test that the triage agent is created with the correct configuration.""""""
    billing_agent = create_billing_agent()
    technical_agent = create_technical_agent()
    account_agent = create_account_agent()
    
    triage_agent = create_triage_agent([billing_agent, technical_agent, account_agent])
    
    assert triage_agent.name == ""TriageAgent""
    assert ""triage agent"" in triage_agent.instructions.lower()
    assert len(triage_agent.handoffs) == 3
",openai-agents-examples/07_agent_with_handoffs.py,
survived,"def test_create_manager_agent():
    """"""Test that the manager agent is created with the correct configuration.""""""
    research_agent = create_research_agent()
    outline_agent = create_outline_agent()
    content_agent = create_content_agent()
    editor_agent = create_editor_agent()
    
    manager = create_manager_agent([research_agent, outline_agent, content_agent, editor_agent])
    
    assert manager.name == ""ContentManager""
    assert ""content manager"" in manager.instructions.lower()
    assert len(manager.handoffs) == 4
",openai-agents-examples/11_agent_orchestration.py,
survived,"def create_tech_agent() -> Agent:
    """"""
    Create a technology specialist agent.
    
    Returns:
        An Agent instance specialized in technology topics.
    """"""
    instructions = """"""
    You are a technology specialist with expertise in computer science, programming, AI, and digital technologies.
    Provide clear, accurate explanations of technical concepts and their practical applications.
    When discussing programming, focus on concepts rather than writing extensive code.
    Explain how technologies work and their real-world impact.
    """"""
    
    return Agent(
        name=""TechSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for questions about technology, computing, programming, and digital systems.""
    )
",openai-agents-examples/02_multi_agent.py,
survived,"def test_blog_tools():
    """"""Test that the blog tools work correctly.""""""
    # Test outline tool
    outline = generate_blog_outline(
        ""AI Ethics"",
        ""AI Ethics involves principles like transparency, fairness, and accountability.""
    )
    assert ""introduction"" in outline.lower()
    assert ""conclusion"" in outline.lower()
    
    # Test markdown formatting tool
    markdown = format_blog_as_markdown(
        ""AI Ethics"",
        ""# AI Ethics\n\nThis is a blog post about AI ethics.""
    )
    assert ""title"" in markdown.lower()
    assert ""date"" in markdown.lower()
    assert ""ai ethics"" in markdown.lower()
",openai-agents-examples/13_research_blog_system.py,
survived,"def test_run_protected_agent():
    """"""Test that the protected agent can run and produce a response or rejection.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Test with a valid prompt
    valid_prompt = ""Tell me about renewable energy sources""
    valid_response = asyncio.run(run_protected_agent(valid_prompt))
    
    # Verify we got a non-empty response
    assert valid_response
    assert len(valid_response) > 0
    assert ""rejected"" not in valid_response.lower()
    
    # Test with an invalid prompt (contains filtered term)
    invalid_prompt = ""How to hack into a system""
    invalid_response = asyncio.run(run_protected_agent(invalid_prompt))
    
    # Verify we got a rejection message
    assert invalid_response
    assert ""rejected"" in invalid_response.lower()
",openai-agents-examples/10_agent_with_guardrails.py,
survived,"def fix_imports_in_file(file_path):
    """"""Fix imports in a single file.""""""
    with open(file_path, 'r') as f:
        content = f.read()
    
    # First fix the Runner.run syntax error
    if 'from agents import Agent, Runner.run' in content:
        content = content.replace('from agents import Agent, Runner.run', 'from agents import Agent, Runner')
    
    if 'from agents import Agent, Runner.run_sync' in content:
        content = content.replace('from agents import Agent, Runner.run_sync', 'from agents import Agent, Runner')
    
    # Replace incorrect imports with correct ones based on documentation
    replacements = [
        ('from openai.agents import', 'from agents import'),
        ('import openai.agents', 'import agents'),
        ('from openai_agents import', 'from agents import'),
        ('import openai_agents', 'import agents'),
        ('from agents import Agent, run_agent', 'from agents import Agent, Runner'),
        ('from agents import Agent, run_agent_sync', 'from agents import Agent, Runner'),
        ('result = run_agent_sync', 'result = Runner.run_sync'),
        ('result = run_agent', 'result = Runner.run'),
        ('result = await run_agent_sync', 'result = await Runner.run_sync'),
        ('result = await run_agent', 'result = await Runner.run'),
        ('result.output', 'result.final_output'),
        ('return response, context', 'return result.final_output, result.context'),
        ('responses.append(response)', 'responses.append(result.final_output)'),
    ]
    
    new_content = content
    for old, new in replacements:
        new_content = new_content.replace(old, new)
    
    # Also update dependencies in the script header
    if '# dependencies = [' in new_content:
        # Update to use the correct package name and import path
        new_content = new_content.replace(
            '""openai-agents>=0.0.2"",', 
            '""openai>=1.66.0"",  # Includes agents module'
        )
        new_content = new_content.replace(
            '""openai>=1.66.0"",  # Includes agents module', 
            '""openai>=1.66.0"",  # Includes agents module'
        )
    
    if new_content != content:
        with open(file_path, 'w') as f:
            f.write(new_content)
        print(f""Fixed imports in {file_path}"")
    else:
        print(f""No changes needed in {file_path}"")
",openai-agents-examples/fix_imports.py,
survived,"async def test_get_request_not_found():
    request_id = ""nonexistent-id""
    subdomain = ""abcd1234""
    
    mock_redis.get.return_value = None
    
    response = client.get(f""/api/get_request?id={request_id}&subdomain={subdomain}"")
    
    assert response.status_code == 404
    assert response.json() == {""detail"": ""Request not found""}
",backend/tests/test_endpoints.py,
survived,"    async def reply_message(
        self,
        message_source: platform_message.MessageChain,
        message: platform_message.MessageChain,
    ) -> dict:
        """"""å›žå¤æ¶ˆæ¯""""""
        source_components = [comp for comp in message_source if isinstance(comp, platform_message.Source)]
        if source_components:
            source = source_components[0]
            session_key = getattr(source, 'session_id', 'webchatperson')
        else:
            session_key = 'webchatperson'
            
        return await self.send_message('person', session_key, message)
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"        async def get_messages(session_type: str) -> str:
            """"""èŽ·å–è°ƒè¯•æ¶ˆæ¯åŽ†å²""""""
            try:
                if session_type not in ['person', 'group']:
                    return self.http_status(400, -1, 'session_type must be person or group')
                
                webchat_adapter = None
                for bot in self.ap.platform_mgr.bots:
                    if hasattr(bot.adapter, '__class__') and bot.adapter.__class__.__name__ == 'WebChatAdapter':
                        webchat_adapter = bot.adapter
                        break
                
                if not webchat_adapter:
                    return self.http_status(404, -1, 'WebChat adapter not found')
                
                messages = webchat_adapter.get_debug_messages(session_type)
                
                return self.success(data={'messages': messages})
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')
",pkg/api/http/controller/groups/debug/webchat.py,WebChatDebugRouterGroup
survived,"    async def send_message(
        self,
        target_type: str,
        target_id: str,
        message: platform_message.MessageChain,
    ) -> dict:
        """"""å‘é€æ¶ˆæ¯åˆ°è°ƒè¯•ä¼šè¯""""""
        session_key = target_id
        
        if session_key not in self.debug_messages:
            self.debug_messages[session_key] = []
            
        message_data = {
            'id': len(self.debug_messages[session_key]) + 1,
            'type': 'bot',
            'content': str(message),
            'timestamp': datetime.now().isoformat(),
            'message_chain': [component.__dict__ for component in message]
        }
        
        self.debug_messages[session_key].append(message_data)
        
        await self.logger.info(f'WebChatå‘é€æ¶ˆæ¯åˆ° {session_key}: {message}')
        
        return {'success': True, 'message_id': message_data['id']}
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"async def delete_report(slug: str, api_key: str = Depends(verify_admin_api_key)):
    try:
        set_status(slug, ReportStatus.DELETED.value)
        return ORJSONResponse(
            content={""message"": f""Report {slug} marked as deleted""},
            headers={
                ""Content-Type"": ""application/json"",
                ""Access-Control-Allow-Origin"": ""*"",
            },
        )
    except ValueError as e:
        slogger.error(f""ValueError: {e}"", exc_info=True)
        raise HTTPException(status_code=404, detail=str(e)) from e
    except Exception as e:
        slogger.error(f""Exception: {e}"", exc_info=True)
        raise HTTPException(status_code=500, detail=""Internal server error"") from e",server/src/routers/admin_report.py,
survived,"    def navigate_to_result(self, url: str):
        """"""Navigate to a search result.""""""
        self.show_results = False
        return rx.redirect(url)
",pcweb/components/docpage/navbar/typesense.py,TypesenseSearchState
survived,"def test_catalog_yaml():
    """"""Test that catalog.yaml is valid and follows the expected structure.""""""
    catalog_path = Path(__file__).parent.parent / ""vlmrun"" / ""hub"" / ""catalog.yaml""
    assert catalog_path.exists(), ""catalog.yaml file not found""

    with open(catalog_path, ""r"") as f:
        catalog = parse_yaml_raw_as(CatalogYaml, f.read())

    # Basic validation
    assert catalog.apiVersion == ""v1"", ""API version must be v1""
    assert len(catalog.schemas) > 0, ""Catalog must contain at least one schema""

    # Schema-specific validation
    for entry in catalog.schemas:
        # Domain format validation
        assert ""."" in entry.domain, ""Domain must be in format: category.name""
        category, name = entry.domain.split(""."", 1)
        assert category and name, ""Both category and name must be non-empty""

        # Schema path validation
        assert entry.schema.startswith(""vlmrun.hub.schemas.""), ""Schema must be in vlmrun.hub.schemas package""

        # Version format validation (basic semver check)
        version_parts = entry.version.split(""."")
        assert len(version_parts) == 3, ""Version must follow semver format (X.Y.Z)""
        assert all(part.isdigit() for part in version_parts), ""Version parts must be numeric""

        # Sample data URL validation
        assert entry.sample_data.startswith(
            ""https://storage.googleapis.com/vlm-data-public-prod/""
        ), ""Sample data must be in Google Cloud Storage""

        # Metadata validation
        assert ""tags"" in entry.metadata, ""Metadata must include tags""
        assert isinstance(entry.metadata[""tags""], list), ""Tags must be a list""
        assert len(entry.metadata[""tags""]) > 0, ""Must have at least one tag""
        assert all(isinstance(tag, str) for tag in entry.metadata[""tags""]), ""All tags must be strings""

        # Content validation
        assert len(entry.prompt) >= 10, ""Prompt must be descriptive (min 10 chars)""
        assert len(entry.description) >= 20, ""Description must be detailed (min 20 chars)""",tests/test_catalog.py,
survived,"def test_github_issue_3149_reproduction():
    """"""Test that reproduces the exact issue from GitHub issue #3149.""""""
    task = Task(
        description=""Test task for issue reproduction"",
        expected_output=""Test output"",
        output_file=""test_output.txt"",
        create_directory=True,
    )
    
    assert task.create_directory is True
    assert task.output_file == ""test_output.txt""
",tests/task_test.py,
survived,"    def __init__(self, transaction: str, required_signers: list = None, signer: str = None):
        self.transaction = transaction
        self.requiredSigners = required_signers or []
        self.signer = signer
",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,SolanaSmartWalletTransactionParams
survived,"def change_password(user_id: str, current_password: str, new_password: str) -> Tuple[bool, Dict]:
    """"""
    Change a user's password.
    
    Args:
        user_id: The ID of the user
        current_password: The current password
        new_password: The new password
        
    Returns:
        Tuple of (success, result) where result contains a success message or error message
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""current_password"": current_password, ""new_password"": new_password},
        [""current_password"", ""new_password""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Validate new password strength
    password_validation = validate_password_strength(new_password)
    if not password_validation[""is_valid""]:
        errors = []
        if not password_validation[""length""]:
            errors.append(""Password must be at least 8 characters"")
        if not password_validation[""uppercase""]:
            errors.append(""Password must contain at least one uppercase letter"")
        if not password_validation[""lowercase""]:
            errors.append(""Password must contain at least one lowercase letter"")
        if not password_validation[""digit""]:
            errors.append(""Password must contain at least one digit"")
        if not password_validation[""special_char""]:
            errors.append(""Password must contain at least one special character"")
        
        return False, {""error"": errors}
    
    # In a real application, this would verify the current password and update it
    # For this mock, we'll just print the change
    print(f""[PASSWORD] User {user_id} password changed"")
    
    return True, {""message"": ""Password changed successfully""}",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"def update_user_profile(user_id: str, profile_data: Dict) -> Tuple[bool, Dict]:
    """"""
    Update a user's profile data.
    
    Args:
        user_id: The ID of the user to update
        profile_data: The profile data to update
        
    Returns:
        Tuple of (success, result) where result is either updated user data or error messages
    """"""
    # Get the current user data
    user_data = get_user_by_id(user_id)
    if not user_data:
        return False, {""error"": ""User not found""}
    
    # Validate email if provided
    if ""email"" in profile_data:
        if not validate_email(profile_data[""email""]):
            return False, {""error"": ""Invalid email format""}
    
    # In a real application, this would update the user in the database
    # For this mock, we'll just print the update
    print(f""[UPDATE] User {user_id} profile updated:"")
    for key, value in profile_data.items():
        print(f""  {key}: {value}"")
    
    # Return success with mock updated data
    updated_user = {**user_data, **profile_data}
    return True, {""user"": updated_user}
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"    def __init__(self, name=""Data Processing Pipeline""):
        """"""Initialize the data processing pipeline.""""""
        super().__init__(name)
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,DataProcessingPipeline
survived,"def save_csv_file(data, file_path, fieldnames=None):
    """"""Save data to a CSV file.""""""
    if not data:
        raise ValueError(""No data to save"")
    
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
    
    if fieldnames is None:
        fieldnames = data[0].keys()
    
    with open(file_path, 'w', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def get_task(task_id):
        """"""Get a task by ID.""""""
        task_data = db.get(""tasks"", task_id)
        if not task_data:
            return None
        return task_data
",codebase-architectures/vertical-slice-architecture/features/tasks/service.py,TaskService
survived,"    def __init__(self):
        """"""Initialize the database.""""""
        self.data = {}
        self.logger = Logger.get_logger(""database"")
        Logger.info(self.logger, ""Database initialized"")
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase
survived,"def send_user_alert(user_id: str, message: str, level: str = ""info"", 
                   email: Optional[str] = None, phone: Optional[str] = None,
                   additional_data: Optional[Dict] = None) -> Tuple[bool, Dict]:
    """"""
    Send an alert to a user through multiple channels.
    
    Args:
        user_id: The ID of the user to alert
        message: The alert message
        level: Alert level (info, warning, error)
        email: Optional email address to send the alert to
        phone: Optional phone number to send the alert to
        additional_data: Additional data for the alert
        
    Returns:
        Tuple of (success, result) with notification details
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""user_id"": user_id, ""message"": message},
        [""user_id"", ""message""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Validate message length
    if not validate_string_length(message, min_length=1, max_length=500):
        return False, {""error"": ""Message must be between 1 and 500 characters""}
    
    # Validate level
    valid_levels = [""info"", ""warning"", ""error""]
    if level not in valid_levels:
        return False, {""error"": f""Level must be one of: {', '.join(valid_levels)}""}
    
    # Create the alert notification
    notification = create_alert(
        user_id=user_id,
        message=message,
        level=level,
        data=additional_data
    )
    
    # Send email if provided
    email_sent = False
    if email:
        if validate_email(email):
            subject = f""Alert: {level.capitalize()}""
            email_sent = send_email_notification(email, subject, message)
        else:
            return False, {""error"": ""Invalid email format""}
    
    # Send SMS if provided
    sms_sent = False
    if phone:
        sms_sent = send_sms_notification(phone, message)
    
    return True, {
        ""notification"": notification,
        ""channels"": {
            ""in_app"": True,
            ""email"": email_sent,
            ""sms"": sms_sent
        }
    }
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,
survived,"def format_currency(amount):
    """"""Format a number as currency.""""""
    try:
        return f""${float(amount):.2f}""
    except (ValueError, TypeError):
        return ""N/A""
",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def configure_output(self, config):
        """"""
        Configure the output stage.
        
        Args:
            config: Dictionary with output configuration
        """"""
        self.output_config = config",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,DataProcessingPipeline
survived,"def register_new_user(username: str, password: str, email: str) -> Tuple[bool, Dict]:
    """"""
    Register a new user with validation.
    
    Args:
        username: The username for the new user
        password: The password for the new user
        email: The email for the new user
        
    Returns:
        Tuple of (success, result) where result is either user data or error messages
    """"""
    # Validate required fields
    missing_fields = validate_required_fields(
        {""username"": username, ""password"": password, ""email"": email},
        [""username"", ""password"", ""email""]
    )
    
    if missing_fields:
        return False, {""error"": f""Missing required fields: {', '.join(missing_fields)}""}
    
    # Validate username
    if not validate_username(username):
        return False, {""error"": ""Username must be 3-20 characters, alphanumeric with underscores""}
    
    # Validate email
    if not validate_email(email):
        return False, {""error"": ""Invalid email format""}
    
    # Validate password strength
    password_validation = validate_password_strength(password)
    if not password_validation[""is_valid""]:
        errors = []
        if not password_validation[""length""]:
            errors.append(""Password must be at least 8 characters"")
        if not password_validation[""uppercase""]:
            errors.append(""Password must contain at least one uppercase letter"")
        if not password_validation[""lowercase""]:
            errors.append(""Password must contain at least one lowercase letter"")
        if not password_validation[""digit""]:
            errors.append(""Password must contain at least one digit"")
        if not password_validation[""special_char""]:
            errors.append(""Password must contain at least one special character"")
        
        return False, {""error"": errors}
    
    try:
        # Register the user
        user_data = register_user(username, password, email)
        return True, {""user"": user_data}
    except ValueError as e:
        return False, {""error"": str(e)}
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"def display_result(result):
    """"""Display a result.""""""
    if result.get(""success""):
        print(""âœ… "" + result.get(""message"", ""Operation successful""))
        
        if ""data"" in result:
            data = result[""data""]
            if isinstance(data, list):
                for item in data:
                    print_item(item)
            else:
                print_item(data)
    else:
        print(""âŒ "" + result.get(""message"", ""Operation failed""))
",codebase-architectures/layered-architecture/main.py,
survived,"    def __init__(self, title, description=None, user_id=None, status=""pending"", id=None):
        self.id = id or generate_id()
        self.title = title
        self.description = description
        self.user_id = user_id
        self.status = status
        self.created_at = get_timestamp()
        self.updated_at = self.created_at
",codebase-architectures/vertical-slice-architecture/features/tasks/model.py,Task
survived,"def generate_id():
    """"""Generate a unique ID.""""""
    return str(uuid.uuid4())
",codebase-architectures/vertical-slice-architecture/shared/utils.py,
survived,"    def get_all_tasks():
        """"""Get all tasks.""""""
        return db.get_all(""tasks"")
",codebase-architectures/vertical-slice-architecture/features/tasks/service.py,TaskService
survived,"    def insert(self, collection_name, id, item):
        """"""Insert an item into a collection.""""""
        if collection_name not in self.data:
            self.create_collection(collection_name)
        self.data[collection_name][id] = item
        return id
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"def generate_report_filename(prefix=""report"", extension=""json""):
    """"""Generate a filename for a report with timestamp.""""""
    timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
    return f""{prefix}_{timestamp}.{extension}""",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def run(self):
        """"""
        Run the pipeline by executing all stages in sequence.
        
        Returns:
            dict: Pipeline results including data and metadata
        """"""
        self.metadata[""started_at""] = datetime.now().isoformat()
        self.metadata[""status""] = ""running""
        
        print(f""\n=== Starting Pipeline: {self.name} ==="")
        
        # Execute each stage
        for i, stage in enumerate(self.stages):
            stage_name = stage[""name""]
            stage_instance = stage[""instance""]
            
            print(f""\n--- Stage {i+1}: {stage_name} ---"")
            
            try:
                # Execute the stage
                if i == 0:
                    # First stage doesn't take input from previous stage
                    result = self._execute_first_stage(stage_instance)
                else:
                    # Pass result from previous stage
                    previous_result = self.results[self.stages[i-1][""name""]]
                    result = self._execute_stage(stage_instance, previous_result)
                
                # Store the result
                self.results[stage_name] = result
                
                # Update stage status
                stage[""status""] = result[""metadata""][""status""]
                
                # Check for errors
                if result[""metadata""][""status""] in [""error"", ""skipped""]:
                    print(f""Stage {stage_name} {result['metadata']['status']}"")
                    for error in result[""metadata""].get(""errors"", []):
                        print(f""  Error: {error}"")
                    
                    # Add errors to pipeline metadata
                    self.metadata[""errors""].append({
                        ""stage"": stage_name,
                        ""errors"": result[""metadata""].get(""errors"", [])
                    })
                else:
                    print(f""Stage {stage_name} completed successfully"")
            
            except Exception as e:
                # Handle unexpected errors
                error_message = f""Unexpected error in stage {stage_name}: {str(e)}""
                print(f""  Error: {error_message}"")
                
                # Update stage status
                stage[""status""] = ""error""
                
                # Add error to pipeline metadata
                self.metadata[""errors""].append({
                    ""stage"": stage_name,
                    ""errors"": [error_message]
                })
        
        # Update pipeline status
        self.metadata[""completed_at""] = datetime.now().isoformat()
        if self.metadata[""errors""]:
            self.metadata[""status""] = ""completed_with_errors""
        else:
            self.metadata[""status""] = ""completed""
        
        # Calculate total execution time
        start_time = datetime.fromisoformat(self.metadata[""started_at""])
        end_time = datetime.fromisoformat(self.metadata[""completed_at""])
        execution_time = (end_time - start_time).total_seconds()
        self.metadata[""execution_time_seconds""] = execution_time
        
        print(f""\n=== Pipeline {self.name} {self.metadata['status']} ==="")
        print(f""Total execution time: {execution_time:.2f} seconds"")
        
        return self._create_pipeline_result()
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager
survived,"def get_user_alerts(user_id: str, unread_only: bool = False, 
                   level: Optional[str] = None) -> List[Dict]:
    """"""
    Get alerts for a user with optional filtering.
    
    Args:
        user_id: The ID of the user
        unread_only: Whether to return only unread alerts
        level: Optional filter by alert level
        
    Returns:
        List of alert notifications
    """"""
    # Get all notifications for the user
    notifications = get_user_notifications(user_id, unread_only)
    
    # Filter to only alert type notifications
    alerts = [n for n in notifications if n[""type""] == ""alert""]
    
    # Filter by level if specified
    if level:
        alerts = [a for a in alerts if a[""data""].get(""level"") == level]
    
    return alerts
",codebase-architectures/atomic-composable-architecture/capabilities/alerting.py,
survived,"    def undo_edit(path: str) -> FileOperationResult:
        """"""
        Placeholder for undo_edit functionality.
        In a real implementation, you would need to track edit history.

        Args:
            path: The path to the file whose last edit should be undone

        Returns:
            FileOperationResult with message about undo functionality
        """"""
        try:
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                Logger.error(app_logger, f""[undo_edit] {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            message = ""Undo functionality is not implemented in this version.""
            Logger.warning(app_logger, f""[undo_edit] {message}"")
            return FileOperationResult(True, message)
        except Exception as e:
            error_msg = f""Error in undo_edit: {str(e)}""
            Logger.error(app_logger, f""[undo_edit] {error_msg}"", exc_info=True)
            return FileOperationResult(False, error_msg)",example-agent-codebase-arch/layered-architecture/services/file_service.py,FileService
survived,"    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """"""
        Process the data from the previous stage by formatting the results for Claude.
        
        Args:
            data: The data from the previous stage containing the operation result
            
        Returns:
            Dictionary with the formatted result for Claude
        """"""
        try:
            # Check if there was an error in the previous stages
            if ""error"" in data:
                console.log(f""[output_stage] Error from previous stage: {data['error']}"")
                return {""error"": data[""error""], ""stage"": ""output""}
                
            result = data.get(""result"")
            if not result:
                error_msg = ""No result found in data from previous stage""
                console.log(f""[output_stage] Error: {error_msg}"")
                return {""error"": error_msg, ""stage"": ""output""}
                
            console.log(f""[output_stage] Formatting result for Claude"")
            
            # Format the result for Claude
            if isinstance(result, FileOperationResult):
                formatted_result = result.to_response()
            else:
                # If the result is not a FileOperationResult, return it as is
                formatted_result = result
                
            console.log(f""[output_stage] Formatted result: {formatted_result}"")
            
            return formatted_result
                
        except Exception as e:
            error_msg = f""Error in output stage: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[output_stage] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return {""error"": error_msg, ""stage"": ""output""}",example-agent-codebase-arch/pipeline-architecture/steps/output_stage.py,OutputStage
survived,"def normalize_path(path: str) -> str:
    """"""
    Normalize file paths to handle various formats (absolute, relative, Windows paths, etc.)

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """"""
    if not path:
        return path

    # Handle Windows backslash paths if provided
    path = path.replace(""\\"", os.sep)

    is_windows_path = False
    if os.name == ""nt"" and len(path) > 1 and path[1] == "":"":
        is_windows_path = True

    # Handle /repo/ paths from Claude (tool use convention)
    if path.startswith(""/repo/""):
        path = os.path.join(os.getcwd(), path[6:])
        return path

    if path.startswith(""/""):
        # Handle case when Claude provides paths with leading slash
        if path == ""/"" or path == ""/."":
            # Special case for root directory
            path = os.getcwd()
        else:
            # Replace leading slash with current working directory
            path = os.path.join(os.getcwd(), path[1:])
    elif path.startswith(""./""):
        # Handle relative paths starting with ./
        path = os.path.join(os.getcwd(), path[2:])
    elif not os.path.isabs(path) and not is_windows_path:
        # For non-absolute paths that aren't Windows paths either
        path = os.path.join(os.getcwd(), path)

    return path
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,
survived,"    def undo_edit(path: str) -> FileOperationResult:
        """"""
        Placeholder for undo_edit functionality.
        In a real implementation, you would need to track edit history.

        Args:
            path: The path to the file whose last edit should be undone

        Returns:
            FileOperationResult with message about undo functionality
        """"""
        try:
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                console.log(f""[undo_edit] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            message = ""Undo functionality is not implemented in this version.""
            console.print(f""[yellow]{message}[/yellow]"")
            console.log(f""[undo_edit] {message}"")
            return FileOperationResult(True, message)
        except Exception as e:
            error_msg = f""Error in undo_edit: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[undo_edit] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/service.py,FileOperationService
survived,"def test_create_crew_with_parent_folder_and_trailing_slash(mock_load_env, mock_write_env, mock_copy_template, temp_dir):
    mock_load_env.return_value = {}
    
    with tempfile.TemporaryDirectory() as work_dir:
        parent_path = Path(work_dir) / ""parent""
        parent_path.mkdir()
        
        create_crew(""child-crew/"", skip_provider=True, parent_folder=parent_path)
        
        crew_path = parent_path / ""child_crew""
        assert crew_path.exists()
        assert not (crew_path / ""src"").exists()",tests/cli/test_create_crew.py,
survived,"    def _persist_state(flow_instance: Any, method_name: str) -> None:
        """"""Helper to persist state with error handling.""""""
        try:
            # Get flow UUID from state
            state = getattr(flow_instance, 'state', None)
            if state is None:
                raise ValueError(""Flow instance has no state"")
                
            flow_uuid: Optional[str] = None
            if isinstance(state, dict):
                flow_uuid = state.get('id')
            elif isinstance(state, BaseModel):
                flow_uuid = getattr(state, 'id', None)
                
            if not flow_uuid:
                raise ValueError(
                    ""Flow state must have an 'id' field for persistence""
                )
                
            # Persist the state
            persistence.save_state(
                flow_uuid=flow_uuid,
                method_name=method_name,
                state_data=state,
            )
        except Exception as e:
            logger.error(
                f""Failed to persist state for method {method_name}: {str(e)}""
            )
            raise RuntimeError(f""State persistence failed: {str(e)}"") from e
",src/crewai/flow/persistence/decorators.py,
survived,"            async def async_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original async method
                result = await method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
",src/crewai/flow/persistence/decorators.py,
survived,"        def count_up(self):
            self.state.counter += 1
            self.state.message = f""Count is {self.state.counter}""
",tests/test_flow_persistence.py,StructuredFlow
survived,"def test_multiple_method_persistence(tmp_path):
    """"""Test state persistence across multiple method executions.""""""
    db_path = os.path.join(tmp_path, ""test_flows.db"")
    persistence = SQLiteFlowPersistence(db_path)
    
    class MultiStepFlow(Flow[TestState]):
        initial_state = TestState
        
        @start()
        @persist(persistence)
        def step_1(self):
            self.state.counter = 1
            self.state.message = ""Step 1""
        
        @start()
        @persist(persistence)
        def step_2(self):
            self.state.counter = 2
            self.state.message = ""Step 2""
    
    flow = MultiStepFlow(persistence=persistence)
    flow.kickoff()
    
    # Load final state
    final_state = persistence.load_state(flow.state.id)
    assert final_state is not None
    assert final_state[""counter""] == 2
    assert final_state[""message""] == ""Step 2""
",tests/test_flow_persistence.py,
survived,"    def init_db(self) -> None:
        """"""Initialize the persistence backend.
        
        This method should handle any necessary setup, such as:
        - Creating tables
        - Establishing connections
        - Setting up indexes
        """"""
        pass
",src/crewai/flow/persistence/base.py,FlowPersistence
survived,"def superfluid(options: Optional[SuperfluidPluginOptions] = None) -> SuperfluidPlugin:
    """"""
    Create a new instance of the Superfluid plugin.
    
    Args:
        options: Optional configuration options for the plugin
        
    Returns:
        A configured SuperfluidPlugin instance
    """"""
    return SuperfluidPlugin(options)",python/src/plugins/superfluid/goat_plugins/superfluid/__init__.py,
survived,"async def test_xai_create_with_completion_async():
    """"""Test that create_with_completion works with XAI provider in async mode""""""
    client = instructor.from_provider(""xai/grok-3-mini"", mode=instructor.Mode.XAI_JSON, async_client=True)
    
    user, raw_response = await client.chat.completions.create_with_completion(
        response_model=User,
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""You are a helpful assistant that extracts information."",
            },
            {
                ""role"": ""user"",
                ""content"": ""Extract: Jason is 25 years old."",
            },
        ],
    )
    
    assert isinstance(user, User)
    assert user.name.lower() == ""jason""
    assert user.age == 25
    assert hasattr(user, ""_raw_response""), (
        ""The raw response should be available from XAI""
    )
    assert raw_response is not None
    assert user._raw_response == raw_response",tests/llm/test_xai/test_raw_response.py,
survived,"    def supports_stop_words(self) -> bool:
        """"""Return True to indicate that stop words are supported.""""""
        return True
",tests/custom_llm_test.py,CustomLLM
survived,"    def __init__(self, jwt_token: str):
        self.jwt_token = jwt_token
        self.calls = []
        self.stop = []
",tests/custom_llm_test.py,JWTAuthLLM
survived,"    def get_context_window_size(self) -> int:
        """"""Get the context window size of the LLM.
        
        Returns:
            The context window size as an integer.
        """"""
        pass
",src/crewai/llm.py,BaseLLM
survived,"def test_custom_llm_implementation():
    """"""Test that a custom LLM implementation works with create_llm.""""""
    custom_llm = CustomLLM(response=""The answer is 42"")
    
    # Test that create_llm returns the custom LLM instance directly
    result_llm = create_llm(custom_llm)
    
    assert result_llm is custom_llm
    
    # Test calling the custom LLM
    response = result_llm.call(""What is the answer to life, the universe, and everything?"")
    
    # Verify that the custom LLM was called
    assert len(custom_llm.calls) > 0
    # Verify that the response from the custom LLM was used
    assert response == ""The answer is 42""
",tests/custom_llm_test.py,
