status,method,filepath,class_name
survived,"def get_file_content_at_revision(file_path: Path, revision: str) -> Optional[str]:
    try:
        return subprocess.check_output([""git"", ""show"", f""{revision}:{file_path}""], text=True)
    except subprocess.CalledProcessError as e:
        print(f""Warning: Failed to get file content at revision: {e}"", file=sys.stderr)
        return None
",dev/check_function_signatures.py,
deleted,"def prepare_visualization_data(program_records, tensor_table):
    """"""Prepare visualization data for the frntend and raw tensor data for the server.""""""
    # global idx
    visualization_data = []
    raw_tensor_data = {}
    for record in program_records:
        record_uuid = str(uuid.uuid4())[:8]

        if isinstance(record, ExpandDims):
            print(record.input_shape, record.output_shape, record.index)
        if isinstance(record, Dot):
            visualization_data.append(
                {
                    ""type"": ""Dot"",
                    ""input_shape"": record.input_shape,
                    ""other_shape"": record.other_shape,
                    ""output_shape"": record.output_shape,
                    ""uuid"": record_uuid,
                }
            )

            raw_tensor_data[record_uuid] = {
                ""input_data"": torch.tensor(record.input_data),
                ""other_data"": torch.tensor(record.other_data),
                ""intermediate_results"": record.intermediate_results,
            }

        elif isinstance(record, Load):
            global_tensor, slice_tensor = tensor_table[record.ptr]
            print(global_tensor)
            global_coords, slice_coords = extract_load_coords(record, global_tensor)

            visualization_data.append(
                {
                    ""type"": ""Load"",
                    ""global_shape"": global_tensor.shape,
                    ""slice_shape"": record.masks.shape,
                    ""global_coords"": global_coords,
                    ""slice_coords"": slice_coords,
                    ""uuid"": record_uuid,
                }
            )

            raw_tensor_data[record_uuid] = {
                ""global_tensor"": global_tensor.data.cpu(),  # Ensure it's on CPU
                ""dims"": len(global_tensor.data.cpu().shape),
            }
            print(record.masks.shape)

        elif isinstance(record, Store):
            global_tensor, slice_tensor = tensor_table[record.ptr]

            global_coords, slice_coords = extract_load_coords(record, global_tensor)

            visualization_data.append(
                {
                    ""type"": ""Store"",
                    ""global_shape"": global_tensor.shape,
                    ""slice_shape"": record.masks.shape,
                    ""global_coords"": global_coords,
                    ""slice_coords"": slice_coords,
                    ""uuid"": record_uuid,
                }
            )

    return visualization_data, raw_tensor_data, """"
",triton_viz/visualizer/draw.py,
survived,"def pandas_rolling_corrmatrix(a, window=20, min_count=None):
    """"""Compute rolling correlation matrix using pandas.

    Note: Returns pandas MultiIndex DataFrame, not numbagg's 3D array format.
    For benchmark purposes, we compare the raw computation without format conversion.
    """"""
    rolling = pandas_rolling_matrix_setup(a, window, min_count)
    return lambda: rolling.corr()
",numbagg/test/conftest.py,
survived,"    def test_rolling_broadcasting_higher_dims(self, move_func):
        """"""Test that rolling functions broadcast correctly over higher dimensions.""""""
        np.random.seed(42)
        window = 5

        # 3D array: (2, 4, 20) -> output (2, 20, 4, 4)
        data_3d = np.random.randn(2, 4, 20)
        result_3d = move_func(data_3d, window=window)
        assert result_3d.shape == (2, 20, 4, 4)

        # 4D array: (2, 3, 4, 20) -> output (2, 3, 20, 4, 4)
        data_4d = np.random.randn(2, 3, 4, 20)
        result_4d = move_func(data_4d, window=window)
        assert result_4d.shape == (2, 3, 20, 4, 4)

        # Verify correctness - each slice should match individual computation
        for i in range(2):
            single_result = move_func(data_3d[i], window=window)
            assert_allclose(result_3d[i], single_result, rtol=1e-10)",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"def move_nancorrmatrix(a, window, min_count, out):
    """"""
    Moving window correlation matrix gufunc.

    For 2D input, correlates variables (rows) across observations (columns in the window).
    """"""
    n_vars = a.shape[0]
    n_obs = a.shape[1]
    min_count = max(min_count, 1)

    # Initialize running statistics
    sums = np.zeros(n_vars, dtype=a.dtype)
    sums_sq = np.zeros(n_vars, dtype=a.dtype)
    counts = np.zeros(n_vars, dtype=np.int64)

    # Initialize pairwise statistics
    prods = np.zeros((n_vars, n_vars), dtype=a.dtype)
    pair_counts = np.zeros((n_vars, n_vars), dtype=np.int64)

    for t in range(n_obs):
        # Remove old values when window slides
        if t >= window:
            for i in range(n_vars):
                old_val = a[i, t - window]
                if not np.isnan(old_val):
                    sums[i] -= old_val
                    sums_sq[i] -= old_val * old_val
                    counts[i] -= 1

                    # Update pairwise products
                    for j in range(n_vars):
                        old_val_j = a[j, t - window]
                        if not np.isnan(old_val_j):
                            prods[i, j] -= old_val * old_val_j
                            pair_counts[i, j] -= 1

        # Add new values
        for i in range(n_vars):
            new_val = a[i, t]
            if not np.isnan(new_val):
                sums[i] += new_val
                sums_sq[i] += new_val * new_val
                counts[i] += 1

                # Update pairwise products
                for j in range(n_vars):
                    new_val_j = a[j, t]
                    if not np.isnan(new_val_j):
                        prods[i, j] += new_val * new_val_j
                        pair_counts[i, j] += 1

        # Compute correlation matrix for current window
        for i in range(n_vars):
            for j in range(n_vars):
                if i == j:
                    # Diagonal is 1 when we have enough data for correlation
                    # Correlation requires at least 2 observations to compute variance
                    if counts[i] >= max(min_count, 2):
                        out[t, i, j] = 1.0
                    else:
                        out[t, i, j] = np.nan
                else:
                    # Compute correlation
                    n = pair_counts[i, j]
                    # Need at least 2 observations for correlation (to compute variance)
                    if n >= max(min_count, 2) and counts[i] >= 2 and counts[j] >= 2:
                        mean_i = sums[i] / counts[i]
                        mean_j = sums[j] / counts[j]

                        # Compute variances
                        var_i = sums_sq[i] / counts[i] - mean_i * mean_i
                        var_j = sums_sq[j] / counts[j] - mean_j * mean_j

                        # Compute covariance
                        cov = prods[i, j] / n - (sums[i] / counts[i]) * (
                            sums[j] / counts[j]
                        )

                        # Compute correlation
                        if var_i > 0 and var_j > 0:
                            corr = cov / np.sqrt(var_i * var_j)
                            # Clamp to [-1, 1] for numerical stability
                            out[t, i, j] = max(-1.0, min(1.0, corr))
                        else:
                            out[t, i, j] = np.nan
                    else:
                        out[t, i, j] = np.nan
",numbagg/moving_matrix.py,
survived,"    def test_rolling_comparison_with_pandas(self, move_func, window):
        """"""Compare rolling functions with pandas.""""""
        np.random.seed(42)
        n_vars = 4
        n_obs = 20
        data = np.random.randn(n_vars, n_obs)

        # NumBagg result
        numbagg_result = move_func(data, window=window, min_count=window)

        # Pandas result - need to transpose for pandas (wants observations as rows)
        df = pd.DataFrame(data.T)
        if move_func == move_nancorrmatrix:
            pandas_result = df.rolling(window, min_periods=window).corr()
        else:
            pandas_result = df.rolling(window, min_periods=window).cov()

        # Compare each window
        for t in range(window - 1, n_obs):
            # Extract pandas matrix for this timepoint
            pandas_matrix = pandas_result.loc[t].values
            # Compare
            assert_allclose(numbagg_result[t], pandas_matrix, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"def pandas_static_covmatrix(a):
    """"""Compute covariance matrix using pandas.""""""
    df = pandas_matrix_setup(a)
    return lambda: df.cov()
",numbagg/test/conftest.py,
survived,"    def test_perfect_correlation_consistency(self):
        """"""Test with perfectly correlated data.""""""
        np.random.seed(222)

        # Create perfectly correlated data
        n_obs = 15
        a1 = np.random.randn(n_obs)
        a2 = 2 * a1 + 1  # Perfect linear relationship

        alpha = 0.6

        # Compute using non-matrix function
        corr_nonmatrix = move_exp_nancorr(a1, a2, alpha=alpha)

        # Compute using matrix function
        data_matrix = np.array([a1, a2])
        corr_matrix_result = move_exp_nancorrmatrix(data_matrix, alpha=alpha)
        corr_from_matrix = corr_matrix_result[:, 0, 1]

        # They should match and approach 1.0
        assert_allclose(corr_nonmatrix, corr_from_matrix, rtol=1e-10)

        # For later time points with perfect correlation, should be close to 1.0
        final_corr = corr_from_matrix[-1]
        assert abs(final_corr - 1.0) < 1e-10
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency
survived,"    def test_with_nans(self, func):
        """"""Test with NaN values.""""""
        data = np.array(
            [[1, 2, np.nan, 4], [2, 4, 6, np.nan], [np.nan, 1, 2, 3]], dtype=np.float64
        )
        alpha = 0.3
        result = func(data, alpha=alpha)

        # Check shape
        assert result.shape == (4, 3, 3)

        # Should handle NaN gracefully - check that we get some finite values
        assert np.any(np.isfinite(result))
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions
survived,"    def test_bias_correction_edge_cases(self):
        """"""Test bias correction in edge cases.""""""
        # Create scenario where bias correction might be problematic
        data = np.array(
            [[1, np.nan, np.nan, 2], [3, np.nan, np.nan, 4]], dtype=np.float64
        )

        # Very low alpha means slow weight accumulation
        result = move_exp_nancovmatrix(data, alpha=0.01, min_weight=0.001)

        # Should handle the sparse data gracefully
        # Early time steps should be NaN due to insufficient weight
        assert np.isnan(result[0, 0, 1])
        assert np.isnan(result[1, 0, 1])

        # Final result should be valid if enough weight accumulated
        final_result = result[-1]
        if not np.isnan(final_result[0, 1]):
            # If not NaN, should be finite
            assert np.isfinite(final_result[0, 1])
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"    def _format_text(self, data: Any) -> str:
        """"""Format as human-readable text""""""
        if isinstance(data, dict):
            return self._dict_to_text(data)
        elif isinstance(data, list):
            return self._list_to_text(data)
        else:
            return str(data)
",src/haconiwa/scan/formatter.py,OutputFormatter
survived,"    def setup_method(self):
        """"""Setup test environment""""""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.generator = ParallelYAMLGenerator(base_path=self.temp_dir)
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def test_scan_compare_command(self, runner, temp_model_dir):
        """"""Test the scan compare command""""""
        # Create another model for comparison
        model2_dir = temp_model_dir / ""models"" / ""test"" / ""gpt-4""
        model2_dir.mkdir(parents=True)
        
        config2 = {
            ""model_name"": ""gpt-4"",
            ""model_type"": ""language"",
            ""parameters"": ""175B""
        }
        with open(model2_dir / ""config.json"", ""w"") as f:
            json.dump(config2, f)
        
        result = runner.invoke(
            scan_app,
            [""compare"", ""o1-mini"", ""gpt-4"", ""--path"", str(temp_model_dir), ""--format"", ""json""]
        )
        
        assert result.exit_code == 0
        output = json.loads(result.stdout)
        assert 'capabilities' in output
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_scan_model_with_options(self, runner, temp_model_dir):
        """"""Test scan model with various options""""""
        # Test with content inclusion
        result = runner.invoke(
            scan_app,
            [""model"", ""o1-mini"", ""--path"", str(temp_model_dir), ""--content"", ""--format"", ""yaml""]
        )
        
        assert result.exit_code == 0
        assert ""model_name: o1-mini"" in result.stdout
        
        # Test with no prefix stripping
        result = runner.invoke(
            scan_app,
            [""model"", ""o1-mini"", ""--path"", str(temp_model_dir), ""--no-strip-prefix""]
        )
        
        assert result.exit_code == 0
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_dynamic_concurrency_calculation(self):
        """"""Test dynamic max_concurrent calculation""""""
        scan_results = {
            'files': {f'file{i}.py': {} for i in range(20)}
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=20
        )
        
        # Should be min(5, max(1, 20//2)) = 5
        assert config['options']['max_concurrent'] == 5
        
        # Test with fewer files
        scan_results = {
            'files': {'file1.py': {}, 'file2.py': {}}
        }
        
        config = self.generator.generate_from_scan_results(
            scan_results,
            action='refactor',
            max_files=2
        )
        
        # Should be min(5, max(1, 2//2)) = 1
        assert config['options']['max_concurrent'] == 1
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def test_generate_prompt_for_file_by_category(self):
        """"""Test prompt generation based on file category""""""
        # Model file
        prompt = self.generator._generate_prompt_for_file(
            'src/models/user.py',
            'validation',
            None
        )
        assert 'validation methods' in prompt
        
        # API file
        prompt = self.generator._generate_prompt_for_file(
            'src/api/routes.py',
            'endpoints',
            None
        )
        assert 'RESTful CRUD endpoints' in prompt
        
        # Utils file
        prompt = self.generator._generate_prompt_for_file(
            'src/utils/helpers.py',
            'type_hints',
            None
        )
        assert 'type hints' in prompt
        
        # Config file
        prompt = self.generator._generate_prompt_for_file(
            'src/config/settings.py',
            'validation',
            None
        )
        assert 'configuration validation' in prompt
        
        # Service file
        prompt = self.generator._generate_prompt_for_file(
            'src/services/auth.py',
            'implementation',
            None
        )
        assert 'service functionality' in prompt
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def test_extract_files_from_matches(self):
        """"""Test file extraction from match results""""""
        matches = {
            'model': [
                {'path': 'file1.py'},
                {'path': 'file2.py'}
            ],
            'api': [
                {'path': 'file3.py'},
                {'path': 'file4.py'}
            ]
        }
        
        files = self.generator._extract_files_from_matches(matches, max_files=3)
        
        assert len(files) == 3
        assert 'file1.py' in files
        assert 'file2.py' in files
        assert 'file3.py' in files
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _generate_usage_guide(self, model_info: Dict[str, Any]) -> str:
        """"""Generate a usage guide""""""
        lines = [
            f""# Usage Guide: {model_info['name']}"",
            f""\nGenerated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"",
            ""\n## Quick Start""
        ]
        
        # Basic usage
        lines.extend([
            ""\n### Basic Usage"",
            ""\n```python"",
            f""# Using {model_info['name']}"",
            """",
            ""# 1. Import necessary libraries"",
            ""import json"",
            ""from pathlib import Path"",
            """",
            ""# 2. Load model configuration"",
            ""config_path = Path('config.json')"",
            ""with open(config_path, 'r') as f:"",
            ""    config = json.load(f)"",
            """",
            ""# 3. Initialize and use model"",
            ""# Add framework-specific code here"",
            ""```""
        ])
        
        # Common use cases
        lines.extend([
            ""\n## Common Use Cases"",
            f""\nBased on the model structure, {model_info['name']} can be used for:""
        ])
        
        if 'llm' in str(model_info['categories']).lower():
            lines.extend([
                ""\n### Text Generation"",
                ""- Content creation"",
                ""- Code generation"",
                ""- Language translation"",
                ""- Text summarization""
            ])
        
        if 'vision' in str(model_info['categories']).lower():
            lines.extend([
                ""\n### Computer Vision"",
                ""- Image classification"",
                ""- Object detection"",
                ""- Image generation"",
                ""- Visual analysis""
            ])
        
        # Parameters
        if model_info['config']:
            lines.extend([
                ""\n## Model Parameters"",
                ""\nKey configuration options:""
            ])
            
            for key, value in list(model_info['config'].items())[:10]:
                lines.append(f""- `{key}`: {value}"")
        
        # Examples from files
        if model_info['examples']:
            lines.extend([
                ""\n## Code Examples"",
                ""\nExample files available:""
            ])
            
            for example in model_info['examples'][:3]:
                lines.append(f""\n### {example['name']}"")
                if example.get('content'):
                    lines.append(""```python"")
                    lines.append(example['content'][:300])
                    if len(example['content']) > 300:
                        lines.append(""# ... (truncated)"")
                    lines.append(""```"")
        
        # Tips
        lines.extend([
            ""\n## Tips and Tricks"",
            ""\n1. **Performance Optimization**:"",
            ""   - Use appropriate batch sizes"",
            ""   - Enable GPU acceleration if available"",
            ""   - Consider model quantization for deployment"",
            """",
            ""2. **Error Handling**:"",
            ""   - Validate input data formats"",
            ""   - Handle out-of-memory errors gracefully"",
            ""   - Implement timeout mechanisms"",
            """",
            ""3. **Best Results**:"",
            ""   - Preprocess input data appropriately"",
            ""   - Use recommended hyperparameters"",
            ""   - Fine-tune for specific use cases""
        ])
        
        return ""\n"".join(lines)
",src/haconiwa/scan/guide_generator.py,GuideGenerator
survived,"    def test_rolling_comparison_with_pandas(self, move_func, window):
        """"""Compare rolling functions with pandas.""""""
        np.random.seed(42)
        n_vars = 4
        n_obs = 20
        # Moving functions expect (obs, vars) format
        data = np.random.randn(n_obs, n_vars)

        # NumBagg result
        numbagg_result = move_func(data, window=window, min_count=window)

        # Pandas result - data is already in (obs, vars) format that pandas expects
        df = pd.DataFrame(data)
        if move_func == move_nancorrmatrix:
            pandas_result = df.rolling(window, min_periods=window).corr()
        else:
            pandas_result = df.rolling(window, min_periods=window).cov()

        # Compare each window
        for t in range(window - 1, n_obs):
            # Extract pandas matrix for this timepoint
            pandas_matrix = pandas_result.loc[t].values
            # Compare
            assert_allclose(numbagg_result[t], pandas_matrix, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices
survived,"    def test_constant_variables(self, func):
        """"""Test with constant (zero variance) variables.""""""
        data = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [1, 2, 3, 4]], dtype=np.float64)
        result = func(data)

        if func == nancorrmatrix:
            # Correlation with constant variables should be NaN
            assert np.isnan(result[0, 1])  # Two constants
            assert np.isnan(result[0, 2])  # Constant with non-constant
            assert result[2, 2] == 1.0  # Variable with itself
        else:
            # Covariance of constants should be 0
            assert result[0, 0] == 0.0
            assert result[1, 1] == 0.0
            assert result[0, 1] == 0.0
",numbagg/test/test_matrix_functions.py,TestCorrelationCovarianceMatrices
survived,"    def test_simple_matrix(self, func, expected_diag):
        """"""Test simple 2x2 matrix calculation with exponential decay.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 2], [2, 4], [3, 6], [4, 8]], dtype=np.float64)
        alpha = 0.5
        result = func(data, alpha=alpha)

        # Check shape - should be (time, vars, vars)
        assert result.shape == (4, 2, 2)

        # Check diagonal at the end
        final_result = result[-1]
        if expected_diag is not None:
            assert_allclose(
                np.diag(final_result), [expected_diag, expected_diag], rtol=1e-10
            )
        else:
            # For covariance, just check diagonal is non-negative
            assert np.all(np.diag(final_result) >= 0)

        # Check symmetry at each time step
        for t in range(result.shape[0]):
            assert_allclose(result[t], result[t].T, rtol=1e-10)

        # For perfect linear relationship, correlation should be 1
        if func == move_exp_nancorrmatrix:
            # Check that off-diagonal elements approach 1 as we get more data
            assert_allclose(final_result, [[1.0, 1.0], [1.0, 1.0]], rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def test_docker_detection(self, mock_which):
        """"""Test Docker executable detection.""""""

        # Test when Docker is available
        mock_which.return_value = ""/usr/bin/docker""
        assert shutil.which(""docker"") is not None

        # Test when Docker is not available
        mock_which.return_value = None
        assert shutil.which(""docker"") is None
",tests/unit/test_windows_compatibility.py,TestCrossPlatformDetection
survived,"    def __enter__(self):
        """"""Context manager entry.""""""
        return self
",ocode_python/core/context_manager.py,ContextManager
survived,"    def test_run_with_uv_all_options(self, mock_run):
        """"""Test run_with_uv with all options combined.""""""
        mock_run.return_value = Mock(returncode=0)

        with pytest.raises(SystemExit) as exc_info:
            run_with_uv(
                ""server.py"",
                python_version=""3.10"",
                project=Path(""/workspace""),
                with_packages=[""pandas""],
                with_requirements=Path(""reqs.txt""),
                transport=""http"",
                port=9000,
                show_banner=False,
            )

        assert exc_info.value.code == 0

        cmd = mock_run.call_args[0][0]
        expected = [
            ""uv"",
            ""run"",
            ""--python"",
            ""3.10"",
            ""--project"",
            ""/workspace"",
            ""--with"",
            ""fastmcp"",
            ""--with"",
            ""pandas"",
            ""--with-requirements"",
            ""reqs.txt"",
            ""fastmcp"",
            ""run"",
            ""server.py"",
            ""--transport"",
            ""http"",
            ""--port"",
            ""9000"",
            ""--no-banner"",
        ]
        assert cmd == expected
",tests/cli/test_run_with_uv.py,TestRunWithUv
survived,"    def test_run_command_parsing_with_new_options(self):
        """"""Test run command parsing with new uv options.""""""
        command, bound, _ = app.parse_args(
            [
                ""run"",
                ""server.py"",
                ""--python"",
                ""3.11"",
                ""--with"",
                ""pandas"",
                ""--with"",
                ""numpy"",
                ""--project"",
                ""/path/to/project"",
                ""--with-requirements"",
                ""requirements.txt"",
            ]
        )

        assert command is not None
        assert bound.arguments[""server_spec""] == ""server.py""
        assert bound.arguments[""python""] == ""3.11""
        assert bound.arguments[""with_packages""] == [""pandas"", ""numpy""]
        assert bound.arguments[""project""] == Path(""/path/to/project"")
        assert bound.arguments[""with_requirements""] == Path(""requirements.txt"")
",tests/cli/test_cli.py,TestRunCommand
survived,"    def test_with_requirements_option(self):
        """"""Test --with-requirements option for all install commands.""""""
        commands_to_test = [
            [""claude-code"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""claude-desktop"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""cursor"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
            [""mcp-json"", ""server.py"", ""--with-requirements"", ""requirements.txt""],
        ]

        for cmd_args in commands_to_test:
            command, bound, _ = install_app.parse_args(cmd_args)
            assert command is not None
            assert str(bound.arguments[""with_requirements""]) == ""requirements.txt""
",tests/cli/test_install.py,TestInstallCommandParsing
survived,"    def get_checkpoint_path(self, filename: str) -> str:
        """"""Get full path for a checkpoint file.""""""
        return os.path.join(self.checkpoint_dir, filename)
",kura/v1/kura.py,CheckpointManager
survived,"    def setup_checkpoint_dir(self) -> None:
        """"""Create checkpoint directory if it doesn't exist.""""""
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)
            logger.info(f""Created checkpoint directory: {self.checkpoint_dir}"")
",kura/v1/kura.py,CheckpointManager
survived,"    def test_len_method(self):
        """"""Test __len__ method.""""""
        pipeline = LearnerPipeline(steps=[(""scale"", StandardScaler())], learner=MockLearner())
        assert len(pipeline) == 1  # Only transformer steps
",tests/test_learner_pipeline.py,TestLearnerPipelineProperties
survived,"    def test_pre_fitted_transformers(self):
        """"""Test pre-fitted transformers work correctly.""""""
        scaler = StandardScaler()
        X_train = np.array([[1, 2], [3, 4], [5, 6]])
        scaler.fit(X_train)  # Pre-fit

        mock_learner = MockLearner()
        pipeline = LearnerPipeline(steps=[(""scale"", scaler)], learner=mock_learner)

        X = np.array([[2, 3]])
        y = np.array([1])

        pipeline.partial_fit(X, y)

        # Should use pre-fitted scaler
        received_X, _, _ = mock_learner.partial_fit_calls[0]
        expected_X = scaler.transform(X)
        np.testing.assert_array_almost_equal(received_X, expected_X)
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers
survived,"    def policy(self, value):
        """"""Set the policy on the wrapped agent.""""""
        self._agent.policy = value
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def test_noncontextual_pipeline_decay_with_rate(self):
        """"""Test non-contextual pipeline decay with explicit rate.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())
        pipeline = NonContextualAgentPipeline([], agent)

        pipeline.decay(decay_rate=0.7)
",tests/test_agent_pipeline.py,TestCoverage
survived,"    def rng(self):
        """"""Get the random generator from the wrapped agent.""""""
        return self._agent.rng
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline
survived,"    def __init__(self, fitted=False):
        self.fitted = fitted
",tests/test_agent_pipeline.py,MockTransformer
survived,"    def test_transform_single_step(self):
        """"""Test transformation with single step.""""""
        steps = [(""double"", FunctionTransformer(lambda x: x * 2))]
        X = np.array([[1], [2], [3]])
        result = _transform_data(X, steps)
        expected = np.array([[2], [4], [6]])
        np.testing.assert_array_equal(result, expected)
",tests/test_agent_pipeline.py,TestTransformData
survived,"    def setup_method(self):
        """"""Set up test pipeline.""""""
        self.mock_learner = MockLearner()
        # Pre-fit the scaler for testing
        scaler = StandardScaler()
        scaler.fit(np.random.randn(100, 2))  # Fit on dummy data
        self.pipeline = LearnerPipeline(steps=[(""scale"", scaler)], learner=self.mock_learner)
",tests/test_learner_pipeline.py,TestLearnerPipelineInterface
survived,"    def arm_to_update(self):
        """"""Get the arm to update from the wrapped agent.""""""
        return self._agent.arm_to_update
",bayesianbandits/pipelines/_agent.py,NonContextualAgentPipeline
survived,"    def test_recommendation_system_scenario(self, policy_class):
        """"""Test realistic recommendation system scenario.""""""
        # Create product arms
        product_arms = make_arms([f""product_{i}"" for i in range(5)])

        # Create agent with preprocessing
        agent = ContextualAgent(product_arms, policy_class(), random_seed=42)

        # Pre-fit scaler on historical user data
        scaler = StandardScaler()
        historical_users = np.random.randn(1000, 3)  # user features
        scaler.fit(historical_users)

        # Create pipeline with preprocessing
        steps = [(""user_scaler"", scaler)]
        pipeline = AgentPipeline(steps, agent)

        # Simulate user interactions
        n_users = 20
        user_contexts = np.random.randn(n_users, 3)

        # Pull recommendations
        recommendations = pipeline.pull(user_contexts)
        assert len(recommendations) == n_users
        assert all(rec.startswith(""product_"") for rec in recommendations)

        # Simulate rewards and update
        rewards = np.random.beta(2, 5, size=n_users)  # Realistic reward distribution
        pipeline.update(user_contexts, rewards)

        # Verify models were updated
        for arm in pipeline.arms:
            assert hasattr(arm.learner, ""coef_"")
",tests/test_agent_pipeline.py,TestIntegrationScenarios
survived,"    def test_transform_not_fitted_error(self):
        """"""Test helpful error when transformer not fitted.""""""
        steps = [(""mock"", MockTransformer(fitted=False))]
        X = np.array([[1], [2]])

        with pytest.raises(RuntimeError) as exc_info:
            _transform_data(X, steps)

        assert ""not fitted"" in str(exc_info.value)
        assert ""mock"" in str(exc_info.value)
        assert ""FunctionTransformer"" in str(exc_info.value)
",tests/test_agent_pipeline.py,TestTransformData
survived,"    def remove_arm(self, token: TokenType) -> None:
        """"""Remove an arm from the wrapped agent.""""""
        self._agent.remove_arm(token)
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def _analyze_project_structure(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze the overall project structure.""""""
        structure = {""directories"": [], ""key_files"": [], ""patterns"": []}
        try:
            for root, dirs, files in os.walk(project_path):
                rel_path = os.path.relpath(root, project_path)
                if rel_path != ""."":
                    structure[""directories""].append(rel_path)
                for file in files:
                    if file in [""README.md"", ""setup.py"", ""package.json"", ""Cargo.toml"", ""pom.xml""]:
                        structure[""key_files""].append(os.path.join(rel_path, file))
        except Exception as e:
            structure[""error""] = str(e)
        return structure
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _generate_validation_criteria(self, requirements: str, analysis: Dict[str, Any]) -> str:
        """"""Generate validation criteria based on requirements and analysis.""""""
        return f""1. Implementation matches requirements: {requirements}\n2. Follows identified code patterns\n3. Maintains architectural consistency""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _identify_testing_frameworks(self, project_path: str) -> List[str]:
        """"""Identify testing frameworks in use.""""""
        return [""pytest"", ""unittest""]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def test_basic_functionality():
    """"""Test basic functionality of ContextAgent methods.""""""
    print(""\nðŸ§ª Testing Basic Functionality..."")
    
    try:
        from praisonaiagents import create_context_agent
        
        context_agent = create_context_agent()
        
        # Test codebase analysis with current directory
        test_path = str(Path(__file__).parent)
        analysis = context_agent.analyze_codebase_patterns(test_path)
        assert isinstance(analysis, dict), ""analyze_codebase_patterns should return dict""
        print(""âœ… analyze_codebase_patterns works correctly"")
        
        # Test context document generation
        context_doc = context_agent.generate_context_document(
            project_path=test_path,
            requirements=""Test feature implementation""
        )
        assert isinstance(context_doc, str), ""generate_context_document should return string""
        assert len(context_doc) > 100, ""Context document should be substantial""
        print(""âœ… generate_context_document works correctly"")
        
        # Test validation loop creation
        validation = context_agent.create_validation_loop(
            implementation_requirements=""Test implementation"",
            success_criteria=[""Test passes"", ""Code works""]
        )
        assert isinstance(validation, dict), ""create_validation_loop should return dict""
        assert 'validation_steps' in validation, ""Should have validation_steps""
        print(""âœ… create_validation_loop works correctly"")
        
        # Test prompt enhancement
        enhanced = context_agent.enhance_prompt_with_context(
            base_prompt=""Test prompt"",
            context_data={""test"": ""data""}
        )
        assert isinstance(enhanced, str), ""enhance_prompt_with_context should return string""
        assert len(enhanced) > len(""Test prompt""), ""Enhanced prompt should be longer""
        print(""âœ… enhance_prompt_with_context works correctly"")
        
        # Test PRP generation
        prp = context_agent.generate_prp(
            feature_request=""Test feature"",
            context_analysis=analysis
        )
        assert isinstance(prp, str), ""generate_prp should return string""
        assert ""PRP"" in prp, ""Should contain PRP reference""
        print(""âœ… generate_prp works correctly"")
        
        return True
        
    except Exception as e:
        print(f""âŒ Functionality test failed: {e}"")
        return False
",test_context_agent.py,
survived,"    def _extract_architecture_guidance(self, context_data: Dict[str, Any]) -> str:
        """"""Extract architecture guidance from context data.""""""
        return ""Follow established architectural patterns identified in the analysis.""
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _analyze_docstring_format(self, project_path: str) -> Dict[str, Any]:
        """"""Analyze docstring format conventions.""""""
        return {""format"": ""google"", ""completeness"": ""partial""}
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _generate_executable_tests(self, requirements: str, criteria: List[str]) -> List[Dict[str, str]]:
        """"""Generate executable test specifications.""""""
        return [{""type"": ""unit_test"", ""description"": f""Test {criterion}""} for criterion in criteria]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def estimate_tokens_for_request(self, request: FenicCompletionsRequest) -> TokenEstimate:
        """"""Estimate the number of tokens for a request.

        Args:
            request: The request to estimate tokens for

        Returns:
            TokenEstimate: The estimated token usage
        """"""
        return self._core.estimate_tokens_for_request(request)
",src/fenic/_inference/openai/openai_batch_chat_completions_client.py,OpenAIBatchChatCompletionsClient
survived,"    def __init__(self, params: list[str]) -> None:
        self.params = params
",dev/clint/src/clint/rules/docstring_param_order.py,DocstringParamOrder
survived,"    def _message(self) -> str:
        return ""Module loaded by `LazyLoader` must be imported in `TYPE_CHECKING` block.""",dev/clint/src/clint/rules/lazy_module.py,LazyModule
survived,"    def id(self) -> str:
        return self._generated_id
",dev/clint/src/clint/rules/base.py,Rule
survived,"    def name(self) -> str:
        """"""
        The name of this rule.
        """"""
        return self._CLASS_NAME_TO_RULE_NAME_REGEX.sub(""-"", self.__class__.__name__).lower()",dev/clint/src/clint/rules/base.py,Rule
survived,"    def example(cls) -> ""ModelVersionTagSetPayload"":
        return cls(
            name=""example_model"",
            version=""1"",
            key=""example_key"",
            value=""example_value"",
        )
",mlflow/webhooks/types.py,ModelVersionTagSetPayload
survived,"    def example(cls) -> ""ModelVersionCreatedPayload"":
        return cls(
            name=""example_model"",
            version=""1"",
            source=""runs:/abcd1234abcd5678/model"",
            run_id=""abcd1234abcd5678"",
            tags={""example_key"": ""example_value""},
            description=""An example model version"",
        )
",mlflow/webhooks/types.py,ModelVersionCreatedPayload
survived,"            def w_NEW(vm: 'SPyVM', wop_cls: W_OpArg,
                     *args_wop: W_OpArg) -> W_OpImpl:
                # Support overloading based on argument count
                if len(args_wop) == 1:
                    # Point(x) -> Point(x, x)
                    @builtin_func('ext', 'new_point_single')
                    def w_new(vm: 'SPyVM', w_cls: W_Type, w_x: W_I32) -> W_Point:
                        return W_Point(w_x, w_x)
                    return W_OpImpl(w_new)
                else:
                    # Normal Point(x, y)
                    @builtin_func('ext', 'new_point')
                    def w_new(vm: 'SPyVM', w_cls: W_Type,
                              w_x: W_I32, w_y: W_I32) -> W_Point:
                        return W_Point(w_x, w_y)
                    return W_OpImpl(w_new)
",spy/tests/compiler/test_operator_call.py,TestCallOp.W_Point
survived,"    def test_oparg_properties(self):
        mod = self.compile(
        """"""
        from operator import OpArg

        def foo() -> tuple:
            arg = OpArg('blue', i32, 42)
            return (arg.color, arg.static_type, arg.blueval)
        """""")
        w_tup = mod.foo(unwrap=False)
        w_color, w_type, w_blueval = w_tup.items_w
        assert self.vm.unwrap_str(w_color) == 'blue'
        assert w_type is B.w_i32
        assert self.vm.unwrap_i32(w_blueval) == 42
",spy/tests/compiler/test_opimpl.py,TestOpImpl
survived,"    def test_definitions(self, mock_definitions):
        """"""Test the implementation of the definitions method""""""
        # Setup mock response
        mock_definitions.return_value = [
            (""GO:0005634"", ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated."", {}),
            (""GO:0005635"", ""The double lipid bilayer enclosing the nucleus and separating its contents from the rest of the cytoplasm."", {})
        ]
        
        # Test definitions retrieval
        definitions = list(self.oi.definitions([""GO:0005634"", ""GO:0005635""], include_metadata=True))
        
        # Check that we got two definitions back with expected content
        self.assertEqual(len(definitions), 2)
        
        # Check first definition
        self.assertEqual(definitions[0][0], ""GO:0005634"")
        self.assertEqual(definitions[0][1], ""A membrane-bounded organelle of eukaryotic cells in which chromosomes are housed and replicated."")
        self.assertEqual(definitions[0][2], {}) # Empty metadata dict
        
        # Check second definition
        self.assertEqual(definitions[1][0], ""GO:0005635"")
        self.assertEqual(definitions[1][1], ""The double lipid bilayer enclosing the nucleus and separating its contents from the rest of the cytoplasm."")
        self.assertEqual(definitions[1][2], {}) # Empty metadata dict
        
        # Verify the mock was called correctly
        mock_definitions.assert_called_with([""GO:0005634"", ""GO:0005635""], include_metadata=True)
",tests/test_implementations/test_ols.py,TestOlsImplementation
survived,"def test_api_key_logging():
    """"""Test that api_key provision is logged correctly.""""""
    from unittest.mock import patch, MagicMock

    # Mock the openai module
    with patch(""openai.OpenAI"") as mock_openai_class:
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client

        # Mock the from_openai import
        with patch(""instructor.from_openai"") as mock_from_openai:
            mock_instructor = MagicMock()
            mock_from_openai.return_value = mock_instructor

            # Mock logger
            with patch(""instructor.auto_client.logger"") as mock_logger:
                # Test that providing api_key triggers debug log
                from_provider(""openai/gpt-4"", api_key=""test-key"")

                # Check that debug was called with api_key message and length
                debug_calls = [
                    call
                    for call in mock_logger.debug.call_args_list
                    if ""API key provided"" in str(call) and ""length:"" in str(call)
                ]
                assert len(debug_calls) > 0, (
                    ""Expected debug log for API key provision with length""
                )

                # Verify the length is logged correctly (test-key is 8 characters)
                mock_logger.debug.assert_called_with(
                    ""API key provided for %s provider (length: %d characters)"",
                    ""openai"",
                    8,
                    extra={""provider"": ""openai"", ""operation"": ""initialize""},
                )",tests/test_auto_client.py,
survived,"    def test_axis_parameter(self):
        # Test with different axes
        data = np.random.randn(3, 4, 5)

        # Default should correlate along last axis
        result_default = nancorrmatrix(data)
        assert result_default.shape == (3, 4, 4)

        # Test with axis=0
        result_0 = nancorrmatrix(data, axis=0)
        assert result_0.shape == (4, 5, 5)

        # Test with axis=1
        result_1 = nancorrmatrix(data, axis=1)
        assert result_1.shape == (3, 5, 5)
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix
survived,"        def __get__(self, obj: Any, objtype: Any = None) -> Any:
            warnings.warn(
                ""`jaxls.FactorGraph` has been renamed `jaxls.LeastSquaresProblem`"",
                DeprecationWarning,
                stacklevel=2,
            )

            class FactorGraph:
                @staticmethod
                def make(*args, **kwargs):
                    from ._core import FactorGraph

                    if ""factors"" in kwargs:
                        kwargs[""costs""] = kwargs.pop(""factors"")

                    warnings.warn(
                        ""`jaxls.FactorGraph` has been renamed `jaxls.FactorGraph`"",
                        DeprecationWarning,
                        stacklevel=2,
                    )

                    return FactorGraph(*args, **kwargs)

            return FactorGraph
",src/jaxls/__init__.py,_FactorGraphDescriptor
survived,"    async def progress_tool(context: Context) -> None:
        await context.report_progress(progress=1, total=10, message=""test"")
",tests/server/middleware/test_middleware.py,
survived,"    async def test_call_tool_on_parent_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.call_tool(""add"", {""a"": 1, ""b"": 2})

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""tools/call"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_call_tool"", times=1)

        assert nested_middleware.assert_called(times=0)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    def assert_not_called(self, hook: str | None = None, method: str | None = None):
        """"""Assert that a hook was not called.""""""
        calls = self.get_calls(hook=hook, method=method)
        assert len(calls) == 0, f""Expected {hook!r} to not be called""
        return True
",tests/server/middleware/test_middleware.py,RecordingMiddleware
survived,"        async def log_tool(context: Context) -> None:
            await context.info(message=""test log"")
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    async def _list_resources(self, apply_middleware: bool = True) -> list[Resource]:
        """"""
        List all available resources.
        """"""

        if (resources := self._cache.get(""resources"")) is self._cache.NOT_FOUND:
            resources: list[Resource] = []

            # iterate such that new mounts overwrite older ones
            for mounted_server in self._mounted_servers:
                try:
                    if apply_middleware:
                        server_resources = (
                            await mounted_server.server._middleware_list_resources()
                        )
                    else:
                        server_resources = await mounted_server.server._list_resources()
                    # Apply prefix to each resource key if prefix exists
                    if mounted_server.prefix:
                        for resource in server_resources:
                            resource = resource.with_key(
                                add_resource_prefix(
                                    resource.key,
                                    mounted_server.prefix,
                                    self.resource_prefix_format,
                                )
                            )
                            resources.append(resource)
                    else:
                        resources.extend(server_resources)
                except Exception as e:
                    logger.warning(
                        f""Failed to get resources from mounted server '{mounted_server.prefix}': {e}""
                    )
                    continue
            resources.extend(self._resource_manager.get_resources().values())
            self._cache.set(""resources"", resources)
        return resources
",src/fastmcp/server/server.py,FastMCP
survived,"    async def test_read_resource_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.read_resource(""resource://nested/test"")

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/read"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_read_resource"", times=1)

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(method=""resources/read"", times=3)
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(hook=""on_read_resource"", times=1)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    async def handle_progress(
        progress_token: str | int,
        progress: float,
        total: float | None,
        message: str | None,
    ):
        print(""HI"")
",tests/server/middleware/test_middleware.py,
survived,"    async def test_read_resource(
        self, mcp_server: FastMCP, recording_middleware: RecordingMiddleware
    ):
        async with Client(mcp_server) as client:
            await client.read_resource(""resource://test"")

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/read"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_read_resource"", times=1)
",tests/server/middleware/test_middleware.py,TestMiddlewareHooks
survived,"def compile(query: str, target: str = None) -> str:
    """"""
    Compile a Wvlet query into SQL.
    
    Args:
        query (str): The Wvlet query to compile.
        target (str): Optional target database (e.g., 'trino', 'duckdb').
    
    Returns:
        str: The compiled SQL query.
    
    Raises:
        ValueError: If the compilation fails.
    """"""
    compiler = WvletCompiler(target=target)
    return compiler.compile(query)",sdks/python/wvlet/__init__.py,
deleted,"def get_git_version():
    """"""Get the current version from Git tags or fallback to 'latest'.""""""
    try:
        # Check if current HEAD matches any tag (exact release)
        result = subprocess.run(
            [""git"", ""tag"", ""--points-at"", ""HEAD""],
            capture_output=True,
            text=True,
            cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        )
        if result.returncode == 0 and result.stdout.strip():
            tags = result.stdout.strip().split('\n')
            # Prefer semantic version tags
            for tag in tags:
                if tag.startswith('v') and len(tag.split('.')) >= 3:
                    return tag[1:]  # Remove 'v' prefix
            # Fallback to first tag
            return tags[0][1:] if tags[0].startswith('v') else tags[0]
    except (subprocess.SubprocessError, FileNotFoundError):
        pass

    try:
        # Get the latest semantic version tag for development builds
        result = subprocess.run(
            [""git"", ""tag"", ""--list""],
            capture_output=True,
            text=True,
            cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        )
        if result.returncode == 0 and result.stdout.strip():
            # Filter for semantic version tags and get the latest
            tags = result.stdout.strip().split('\n')
            version_tags = []
            for tag in tags:
                if tag.startswith('v') and len(tag.split('.')) >= 3:
                    try:
                        # Check if it's a proper semantic version
                        parts = tag[1:].split('.')
                        if len(parts) >= 3 and all(part.isdigit() for part in parts[:3]):
                            version_tags.append(tag)
                    except (ValueError, IndexError):
                        continue
            
            if version_tags:
                # Sort by version number and get the latest
                version_tags.sort(key=lambda x: [int(part) for part in x[1:].split('.')[:3]])
                latest_tag = version_tags[-1]
                
                # Check if we're on the exact tag or ahead of it
                try:
                    result = subprocess.run(
                        [""git"", ""describe"", ""--tags"", ""--exact-match"", ""HEAD""],
                        capture_output=True,
                        text=True,
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                    )
                    if result.returncode == 0:
                        # We're on an exact tag
                        return latest_tag[1:]
                except (subprocess.SubprocessError, FileNotFoundError):
                    pass
                
                # We're ahead of the latest tag, add dev suffix
                return f""{latest_tag[1:]}.dev""
    except (subprocess.SubprocessError, FileNotFoundError):
        pass
    
    # Default fallback
    return ""latest""
",docs/source/conf.py,
survived,"    def __init__(self):
        self.calls = 0
        self.node_id = ""X""
",tests/test_register_mesh_backoff.py,StubClient
survived,"    def add_check(self, check: Callable[[str], Awaitable[str]]) -> None:
        """"""Register a custom check plugin.""""""

        self.checks.append(check)
",src/meta_agent/policy.py,PolicyChecker
survived,"def test_settings_loads_dotenv(tmp_path, monkeypatch):
    env = tmp_path / "".env""
    env.write_text(""OPENAI_API_KEY=abc\nAGI_INSIGHT_BUS_PORT=1234\n"", encoding=""utf-8"")
    monkeypatch.chdir(tmp_path)
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    import src.utils.config as cfg
    importlib.reload(cfg)
    settings = cfg.Settings()
    assert settings.openai_api_key == ""abc""
    assert settings.bus_port == 1234
",tests/test_root_config.py,
survived,"    def test_sitecustomize_meta_importer(self) -> None:
        """"""Verify Jac modules importable without importing jaclang.""""""
        with tempfile.TemporaryDirectory() as tmpdir:
            Path(tmpdir, ""mymod.jac"").write_text(
                'with entry {print(""via meta"");}'
            )
            env = os.environ.copy()
            project_root = Path(__file__).resolve().parents[2]
            env[""PYTHONPATH""] = os.pathsep.join([str(project_root), tmpdir])
            proc = subprocess.run(
                [sys.executable, ""-c"", ""import mymod""],
                capture_output=True,
                text=True,
                cwd=tmpdir,
                env=env,
            )
            self.assertEqual(proc.returncode, 0, proc.stderr)
            self.assertEqual(proc.stdout.strip(), ""via meta"")",jac/jaclang/tests/test_language.py,JacLanguageTests
survived,"def main(directory: Path) -> int:
    service_worker = directory / ""service-worker.js""
    workbox = directory / ""lib"" / ""workbox-sw.js""
    if not service_worker.exists():
        raise FileNotFoundError(service_worker)
    if not workbox.exists():
        raise FileNotFoundError(workbox)
    expected = parse_expected_hash(service_worker)
    actual = compute_hash(workbox)
    if expected != actual:
        print(f""Hash mismatch: expected {expected}, got {actual}"")
        return 1
    return 0
",scripts/verify_workbox_hash.py,
survived,"async def test_shutdown_without_signal(monkeypatch: pytest.MonkeyPatch) -> None:
    loop = asyncio.get_running_loop()

    def boom(*_: object) -> None:
        raise NotImplementedError

    monkeypatch.setattr(loop, ""add_signal_handler"", boom)

    shutdown_event = asyncio.Event()
    dummy = DummyManager()

    async def factory() -> qm.QueueManager:
        return cast(qm.QueueManager, dummy)

    async def run_manager(manager: DummyManager, *args: object, **kwargs: object) -> None:
        await manager.run()

    def setup_shutdown_handlers_stub(
        manager: DummyManager, shutdown: asyncio.Event
    ) -> DummyManager:
        manager.shutdown = shutdown
        return manager

    monkeypatch.setattr(supervisor, ""run_manager"", run_manager)
    monkeypatch.setattr(supervisor, ""setup_shutdown_handlers"", setup_shutdown_handlers_stub)

    task = asyncio.create_task(
        supervisor.runit(
            factory,
            dequeue_timeout=timedelta(seconds=1),
            batch_size=1,
            restart_delay=timedelta(seconds=0),
            restart_on_failure=False,
            shutdown=shutdown_event,
            mode=types.QueueExecutionMode.continuous,
            max_concurrent_tasks=None,
            shutdown_on_listener_failure=False,
        )
    )

    await asyncio.sleep(0.1)
    shutdown_event.set()
    await task",test/windows/test_shutdown.py,
survived,"    def _check_allowed(self, diff: str) -> None:
        files = _files_from_diff(diff)
        for f in files:
            if not any(fnmatch.fnmatch(f, pat) for pat in self.allowed):
                raise ValueError(f""file '{f}' not allowed"")
",src/agents/self_improver_agent.py,SelfImproverAgent
survived,"    def __init__(
        self,
        model: str = ""flux-schnell"",
        timeout: int = 60,
        proxies: dict = {},
    ):
        """"""Initialize your PixelMuse provider with custom settings! âš™ï¸

        Args:
            model (str): Which model to use (default: flux-schnell)
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
            logging (bool): Enable fire logging (default: True)
        """"""
        self.api_endpoint = ""https://www.pixelmuse.studio/api/predictions""
        self.headers = {
            ""accept"": ""*/*"",
            ""accept-language"": ""en-US,en;q=0.9"",
            ""content-type"": ""application/json"",
            ""origin"": ""https://www.pixelmuse.studio"",
            ""referer"": ""https://www.pixelmuse.studio/"",
            ""sec-ch-ua"": '""Chromium"";v=""134"", ""Not:A-Brand"";v=""24"", ""Google Chrome"";v=""134""',
            ""sec-ch-ua-mobile"": ""?0"",
            ""sec-ch-ua-platform"": '""Windows""',
            ""sec-fetch-dest"": ""empty"",
            ""sec-fetch-mode"": ""cors"",
            ""sec-fetch-site"": ""same-origin"",
            ""user-agent"": LitAgent().random(),
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.model = model
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""webp""
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager
survived,"    def save(
        self,
        response: List[bytes],
        name: str = None,
        dir: str = os.getcwd(),
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your fire images! ðŸ’¾

        Args:
            response (List[bytes]): Your generated images
            name (str, optional): Custom name (default: uses prompt)
            dir (str, optional): Where to save (default: current directory)
            filenames_prefix (str, optional): Add prefix to filenames

        Returns:
            List[str]: Where your images were saved
        """"""
        assert isinstance(response, list), f""Response gotta be a list, not {type(response)} ðŸ¤”""
        name = self.prompt if name is None else name

        filenames = []
        count = 0
        if self.logging:
            logger.info(f""Saving {len(response)} images... ðŸ’¾"")

        for image_bytes in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            with open(absolute_path_to_file, ""wb"") as fh:
                fh.write(image_bytes)

        if self.logging:
            logger.success(f""Images saved successfully! Check {dir} ðŸŽ‰"")
        return filenames",webscout/Provider/TTI/huggingface.py,HFimager
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}):
        """"""Initialize your MagicStudio provider with custom settings! âš™ï¸""""""
        self.api_endpoint = ""https://ai-api.magicstudio.com/api/ai-art-generator""
        self.headers = {
            ""Accept"": ""application/json, text/plain, */*"",
            ""User-Agent"": agent.random(),
            ""Origin"": ""https://magicstudio.com"",
            ""Referer"": ""https://magicstudio.com/ai-art-generator/"",
            ""DNT"": ""1"",
            ""Sec-GPC"": ""1""
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""jpg""
",webscout/Provider/TTI/magicstudio.py,MagicStudioImager
survived,"    def __init__(self, timeout: int = 60, proxies: dict = None):
        """"""Initialize your FastFluxImager provider with custom settings

        Examples:
            >>> provider = FastFluxImager(timeout=120)
            >>> provider = FastFluxImager(proxies={""http"": ""http://proxy:8080""})

        Args:
            timeout (int): HTTP request timeout in seconds (default: 60)
            proxies (dict, optional): Proxy configuration for requests
            logging (bool): Enable/disable logging (default: True)
        """"""
        self.api_endpoint = ""https://api.fastflux.co/v1/images/generate""
        self.headers = {
            ""accept"": ""application/json, text/plain, */*"",
            ""content-type"": ""application/json"",
            ""origin"": ""https://fastflux.co"",
            ""referer"": ""https://fastflux.co/"",
            ""user-agent"": agent.random()
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        if proxies:
            self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
        self.logging = True
",webscout/Provider/TTI/fastflux.py,FastFluxImager
survived,"    def __init__(self, timeout: int = 60, proxies: dict = {}, logging: bool = True):
        """"""Initialize your Nexra provider with custom settings! âš™ï¸

        Args:
            timeout (int): Request timeout in seconds (default: 60)
            proxies (dict): Proxy settings for requests (default: {})
            logging (bool): Enable fire logging (default: True)
        """"""
        self.url = ""https://nexra.aryahcr.cc/api/image/complements""
        self.headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
            ""User-Agent"": agent.random()
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.proxies.update(proxies)
        self.timeout = timeout
        self.prompt: str = ""AI-generated image - webscout""
        self.image_extension: str = ""png""
        self.logging = logging
        if self.logging:
            logger.info(""Nexra provider initialized! ðŸš€"")
",webscout/Provider/TTI/nexra.py,NexraImager
survived,"    def create(*_a: object, **_k: object) -> None:
        raise FailError(""boom"")
",tests/test_llm_client_error_handling.py,
survived,"def eval_monad_grad(klong, a):
    """"""

        âˆ‡a                                                     [Grad]

        Return a function that computes the numeric gradient of ``a``.

    """"""
    return KGLambda(lambda x, fn=a, k=klong: grad_of_fn(k, fn, x))
",klongpy/monads.py,
survived,"def eval_dyad_grad(klong, a, b):
    """"""

        aâˆ‡b                                                    [Grad]

        Compute the numeric gradient of the monadic function ``b`` at ``a``.

    """"""
    if isinstance(a, KGSym):
        orig = klong[a]

        def func(v):
            klong[a] = v
            try:
                return klong.call(KGCall(b, [v], 1)) if isinstance(b, (KGSym, KGLambda, KGFn, KGCall)) else b(v)
            finally:
                klong[a] = orig

        return numeric_grad(func, orig)
    else:
        return grad_of_fn(klong, b, a)
",klongpy/dyads.py,
survived,"    def test_scalar_grad_torch(self):
        klong = KlongInterpreter()
        klong['sin'] = lambda x: np.sin(x)
        klong['cos'] = lambda x: np.cos(x)
        klong('g::âˆ‡{sin(x)+x*x}')
        r = klong('g(3.14)')
        x = torch.tensor(3.14, dtype=torch.float64, requires_grad=True)
        f = torch.sin(x) + x * x
        f.backward()
        self.assertTrue(np.isclose(r, x.grad.item(), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"    def func() -> str:
        calls[""n""] += 1
        raise ValueError(""fail"")
",tests/test_retry_wrapper.py,
survived,"def _make_client() -> TestClient:
    return TestClient(cast(Any, api.app))
",tests/test_insight_endpoint.py,
survived,"def _setup_simulations() -> None:
    api._simulations.clear()
    api._simulations[""a""] = api.ResultsResponse(
        id=""a"",
        forecast=[api.ForecastPoint(year=1, capability=0.1)],
        population=None,
    )
    api._simulations[""b""] = api.ResultsResponse(
        id=""b"",
        forecast=[api.ForecastPoint(year=1, capability=0.9)],
        population=None,
    )
",tests/test_insight_endpoint.py,
survived,"        async def json(self) -> dict:
            raise NotImplementedError(""aiohttp is required for network access"")
",src/meta_agent/services/telemetry_client.py,_DummyResponse
survived,"    def load_dotenv(*_args, **_kwargs) -> None:
        """"""Fallback no-op for environments without python-dotenv.""""""
        return None
",src/meta_agent/services/llm_service.py,
survived,"    def __init__(self, *_, **__):
        pass
",src/aiohttp/__init__.py,TCPConnector
survived,"    def first_markdown_cell(nb_path: Path) -> str:
        try:
            data = json.loads(nb_path.read_text(encoding=""utf-8""))
        except Exception:
            return """"
        for cell in data.get(""cells"", []):
            if cell.get(""cell_type"") == ""markdown"":
                src = cell.get(""source"", """")
                if isinstance(src, list):
                    src_text = """".join(src)
                else:
                    src_text = str(src)
                return src_text
        return """"
",scripts/verify_disclaimer_snippet.py,
survived,"    def __init__(self, default_direction: str = ""TB"") -> None:
        self.default_direction = default_direction
",src/meta_agent/ux/diagram_generator.py,DiagramGenerator
survived,"    def success(self, message: str, *, level: int = 1) -> None:
        """"""Output a success message.""""""
        self._echo(message, fg=""green"", bold=True, level=level)
",src/meta_agent/ux/cli_output.py,CLIOutput
survived,"    def error(self, message: str, *, level: int = 1) -> None:
        """"""Output an error message.""""""
        self._echo(message, fg=""red"", bold=True, err=True, level=level)",src/meta_agent/ux/cli_output.py,CLIOutput
survived,"def test_menu(monkeypatch, capsys):
    inputs = iter([""3"", ""2""])
    monkeypatch.setattr(""builtins.input"", lambda _: next(inputs))
    inter = Interactive()
    result = inter.menu(""Pick one"", [""a"", ""b""])
    out = capsys.readouterr().out
    assert ""Invalid choice"" in out
    assert result == ""b""
",tests/ux/test_interactive.py,
survived,"    def test_none_library(self):
        os.environ[""ALPHA_KAFKA_BROKER""] = ""localhost:9092""
        orig = base_mod.KafkaProducer
        base_mod.KafkaProducer = None
        self.assertIsNone(base_mod._kafka_producer())
        base_mod.KafkaProducer = orig
        os.environ.pop(""ALPHA_KAFKA_BROKER"", None)
",tests/test_base_helpers.py,TestKafkaProducer
survived,"    def test_env_seconds(self):
        from alpha_factory_v1.backend.agents.ping_agent import _env_seconds, _MIN_INTERVAL
        os.environ.pop(""X_INT"", None)
        self.assertEqual(_env_seconds(""X_INT"", 42), 42)
        os.environ[""X_INT""] = ""2""
        self.assertEqual(_env_seconds(""X_INT"", 10), _MIN_INTERVAL)
        os.environ[""X_INT""] = ""15""
        self.assertEqual(_env_seconds(""X_INT"", 1), 15)
        os.environ[""X_INT""] = ""bad""
        self.assertEqual(_env_seconds(""X_INT"", 7), 7)
        os.environ.pop(""X_INT"", None)
",tests/test_base_helpers.py,TestEnvSeconds
survived,"    def test_prom_metrics_none(self):
        orig_counter = base_mod.Counter
        orig_gauge = base_mod.Gauge
        base_mod.Counter = None
        base_mod.Gauge = None
        run, err, lat = base_mod._prom_metrics(""x"")
        self.assertIsNone(run)
        self.assertIsNone(err)
        self.assertIsNone(lat)
        base_mod.Counter = orig_counter
        base_mod.Gauge = orig_gauge
",tests/test_base_helpers.py,TestPromMetrics
survived,"    def test_demo_shell_scripts(self) -> None:
        """"""All demo shell scripts should be executable and have shebangs.""""""
        base = Path(validate_demos.DEFAULT_DIR)
        for script in base.rglob(""*.sh""):
            with self.subTest(script=script.name):
                content = script.read_text(errors=""ignore"")
                self.assertTrue(content.startswith(""#!/""), f""{script} missing shebang"")
                self.assertTrue(script.stat().st_mode & 0o111, f""{script} not executable"")",tests/test_demos.py,TestDemos
survived,"def cli() -> None:
    """"""Run a short evolutionary loop and print the champion genome.""""""
    import argparse

    parser = argparse.ArgumentParser(description=""AI-GA Meta-Evolver demo"")
    parser.add_argument(""--gens"", type=int, default=5, help=""Generations to run"")
    args = parser.parse_args()

    from curriculum_env import CurriculumEnv

    evolver = MetaEvolver(env_cls=CurriculumEnv)
    evolver.run_generations(args.gens)
    print(evolver.latest_log())

    try:
        df = evolver.history_plot()
        print(df.tail())
    except Exception:  # pragma: no cover - pandas optional
        pass
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,
survived,"    def test_excluded_fields(self):
        """"""Test that fields marked with exclude=True are not included.""""""

        class Base(DeclarativeBase):
            pass

        class User(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""users""

            id: Mapped[int] = mapped_column(primary_key=True)
            username: Mapped[str] = mapped_column()
            password_hash: Mapped[str] = mapped_column(info={""exclude"": True})
            secret_token: Mapped[str] = mapped_column(
                info={""exclude"": True, ""description"": ""Should not appear""}
            )

        UserEnrichModel = User.__enrich_model__()
        fields = UserEnrichModel.model_fields

        # Check included fields
        assert ""id"" in fields
        assert ""username"" in fields

        # Check excluded fields
        assert ""password_hash"" not in fields
        assert ""secret_token"" not in fields
",tests/test_sqlalchemy_integration.py,TestBasicModel
survived,"async def get_order_user(order_id: int, ctx: EnrichContext) -> UserEnrichModel | None:
    """"""Get the user who placed a specific order.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        order = await session.get(Order, order_id)
        if not order:
            return None

        # Load the user (SQLAlchemy will handle the join)
        await session.refresh(order, [""user""])
        user = order.user

        return UserEnrichModel(
            id=user.id,
            username=user.username,
            email=user.email,
            full_name=user.full_name,
            is_active=user.is_active,
            created_at=user.created_at,
        )
",examples/sqlalchemy_shop/app.py,
survived,"    def test_excluded_relationship(self):
        """"""Test that relationships marked with exclude=True are not included.""""""

        class Base(DeclarativeBase):
            pass

        class User(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""users""

            id: Mapped[int] = mapped_column(primary_key=True)
            username: Mapped[str] = mapped_column()
            secret_orders: Mapped[list[""Order""]] = relationship(
                info={""exclude"": True}, overlaps=""public_orders""
            )
            public_orders: Mapped[list[""Order""]] = relationship(
                info={""description"": ""Public orders""}, overlaps=""secret_orders""
            )

        class Order(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""orders""
            id: Mapped[int] = mapped_column(primary_key=True)
            user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))

        UserEnrichModel = User.__enrich_model__()
        fields = UserEnrichModel.model_fields

        # Check that excluded relationship is not included
        assert ""secret_orders"" not in fields
        assert ""public_orders"" in fields
",tests/test_sqlalchemy_integration.py,TestRelationships
survived,"    def test_many_to_one_relationship(self):
        """"""Test many-to-one relationship conversion.""""""

        class Base(DeclarativeBase):
            pass

        class Order(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""orders""

            id: Mapped[int] = mapped_column(primary_key=True)
            user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))
            user: Mapped[""User""] = relationship(
                info={""description"": ""Customer who placed the order""}
            )

        class User(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""users""

            id: Mapped[int] = mapped_column(primary_key=True)
            username: Mapped[str] = mapped_column()

        OrderEnrichModel = Order.__enrich_model__()
        fields = OrderEnrichModel.model_fields

        # Check that user field exists and is a Relationship
        assert ""user"" in fields
        assert isinstance(fields[""user""].default, Relationship)
        assert fields[""user""].default.description == ""Customer who placed the order""

        # Type should be just ""UserEnrichModel"" (not List)
        assert ""UserEnrichModel"" in str(fields[""user""].annotation)
        assert ""List"" not in str(fields[""user""].annotation)
",tests/test_sqlalchemy_integration.py,TestRelationships
survived,"def _str_tkn(text: str) -> int:
    # naÃ¯ve token estimate â‰ˆâ€‘ 1 token / 4 chars in English
    return max(1, math.ceil(len(text)/4))
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,
survived,"    def emit(self, topic: str, msg: dict):
        A2ABus.publish(topic, msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Agent
survived,"    def __init__(self,
                 endpoint: str = ""openai:gpt-4o"",
                 temperature: float = 0.2,
                 max_tokens: int = 2048,
                 context_len: int = 8192,
                 stream: bool = False,
                 timeout: int = 120,
                 **extra):
        self.endpoint = endpoint
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.context_len = context_len
        self.stream = stream
        self.timeout = timeout
        self.extra = extra
        self._backend, self._model = self._parse(endpoint)
        self._client = self._init_backend()
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,LMClient
survived,"def serve_lineage(path: pathlib.Path, port: int=8000):
    import flask, threading
    app = flask.Flask(""lineage-viewer"")

    @app.route(""/"")
    def idx():
        return VIEW_HTML

    @app.route(""/log"")
    def log():
        if not path.exists():
            return ""(no events yet)""
        return flask.escape(path.read_text(""utf-8""))

    th = threading.Thread(target=app.run, kwargs=dict(port=port, host=""0.0.0.0"", debug=False))
    th.daemon = True
    th.start()
    LOGGER.info(""Lineage viewer at http://localhost:%d"", port)
    return th
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,
survived,"    def _risk_assess(self, prompt:str, response:str) -> float:
        # toy heuristic: long responses & code carry more risk
        return min(1.0, 0.1 + 0.9*(len(response)/4000))
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,Agent
survived,"    def __init__(self,
                 name: str,
                 role: str = ""autonomousâ€‘agent"",
                 provider: str | None = None,
                 objectives: Optional[ObjectiveWeights] = None,
                 lineage_dir: str | pathlib.Path = ""./lineage"",
                 rate_limit_tps: float = 3.0):
        self.name = name
        self.role = role
        self.id = f""{name}-{_sha(uuid.uuid4().hex)}""
        self.objectives = objectives or ObjectiveWeights()
        self.lm = LMClient(provider or os.getenv(""ALPHA_PROVIDER"", ""openai:gpt-4o""))
        self.tracer = LineageTracer(pathlib.Path(lineage_dir)/f""{self.id}.jsonl"")
        self.tracer.log(""init"", role=role, provider=self.lm.endpoint)
        GLOBAL_LIMITER._tps = rate_limit_tps
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,Agent
survived,"    def score(self, metrics: Dict[str,float]) -> float:
        return (
            self.latency * (1/ (1+metrics.get(""latency"",0))) +
            self.cost    * (1/ (1+metrics.get(""cost"",0))) +
            self.carbon  * (1/ (1+metrics.get(""carbon"",0))) +
            self.risk    * (1- metrics.get(""risk"",0))
        )
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,ObjectiveWeights
survived,"    def run(self, prompt: str, context: Optional[Iterable[Dict[str,str]]]=None, **kw) -> Dict[str,Any]:
        ctx: List[Dict[str,str]] = list(context or [])
        ctx.append({""role"":""user"", ""content"": prompt})
        t0 = time.perf_counter()
        output = self.lm.chat(ctx, **kw)
        latency = time.perf_counter()-t0
        tokens_in = _str_tkn(prompt)
        tokens_out = _str_tkn(output)
        cost = self._estimate_cost(tokens_in,tokens_out)
        carbon = cost*0.00015 # placeholder multiplier (avg kgCO2 per $ cloud)
        risk = self._risk_assess(prompt, output)
        metrics = dict(latency=latency, cost=cost, carbon=carbon, risk=risk)
        score = self.objectives.score(metrics)
        self.tracer.log(""run"", prompt=prompt[:120], response=output[:120], metrics=metrics, score=score)
        return {""response"": output, ""metrics"": metrics, ""score"": score}
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,Agent
survived,"    def train_once(self)->float:
        if len(self.buffer)<CFG.train_batch: return 0.0
        obs,rew=zip(*random.sample(self.buffer, CFG.train_batch))
        obs_t=torch.tensor(obs, device=CFG.device, dtype=torch.float32)
        rew_t=torch.tensor(rew, device=CFG.device)
        _,v,_=self.net.initial(obs_t)
        loss=F.mse_loss(v.squeeze(),rew_t)
        self.opt.zero_grad(); loss.backward(); self.opt.step()
        return float(loss.item())
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Learner
survived,"def test_json_parser(tmp_path):
    json_file = tmp_path / ""data.json""
    json_file.write_text(json.dumps({""a"": 1, ""b"": 2}), encoding=""utf-8"")
    parser = JsonParser(file_path=str(json_file))
    result = parser.parse(file_path=str(json_file))
    assert result[""title""] == ""json""
    assert ""\""a\"""" in result[""content""]
    assert result[""lifecycle""][0][""life_metadata""][""source_file""] == str(json_file)
",tests/test_json_parser.py,
survived,"    def parse(self, file_path: str) -> MarkdownOutputVo:
        try:
            content = self.read_json_file(file_path)
            lifecycle = self.generate_lifecycle(
                source_file=file_path,
                domain=""Technology"",
                usage_purpose=""Documentation"",
                life_type=""LLM_ORIGIN"",
            )
            output_vo = MarkdownOutputVo(self.get_file_extension(file_path), content)
            output_vo.add_lifecycle(lifecycle)
            return output_vo.to_dict()
        except Exception as e:
            raise e",datamax/parser/json_parser.py,JsonParser
survived,"def test_root_serves_index() -> None:
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface import api_server

    client = TestClient(api_server.app)
    resp = client.get(""/"")
    assert resp.status_code == 200
    assert ""<div id=\""root\""></div>"" in resp.text",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_static.py,
survived,"    def eval(self):
        pass
",tests/test_multi_contributor.py,DummyModel
survived,"def test_csp_no_violations() -> None:
    dist = Path(__file__).resolve().parents[2] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            violations = []
            page.on(
                ""console"",
                lambda msg: violations.append(msg.text)
                if ""Content Security Policy"" in msg.text
                else None,
            )
            page.on(""pageerror"", lambda err: violations.append(str(err)))
            page.goto(url)
            page.wait_for_selector(""#controls"")
            assert not any(""Content Security Policy"" in v for v in violations)
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
",tests/security/test_csp.py,
survived,"    async def restart(self, bus: object, ledger: object) -> None:
        if self.task:
            self.task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self.task
        try:
            close = getattr(self.agent, ""close"")
        except AttributeError:
            pass
        else:
            close()
        self.agent = self.cls(bus, ledger)
        self.error_count = 0
        self.restarts += 1
        self.restart_streak += 1
        self.start(bus, ledger)
",alpha_factory_v1/backend/orchestrator_utils.py,AgentRunner
survived,"async def monitor_agents(
    runners: Dict[str, AgentRunner],
    bus: object,
    ledger: object,
    *,
    err_threshold: int = 3,
    backoff_exp_after: int = 3,
    on_restart: Callable[[AgentRunner], None] | None = None,
) -> None:
    """"""Restart crashed or stalled agents and apply exponential backoff.""""""
    while True:
        await asyncio.sleep(2)
        now = time.time()
        for r in list(runners.values()):
            needs_restart = False
            if r.task and r.task.done():
                needs_restart = True
            elif r.error_count >= err_threshold:
                needs_restart = True
            elif now - r.last_beat > r.period * 5:
                needs_restart = True
            if needs_restart:
                delay = random.uniform(0.5, 1.5)
                if r.restart_streak >= backoff_exp_after:
                    delay *= 2 ** (r.restart_streak - backoff_exp_after + 1)
                await asyncio.sleep(delay)
                await r.restart(bus, ledger)
                if on_restart:
                    on_restart(r)
",alpha_factory_v1/backend/orchestrator_utils.py,
survived,"    def __init__(self, agent: object) -> None:
        self.cls: Callable[..., object] = type(agent)
        self.agent = agent
        self.period = getattr(agent, ""CYCLE_SECONDS"", 1.0)
        self.capabilities = getattr(agent, ""CAPABILITIES"", [])
        self.last_beat = time.time()
        self.restarts = 0
        self.task: asyncio.Task[None] | None = None
        self.error_count = 0
        self.restart_streak = 0
",alpha_factory_v1/backend/orchestrator_utils.py,AgentRunner
survived,"def evaluate_agent(code: str) -> dict[str, float]:
    """"""Return accuracy, novelty SimHash and execution latency.""""""

    import random
    import time
    from hashlib import blake2b

    start = time.perf_counter()
    h = blake2b(code.encode(), digest_size=8).digest()
    simhash = int.from_bytes(h, ""big"")
    rng = random.Random(simhash & 0xFFFF)
    accuracy = 0.5 + rng.random() * 0.5
    latency_ms = (time.perf_counter() - start) * 1000
    return {
        ""accuracy"": accuracy,
        ""novelty_simhash"": float(simhash),
        ""latency_ms"": latency_ms,
    }
",src/eval/fitness.py,
survived,"def test_blocks_insider_message() -> None:
    agent = _make_agent()
    env = messaging.Envelope(
        sender=""market"",
        recipient=""safety"",
        payload={""analysis"": ""buy AAPL tomorrow""},
        ts=0.0,
    )
    asyncio.run(agent.handle(env))
    assert agent.bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_codegen_safety.py,
survived,"    def test_multiple_backends_agents_created(self) -> None:
        settings = config.Settings(
            bus_port=0,
            offline=True,
            island_backends={""openai"": ""gpt-4o"", ""anth"": ""claude-opus""},
        )
        orch = orchestrator.Orchestrator(settings)
        self.assertEqual(orch.island_backends, settings.island_backends)
        # eight agents per island
        self.assertEqual(len(orch.runners), 16)
        islands = {name.split(""_"")[-1] if ""_"" in name else ""openai"" for name in orch.runners}
        self.assertIn(""openai"", islands)
        self.assertIn(""anth"", islands)
        for name, runner in orch.runners.items():
            if name.endswith(""_anth""):
                self.assertEqual(runner.agent.backend, ""claude-opus"")
            elif name.endswith(""_openai"") or name == ""planning"":
                # default island uses openai when island name 'openai'
                pass
",tests/test_island_backends.py,TestIslandBackends
survived,"    def test_venv_pip_posix(self):
        with mock.patch.object(os, 'name', 'posix'):
            self.assertEqual(
                quickstart._venv_pip(Path('/tmp/venv')),
                Path('/tmp/venv/bin/pip')
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest
survived,"def main(base_dir: str = DEFAULT_DIR) -> int:
    failures = []
    for entry in os.listdir(base_dir):
        path = os.path.join(base_dir, entry)
        if os.path.isdir(path):
            if entry.startswith('.') or entry.startswith('__'):
                continue
            readme = os.path.join(path, ""README.md"")
            if not os.path.isfile(readme):
                failures.append(f""Missing README.md in {entry}"")
    if failures:
        for msg in failures:
            print(f""ERROR: {msg}"", file=sys.stderr)
        return 1
    print(""All demo directories contain README.md"")
    return 0
",alpha_factory_v1/demos/validate_demos.py,
survived,"    def _simulate_worker(env_cls, archive, js: str):
        g = Genome.from_json(js)
        env = env_cls()
        obs_dim, act_dim = env.observation_space.shape[0], env.action_space.n
        net = EvoNet(obs_dim, act_dim, g).to(Device)
        obs, _ = env.reset()
        total, bc = 0.0, []
        for _ in range(env.genome.max_steps):
            with torch.no_grad():
                a = net(torch.tensor(obs, dtype=torch.float32, device=Device)).argmax().item()
            obs, rew, done, truncated, _ = env.step(a)
            total += rew; bc.append(obs)
            if done or truncated:
                break
        bc_vec = np.mean(bc, axis=0)
        if g.novelty_weight and archive:
            novelty = float(np.mean([np.linalg.norm(bc_vec - a) for a in archive]))
            total += g.novelty_weight * novelty
        return total, bc_vec
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"            def inc(self, *a, **k):
                self.calls.append(""inc"")
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest.DummyMetric
survived,"    def test_metrics_setup_with_stubs(self):
        class DummyMetric:
            def __init__(self, *a, **k):
                self.calls = []
            def inc(self, *a, **k):
                self.calls.append(""inc"")
            def set(self, *a, **k):
                self.calls.append(""set"")
            def observe(self, *a, **k):
                self.calls.append(""observe"")

        prom_stub = SimpleNamespace(Counter=DummyMetric, Gauge=DummyMetric, Histogram=DummyMetric)
        with mock.patch.object(ping_agent, ""_Prom"", prom_stub):
            ping_agent.PingAgent.__bases__ = (NewAgentBase,)
            agent = ping_agent.PingAgent()
            agent.orchestrator = self.orc
            asyncio.run(agent.setup())
            self.assertIsInstance(agent._prom_ping_total, DummyMetric)
            asyncio.run(agent.step())
            self.assertIn(""inc"", agent._prom_ping_total.calls)
            self.assertTrue(any(c == ""observe"" for c in agent._prom_cycle_hist.calls))
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest
survived,"    def test_env_seconds_bad_value(self):
        with mock.patch.dict(""os.environ"", {""X"": ""bad""}):
            val = ping_agent._env_seconds(""X"", 7)
        self.assertEqual(val, 7)
",alpha_factory_v1/tests/test_ping_agent.py,EnvSecondsTest
survived,"def test_evaluate_logs(monkeypatch, tmp_path, caplog):
    fake_rc = MagicMock()
    fake_rc.execute_and_collect.return_value = CollectionResult(0, '', '', 0.1)
    fake_reporter = MagicMock()
    fake_reporter.generate_report.return_value = ''
    harness = EvaluationHarness(fake_rc, fake_reporter)
    with caplog.at_level('INFO', logger='meta_agent.evaluation.harness'):
        harness.evaluate(tmp_path)
    assert any('Starting evaluation for' in r.getMessage() for r in caplog.records)",tests/unit/test_evaluation_harness.py,
survived,"def test_init_creates_default_modules(monkeypatch):
    fake_rc_cls = MagicMock()
    fake_rc = MagicMock()
    fake_rc_cls.return_value = fake_rc
    fake_reporter_cls = MagicMock()
    fake_reporter = MagicMock()
    fake_reporter_cls.return_value = fake_reporter
    monkeypatch.setattr(
        'meta_agent.evaluation.harness.ResultCollectionModule', fake_rc_cls
    )
    monkeypatch.setattr('meta_agent.evaluation.harness.ReportingModule', fake_reporter_cls)

    harness = EvaluationHarness()
    assert harness.result_collector is fake_rc
    assert harness.reporter is fake_reporter
",tests/unit/test_evaluation_harness.py,
survived,"        def fake_check_pkg(name: str) -> bool:
            # Required packages are always present
            if name in {""pytest"", ""prometheus_client""}:
                return True
            # Simulate all optional deps missing
            if name in preflight.OPTIONAL_DEPS:
                return False
            return True
",tests/test_preflight_optional_missing.py,TestPreflightOptionalMissing
survived,"def safe_current_wb_run_step() -> int | None:
    try:
        import wandb

        wandb_run = wandb.run
        if wandb_run is None:
            return None
    except ImportError:
        return None
    else:
        try:
            return int(wandb_run.step)
        except Exception:
            return None
",weave/trace/weave_client.py,
survived,"        def _fake_import(name: str, *args: object, **kwargs: object) -> object:
            if name == ""agents"":
                raise ModuleNotFoundError
            return orig_import_module(name, *args, **kwargs)
",tests/test_build_core_agent.py,TestBuildCoreAgent
survived,"def example4():
    x = fetchSomething()
    if x > 0:
        doPos(x)
    else:
        doNeg(x)",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"def example2(flag):
    if flag:
        None
    else:
        None",tests/rosetta/transpiler/Python/conditional-structures-2.py,
survived,"def real(f):
    r = 0.0
    i = len(f) - 1
    while i > 0:
        r = (float(f[i][""b""])) // ((float(f[i][""a""])) + r)
        i = i - 1
    if len(f) > 0:
        r = r + (float(f[0][""a""]))
    return r
",tests/rosetta/transpiler/Python/continued-fraction.py,
survived,"    def eat(self):
        print(""mm, that "" + self.value + "" was good!"")
",tests/rosetta/transpiler/Python/constrained-genericity-3.py,PeelFirst
survived,"def example8(b1, b2):
    if b1:
        None
    if b2:
        None",tests/rosetta/transpiler/Python/conditional-structures-8.py,
survived,"    async def setup(self):
        self.iter = 0
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv
survived,"def test_available_scenarios() -> None:
    names = set(replay.available_scenarios())
    assert EXPECTED.issubset(names)
",tests/test_replay_scenarios.py,
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Delete cache entries older than ``stale_after``.""""""",src/cachier/cores/base.py,_BaseCore
survived,"    def add(x):
        return x + 1
",tests/test_cleanup.py,
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Remove stale entries from the Redis cache.""""""
        redis_client = self._resolve_redis_client()
        pattern = f""{self.key_prefix}:{self._func_str}:*""
        try:
            keys = redis_client.keys(pattern)
            threshold = datetime.now() - stale_after
            for key in keys:
                ts = redis_client.hget(key, ""timestamp"")
                if ts is None:
                    continue
                try:
                    ts_val = datetime.fromisoformat(ts.decode(""utf-8""))
                except Exception as exc:
                    warnings.warn(
                        f""Redis timestamp parse failed: {exc}"", stacklevel=2
                    )
                    continue
                if ts_val < threshold:
                    redis_client.delete(key)
        except Exception as e:
            warnings.warn(
                f""Redis delete_stale_entries failed: {e}"", stacklevel=2
            )",src/cachier/cores/redis.py,_RedisCore
survived,"    def __init__(self, *args, **kwargs):
        pass
",stubs/openai_agents/__init__.py,OpenAIAgent
survived,"    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)
",tests/test_selfheal_entrypoint_offline.py,
survived,"def projection_from_json(path: str | Path) -> Dict[str, Dict[str, Any]]:
    """"""Return discounted cash flow projections loaded from ``path``.

    Each top-level key in the JSON file should map to a sector. The value must be
    a mapping accepted by :func:`delta_sector_to_dcf` with keys ``delta_revenue``,
    ``margin``, ``discount_rate`` and ``years``.
    """"""
    data = json.loads(Path(path).read_text(encoding=""utf-8""))
    results: Dict[str, Dict[str, Any]] = {}
    for sector, vals in data.items():
        if not isinstance(vals, dict):
            continue
        results[sector] = delta_sector_to_dcf(vals)
    return results",alpha_factory_v1/core/finance/wealth_projection.py,
survived,"    def test_no_log_skips_directory(self) -> None:
        with tempfile.TemporaryDirectory() as home:
            env = os.environ.copy()
            env[""HOME""] = home
            env.pop(""ALPHA_CONVERSION_LEDGER"", None)
            result = subprocess.run(
                [sys.executable, STUB, ""--alpha"", ""skip"", ""--no-log""],
                capture_output=True,
                text=True,
                env=env,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            default_dir = Path(home) / "".aiga""
            self.assertFalse(default_dir.exists())
",tests/test_alpha_conversion_stub.py,TestAlphaConversionStub
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/join_multi.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_left_join.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/left_join_multi.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join.py,Supplier
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/join_multi.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_join.py,Order
survived,"        def save(self, *args, **kwargs):
            pass
",tests/conftest.py,DummyDocxDocument
survived,"    async def _fake_comment(_: float) -> str:
        return ""ok""
",tests/test_alpha_agi_business_3_v1.py,
survived,"def test_macro_entrypoint_launch(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""ADK launch should be triggered when the env flag is set.""""""

    monkeypatch.setenv(""ALPHA_FACTORY_ENABLE_ADK"", ""1"")
    monkeypatch.setenv(""OPENAI_API_KEY"", ""dummy"")

    # Provide a minimal openai_agents stub when the package is absent
    if ""openai_agents"" not in sys.modules:
        stub = ModuleType(""openai_agents"")

        class _Agent:
            def __init__(self, *a, **kw):
                self.name = kw.get(""name"", ""agent"")

        class _OpenAI:
            def __init__(self, *a, **kw):
                pass

            def __call__(self, *_a, **_k):
                return """"

        def _tool(*_a, **_kw):
            def _decorator(func):
                return func

            return _decorator

        stub.Agent = _Agent
        stub.OpenAIAgent = _OpenAI
        stub.Tool = _tool
        sys.modules[""openai_agents""] = stub

    mod_path = ""alpha_factory_v1.demos.macro_sentinel.agent_macro_entrypoint""
    sys.modules.pop(mod_path, None)

    with patch(""alpha_factory_v1.backend.adk_bridge.auto_register""), \
         patch(""alpha_factory_v1.backend.adk_bridge.maybe_launch"") as maybe_launch:
        importlib.import_module(mod_path)
        maybe_launch.assert_called_once_with()
",tests/test_macro_adk_integration.py,
survived,"    def Counter(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Counter, name, desc, labels)
",alpha_factory_v1/backend/agents/__init__.py,
survived,"    def strip_comments(s: str) -> str:
        return ""\n"".join([ln.split(""//"")[0].rstrip() for ln in s.splitlines()])
",tools/any2mochi/py/run_all.py,
survived,"    def parse_callable(self, node: ast.expr | None) -> tuple[list[str], str] | None:
        if not isinstance(node, ast.Subscript):
            return None
        if not (
            isinstance(node.value, ast.Attribute) and node.value.attr == ""Callable""
        ):
            return None
        if not (isinstance(node.slice, ast.Tuple) and len(node.slice.elts) == 2):
            return None
        args_node, ret_node = node.slice.elts
        if isinstance(args_node, ast.List):
            arg_types = [self.convert_type(e) for e in args_node.elts]
        else:
            arg_types = [self.convert_type(args_node)]
        ret_type = self.convert_type(ret_node)
        return arg_types, ret_type
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    def __init__(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyBlocks
survived,"        def __init__(self, *a, **k):
            pass
",tests/test_agent_experience_entrypoint.py,DummyMemory
survived,"        def __init__(self, *a, **k):
            pass
",tests/test_agent_experience_entrypoint.py,DummyConfig
survived,"def _run_main(monkeypatch: pytest.MonkeyPatch, openai_key: str | None, base_url: str | None) -> str | None:
    recorded: dict[str, str | None] = {}

    stub = types.ModuleType(""openai_agents"")

    def Tool(*_a, **_k):
        def dec(f):
            return f
        return dec

    class DummyMemory:
        def __init__(self, *a, **k):
            pass

        def recent(self, _n: int):
            return []

    class DummyAgent:
        def __init__(self, *a, **k) -> None:
            self.memory = DummyMemory()

        async def act(self) -> str:
            return ""done""

        def observe(self, *_a) -> None:
            pass

    def DummyOpenAIAgent(*_a, **kw):
        recorded[""base_url""] = kw.get(""base_url"")
        return object()

    stub.Agent = DummyAgent
    stub.OpenAIAgent = DummyOpenAIAgent
    stub.Tool = Tool
    stub.memory = types.SimpleNamespace(LocalQdrantMemory=DummyMemory)

    gr_stub = types.SimpleNamespace(Blocks=DummyBlocks, mount_gradio_app=mount_gradio_app)

    class DummyConfig:
        def __init__(self, *a, **k):
            pass

    class DummyServer:
        def __init__(self, *a, **k):
            pass

        async def serve(self) -> None:
            pass

    uvicorn_stub = types.SimpleNamespace(Config=DummyConfig, Server=DummyServer)

    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    monkeypatch.setitem(sys.modules, ""gradio"", gr_stub)
    monkeypatch.setitem(sys.modules, ""uvicorn"", uvicorn_stub)

    if openai_key is None:
        monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    else:
        monkeypatch.setenv(""OPENAI_API_KEY"", openai_key)
    if base_url is None:
        monkeypatch.delenv(""LLM_BASE_URL"", raising=False)
    else:
        monkeypatch.setenv(""LLM_BASE_URL"", base_url)

    module_name = ""alpha_factory_v1.demos.era_of_experience.agent_experience_entrypoint""
    sys.modules.pop(module_name, None)
    mod = importlib.import_module(module_name)

    async def one_event():
        yield {""id"": 1, ""t"": ""0"", ""user"": ""a"", ""kind"": ""health"", ""payload"": {}}

    monkeypatch.setattr(mod, ""experience_stream"", one_event)
    monkeypatch.setattr(mod.asyncio, ""sleep"", lambda *_a, **_kw: None)
    asyncio.run(mod.main())
    return recorded.get(""base_url"")
",tests/test_agent_experience_entrypoint.py,
survived,"def test_plain_table_handles_no_rows() -> None:
    assert cli._plain_table([""h1"", ""h2""], []) == ""h1 | h2""",tests/test_demo_cli.py,
survived,"    def __init__(self, *a, **k):
        pass
",tests/test_selfheal_env.py,DummyBlocks
survived,"def prefer_medium_model() -> ModelPreferences:
    """"""Balanced model preferences for general use.""""""

    return ModelPreferences(
        hints=[
            ModelHint(name=""gpt-4o-2024-05-13""),
            ModelHint(name=""claude-3-sonnet-20240229""),
            ModelHint(name=""llama-3-70b""),
        ],
        costPriority=0.5,
        speedPriority=0.6,
        intelligencePriority=0.6,
    )
",src/enrichmcp/context.py,
survived,"def _make_client(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> TestClient:
    monkeypatch.setenv(""STORAGE_PATH"", str(tmp_path))
    from alpha_factory_v1.demos.alpha_agi_insight_v1.src import evolution_worker

    return TestClient(evolution_worker.app)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_evolution_worker.py,
survived,"def test_rejects_absolute_path(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(tmp_path, monkeypatch)

    info = tarfile.TarInfo(""/abs.txt"")
    info.size = 0
    payload = _make_tar(info)

    resp = client.post(""/mutate"", files={""tar"": (""bad.tar"", payload, ""application/x-tar"")})
    assert resp.status_code == 400",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_evolution_worker.py,
survived,"    def sample_topics(self, d):

        doc = self.corpus[d]

        # initialise level counts
        doc_levels = self.levels[d]
        level_counts = np.zeros(self.num_levels, dtype=int)
        for c in doc_levels:
            level_counts[c] += 1

        # get the leaf node and populate the path
        path = np.zeros(self.num_levels, dtype=object)
        node = self.document_leaves[d]
        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4
            path[level] = node
            node = node.parent

        # sample a new level for each word
        level_weights = np.zeros(self.num_levels)
        for n in range(len(doc)):

            w = doc[n]
            word_level = doc_levels[n]

            # remove from model
            level_counts[word_level] -= 1
            node = path[word_level]
            node.word_counts[w] -= 1
            node.total_words -= 1

            # pick new level
            for level in range(self.num_levels):
                level_weights[level] = (self.alpha + level_counts[level]) *                     \
                    (self.eta + path[level].word_counts[w]) /                                   \
                    (self.eta_sum + path[level].total_words)
            level_weights = level_weights / np.sum(level_weights)
            level = self.random_state.multinomial(1, level_weights).argmax()

            # put the word back into the model
            doc_levels[n] = level
            level_counts[level] += 1
            node = path[level]
            node.word_counts[w] += 1
            node.total_words += 1
",src/hlda/sampler.py,HierarchicalLDA
survived,"def _write_executable(path: Path, content: str) -> None:
    path.write_text(content)
    path.chmod(0o755)
",tests/test_cross_industry_patch.py,
survived,"def _build_page_cache(cfg, B: Axis, Pos: Axis, page_size: int = 2) -> PageCache:
    pages_per_seq = (Pos.size + page_size - 1) // page_size
    Seq = Axis(""seq"", B.size)
    Page = Axis(""page"", pages_per_seq)
    MaxPage = Axis(""max_page"", pages_per_seq * B.size)
    Slot = Axis(""slot"", page_size)
    return PageCache.init(Seq, Page, Slot, cfg.KVHeads, cfg.HeadSize, MaxPage, dtype=jnp.float32)
",tests/test_attention.py,
survived,"    def tearDown(self):
        self.patcher.stop()
",tests/test_sys_fn_kdb.py,TestKdbIPC
survived,"def test_distribution_zip(tmp_path: Path) -> None:
    zip_path = BROWSER_DIR / ""insight_browser.zip""
    if zip_path.exists():
        zip_path.unlink()
    result = subprocess.run([
        ""npm"",
        ""run"",
        ""build:dist"",
    ], cwd=BROWSER_DIR, capture_output=True, text=True)
    assert result.returncode == 0, result.stderr
    assert zip_path.exists(), ""insight_browser.zip missing""
    assert zip_path.stat().st_size <= 3 * 1024 * 1024, ""zip size exceeds 3 MiB""
    with zipfile.ZipFile(zip_path) as zf:
        names = zf.namelist()
    expected = {
        ""index.html"",
        ""insight.bundle.js"",
        ""service-worker.js"",
        ""manifest.json"",
        ""style.css"",
        ""insight_browser_quickstart.pdf"",
    }
    # ensure expected files exist
    for name in expected:
        assert name in names, f""{name} missing from zip""
    # ensure assets directory exists and contains files
    assert any(n.startswith(""assets/"") for n in names), ""assets directory missing""
    # ensure no unexpected files
    allowed_prefixes = {""assets/""}
    for name in names:
        if name in expected:
            continue
        if any(name.startswith(p) for p in allowed_prefixes):
            continue
        pytest.fail(f""Unexpected file {name} in zip"")",tests/test_distribution_zip.py,
survived,"    def load_weights(self, path: str) -> None:
        self.loaded = path
",tests/test_orchestrator_rest.py,DummyAgent
survived,"        def set(self, *_a: Any, **_kw: Any) -> None: ...
",src/monitoring/metrics.py,_N
survived,"    def test_infer_fastmcp_v1_server(self):
        """"""FastMCP 1.0 server instances should infer to FastMCPTransport.""""""
        from mcp.server.fastmcp import FastMCP as FastMCP1

        server = FastMCP1()
        transport = infer_transport(server)
        assert isinstance(transport, FastMCPTransport)",tests/client/test_client.py,TestInferTransport
survived,"def test_compare_df_mixed_types_value_mismatch():
    df1 = pd.DataFrame({
        'int_col': [1, 2],
        'float_col': [1.5, 2.5],
        'date_col': pd.to_datetime(['2023-01-01', '2023-01-02']),
        'str_col': ['x', 'y'],
    })
    df2 = df1.copy()
    df2.loc[1, 'float_col'] = 9.9
    assert not compare_df(df1, df2, question=""mixed"")
",backend/tests/test_utils_sql_compare_df.py,
survived,"def test_download_with_retry_fallback(tmp_path: Path, requests_mock: requests_mock.Mocker) -> None:
    path = tmp_path / ""out""
    monkeypatch = pytest.MonkeyPatch()
    monkeypatch.setattr(fa, ""FALLBACK_GATEWAYS"", [""https://alt.gateway/ipfs""])
    url_primary = f""{fa.GATEWAY}/CID""
    url_alt = ""https://alt.gateway/ipfs/CID""
    requests_mock.get(url_primary, status_code=500)
    requests_mock.get(url_alt, text=""data"")
    try:
        fa.download_with_retry(""CID"", path, attempts=1)
    finally:
        monkeypatch.undo()
    assert path.read_text() == ""data""
",tests/test_fetch_assets.py,
survived,"def test_download_error(tmp_path: Path, requests_mock: ""requests_mock.Mocker"") -> None:
    monkeypatch_file_list = [""dummy.txt""]
    url = dg.model_urls(""117M"")[0].replace(""checkpoint"", ""dummy.txt"")
    requests_mock.get(url, status_code=404)

    dest_dir = tmp_path / ""models""
    with pytest.MonkeyPatch.context() as m:
        m.setattr(dg, ""_FILE_LIST"", monkeypatch_file_list)
        with pytest.raises(Exception):
            dg.download_openai_gpt2(""117M"", dest=dest_dir, attempts=1)",tests/test_download_openai_gpt2.py,
survived,"def test_missing_token(monkeypatch: pytest.MonkeyPatch) -> None:
    client = _make_client(monkeypatch)
    resp = client.get(""/agents"")
    assert resp.status_code == 403
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,
survived,"def test_env_required(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.delenv(""API_TOKEN"", raising=False)
    with pytest.raises(RuntimeError):
        orchestrator._build_rest({""dummy"": DummyRunner()})",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,
survived,"def _dir_checksum(path: Path) -> str:
    hasher = hashlib.sha256()
    for file in sorted(path.rglob(""*"")):
        if file.is_file():
            hasher.update(file.relative_to(path).as_posix().encode())
            hasher.update(file.read_bytes())
    return hasher.hexdigest()
",tests/test_checksum.py,
survived,"    def project(self, vec: Sequence[float] | np.ndarray) -> np.ndarray:
        """"""Return ``vec`` multiplied by the current projection matrix.""""""
        if self._counter % self.steps == 0:
            self._proj = self._new_projection()
        self._counter += 1
        if np is not None:
            arr = np.asarray(vec, dtype=""float32"")
            if arr.ndim == 1:
                arr = arr.reshape(1, -1)
            return (arr @ np.asarray(self._proj).T).reshape(-1)
        arr = [float(x) for x in vec]
        out = [sum(a * b for a, b in zip(arr, col)) for col in zip(*self._proj)]
        if np is not None:
            return np.asarray(out, dtype=""float32"")
        return out  # type: ignore[return-value]",src/agents/guards/embedding_orthogonaliser.py,EmbeddingOrthogonaliser
survived,"    def prompt_video(self) -> str | None:
        prompt = self.properties.get(""prompt_video"")
        if prompt is None:
            return None
        if not isinstance(prompt, str):
            raise ValueError(""Invalid prompt_video. prompt_video must be a string."")
        return prompt
",libs/core/kiln_ai/datamodel/extraction.py,ExtractorConfig
survived,"    def _byte_to_char_idx(self, line_no: int, byte_idx: int) -> int:
        line_bytes = self._src_lines_bytes[line_no]
        return len(line_bytes[:byte_idx].decode(""utf-8""))
",src/flynt/code_editor.py,CodeEditor
survived,"def test_broadcast_merkle_root_handles_corrupt_db(tmp_path: Path) -> None:
    ledger_path = tmp_path / ""ledger.db""
    ledger = Ledger(str(ledger_path), rpc_url=""http://rpc.test"", broadcast=True)
    ledger.log(messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0))
    ledger.compute_merkle_root()
    # truncate database file to simulate missing pages
    data = ledger_path.read_bytes()
    ledger_path.write_bytes(data[: len(data) // 2])
    with mock.patch.object(insight_logging, ""_log"") as log:
        asyncio.run(ledger.broadcast_merkle_root())
        log.warning.assert_called()",tests/test_ledger_corruption.py,
survived,"def test_bus_extreme_envelopes() -> None:
    """"""Large or malformed messages should not crash the bus.""""""

    bus = messaging.A2ABus(config.Settings(bus_port=0))
    received: list[object] = []

    async def handler(env: object) -> None:
        received.append(env)

    bus.subscribe(""x"", handler)

    async def run() -> None:
        for size in (0, 1, 100, 1000, 10000, 50000):
            env = messaging.Envelope(sender=""s"" * size, recipient=""x"", ts=1e308)
            env.payload[""data""] = ""p"" * size
            bus.publish(""x"", env)
        bus.publish(""x"", messaging.Envelope(sender="""", recipient=""x"", ts=float(""inf"")))
        bus.publish(""x"", messaging.Envelope(sender="""", recipient=""x"", ts=float(""-inf"")))
        bus.publish(""x"", types.SimpleNamespace(sender=None, recipient=""x"", payload={}, ts=None))
        await asyncio.sleep(0)

    asyncio.run(run())
    assert received",tests/test_bus_fuzz.py,
survived,"def test_merkle_root_ignores_corrupt_rows(tmp_path: Path) -> None:
    ledger = Ledger(str(tmp_path / ""ledger.db""), broadcast=False)
    env1 = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
    env2 = messaging.Envelope(sender=""b"", recipient=""c"", payload={""v"": 2}, ts=1.0)
    ledger.log(env1)
    ledger.log(env2)

    baseline = ledger.compute_merkle_root()

    # insert rows with missing hash and invalid hash
    ledger.conn.execute(
        ""INSERT INTO messages (ts, sender, recipient, payload) VALUES (?, ?, ?, ?)"",
        (2.0, ""x"", ""y"", ""{oops""),
    )
    ledger.conn.execute(
        ""INSERT INTO messages (ts, sender, recipient, payload, hash) VALUES (?, ?, ?, ?, ?)"",
        (3.0, ""y"", ""z"", ""{}"", ""zz""),
    )
    ledger.conn.commit()

    root = ledger.compute_merkle_root()
    assert root == baseline

    # further logging should still succeed
    ledger.log(messaging.Envelope(sender=""c"", recipient=""d"", payload={""v"": 3}, ts=2.0))
    ledger.compute_merkle_root()
",tests/test_ledger_malformed_rows.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q2.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q1.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q11.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q15.py,Auto1
survived,"def test_Q20_finds_complete_cast_Iron_Man_movie():
    assert result == [Auto1(complete_downey_ironman_movie=""Iron Man"")]
",tests/dataset/job/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto11
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto8
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q5.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q14.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto6
survived,"def test_Q28_finds_euro_dark_movie_with_minimal_values():
    assert result == Auto1(
        movie_company=""Euro Films Ltd."",
        rating=7.2,
        complete_euro_dark_movie=""Dark Euro Film"",
    )
",tests/dataset/job/compiler/py/q28.py,
survived,"def test_Q23_finds_US_internet_movie_with_verified_cast():
    assert result == [Auto1(movie_kind=""movie"", complete_us_internet_movie=""Web Movie"")]
",tests/dataset/job/compiler/py/q23.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q10.py,Auto4
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/job/compiler/py/q15.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q8.py,Auto4
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q18.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q14.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto11
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto14
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q7.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto11
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto16
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q27.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto11
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,CustomerAddres
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q21.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CustomerAddress
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,DateDim
survived,"def _q4():
    _src = catalog_sales
    _rows = _query(
        _src,
        [{""items"": date_dim, ""on"": lambda cs, d: d.d_date_sk == cs.cs_sold_date_sk}],
        {""select"": lambda cs, d: (cs, d)},
    )
    _groups = _group_by(_rows, lambda cs, d: cs.cs_call_center_sk)
    _items5 = _groups
    return [
        Auto4(
            cs_call_center_sk=g.key,
            sales=_sum([x[0].cs_ext_sales_price for x in g]),
            profit=_sum([x[0].cs_net_profit for x in g]),
        )
        for g in _items5
    ]
",tests/dataset/tpc-ds/compiler/py/q77.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Store
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q63.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,StoreReturn
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,Auto1
survived,"def test_TPCDS_Q54_simplified():
    assert result == [
        Auto1(segment=1, num_customers=1, segment_base=50),
        Auto1(segment=0, num_customers=1, segment_base=0),
    ]
",tests/dataset/tpc-ds/compiler/py/q54.py,
survived,"def test_TPCDS_Q84_sample():
    assert result == 84.0
",tests/dataset/tpc-ds/compiler/py/q84.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,Auto3
survived,"def _q0():
    _src = inventory
    _rows = _query(
        _src,
        [
            {""items"": date_dim, ""on"": lambda inv, d: inv.inv_date_sk == d.d_date_sk},
            {""items"": item, ""on"": lambda inv, d, i: inv.inv_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda inv, d, i: (inv, d, i),
            ""where"": lambda inv, d, i: d.d_month_seq >= 0 and d.d_month_seq <= 11,
        },
    )
    _groups = _group_by(
        _rows,
        lambda inv, d, i: Auto2(
            product_name=i.i_product_name,
            brand=i.i_brand,
            _class=i.i_class,
            category=i.i_category,
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            i_product_name=g.key[""product_name""],
            i_brand=g.key[""brand""],
            i_class=g.key[""_class""],
            i_category=g.key[""category""],
            qoh=(
                sum([x[0].inv_quantity_on_hand for x in g])
                / len([x[0].inv_quantity_on_hand for x in g])
                if [x[0].inv_quantity_on_hand for x in g]
                else 0
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q22.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q97.py,Auto1
survived,"def abs(x):
    if x >= 0.0:
        return x
    return -x
",tests/dataset/tpc-ds/compiler/py/q53.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Auto4
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q19.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,DateDim
survived,"def _q0():
    _src = catalog_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": warehouse,
                ""on"": lambda cs, w: cs.cs_warehouse_sk == w.w_warehouse_sk,
            },
            {
                ""items"": ship_mode,
                ""on"": lambda cs, w, sm: cs.cs_ship_mode_sk == sm.sm_ship_mode_sk,
            },
            {
                ""items"": call_center,
                ""on"": lambda cs, w, sm, cc: cs.cs_call_center_sk
                == cc.cc_call_center_sk,
            },
        ],
        {""select"": lambda cs, w, sm, cc: (cs, w, sm, cc)},
    )
    _groups = _group_by(
        _rows,
        lambda cs, w, sm, cc: Auto2(
            warehouse=w.w_warehouse_name[0:20], sm_type=sm.sm_type, cc_name=cc.cc_name
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            warehouse=g.key[""warehouse""],
            sm_type=g.key[""sm_type""],
            cc_name=g.key[""cc_name""],
            d30=len(
                [x for x in g if x[0].cs_ship_date_sk - x[0].cs_sold_date_sk <= 30]
            ),
            d60=len(
                [
                    x
                    for x in g
                    if x[0].cs_ship_date_sk - x[0].cs_sold_date_sk > 30
                    and x[0].cs_ship_date_sk - x[0].cs_sold_date_sk <= 60
                ]
            ),
            d90=len(
                [
                    x
                    for x in g
                    if x[0].cs_ship_date_sk - x[0].cs_sold_date_sk > 60
                    and x[0].cs_ship_date_sk - x[0].cs_sold_date_sk <= 90
                ]
            ),
            d120=len(
                [
                    x
                    for x in g
                    if x[0].cs_ship_date_sk - x[0].cs_sold_date_sk > 90
                    and x[0].cs_ship_date_sk - x[0].cs_sold_date_sk <= 120
                ]
            ),
            dmore=len(
                [x for x in g if x[0].cs_ship_date_sk - x[0].cs_sold_date_sk > 120]
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q99.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,DateDim
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q42.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto5
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q22.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q51.py,WebSale
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q93.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto3
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,CallCenter
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,CatalogSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q76.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto3
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q79.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q6.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q20.py,_Group
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q44.py,
survived,"def test_TPCDS_Q4_result():
    assert result == [
        Auto1(
            customer_id=""C1"",
            customer_first_name=""Alice"",
            customer_last_name=""A"",
            customer_login=""alice"",
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q4.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q23.py,_Group
survived,"def _q0():
    _src = item
    _rows = _query(
        _src,
        [
            {""items"": inventory, ""on"": lambda i, inv: i.i_item_sk == inv.inv_item_sk},
            {""items"": date_dim, ""on"": lambda i, inv, d: inv.inv_date_sk == d.d_date_sk},
            {
                ""items"": catalog_sales,
                ""on"": lambda i, inv, d, cs: cs.cs_item_sk == i.i_item_sk,
            },
        ],
        {
            ""select"": lambda i, inv, d, cs: (i, inv, d, cs),
            ""where"": lambda i, inv, d, cs: (
                (
                    (
                        (i.i_current_price >= 20 and i.i_current_price <= 50)
                        and i.i_manufact_id >= 800
                    )
                    and i.i_manufact_id <= 803
                )
                and inv.inv_quantity_on_hand >= 100
            )
            and inv.inv_quantity_on_hand <= 500,
        },
    )
    _groups = _group_by(
        _rows,
        lambda i, inv, d, cs: Auto2(
            id=i.i_item_id, desc=i.i_item_desc, price=i.i_current_price
        ),
    )
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: _sort_key(g.key[""id""]))
    return [
        Auto1(
            i_item_id=g.key[""id""],
            i_item_desc=g.key[""desc""],
            i_current_price=g.key[""price""],
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q37.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,DateDim
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q65.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,StoreReturn
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Store
survived,"def test_TPCDS_Q41_simplified():
    assert result == [""Blue Shirt"", ""Red Dress""]
",tests/dataset/tpc-ds/compiler/py/q41.py,
survived,"def _q0():
    _src = call_center
    _rows = _query(
        _src,
        [
            {
                ""items"": catalog_returns,
                ""on"": lambda cc, cr: cc.cc_call_center_sk == cr.cr_call_center_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda cc, cr, d: cr.cr_returned_date_sk == d.d_date_sk,
            },
            {
                ""items"": customer,
                ""on"": lambda cc, cr, d, c: cr.cr_returning_customer_sk
                == c.c_customer_sk,
            },
            {
                ""items"": customer_demographics,
                ""on"": lambda cc, cr, d, c, cd: c.c_current_cdemo_sk == cd.cd_demo_sk,
            },
            {
                ""items"": household_demographics,
                ""on"": lambda cc, cr, d, c, cd, hd: c.c_current_hdemo_sk
                == hd.hd_demo_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda cc, cr, d, c, cd, hd, ca: c.c_current_addr_sk
                == ca.ca_address_sk,
            },
        ],
        {
            ""select"": lambda cc, cr, d, c, cd, hd, ca: (cc, cr, d, c, cd, hd, ca),
            ""where"": lambda cc, cr, d, c, cd, hd, ca: (
                (
                    (
                        (d.d_year == 2001 and d.d_moy == 5)
                        and cd.cd_marital_status == ""M""
                    )
                    and cd.cd_education_status == ""Unknown""
                )
                and hd.hd_buy_potential == ""1001-5000""
            )
            and ca.ca_gmt_offset == -6,
        },
    )
    _groups = _group_by(
        _rows,
        lambda cc, cr, d, c, cd, hd, ca: Auto2(
            id=cc.cc_call_center_id, name=cc.cc_name, mgr=cc.cc_manager
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            Call_Center=g.key[""id""],
            Call_Center_Name=g.key[""name""],
            Manager=g.key[""mgr""],
            Returns_Loss=_sum([x[1].cr_net_loss for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q91.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q46.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q31.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q79.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q15.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q8.py,CustomerAddres
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q28.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,Auto3
survived,"def _q2():
    _groups = {}
    _order = []
    for g in grouped:
        _k = g.i_class
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(g)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto4(_class=cg.key, total=sum([x.itemrevenue for x in cg])) for cg in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q98.py,
survived,"def _q0():
    _src = date_dim
    _rows = _query(
        _src,
        [
            {
                ""items"": store_sales,
                ""on"": lambda dt, ss: dt.d_date_sk == ss.ss_sold_date_sk,
            },
            {""items"": item, ""on"": lambda dt, ss, i: ss.ss_item_sk == i.i_item_sk},
        ],
        {
            ""select"": lambda dt, ss, i: (dt, ss, i),
            ""where"": lambda dt, ss, i: i.i_manufact_id == 100 and dt.d_moy == 12,
        },
    )
    _groups = _group_by(
        _rows,
        lambda dt, ss, i: Auto2(
            d_year=dt.d_year, brand_id=i.i_brand_id, brand=i.i_brand
        ),
    )
    _items1 = _groups
    _items1 = sorted(
        _items1,
        key=lambda g: _sort_key(
            [
                g.key[""d_year""],
                -_sum([x[1].ss_ext_sales_price for x in g]),
                g.key[""brand_id""],
            ]
        ),
    )
    return [
        Auto1(
            d_year=g.key[""d_year""],
            brand_id=g.key[""brand_id""],
            brand=g.key[""brand""],
            sum_agg=_sum([x[1].ss_ext_sales_price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q3.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,DateDim
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q32.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q53.py,_Group
survived,"def test_TPCDS_Q83_sample():
    assert result == 83
",tests/dataset/tpc-ds/compiler/py/q83.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q86.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q56.py,StoreSale
survived,"def test_TPCDS_Q6_result():
    assert result == [Auto1(state=""CA"", cnt=10)]
",tests/dataset/tpc-ds/compiler/py/q6.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q2.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,StoreSale
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q8.py,
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q1.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q39.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Auto3
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q13.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q45.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,CustomerAddres
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,Auto1
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q22.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q14.py,Item
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q22.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,WebSite
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q65.py,Auto2
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q96.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q39.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Store
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q99.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,DateDim
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q79.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q76.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CatalogSale
survived,"def test_TPCDS_Q89_sample():
    assert result == 89.0
",tests/dataset/tpc-ds/compiler/py/q89.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q59.py,Auto1
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q33.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,CatalogSale
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q70.py,
survived,"def test_TPCDS_Q63_simplified():
    assert result == 63
",tests/dataset/tpc-ds/compiler/py/q63.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q53.py,Auto1
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q30.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q4.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,Inventory
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q24.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Inventory
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Auto3
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q67.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,CallCenter
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Store
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q19.py,_Group
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q28.py,_Group
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q34.py,_Group
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q56.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,Auto1
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q19.py,
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": store_returns,
                ""on"": lambda ss, sr: ss.ss_ticket_number == sr.sr_ticket_number
                and ss.ss_item_sk == sr.sr_item_sk,
            },
            {""items"": store, ""on"": lambda ss, sr, s: ss.ss_store_sk == s.s_store_sk},
            {""items"": item, ""on"": lambda ss, sr, s, i: ss.ss_item_sk == i.i_item_sk},
            {
                ""items"": customer,
                ""on"": lambda ss, sr, s, i, c: ss.ss_customer_sk == c.c_customer_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda ss, sr, s, i, c, ca: c.c_current_addr_sk
                == ca.ca_address_sk,
            },
        ],
        {
            ""select"": lambda ss, sr, s, i, c, ca: (ss, sr, s, i, c, ca),
            ""where"": lambda ss, sr, s, i, c, ca: (
                c.c_birth_country != ca.ca_country.upper() and s.s_zip == ca.ca_zip
            )
            and s.s_market_id == 5,
        },
    )
    _groups = _group_by(
        _rows,
        lambda ss, sr, s, i, c, ca: Auto3(
            last=c.c_last_name,
            first=c.c_first_name,
            store_name=s.s_store_name,
            color=i.i_color,
        ),
    )
    _items1 = _groups
    return [
        Auto2(
            c_last_name=g.key[""last""],
            c_first_name=g.key[""first""],
            s_store_name=g.key[""store_name""],
            color=g.key[""color""],
            netpaid=sum([x[0].ss_net_paid for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q24.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q34.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q74.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Customer
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q98.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,WebReturn
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q52.py,Auto1
survived,"def test_TPCDS_Q66_simplified():
    assert result == 66
",tests/dataset/tpc-ds/compiler/py/q66.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q1.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,W
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q14.py,_Group
survived,"def _sort_key(k):
    if hasattr(k, ""__dataclass_fields__""):
        return str(k)
    if isinstance(k, list):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, tuple):
        return tuple((_sort_key(x) for x in k))
    if isinstance(k, dict):
        return str(k)
    return k
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q44.py,StoreSale
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q1.py,_Group
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q77.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,CustomerDemographic
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,StoreSale
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q21.py,_Group
survived,"def _q0():
    _groups = {}
    _order = []
    for j in joined:
        _k = j.s
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(j)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            s_store_name=g.key.s_store_name,
            d30=len([1 for x in g if x.diff <= 30]),
            d31_60=len([1 for x in g if x.diff > 30 and x.diff <= 60]),
            d61_90=len([1 for x in g if x.diff > 60 and x.diff <= 90]),
            d91_120=len([1 for x in g if x.diff > 90 and x.diff <= 120]),
            d_gt_120=len([1 for x in g if x.diff > 120]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q9.py,StoreSale
survived,"def test_population_df() -> None:
    pop = [mats.Individual([0.0, 0.0]), mats.Individual([1.0, 1.0])]
    for i, ind in enumerate(pop):
        ind.fitness = (i * 1.0, i * 2.0, i * 3.0)
        ind.rank = i
    df = web_app.population_df(pop)
    assert set(df.columns) == {""effectiveness"", ""risk"", ""complexity"", ""rank""}
    assert len(df) == 2",tests/test_web_app.py,
survived,"        def handle_request(req: Request) -> None:
            if req.url.endswith("".js"") and not req.url.endswith(""sw.js""):
                js_requests.append(req.url)
            elif req.url.endswith("".map""):
                map_requests.append(req.url)
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_single_network_request.py,
survived,"async def async_setup_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:
    """"""Set up Gree from a config entry.""""""
    if DOMAIN not in hass.data:
        hass.data[DOMAIN] = {}

    hass.data[DOMAIN][entry.entry_id] = entry.data
    await hass.config_entries.async_forward_entry_setups(entry, PLATFORMS)
    return True
",custom_components/gree/__init__.py,
survived,"    def __init__(self, *args, **kwargs):
        self.chat = _Chat()
",openai/__init__.py,OpenAI
survived,"    def __init__(self, request=None):
        self.request = request
",openai/__init__.py,APITimeoutError
survived,"def unicode_escape_map(literal: str) -> Dict[str, str]:
    """"""Return mapping of characters to their unicode escape sequences.""""""
    quote = get_quote_type(literal)
    if quote is None:
        return {}
    idx = 0
    while idx < len(literal) and literal[idx] in ""furbFURB"":
        idx += 1
    body = literal[idx + len(quote) : -len(quote)]
    mapping: Dict[str, str] = {}
    for m in unicode_escape_re.finditer(body):
        esc = m.group(0)
        try:
            char = codecs.decode(esc, ""unicode_escape"")
        except Exception:  # noqa: S112
            continue
        mapping[char] = esc
    return mapping
",src/flynt/utils/utils.py,
survived,"def test_chat_formatter_final_only():
    training_data = ModelTrainingData(
        input=""test input"",
        system_message=""system message"",
        final_output=""test output"",
    )
    expected = generate_chat_message_response(training_data)[""messages""]

    formatter = get_chat_formatter(
        strategy=ChatStrategy.final_only,
        system_message=""system message"",
        user_input=""test input"",
    )

    first = formatter.next_turn()
    assert [m.__dict__ for m in first] == expected[:2]

    assert formatter.next_turn(""test output"") is None
    assert formatter.message_dicts() == expected
",libs/core/kiln_ai/adapters/chat/test_chat_formatter.py,
survived,"    def __init__(self, status_code: int, text: str):
        self.status_code = status_code
        self.text = text
",alpha_factory_v1/requests.py,Response
survived,"def get(url: str, **kwargs):
    with _request.urlopen(url) as resp:
        data = resp.read().decode()
        return Response(resp.getcode(), data)",alpha_factory_v1/requests.py,
survived,"    def body(self) -> Union[str, bytes]:
        return self.request.body
",src/graphql_server/webob/views.py,WebobHTTPRequestAdapter
survived,"    def headers(self) -> Mapping[str, str]:
        return self.request.headers
",src/graphql_server/webob/views.py,WebobHTTPRequestAdapter
survived,"def test_load_passes_revision():
    model_mock = MagicMock()
    model_mock.config = MagicMock(eos_token_id=None)
    processor_mock = MagicMock()

    with patch(""mlx_vlm.utils.get_model_path"") as mock_get_model_path, patch(
        ""mlx_vlm.utils.load_model"",
        return_value=model_mock,
    ) as mock_load_model, patch(
        ""mlx_vlm.utils.load_processor"",
        return_value=processor_mock,
    ) as mock_load_processor, patch(
        ""mlx_vlm.utils.load_image_processor"", return_value=None
    ):
        mock_get_model_path.return_value = Path(""/tmp/model"")

        model, processor = load(""repo"", revision=""abc"")

        assert model is model_mock
        assert processor is processor_mock
        mock_get_model_path.assert_called_with(""repo"", revision=""abc"")",mlx_vlm/tests/test_utils.py,
survived,"    async def run(self, stop_event: asyncio.Event) -> None:
        """"""Drive the orchestrator until ``stop_event`` is set.""""""
        await self.start()
        await self.scheduler.run(stop_event)
        await self.stop()
",alpha_factory_v1/backend/orchestrator.py,Orchestrator
survived,"def test_vote_and_merge_accepts_patch(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    diff = """"""--- a/metric.txt
+++ b/metric.txt
@@
-1
+2
""""""
    reg = StakeRegistry()
    reg.set_stake(""orch"", 1.0)
    with (
        patch.object(harness, ""_run_tests"", return_value=0),
        patch.object(harness, ""run_preflight""),
        patch.object(
            harness.patcher_core, ""apply_patch"", lambda d, repo_path: (Path(repo_path) / ""metric.txt"").write_text(""2\n"")
        ),
    ):
        accepted = harness.vote_and_merge(repo, diff, reg)
    assert accepted
    assert (repo / ""metric.txt"").read_text().strip() == ""2""
",tests/test_self_evolution.py,
survived,"    def __init__(self, *a, **k):
        pass
",tests/resources/sentence_transformers.py,SentenceTransformer
survived,"    def adder(x):
        return x + n
",tests/human/python/closure.py,
survived,"def sum3(a: int, b: int, c: int) -> int:
    return a + b + c
",tests/human/x/python/fun_three_args.py,
survived,"def test_describe_model_with_literal_type():
    """"""Test describe_model with Literal field types.""""""
    app = EnrichMCP(""Enum API"", description=""A model with Literal fields"")

    @app.entity(description=""Entity using Literal"")
    class Item(EnrichModel):
        status: Literal[""pending"", ""complete""] = Field(description=""Item status"")

    description = app.describe_model()

    assert ""## Item"" in description
    assert ""- **status** (Literal): Item status"" in description",tests/test_model_description.py,
survived,"    def get_help(self, ctx: click.Context) -> str:  # pragma: no cover - CLI
        help_text = super().get_help(ctx)
        return f""{DISCLAIMER}\n\n{help_text}""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,DisclaimerGroup
survived,"def test_call_summary_deep_merge(client):
    @weave.op()
    def my_op():
        call = call_context.get_current_call()
        call.summary[""nested""] = {""foo"": 1}
        return ""done""

    my_op()
    calls = list(client.get_calls())
    assert len(calls) == 1
    summary = calls[0].summary
    assert summary[""nested""][""foo""] == 1
    assert summary[RESERVED_SUMMARY_STATUS_COUNTS_KEY][tsi.TraceStatus.SUCCESS] == 1",tests/trace/test_current_call.py,
survived,"def test_call_attributes_update_and_delete_forbidden(client):
    @weave.op()
    def my_op():
        call = call_context.get_current_call()
        with pytest.raises(TypeError):
            call.attributes.update({""extra"": 1})
        with pytest.raises(TypeError):
            del call.attributes[""weave""]
        return 1

    with weave.attributes({""env"": ""prod""}):
        my_op()
    calls = list(client.get_calls())
    assert len(calls) == 1
    # Original attribute is preserved
    assert calls[0].attributes[""env""] == ""prod""
    assert ""extra"" not in calls[0].attributes
",tests/trace/test_current_call.py,
survived,"def test_simulate_sets_llama_path() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio""):
        with patch.object(cli.orchestrator, ""Orchestrator""):
            result = runner.invoke(
                cli.main,
                [
                    ""simulate"",
                    ""--horizon"",
                    ""1"",
                    ""--offline"",
                    ""--llama-model-path"",
                    ""weights.bin"",
                ],
            )
    assert result.exit_code == 0
    assert os.environ.get(""LLAMA_MODEL_PATH"") == ""weights.bin""
",tests/test_demo_cli.py,
survived,"        async def get_event():
            gen = demo.experience_stream()
            return await anext(gen)
",tests/test_era_experience.py,TestEraOfExperience
survived,"def safe_eval(expression: str) -> float:
    tree = ast.parse(expression, mode=""eval"")
    allowed = (
        ast.Expression,
        ast.BinOp,
        ast.UnaryOp,
        ast.Constant,
        ast.Add,
        ast.Sub,
        ast.Mult,
        ast.Div,
        ast.Pow,
        ast.USub,
        ast.Load,
    )
    for n in ast.walk(tree):
        if not isinstance(n, allowed):
            raise ValueError(""Unsupported expression"")
    return _eval_node(tree.body)
",tests/test_safe_eval_security.py,
survived,"def test_init_creates_manager(monkeypatch):
    fake_cls = MagicMock()
    fake_instance = MagicMock()
    fake_cls.return_value = fake_instance
    monkeypatch.setattr(exec_mod, ""SandboxManager"", fake_cls)
    module = ExecutionModule()
    assert module.sandbox_manager is fake_instance
",tests/unit/test_execution_module.py,
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    snippet_path = repo_root / ""docs"" / ""DISCLAIMER_SNIPPET.md""
    disclaimer_text = snippet_path.read_text(encoding=""utf-8"").splitlines()[0].strip()

    missing: list[Path] = []
    for path in repo_root.rglob(""*.md""):
        if path == snippet_path or "".git"" in path.parts:
            continue
        try:
            first_line = path.read_text(encoding=""utf-8"").splitlines()[0].strip()
        except Exception:
            first_line = """"
        if ""docs/DISCLAIMER_SNIPPET.md"" not in first_line and not first_line.startswith(disclaimer_text):
            missing.append(path)

    if missing:
        print(""Missing disclaimer snippet in the following files:"", file=sys.stderr)
        for p in missing:
            print(f""  {p.relative_to(repo_root)}"", file=sys.stderr)
        return 1
    return 0
",scripts/verify_disclaimer_snippet.py,
survived,"def test_maybe_await_sync():
    result = asyncio.run(maybe_await(_sync_fn, 5))
    assert result == 6
",tests/test_agent_runner_utils.py,
survived,"def best_alpha(signals: Dict[str, str]) -> str:
    """"""Select the most actionable alpha message.""""""
    yc = signals.get(""yield_curve"", """")
    sc = signals.get(""supply_chain"", """")

    # simple heuristics
    if ""bottleneck"" in sc.lower():
        return sc
    if ""long bonds"" in yc.lower():
        return yc
    return yc if yc else sc
",alpha_factory_v1/demos/era_of_experience/alpha_report.py,
survived,"async def trigger_opportunity() -> str:
    resp = requests.post(f""{HOST}/agent/alpha_opportunity/trigger"", timeout=5)
    resp.raise_for_status()
    return ""alpha_opportunity queued""
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def _list_agents() -> list[str]:
    resp = requests.get(f""{HOST}/agents"", timeout=5)
    resp.raise_for_status()
    return resp.json()
",alpha_factory_v1/demos/alpha_agi_business_v1/gradio_dashboard.py,
survived,"    def test_launcher_compiles(self) -> None:
        path = Path('alpha_factory_v1/demos/alpha_agi_business_v1/run_business_v1_local.py')
        py_compile.compile(path, doraise=True)
",tests/test_alpha_business_v1_script.py,TestAlphaBusinessV1Script
survived,"def _set_business_host(host: str) -> None:
    """"""Propagate the orchestrator base URL to the bridge module.

    This helper updates the ``BUSINESS_HOST`` environment variable and
    mirrors the value inside :mod:`openai_agents_bridge` when that module
    is available.  It keeps the launcher functional even when the bridge
    was imported prior to setting the environment variable.
    """"""

    os.environ[""BUSINESS_HOST""] = host
    try:  # soft dependency
        from alpha_factory_v1.demos.alpha_agi_business_v1 import (
            openai_agents_bridge,
        )

        openai_agents_bridge.HOST = host
    except Exception:
        pass
",alpha_factory_v1/demos/alpha_agi_business_v1/run_business_v1_local.py,
survived,"def test_metrics_update_during_sim() -> None:
    dist = Path(__file__).resolve().parents[1] / (
        ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/index.html""
    )
    url = dist.as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_selector(""#simulator-panel"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-gen').value=2"")
            page.evaluate(""document.querySelector('#simulator-panel #sim-pop').value=3"")
            page.click(""#simulator-panel #sim-start"")
            page.wait_for_function(""document.querySelector('#worker-time').textContent.includes('ms')"")
            page.wait_for_function(""document.querySelector('#heap').textContent !== ''"")
            page.wait_for_function(""document.querySelector('#fps-value').textContent.includes('fps')"")
            page.click('#simulator-panel #sim-cancel')
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",tests/test_browser_ui.py,
survived,"    def __init__(self, base_url: str):
        self._base_url = base_url
",tests/integrations/openai/test_openai_sdk.py,DummyClient
survived,"    async def dummy_fn(completion, **kwargs):
        captured.update(kwargs)
        return ""ok""
",tests/integrations/openai/test_openai_sdk.py,
survived,"        async def stop_merkle_task(self) -> None:
            pass
",tests/test_orchestrator.py,DummyLedger
survived,"        def Add(a, b):
            return a + b
",tests/transpiler/x/py/go_auto.py,testpkg
survived,"    def test_version_flag(self):
        args = _parse_with(['--version'])
        self.assertTrue(args.version)
",alpha_factory_v1/tests/test_cli.py,CliParseTest
survived,"def test_cycle_emits_blocked_to_memory() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    chaos = chaos_agent.ChaosAgent(bus, led, burst=1)
    guardian = safety_agent.SafetyGuardianAgent(bus, led)

    asyncio.run(chaos.run_cycle())
    topic, env = bus.published[0]
    assert topic == ""safety""

    asyncio.run(guardian.handle(env))
    assert bus.published[-1][0] == ""memory""
    assert bus.published[-1][1].payload[""status""] == ""blocked""",tests/test_chaos_agent.py,
survived,"    def add(self, entry: ArchiveEntry) -> None:
        with Session(self.engine) as session:
            session.merge(_ArchiveRow(**dataclasses.asdict(entry)))
            session.commit()
",src/archive/db.py,ArchiveDB
survived,"    def add(self, meta: dict[str, Any], score: float) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(""INSERT INTO agents(meta, score) VALUES (?, ?)"", (json.dumps(meta), score))
",src/archive/__init__.py,Archive
survived,"    def __init__(self, path: str | Path) -> None:
        self.path = Path(path)
        self.engine = create_engine(f""sqlite:///{self.path}"")
        Base.metadata.create_all(self.engine)
        with Session(self.engine) as session:
            exists = session.query(_ArchiveRow).first() is not None
        if not exists:
            self._migrate_legacy()
",src/archive/db.py,ArchiveDB
survived,"def softmax(arr: np.ndarray) -> np.ndarray:
    exp = np.exp(arr - np.max(arr))
    return exp / exp.sum()
",tests/test_selector.py,
survived,"    def __init__(self, host: str, port: int, use_https: bool = False, level: LogLevel = LogLevel.DEBUG):
        super().__init__(level)
        self.host = host
        self.port = port
        self.use_https = use_https
",webscout/litlogger/handlers.py,NetworkHandler
survived,"    def emit(self, message: str, level: LogLevel):
        if level < self.level:
            return
        with socket.create_connection((self.host, self.port), timeout=5) as sock:
            sock.sendall(message.encode() + b""\n"")",webscout/litlogger/handlers.py,TCPHandler
survived,"    def log(self, level: LogLevel, message: str):
        if self.async_mode:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                return asyncio.create_task(self._log_async(level, message))
            return loop.run_until_complete(self._log_async(level, message))
        self._log(level, message)
",webscout/litlogger/logger.py,Logger
survived,"    def __init__(self, path: str, level: LogLevel = LogLevel.DEBUG, max_bytes: int = 0, backups: int = 0):
        super().__init__(level)
        self.path = Path(path)
        self.max_bytes = max_bytes
        self.backups = backups
        self._open()
",webscout/litlogger/handlers.py,FileHandler
survived,"    def __exit__(self, exc_type, exc, tb):
        if exc_type:
            self.exception(str(exc))
        return False",webscout/litlogger/logger.py,Logger
survived,"        def json(self):
            return {""model_list"": sample}
",libs/core/kiln_ai/adapters/test_remote_config.py,FakeResponse
survived,"        def __init__(self, *_: object, **__: object) -> None:
            pass
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,OpenAIAgent
survived,"            def _decorator(func):
                return func
",tests/test_aiga_agents_import.py,TestAigaAgentsImport
survived,"    def findChildren(self, recursive=True):
        result = []
        for child in self.children:
            result.append(child)
            if recursive:
                result.extend(child.findChildren(recursive))
        return result
",tests/conftest.py,_Node
survived,"    async def stop_merkle_task(self) -> None:
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:  # pragma: no cover - expected
                pass
            self._task = None
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"    def append(self, new_child: Union['Tag', NavigableString, str]) -> None:
        """"""Append a new child to this tag.""""""
        if isinstance(new_child, str):
            new_child = NavigableString(new_child)
        new_child.parent = self
        self.contents.append(new_child)
",webscout/scout/element.py,Tag
survived,"def test_labels_allowed_characters():

    runner = ScenarioRunner(uri=GMT_DIR, uri_type='folder', filename='tests/data/usage_scenarios/labels_stress_allowed.yml', skip_unsafe=False, skip_system_checks=True, dev_cache_build=True, dev_no_sleeps=True, dev_no_metrics=True, dev_no_phase_stats=True)
    with Tests.RunUntilManager(runner) as context:
        context.run_until('setup_services')
        labels = get_labels()

        assert labels.get('TESTALLOWED') == 'alpha-num123_', Tests.assertion_info('TESTALLOWED label', labels)
        assert labels.get('test.label') == 'example.com', Tests.assertion_info('test.label label', labels)
        assert labels.get('OTHER_LABEL') == 'http://localhost:8080', Tests.assertion_info('OTHER_LABEL label', labels)
",tests/test_usage_scenario.py,
survived,"def test_schema_checker_network_alias():
    usage_scenario_name = 'schema_checker_valid_network_alias.yml'
    usage_scenario_path = os.path.join(CURRENT_DIR, '../data/usage_scenarios/schema_checker/', usage_scenario_name)
    with open(usage_scenario_path, encoding='utf8') as file:
        usage_scenario = yaml.safe_load(file)
    schema_checker = SchemaChecker(validate_compose_flag=True)
    schema_checker.check_usage_scenario(usage_scenario)
",tests/lib/test_schema_checker.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q8.py,
survived,"def test_run_inference_raises_for_empty_glob(tmp_path):
    config = InferenceConfig(
        input_path=str(tmp_path),
        output_path=str(tmp_path / ""out""),
        model_name=""dummy"",
        model_type=""fasttext"",
        attribute_name=""test"",
    )

    with pytest.raises(FileNotFoundError):
        ray.get(run_inference.remote(config))",tests/test_classification_inference_empty_glob.py,
survived,"        async def run(self, *_: Any, **__: Any) -> Dict[str, Any]:
            return {""status"": ""error"", ""error"": ""agents SDK unavailable""}
",src/meta_agent/agents/guardrail_designer_agent.py,_Agent
survived,"def _kafka_producer() -> Optional[KafkaProducer]:
    if not _KAFKA_BROKER or KafkaProducer is None:
        return None
    try:
        return KafkaProducer(
            bootstrap_servers=_KAFKA_BROKER,
            value_serializer=lambda v: v.encode() if isinstance(v, str) else v,
        )
    except Exception:  # noqa: BLE001
        logger.exception(""Kafka producer init failed"")
        return None
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def Gauge(name: str, desc: str, labels=None):  # type: ignore[misc]
        return _get_metric(_Gauge, name, desc, labels)
",alpha_factory_v1/backend/agents/registry.py,
survived,"def list_agents(detail: bool = False):
    with _REGISTRY_LOCK:
        metas = sorted(AGENT_REGISTRY.values(), key=lambda m: m.name)
    return [m.as_dict() if detail else m.name for m in metas]
",alpha_factory_v1/backend/agents/registry.py,
survived,"def _emit_kafka(topic: str, payload: str) -> None:
    if _PRODUCER is None:
        return
    try:
        _PRODUCER.send(topic, payload)
        _PRODUCER.flush()
    except Exception:  # noqa: BLE001
        logger.exception(""Kafka emit failed (topic=%s)"", topic)
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def to_json(self) -> str:
        return json.dumps(self.as_dict(), separators=("","", "":""))
",alpha_factory_v1/backend/agents/registry.py,AgentMetadata
survived,"def _session() -> requests.Session:
    """"""Return a session with basic retry logic.""""""
    retry = Retry(total=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    s = requests.Session()
    s.mount(""https://"", adapter)
    s.mount(""http://"", adapter)
    return s
",scripts/download_hf_gpt2.py,
survived,"def _benchmark_engine(
    engine: Engine,
    routes: list[tuple[Vertex, Vertex]],
    repetitions: int,
) -> tuple[list[float], int, dict]:
    """"""Benchmark a planning engine.""""""

    times: list[float] = []
    successful = 0

    for rep in range(repetitions):
        print(f""   Repetition {rep + 1}/{repetitions}..."")
        rep_start = time.time()

        for i, (start, goal) in enumerate(routes):
            route_start = time.time()
            try:
                result = engine.plan(start=start, goal=goal)
                if result.edges:
                    successful += 1
                times.append(time.time() - route_start)
            except Exception as e:  # pragma: no cover - diagnostic
                print(f""      Route {i + 1} failed: {e}"")
                continue

        print(f""      Completed in {time.time() - rep_start:.3f}s"")

    return times, successful, engine.get_stats()
",python/examples/osm_cache_performance_test.py,
survived,"async def test_broadcast_merkle_root_devnet_e2e() -> None:
    if os.getenv(""PYTEST_NET_OFF"") == ""1"" or not await _devnet_available():
        pytest.skip(""network disabled or devnet unreachable"")
    tmp = tempfile.TemporaryDirectory()
    ledger = Ledger(os.path.join(tmp.name, ""l.db""), rpc_url=""https://api.devnet.solana.com"", broadcast=True)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    try:
        await ledger.broadcast_merkle_root()
    finally:
        await ledger.stop_merkle_task()
        ledger.close()
        tmp.cleanup()",tests/test_ledger_devnet_e2e.py,
survived,"    def test_simulate_returns_series(self) -> None:
        try:
            import numpy as np  # noqa: F401
            import pandas as pd  # noqa: F401
        except ModuleNotFoundError:
            self.skipTest(""numpy/pandas not available"")
        sim = simulation_core.MonteCarloSimulator(n_paths=3, horizon=2)
        factors = sim.simulate(
            {
                ""yield_10y"": 4.0,
                ""yield_3m"": 4.5,
                ""stable_flow"": 10.0,
                ""es_settle"": 5000.0,
            }
        )
        self.assertIsInstance(factors, pd.Series)
        self.assertEqual(len(factors), 3)
        self.assertEqual(factors.name, ""es_factor"")
",tests/test_macro_sentinel.py,TestMacroSentinel
survived,"def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(description=""Alpha-Factory launcher"")
    ap.add_argument(""--dev"", action=""store_true"", help=""Enable dev mode"")
    ap.add_argument(""--preflight"", action=""store_true"", help=""Run environment checks and exit"")
    ap.add_argument(""--port"", type=int, help=""REST API port"")
    ap.add_argument(""--metrics-port"", type=int, help=""Prometheus metrics port"")
    ap.add_argument(""--a2a-port"", type=int, help=""A2A gRPC port"")
    ap.add_argument(""--enabled"", help=""Comma separated list of enabled agents"")
    ap.add_argument(""--loglevel"", default=""INFO"", help=""Log level"")
    return ap.parse_args()
",alpha_factory_v1/run.py,
survived,"    def test_bridge_market_data_output(self) -> None:
        """"""Bridge handles CSV input and prints agent summary.""""""
        import tempfile

        with tempfile.NamedTemporaryFile(""w"", delete=False) as fh:
            fh.write(""1,2,3"")
            csv_file = fh.name

        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.openai_agents_bridge"",
                ""--episodes"",
                ""1"",
                ""--market-data"",
                csv_file,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def test_main_stops_a2a(monkeypatch) -> None:
    """"""`_A2A.stop()` should be called when the loop exits.""""""
    mod = importlib.import_module(MODULE)

    class DummySocket:
        def __init__(self) -> None:
            self.started = False
            self.stopped = False

        def start(self) -> None:
            self.started = True

        def stop(self) -> None:
            self.stopped = True

        def sendjson(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - unused
            pass

    dummy = DummySocket()

    monkeypatch.setattr(mod, ""_A2A"", dummy)
    monkeypatch.setattr(mod, ""ADKClient"", None)

    async def _llm(_: float) -> str:
        return ""ok""

    monkeypatch.setattr(mod, ""_llm_comment"", _llm)

    asyncio.run(mod.main([""--cycles"", ""1"", ""--interval"", ""0""]))

    assert dummy.started
    assert dummy.stopped
",tests/test_alpha_agi_business_3_v1.py,
survived,"    def fake_run(
        cmd: list[str],
        repo_dir: str,
        *,
        image: str | None = None,
        mounts: dict[str, str] | None = None,
    ) -> tuple[int, str]:
        if ""pytest"" in cmd:
            res = subprocess.run(
                [""pytest"", ""-q"", ""--color=no""],
                cwd=repo_dir,
                capture_output=True,
                text=True,
            )
            return res.returncode, res.stdout + res.stderr
        if ""patch"" in cmd:
            ok, out = diff_utils.apply_diff(patch, repo_dir=repo_dir)
            return (0 if ok else 1), out
        return 0, """"
",tests/test_self_healer_sandbox.py,
survived,"    def result_processor(
        self, dialect: Dialect, coltype: Any
    ) -> Callable[[list[float] | None], FloatVector | None]:
        def process(value: list[float] | None) -> FloatVector | None:
            return np.asarray(value, dtype=np.float32) if value is not None else None

        return process
",src/raglite/_typing.py,DuckDBVec
deleted,"    def dot_distance(self, other: FloatVector) -> Operators:
        """"""Compute the dot product distance.""""""
        if self._is_postgres():
            return self.op(""<#>"", return_type=Float)(other)
        if self._is_duckdb():
            return func.array_negative_inner_product(self.expr, other)
        return self.op(""<#>"", return_type=Float)(other)
",src/raglite/_typing.py,EmbeddingComparator
survived,"def run(cmd: Sequence[str], **kwargs: Any) -> None:
    """"""Run ``cmd`` and raise ``CalledProcessError`` on failure.""""""
    print(""+"", "" "".join(cmd))
    subprocess.run(cmd, check=True, **kwargs)
",scripts/publish_demo_gallery.py,
survived,"    def _get_metric(cls, name: str, desc: str):
        if name in getattr(REGISTRY, ""_names_to_collectors"", {}):
            return REGISTRY._names_to_collectors[name]
        return cls(name, desc)
",alpha_factory_v1/backend/agents/ping_agent.py,
survived,"def _subdir_url() -> str:
    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"")
    repo_path = repo_path.removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    return f""https://{org}.github.io/{repo}/alpha_factory_v1/demos/""
",scripts/open_subdir_gallery.py,
survived,"def test_regression_guard(monkeypatch) -> None:
    alerts: list[str] = []
    monkeypatch.setattr(orchestrator.alerts, ""send_alert"", lambda m: alerts.append(m))
    runner = DummyRunner()
    runners = {""aiga_evolver"": runner}

    async def drive() -> float:
        guard = asyncio.create_task(orchestrator._regression_guard(runners))
        start = time.time()
        for v in [1.0, 0.7, 0.5, 0.3]:
            metrics.dgm_best_score.set(v)
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.5)
        duration = time.time() - start
        guard.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await guard
        return duration

    dur = asyncio.run(drive())
    assert runner.task.cancelled
    assert dur < 10
    assert alerts
",tests/test_governance.py,
survived,"async def _regression_guard(runners: Dict[str, AgentRunner]) -> None:
    history: deque[float] = deque(maxlen=3)
    while True:
        await asyncio.sleep(1)
        try:
            sample = metrics.dgm_best_score.collect()[0].samples[0]
            score = float(sample.value)
        except Exception:  # pragma: no cover - metrics optional
            continue
        history.append(score)
        if (
            len(history) == 3
            and history[1] <= history[0] * 0.8
            and history[2] <= history[1] * 0.8
        ):
            runner = runners.get(""aiga_evolver"")
            if runner and runner.task:
                runner.task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await runner.task
            alerts.send_alert(""Evolution paused due to metric regression"")
            history.clear()
",alpha_factory_v1/backend/orchestrator.py,
survived,"        async def _run() -> None:
            while True:
                await asyncio.sleep(0.1)
",tests/test_governance.py,DummyRunner
survived,"def test_manual_build_missing_tsc(tmp_path: Path) -> None:
    work = tmp_path / ""browser""
    shutil.copytree(BROWSER_DIR, work)
    # provide required .env
    (work / "".env"").write_text((BROWSER_DIR / "".env.sample"").read_text())
    (work / ""build"" / ""__init__.py"").touch()

    # scrub placeholder text to avoid asset download
    for sub in (""wasm"", ""wasm_llm""):
        d = work / sub
        if d.exists():
            for p in d.rglob(""*""):
                if p.is_file():
                    data = p.read_bytes().replace(b""placeholder"", b"""")
                    p.write_bytes(data)
    bundle = work / ""lib"" / ""bundle.esm.min.js""
    bundle.write_text(bundle.read_text().replace(""Placeholder"", """"))

    # isolate PATH with node only
    bin_dir = tmp_path / ""bin""
    bin_dir.mkdir()
    node = shutil.which(""node"")
    assert node
    os.symlink(node, bin_dir / ""node"")

    env = os.environ.copy()
    env[""PATH""] = str(bin_dir)

    result = subprocess.run(
        [sys.executable, ""manual_build.py""],
        cwd=work,
        capture_output=True,
        text=True,
        env=env,
    )
    assert result.returncode != 0
    assert ""TypeScript compiler not found"" in result.stderr",tests/test_manual_build_missing_tsc.py,
survived,"async def hb_watch(runners: Dict[str, AgentRunner]) -> None:
    while True:
        now = time.time()
        for n, r in runners.items():
            alive = int(now - r.last_beat < r.period * 3.0)
            MET_UP.labels(n).set(alive)
        await asyncio.sleep(5)
",alpha_factory_v1/backend/agent_runner.py,
survived,"    def publish(self, topic: str, msg: Dict[str, Any]) -> None:
        if self._producer:
            self._producer.send(topic, msg)
        else:
            assert self._queues is not None
            self._queues.setdefault(topic, asyncio.Queue()).put_nowait(msg)
",alpha_factory_v1/backend/agent_runner.py,EventBus
survived,"def utc_now() -> str:
    return datetime.now(timezone.utc).isoformat(timespec=""milliseconds"")
",alpha_factory_v1/backend/agent_runner.py,
survived,"    def _calc_next(self) -> None:
        now = time.time()
        if self.spec:
            with contextlib.suppress(ModuleNotFoundError, ValueError):
                from croniter import croniter  # type: ignore

                self.next_ts = croniter(self.spec, datetime.fromtimestamp(now)).get_next(float)
                return
        self.next_ts = now + self.period
",alpha_factory_v1/backend/agent_runner.py,AgentRunner
survived,"                    def __init__(self) -> None:
                        self.nodes: set[str] = set()
                        self._edges: list[tuple[str, str, str, dict[str, Any]]] = []
",alpha_factory_v1/backend/memory_graph.py,GraphMemory._Stub
survived,"    def model_dump(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:
        """"""Return model data as a dictionary across Pydantic versions.""""""
        base = getattr(super(), ""model_dump"", None)
        if callable(base):
            return base(*args, **kwargs)
        return self.dict(*args, **kwargs)
",src/meta_agent/models/spec_schema.py,SpecSchema
survived,"        def model_dump(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:
            ...
",src/meta_agent/models/spec_schema.py,_ModelDumpProtocol
survived,"async def lifespan(app: EnrichMCP) -> AsyncIterator[dict[str, Any]]:
    async with httpx.AsyncClient(base_url=BACKEND_URL) as client:
        yield {""client"": client}
",examples/shop_api_gateway/app.py,
survived,"def format2(f):
    s = str(f)
    idx = indexOf(s, ""."")
    if idx < 0:
        s = s + "".00""
    else:
        need = idx + 3
        if len(s) > need:
            s = s[0:need]
        else:
            while len(s) < need:
                s = s + ""0""
    return s
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def main():
    n = True
    print((""true"" if n else ""false""))
    print(""bool"")
    n = not n
    print((""true"" if n else ""false""))
    x = 5
    y = 8
    print(""x == y:"", (1 if x == y else 0))
    print(""x < y:"", (1 if x < y else 0))
    print(""\nConvert String into Boolean Data type\n"")
    str1 = ""japan""
    print(""Before :"", ""string"")
    bolStr = parseBool(str1)
    print(""After :"", ""bool"")
",tests/rosetta/transpiler/Python/boolean-values.py,
survived,"def stringSearch(h, n):
    result = []
    start = 0
    hlen = len(h)
    nlen = len(n)
    while start < hlen:
        idx = indexOfStr(h[start:hlen], n)
        if idx >= 0:
            result = result + [start + idx]
            start = start + idx + nlen
        else:
            break
    return result
",tests/rosetta/transpiler/Python/boyer-moore-string-search.py,
survived,"def bigrat(a, b):
    return (Fraction(a)) // (Fraction(b))
",tests/rosetta/transpiler/Python/calkin-wilf-sequence.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"def main():
    go1 = ""hello C""
    c2 = strdup(go1)
    print(c2)
",tests/rosetta/transpiler/Python/call-a-foreign-language-function.py,
survived,"def partialSum(x):
    return lambda y: mysum(x, y)
",tests/rosetta/transpiler/Python/call-a-function-12.py,
survived,"def shiftRune(r, k):
    if r >= ""a"" and r <= ""z"":
        return chr(((ord(r) - 97 + k) % 26) + 97)
    if r >= ""A"" and r <= ""Z"":
        return chr(((ord(r) - 65 + k) % 26) + 65)
    return r
",tests/rosetta/transpiler/Python/caesar-cipher-1.py,
survived,"    def New():
        global sn
        sn = sn + 1
        b = Box(secret=sn)
        if sn == 1:
            b = dataclasses.replace(b, Contents=""rabbit"")
        else:
            if sn == 2:
                b = dataclasses.replace(b, Contents=""rock"")
        return b
",tests/rosetta/transpiler/Python/call-an-object-method-2.py,
survived,"def primesUpTo(n):
    sieve = []
    i = 0
    while i <= n:
        sieve = sieve + [True]
        i = i + 1
    p = 2
    while p * p <= n:
        if sieve[p]:
            m = p * p
            while m <= n:
                sieve[m] = False
                m = m + p
        p = p + 1
    res = []
    x = 2
    while x <= n:
        if sieve[x]:
            res = res + [x]
        x = x + 1
    return res
",tests/rosetta/transpiler/Python/brilliant-numbers.py,
survived,"def ord(ch):
    upper = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""
    lower = ""abcdefghijklmnopqrstuvwxyz""
    idx = indexOf(upper, ch)
    if idx >= 0:
        return 65 + idx
    idx = indexOf(lower, ch)
    if idx >= 0:
        return 97 + idx
    return 0
",tests/rosetta/transpiler/Python/caesar-cipher-2.py,
survived,"def fields(s):
    words = []
    cur = """"
    i = 0
    while i < len(s):
        ch = s[i:i + 1]
        if ch == "" "" or ch == ""\t"" or ch == ""\n"":
            if len(cur) > 0:
                words = words + [cur]
                cur = """"
        else:
            cur = cur + ch
        i = i + 1
    if len(cur) > 0:
        words = words + [cur]
    return words
",tests/rosetta/transpiler/Python/bulls-and-cows-player.py,
survived,"def newFactory():
    sn = 0
    def New():
        global sn
        sn = sn + 1
        b = Box(secret=sn)
        if sn == 1:
            b = dataclasses.replace(b, Contents=""rabbit"")
        else:
            if sn == 2:
                b = dataclasses.replace(b, Contents=""rock"")
        return b
    def Count():
        return sn
    return [New, Count]
",tests/rosetta/transpiler/Python/call-an-object-method-2.py,
survived,"    def Count():
        return sn
",tests/rosetta/transpiler/Python/call-an-object-method-2.py,
survived,"def new(graphdb_filename, overwrite):
    """"""Create a new empty graph database.""""""
    if not os.path.exists(graphdb_filename) or overwrite:
        click.echo(f""Creating graph database '{graphdb_filename}'"")
        GraphDatabase(graphdb_filename, overwrite=overwrite)
    else:
        click.echo(
            f""Graph database '{graphdb_filename}' already exists. Use -o to overwrite""
        )
",pygs/graphserver/cli.py,
survived,"def test_tool_roundtrip(monkeypatch, tmp_path: Path) -> None:
    monkeypatch.setattr(manager, ""REPO_ROOT"", tmp_path)
    monkeypatch.setattr(""src.self_edit.tools.REPO_ROOT"", tmp_path)
    assert _tool_roundtrip()
",tests/test_archive_policy.py,
survived,"    def __init__(self, db_path: str | Path) -> None:
        self.db = ArchiveDB(db_path)
",src/archive/manager.py,PatchManager
survived,"    def test_closes_on_channel_ready_timeout(self) -> None:
        channel = mock.Mock()
        channel.channel_ready = mock.AsyncMock(side_effect=asyncio.TimeoutError)
        channel.close = mock.AsyncMock()

        async def run() -> None:
            fake_workloadapi = types.SimpleNamespace(X509Source=object, WorkloadApiClient=object)
            with (
                mock.patch(""grpc.aio.insecure_channel"", return_value=channel),
                mock.patch.dict(sys.modules, {""workloadapi"": fake_workloadapi}),
            ):
                with self.assertRaises(asyncio.TimeoutError):
                    await _GrpcTransport.new(""svc:1"", spiffe_id=None, timeout=1.0)

        with mock.patch.dict(os.environ, {""A2A_INSECURE"": ""1""}, clear=False):
            asyncio.run(run())
        channel.close.assert_awaited_once()",tests/test_grpc_transport_timeout.py,TestGrpcTransport
survived,"def set_notification_settings(settings):
    set_global_setting('notification_settings', settings)",users_db.py,
survived,"def get_global_setting(key, default=None):
    conn = get_db()
    c = conn.cursor()
    c.execute('SELECT value FROM global_settings WHERE key=?', (key,))
    row = c.fetchone()
    conn.close()
    if row:
        try:
            return json.loads(row['value'])
        except Exception:
            return row['value']
    return default
",users_db.py,
survived,"def problem_response(exc: HTTPException) -> JSONResponse:
    """"""Return an RFC 7807 compliant response for ``exc``.""""""

    try:
        title = HTTPStatus(exc.status_code).phrase
    except Exception:  # pragma: no cover - unknown status code
        title = str(exc.status_code)

    detail = (
        exc.detail if isinstance(exc.detail, str) else str(exc.detail) if exc.detail else """"
    )

    body: dict[str, Any] = {""type"": ""about:blank"", ""title"": title, ""status"": exc.status_code}
    if detail:
        body[""detail""] = detail

    return JSONResponse(status_code=exc.status_code, content=body)
",src/interface/problem_json.py,
survived,"def main():
    global seed
    print(eqIndices([-7, 1, 5, 2, -4, 3, 0]))
    verylong = []
    i = 0
    while i < 10000:
        seed = (seed * 1664525 + 1013904223) % 2147483647
        verylong = verylong + [seed % 1001 - 500]
        i = i + 1
    print(eqIndices(verylong))
",tests/rosetta/transpiler/Python/equilibrium-index.py,
survived,"def powf(base, exp):
    r = 1.0
    i = 0
    while i < exp:
        r = r * base
        i = i + 1
    sys.exit(r)
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def listString(xs):
    s = ""[""
    i = 0
    while i < len(xs):
        s = s + str(xs[i])
        if i < len(xs) - 1:
            s = s + "" ""
        i = i + 1
    s = s + ""]""
    return s
",tests/rosetta/transpiler/Python/executable-library.py,
survived,"def absBig(x):
    if x < zero:
        sys.exit(zero - x)
    sys.exit(x)
",tests/rosetta/transpiler/Python/euclid-mullin-sequence.py,
survived,"def parseBigInt(str):
    i = 0
    neg = False
    if len(str) > 0 and str[0:1] == ""-"":
        neg = True
        i = 1
    n = 0
    while i < len(str):
        ch = str[i:i + 1]
        d = int(ch)
        n = n * (10) + (d)
        i = i + 1
    if neg:
        n = -n
    sys.exit(n)
",tests/rosetta/transpiler/Python/even-or-odd.py,
survived,"def padInt(f):
    s = str((int(f)))
    if f >= 0:
        return "" "" + s
    return s
",tests/rosetta/transpiler/Python/exponentiation-with-infix-operators-in-or-operating-on-the-base.py,
survived,"def commatize(n):
    s = str(n)
    i = len(s) - 3
    while i >= 1:
        s = """".join(s[0:i]) + "","" + """".join(s[i:len(s)])
        i = i - 3
    sys.exit(s)
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def perimEqual(p1, p2):
    if len(p1) != len(p2):
        return False
    for v in p1:
        if not v in p2:
            return False
    c = copyInts(p1)
    r = 0
    while r < 2:
        i = 0
        while i < len(p1):
            if sliceEqual(c, p2):
                return True
            t = c[len(c) - 1]
            j = len(c) - 1
            while j > 0:
                c[j] = c[j - 1]
                j = j - 1
            c[0] = t
            i = i + 1
        reverse(c)
        r = r + 1
    return False
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,
survived,"def joinInts(xs):
    s = """"
    i = 0
    while i < len(xs):
        if i > 0:
            s = s + "" ""
        s = s + str(xs[i])
        i = i + 1
    return s
",tests/rosetta/transpiler/Python/extensible-prime-generator.py,
survived,"def reverse(xs):
    i = 0
    j = len(xs) - 1
    while i < j:
        t = xs[i]
        xs[i] = xs[j]
        xs[j] = t
        i = i + 1
        j = j - 1
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,
survived,"def harmonic(n):
    sum = 0.0
    i = 1
    while i <= n:
        sum = sum + 1.0 / (float(i))
        i = i + 1
    sys.exit(sum)
",tests/rosetta/transpiler/Python/eulers-constant-0.5772....py,
survived,"def endsWith(s, suf):
    if len(s) < len(suf):
        sys.exit(False)
    sys.exit(s[len(s) - len(suf):len(s)] == suf)
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fermat-numbers.py,
survived,"def fib(a):
    if a < 2:
        return a
    return fib(a - 1) + fib(a - 2)
",tests/rosetta/transpiler/Python/fibonacci-sequence-1.py,
survived,"def interpret(ruleset, input):
    p = parseRules(ruleset)
    if not p.get(""ok""):
        sys.exit({""ok"": False, ""out"": """"})
    out = runRules(p.get(""rules""), input)
    sys.exit({""ok"": True, ""out"": out})
",tests/rosetta/transpiler/Python/execute-a-markov-algorithm.py,
survived,"def isEsthetic(n, b):
    if n == 0:
        sys.exit(False)
    i = n % b
    n = n // b
    while n > 0:
        j = n % b
        if uabs(i, j) != 1:
            sys.exit(False)
        n = n // b
        i = j
    sys.exit(True)
",tests/rosetta/transpiler/Python/esthetic-numbers.py,
survived,"def sliceEqual(a, b):
    i = 0
    while i < len(a):
        if a[i] != b[i]:
            return False
        i = i + 1
    return True
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/file-input-output-1.py,
survived,"async def seed(session: AsyncSession) -> None:
    user = User(id=1, name=""Alice"")
    orders = [Order(id=i, user=user) for i in range(1, 3)]
    session.add_all([user, *orders])
",tests/test_sqlalchemy_autogen_extra.py,
survived,"    def _tool(*_a, **_k):
        def _decorator(func):
            return func

        return _decorator
",tests/test_agents_fallback.py,
survived,"        def _decorator(func):
            return func
",tests/test_agents_fallback.py,
survived,"    def test_history_after_evolve(self) -> None:
        """"""history() should return evolver history after evolve()""""""

        stub = types.ModuleType(""openai_agents"")
        stub.Agent = object
        stub.AgentRuntime = MagicMock()
        stub.OpenAIAgent = MagicMock()

        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator

        stub.Tool = _tool

        env_stub = types.ModuleType(""curriculum_env"")
        env_stub.CurriculumEnv = object

        evo_stub = types.ModuleType(""meta_evolver"")

        class DummyEvolver:
            def __init__(self, *a, **k) -> None:  # pragma: no cover - test stub
                pass

            def run_generations(self, *_a) -> None:  # pragma: no cover - test stub
                pass

            history = [(0, 0.0)]

        evo_stub.MetaEvolver = DummyEvolver

        adk_stub = types.ModuleType(""adk_bridge"")
        adk_stub.auto_register = lambda *_a, **_k: None
        adk_stub.maybe_launch = lambda *_a, **_k: None
        backend_stub = types.ModuleType(""backend"")
        backend_stub.adk_bridge = adk_stub

        with patch.dict(
            sys.modules,
            {
                ""openai_agents"": stub,
                ""alpha_factory_v1.demos.aiga_meta_evolution.curriculum_env"": env_stub,
                ""alpha_factory_v1.demos.aiga_meta_evolution.meta_evolver"": evo_stub,
                ""alpha_factory_v1.backend"": backend_stub,
                ""alpha_factory_v1.backend.adk_bridge"": adk_stub,
            },
        ):
            sys.modules.pop(
                ""alpha_factory_v1.demos.aiga_meta_evolution.utils"", None
            )
            sys.modules.pop(
                ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"",
                None,
            )
            mod = importlib.import_module(
                ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge""
            )

            dummy = MagicMock()
            dummy.history = [(1, 0.5)]
            dummy.latest_log.return_value = ""done""

            with patch.object(mod, ""EVOLVER"", dummy):
                asyncio.run(mod.evolve(1))
                result = asyncio.run(mod.history())
                self.assertEqual(result, {""history"": dummy.history})
                asyncio.run(mod.checkpoint())
                asyncio.run(mod.reset())
                agent = mod.EvolverAgent()
                asyncio.run(agent.policy({""gens"": 1}, None))
                self.assertIn(mod.history, mod.EvolverAgent.tools)

                runpy.run_module(
                    ""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"",
                    run_name=""__main__"",
                )
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime
survived,"    async def stop_merkle_task(self) -> None:  # pragma: no cover - stub
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,DummyLedger
survived,"def softmax(arr: np.ndarray) -> np.ndarray:
    exp = np.exp(arr - np.max(arr))
    return exp / exp.sum()
",tests/test_selector_v2.py,
survived,"    def build(self, inference: InferenceEndpoint, rollout_sink: RolloutSink, seed: int):
        ActorCls = ray.remote(num_cpus=1)(MathEnv)
        actor = ActorCls.remote(
            inference,
            rollout_sink,
            data_source=self.data_source,
            split=self.split,
            model=self.model,
            max_iters=self.max_iters,
            seed=self.seed + seed,
        )
        actor.run.remote()
        return actor",marin/rl/envs/math_env.py,MathEnvConfig
survived,"def test_parquet_round_trip(tmp_path):
    groups = _make_sample_groups()

    # Write groups twice to verify appending additional parts is okay.
    write_rollout_groups(groups[:1], str(tmp_path))
    write_rollout_groups(groups[1:], str(tmp_path))

    read_back = list(iter_rollout_groups(str(tmp_path)))

    assert len(read_back) == len(groups)

    original_sorted = _sort_by_id(groups)
    read_sorted = _sort_by_id(read_back)

    for orig, rec in zip(original_sorted, read_sorted, strict=False):
        assert _groups_equal(orig, rec)",tests/rl/test_parquet_store.py,
survived,"    def _fake_load_dataset(name, *_, **__):
        assert name == ""mock""
        return fake_dataset
",tests/rl/test_math_env.py,
survived,"def detect_supply_chain_alpha(threshold: float = 50.0) -> str:
    """"""Return a basic message about supplyâ€‘chain flow levels.""""""
    try:
        data = pd.read_csv(_STABLE_FLOWS_CSV)
    except FileNotFoundError:
        return ""offline data missing""

    flow = float(data[""usd_mn""][0])
    if flow < threshold:
        return f""Flows {flow:.1f}â€¯MÂ USD â€“ potential bottleneck""
    return f""Flows {flow:.1f}â€¯MÂ USD â€“ supply chain normal""
",alpha_factory_v1/demos/era_of_experience/alpha_detection.py,
survived,"def _read_first_row(path: Path) -> Dict[str, str] | None:
    """"""Return the first row of a CSV as a mapping or ``None`` if empty.""""""
    if pd is not None:  # use pandas when available for convenience
        try:
            df = pd.read_csv(path)
        except FileNotFoundError:
            raise
        except Exception:  # pragma: no cover - handle corrupt CSV gracefully
            pass
        else:
            if not df.empty:
                return df.iloc[0].to_dict()

    try:
        with open(path, newline="""") as fh:
            reader = csv.DictReader(fh)
            return next(reader, None)
    except FileNotFoundError:
        raise

    return None
",alpha_factory_v1/demos/era_of_experience/alpha_detection.py,
survived,"    def test_simple_env_runs(self) -> None:
        env = SimpleExperienceEnv()
        state = env.reset()
        self.assertEqual(state, 0)
        state, reward, done, info = env.step(""act"")
        self.assertIsInstance(reward, float)
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def test_stub_agents_instantiable(self) -> None:
        exp = ExperienceAgent()
        fed = FederatedExperienceAgent()
        self.assertTrue(hasattr(exp, ""act""))
        self.assertTrue(hasattr(fed, ""handle_request""))
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def dlt_source(self, uri: str, table: str, **kwargs):
        parsed_uri = urlparse(uri)
        params = parse_qs(parsed_uri.query)
        token = params.get(""token"")
        if not token or not token[0].strip():
            raise MissingValueError(""token"", ""Internet Society Pulse"")

        start_date = kwargs.get(""interval_start"")
        if start_date is None:
            raise MissingValueError(""interval_start"", ""Internet Society Pulse"")

        end_date = kwargs.get(""interval_end"")

        metrics = [table]
        if table == ""all"":
            from ingestr.src.pulse import GLOBAL_METRICS

            metrics = list(GLOBAL_METRICS.keys())

        from ingestr.src.pulse import GLOBAL_METRICS, pulse_source

        for metric in metrics:
            if metric not in GLOBAL_METRICS:
                raise UnsupportedResourceError(metric, ""Internet Society Pulse"")

        src = pulse_source(
            token=token[0],
            start_date=str(start_date),
            end_date=str(end_date) if end_date else None,
            metrics=metrics,
        )
        return src.with_resources(*metrics)",ingestr/src/sources.py,PulseSource
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/best-shuffle.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-b-zier-curves-cubic.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitmap-histogram.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bin-given-limits.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/biorhythms.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-5.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-7.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-3.py,
survived,"def test_queue_never_exceeds_cap() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""navigator.sendBeacon=()=>false;""
            ""Object.defineProperty(navigator,'onLine',{get:()=>false,configurable:true});""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        for _ in range(105):
            page.click(""text=Share"")
            page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert (
            page.evaluate(
                ""JSON.parse(localStorage.getItem('telemetryQueue')).length""
            )
            == 100
        )
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,
survived,"def build_tree(df: pd.DataFrame) -> Figure:
    """"""Return Plotly treemap figure for lineage.""""""
    if df.empty:
        return px.treemap()

    ids = df[""id""].astype(str)
    parents = df[""parent""].fillna("""").astype(str)
    fig = px.treemap(
        df,
        ids=ids,
        parents=parents,
        values=[1] * len(df),
        color=""score"",
        custom_data=[df[""patch""].fillna("""")],
        color_continuous_scale=""Blues"",
    )
    labels = [f""<a href='{p}'>{i}</a>"" if p else str(i) for i, p in zip(ids, df[""patch""].fillna(""""))]
    fig.data[0].text = labels
    fig.data[0].hovertemplate = (
        ""score=%{color}<br>patch=%{customdata[0]}<extra></extra>""
    )
    return fig
",src/interface/lineage_dashboard.py,
survived,"def main(argv: list[str] | None = None) -> None:  # pragma: no cover - entry point
    """"""Launch the lineage dashboard.""""""
    if st is None:
        print(""Streamlit not installed"")
        return

    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        ""--db"",
        default=os.getenv(""ARCHIVE_PATH"", ""archive.db""),
        help=""Path to archive database"",
    )
    parser.add_argument(
        ""--refresh"",
        type=int,
        default=int(os.getenv(""DASH_REFRESH"", ""10"")),
        help=""Auto-refresh interval in seconds"",
    )
    args = parser.parse_args(argv)

    st.set_page_config(page_title=""Lineage Dashboard"", layout=""wide"")
    if st_autorefresh is not None:
        st_autorefresh(interval=args.refresh * 1000, key=""refresh"")

    df = load_df(Path(args.db))
    if df.empty:
        st.info(""Archive empty"")
        return

    fig = build_tree(df)
    st.plotly_chart(fig, use_container_width=True)
",src/interface/lineage_dashboard.py,
survived,"    def _validate_patch(self, patch: str) -> bool:
        """"""Apply ``patch`` in a temporary clone and run quality checks.""""""

        from pathlib import Path
        import shutil
        import subprocess
        import tempfile

        try:
            root = (
                Path(
                    subprocess.check_output([""git"", ""rev-parse"", ""--show-toplevel""], text=True).strip()
                )
            )
        except subprocess.CalledProcessError:
            return False

        with tempfile.TemporaryDirectory() as tmp:
            try:
                subprocess.run(
                    [""git"", ""clone"", ""--local"", str(root), tmp],
                    check=True,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                )
                apply = subprocess.run(
                    [""git"", ""apply"", ""-""],
                    input=patch.encode(),
                    cwd=tmp,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                )
                if apply.returncode != 0:
                    return False

                checks = [
                    [""pytest"", ""-q""],
                    [""ruff"", "".""],
                    [""bandit"", ""-q"", ""-r"", "".""],
                ]
                for cmd in checks:
                    if shutil.which(cmd[0]) is None:
                        continue
                    proc = subprocess.run(
                        cmd,
                        cwd=tmp,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                    )
                    if proc.returncode != 0:
                        return False
            except Exception:
                return False

        return True
",src/simulation/mats_ops.py,SelfRewriteOperator
survived,"    async def step(self) -> None:
        # Pretend to compute an optimal plan
        await self.publish(""alpha.plan"", {""plan"": ""explore_market""})
",alpha_factory_v1/demos/alpha_agi_business_2_v1/alpha_agi_business_2_v1.py,PlanningAgent
survived,"def test_list_profiles_returns_all_names(tmp_path):
    data = """"""\
[default]
username = 'a'
password = 'b'

[second]
username = 'c'
password = 'd'
""""""
    cred_file = tmp_path / ""credentials""
    cred_file.write_text(data, encoding=""UTF-8"")

    profiles = CredentialsProvider.list_profiles(path=str(cred_file))

    assert set(profiles) == {""default"", ""second""}
",tests/dhapi/port/test_credentials_provider.py,
survived,"def test_list_profiles_raise_when_file_missing(tmp_path):
    missing = tmp_path / ""none""
    with pytest.raises(FileNotFoundError):
        CredentialsProvider.list_profiles(path=str(missing))",tests/dhapi/port/test_credentials_provider.py,
survived,"def triple(x):
    return x * 3
",tests/transpiler/x/py/pure_fold.py,
survived,"def add(a, b):
    return a + b
",tests/transpiler/x/py/partial_application.py,
survived,"        def execute_endpoint(payload: Dict[str, Any]) -> Any:
            input_obj = cls.InputSchema(**payload)
            instance = cls()
            result = instance.execute(input_obj.dict())
            return OutputModel(**result)
",servers/server_clear_thought/core/base_tool.py,BaseTool
survived,"def test_seven_seekers_orchestrator():
    client = get_client()
    resp = client.post(
        ""/seven-seekers-orchestrator/execute"",
        json={""query"": ""q""},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""seeker_results"", ""resonance_map"", ""synthesis""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"def create_app() -> FastAPI:
    app = FastAPI()
    for tool_cls in load_tools():
        router = tool_cls.get_router()
        app.include_router(router, prefix=tool_cls.endpoint_path, tags=[tool_cls.slug])
    return app",servers/server_clear_thought/app.py,
survived,"def test_safe_struggle_designer():
    client = get_client()
    resp = client.post(
        ""/safe-struggle-designer/execute"",
        json={""skill"": ""x"", ""current_level"": 1, ""target_level"": 2},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""scaffold_steps"", ""safety_measures"", ""review_intervals""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        points = [
            {""category"": c or ""general"", ""count"": i}
            for i, c in enumerate(payload.get(""categories"") or [""general""], start=1)
        ]
        score = sum(p[""count""] for p in points) / len(points)
        return {
            ""drag_points"": points,
            ""summary_score"": round(score, 2),
        }",servers/server_clear_thought/tools/drag_point_audit.py,DragPointAudit
survived,"def test_meme_disabled() -> None:
    rng = random.Random(0)
    op = SelfRewriteOperator(steps=3, rng=rng, templates=[""meme""], reuse_rate=0.0)
    result = op(""improve quick test"")
    assert result != ""meme""
",tests/test_meme_reuse.py,
survived,"def run_orchestrator(verbose: bool) -> None:
    """"""Run the orchestrator until interrupted.""""""
    orch = orchestrator.Orchestrator()
    if verbose and console is not None:
        console.log(""Starting orchestrator â€¦ press Ctrl+C to stop"")
    try:
        asyncio.run(orch.run_forever())
    except KeyboardInterrupt:  # pragma: no cover - interactive
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def record(
        self, tokens: int, cost: float, latency: float, guardrail_hits: int
    ) -> None:
        cur = self.conn.cursor()
        cur.execute(
            ""INSERT INTO telemetry (timestamp, tokens, cost, latency, guardrail_hits) VALUES (?, ?, ?, ?, ?)"",
            (datetime.utcnow().isoformat(), tokens, cost, latency, guardrail_hits),
        )
        self.conn.commit()
        self.purge_old()
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"            async def improve_policy(policy: list[int]) -> list[int]:
                return [p + 1 for p in policy]
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/meta_rewrite.py,
survived,"    def test_run_demo_with_seed(self) -> None:
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--seed"",
                ""123"",
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"def _load_exfil_patterns() -> list[str]:
    policy_path = _POLICY_DIR / ""deny_exfil.rego""
    try:
        text = policy_path.read_text(encoding=""utf-8"")
    except FileNotFoundError:
        return []
    return re.findall(r're_match\(""([^""]+)"",\s*input.text\)', text)
",src/utils/opa_policy.py,
survived,"    def close(self) -> None:
        if self.conn:
            self.conn.close()
            self.conn = None
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"def _merkle_root(hashes: Iterable[str]) -> str:
    nodes: List[bytes] = [bytes.fromhex(h) for h in hashes]
    if not nodes:
        return cast(str, blake3(b""\x00"").hexdigest())

    while len(nodes) > 1:
        if len(nodes) % 2 == 1:
            nodes.append(nodes[-1])
        next_lvl: List[bytes] = []
        for i in range(0, len(nodes), 2):
            next_lvl.append(blake3(nodes[i] + nodes[i + 1]).digest())
        nodes = next_lvl
    return nodes[0].hex()
",alpha_factory_v1/common/utils/logging.py,
survived,"    def __init__(
        self,
        path: str,
        rpc_url: str | None = None,
        wallet: str | None = None,
        broadcast: bool = True,
        db: str | None = None,
    ) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        db_type = db or os.getenv(""AGI_INSIGHT_DB"", ""sqlite"")
        self.db_type = db_type
        if db_type == ""duckdb"" and duckdb is not None:
            self.conn = duckdb.connect(str(self.path))
            self.conn.execute(
                """"""
                CREATE TABLE IF NOT EXISTS messages (
                    id BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
                    ts DOUBLE,
                    sender TEXT,
                    recipient TEXT,
                    payload TEXT,
                    hash TEXT
                )
                """"""
            )
        elif db_type == ""postgres"":
            if ""psycopg2"" not in globals():
                _log.warning(""AGI_INSIGHT_DB=postgres but psycopg2 not installed â€“ falling back to sqlite"")
                self.conn = sqlite3.connect(str(self.path))
                self.conn.execute(
                    """"""
                    CREATE TABLE IF NOT EXISTS messages (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        ts REAL,
                        sender TEXT,
                        recipient TEXT,
                        payload TEXT,
                        hash TEXT
                    )
                    """"""
                )
            else:
                params = {
                    ""host"": os.getenv(""PGHOST""),
                    ""port"": os.getenv(""PGPORT"", ""5432""),
                    ""user"": os.getenv(""PGUSER""),
                    ""password"": os.getenv(""PGPASSWORD""),
                    ""dbname"": os.getenv(""PGDATABASE"", ""insight""),
                }
                self.conn = psycopg2.connect(**{k: v for k, v in params.items() if v is not None})
                with self.conn, self.conn.cursor() as cur:
                    cur.execute(
                        """"""
                        CREATE TABLE IF NOT EXISTS messages (
                            id BIGSERIAL PRIMARY KEY,
                            ts DOUBLE PRECISION,
                            sender TEXT,
                            recipient TEXT,
                            payload TEXT,
                            hash TEXT
                        )
                        """"""
                    )
        else:
            if db_type == ""duckdb"" and duckdb is None:
                _log.warning(""AGI_INSIGHT_DB=duckdb but duckdb not installed â€“ falling back to sqlite"")
            self.conn = sqlite3.connect(str(self.path))
            self.conn.execute(
                """"""
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    ts REAL,
                    sender TEXT,
                    recipient TEXT,
                    payload TEXT,
                    hash TEXT
                )
                """"""
            )
        self.conn.commit()
        self._task: asyncio.Task[None] | None = None
        self.rpc_url = rpc_url
        self.wallet = wallet
        self.broadcast = broadcast
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"    def grpc_server(self) -> Optional[Any]:
        return self._grpc_server
",alpha_factory_v1/backend/services/api_server_service.py,APIServer
survived,"    async def fake_start_servers(*a, **k):
        events.append(""start"")

        async def sleeper():
            await asyncio.sleep(0)
        task = asyncio.create_task(sleeper())
        server = SimpleNamespace(stop=lambda code=0: events.append(""stop""))
        return task, server
",tests/test_api_server_service.py,
survived,"def test_demo_assets_revision_pinned() -> None:
    expected = ""90fe9b623b3a0ae5475cf4fa8693d43cb5ba9ac5""
    with open(RUN_SCRIPT) as f:
        text = f.read()
    m = re.search(r""DEMO_ASSETS_REV=\$\{DEMO_ASSETS_REV:-([0-9a-f]{40})\}"", text)
    assert m, ""revision variable missing""
    assert m.group(1) == expected

    from alpha_factory_v1.demos.macro_sentinel import data_feeds

    assert data_feeds.DEMO_ASSETS_REV == expected
    for url in data_feeds.OFFLINE_URLS.values():
        assert expected in url",tests/test_macro_launcher.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q15.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q4.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q10.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q9.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q13.py,_Group
survived,"        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)
",tests/test_cross_industry_bridge_runtime.py,TestCrossIndustryBridgeRuntime
survived,"    def test_no_log_flag(self) -> None:
        with tempfile.TemporaryDirectory() as tmp:
            ledger = Path(tmp) / ""no_log.json""
            result = subprocess.run(
                [
                    sys.executable,
                    STUB,
                    ""-n"",
                    ""1"",
                    ""--seed"",
                    ""4"",
                    ""--ledger"",
                    str(ledger),
                    ""--no-log"",
                    ""--model"",
                    ""gpt-4o-mini"",
                ],
                capture_output=True,
                text=True,
            )
            self.assertEqual(result.returncode, 0, result.stderr)
            self.assertFalse(ledger.exists())
",tests/test_cross_alpha_discovery.py,TestCrossAlphaDiscoveryStub
survived,"    def tearDown(self):
        AGENT_REGISTRY.clear()
        AGENT_REGISTRY.update(self._registry_backup)
        CAPABILITY_GRAPH.clear()
        for cap, agents in self._cap_backup.items():
            for name in agents:
                CAPABILITY_GRAPH.add(cap, name)
",tests/test_agents_registry.py,TestAgentRegistryFunctions
survived,"    def setUp(self):
        self._registry_backup = AGENT_REGISTRY.copy()
        self._cap_backup = {k: v[:] for k, v in CAPABILITY_GRAPH.items()}
        AGENT_REGISTRY.clear()
        CAPABILITY_GRAPH.clear()
",tests/test_agents_registry.py,TestAgentRegistryFunctions
survived,"    def tearDown(self):
        AGENT_REGISTRY.clear()
        AGENT_REGISTRY.update(self._registry_backup)
",tests/test_agents_registry.py,TestRegisterDecorator
survived,"    def test_all_agents_instantiable(self):
        for name in list_agents():
            meta = AGENT_REGISTRY[name]
            self.assertIsNotNone(meta.cls)
            # instantiation may fail if optional deps are missing
            try:
                agent = get_agent(name)
            except Exception:
                continue
            self.assertEqual(agent.NAME, name)
",tests/test_agents_integrity.py,TestAgentsIntegrity
survived,"    async def publish(self, topic, msg):
        self.published.append((topic, msg))
",tests/test_ping_agent.py,DummyOrch
survived,"def test_run_with_research(monkeypatch):
    calls = []

    class DummyResearch:
        def research(self, name: str, purpose: str):
            calls.append((name, purpose))
            return [""info""]

    agent = ToolDesignerAgent(research_manager=DummyResearch(), enable_research=True)
    result = await agent.run(VALID_DICT_SPEC)
    assert result['status'] == 'success'
    assert calls == [(VALID_DICT_SPEC['name'], VALID_DICT_SPEC['purpose'])]
",tests/agents/test_tool_designer_agent.py,
survived,"def test_formulate_query():
    mgr = ToolResearchManager(web_search_tool=DummyTool(), enabled=True)
    q = mgr.formulate_query(""foo"", ""does bar"")
    assert ""foo"" in q and ""bar"" in q
",tests/unit/test_tool_research_manager.py,
survived,"    def __call__(self, query: str):
        self.calls.append(query)
        return ""result line 1\nresult line 2""
",tests/unit/test_tool_research_manager.py,DummyTool
survived,"    def __init__(
        self,
        web_search_tool: Optional[Callable[[str], str]] = None,
        enabled: bool = True,
        max_results: int = 3,
    ) -> None:
        self.web_search_tool = web_search_tool or WebSearchTool()
        self.enabled = enabled
        self.max_results = max_results
        self.cache: Dict[str, List[str]] = {}
",src/meta_agent/research_manager.py,ToolResearchManager
survived,"    def test_openai_failure_falls_back(self) -> None:
        os.environ[""OPENAI_API_KEY""] = ""sk-test""
        llm._OPENAI_KEY = ""sk-test""
        llm._sync_embed.cache_clear()
        with patch.object(llm.openai.Embedding, ""create"", side_effect=llm.openai.OpenAIError(""boom"")) as mock_create:

            class _Vec(list):
                def tolist(self):
                    return list(self)

            fake_mod = SimpleNamespace(
                SentenceTransformer=lambda *_: SimpleNamespace(
                    encode=lambda text, normalize_embeddings=True: _Vec([0.1, 0.2])
                )
            )
            with patch.dict(sys.modules, {""sentence_transformers"": fake_mod}):
                vec = llm._sync_embed(""hi"")
        mock_create.assert_called_once()
        self.assertEqual(vec, [0.1, 0.2])
",tests/test_embedder_fallback.py,TestEmbedderFallback
survived,"def test_self_healer_aborts_on_invalid_diff(tmp_path, monkeypatch, caplog):
    repo_src = Path(__file__).parent / ""fixtures"" / ""self_heal_repo""
    repo_path = tmp_path / ""repo""
    shutil.copytree(repo_src, repo_path)
    subprocess.run([""git"", ""init""], cwd=repo_path, check=True)
    subprocess.run([""git"", ""add"", "".""], cwd=repo_path, check=True)
    subprocess.run([""git"", ""commit"", ""-m"", ""init""], cwd=repo_path, check=True)
    commit = subprocess.check_output([""git"", ""rev-parse"", ""HEAD""], cwd=repo_path, text=True).strip()

    workdir = tmp_path / ""work""
    healer = self_healer.SelfHealer(repo_url=str(repo_path), commit_sha=commit)
    healer.working_dir = str(workdir)

    monkeypatch.setattr(patcher_core, ""generate_patch"", lambda *_a, **_k: ""bad"")
    monkeypatch.setattr(llm_client, ""request_patch"", lambda *_a, **_k: ""bad"")
    monkeypatch.setattr(
        diff_utils,
        ""parse_and_validate_diff"",
        lambda diff, repo_dir, allowed_paths=None: None,
    )
    pushed = []

    def fake_push(self):
        pushed.append(True)
        return ""branch""

    monkeypatch.setattr(self_healer.SelfHealer, ""commit_and_push_fix"", fake_push)
    monkeypatch.setattr(self_healer.SelfHealer, ""create_pull_request"", lambda self, branch: 1)

    monkeypatch.setattr(sandbox, ""run_in_docker"", lambda *_a, **_k: (1, ""fail""))

    caplog.set_level(""WARNING"")
    pr = healer.run()

    assert pr is None
    assert not pushed
    assert any(""valid patch"" in rec.getMessage() for rec in caplog.records)
",tests/test_self_healer_pipeline.py,
survived,"  def __init__(self, dbc_name, messages: list[tuple[str | int, int]], bus: int = 0):
    if isinstance(dbc_name, bytes):
      dbc_name = dbc_name.decode(""utf-8"")
    self.dbc_name: str = dbc_name
    self.bus: int = bus
    self.dbc: DBC | None = _get_dbc(dbc_name)
    if not self.dbc:
      raise RuntimeError(f""Can't find DBC: {dbc_name}"")

    self.vl: dict[int | str, dict[str, float]] = {}
    self.vl_all: dict[int | str, dict[str, list[float]]] = {}
    self.ts_nanos: dict[int | str, dict[str, int]] = {}
    self.addresses: set[int] = set()
    self.message_states: dict[int, MessageState] = {}

    for name_or_addr, freq in messages:
      if isinstance(name_or_addr, numbers.Number):
        msg = self.dbc.addr_to_msg.get(int(name_or_addr))
      else:
        msg = self.dbc.name_to_msg.get(name_or_addr)
      if msg is None:
        raise RuntimeError(f""could not find message {name_or_addr!r} in DBC {dbc_name}"")
      if msg.address in self.addresses:
        raise RuntimeError(""Duplicate Message Check: %d"" % msg.address)

      self.addresses.add(msg.address)
      signal_names = list(msg.sigs.keys())
      self.vl[msg.address] = {s: 0.0 for s in signal_names}
      self.vl[msg.name] = self.vl[msg.address]
      self.vl_all[msg.address] = defaultdict(list)
      self.vl_all[msg.name] = self.vl_all[msg.address]
      self.ts_nanos[msg.address] = {s: 0 for s in signal_names}
      self.ts_nanos[msg.name] = self.ts_nanos[msg.address]

      state = MessageState(
        address=msg.address,
        name=msg.name,
        size=msg.size,
        signals=list(msg.sigs.values()),
        ignore_alive=freq == 0,
      )
      if 0 < freq < 10:
        state.frequency = freq
        state.timeout_threshold = (1_000_000_000 / freq) * 10

      self.message_states[msg.address] = state

    self.can_valid: bool = False
    self.bus_timeout: bool = False
    self.can_invalid_cnt: int = CAN_INVALID_CNT
    self.last_nonempty_nanos: int = 0
",opendbc/can/parser.py,CANParser
survived,"async def test_loop():
    counter = {""i"": 0}

    async def step(prompt, **kwargs):
        counter[""i""] += 1
        return counter[""i""]

    wf = Workflow(
        name=""wf"",
        steps=[WorkflowStep(runner=step, mode=StepMode.LOOP, max_iterations=3)],
    )

    result = await wf.run(0)
    assert result == 3
    assert counter[""i""] == 3
",tests/test_workflow.py,
survived,"    def __init__(
        self,
        name: str,
        steps: List[WorkflowStep],
        instruction: str = """",
        description: str = """",
        default_llm: Optional[LLM] = None,
        sdk_context: Optional[SDKContext] = None,
        default_user_id: str = ""default_user"",
        default_session_id: str = ""default_chat"",
        workflow_id: Optional[str] = None,
    ) -> None:
        self.id = workflow_id or str(uuid.uuid4())
        self.name = name
        self.instruction = instruction
        self.description = description
        self.default_llm = default_llm
        self.sdk_context = sdk_context or SDKContext.get_instance()
        self.default_user_id = default_user_id
        self.default_session_id = default_session_id
        self.steps = steps
        self.sdk_context.add_resource(self, resource_type=""workflow"")
",swarmzero/workflow.py,Workflow
survived,"def test_two_contiguous_selectors():
    B, X, Y = Axis(""batch"", 3), Axis(""x"", 5), Axis(""y"", 7)
    a = hax.arange((B, X, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a[""x"", ix, ""y"", iy]
    assert out.axes == (B,)
    ref = a.array[jnp.arange(3), ix.array, iy.array]
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,
survived,"    def is_finished(self) -> bool:
        return self.status is SequenceStatus.FINISHED
",src/levanter/inference/sequence.py,Sequence
survived,"    def add_request(self, prompt: str | List[int], sampling_params: SamplingParams, scheduler: Scheduler) -> None:
        prompt_ids = (
            self.tokenizer.encode(prompt, add_special_tokens=False)
            if isinstance(prompt, str)
            else prompt
        )
        seq = Sequence(list(prompt_ids), sampling_params)
        scheduler.add(seq)
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        self.trainer_cfg = TrainerConfig()
        self.Vocab = round_axis_for_partitioning(
            Axis(""vocab"", len(self.tokenizer)), self.trainer_cfg.compute_axis_mapping
        )
        converter = LlamaConfig().hf_checkpoint_converter()
        converter = converter.replaced(reference_checkpoint=RepoRef(model_path), tokenizer=self.tokenizer)
        self.model = cast(
            LlamaLMHeadModel,
            converter.load_pretrained(
                LlamaLMHeadModel,
                ref=RepoRef(model_path),
                dtype=self.trainer_cfg.mp.compute_dtype,
            ),
        )
        self.sampler = Sampler(self.Vocab)
        self.eos = self.tokenizer.eos_token_id or -1
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"    def schedule(self) -> tuple[List[Sequence], bool]:
        if self.waiting:
            seq = self.waiting.popleft()
            self.running.append(seq)
            return [seq], True
        seqs = [s for s in self.running if not s.is_finished]
        return seqs, False
",src/levanter/inference/scheduler.py,Scheduler
survived,"    def _prefill(self, seq: Sequence, cache, page_table):
        real_len = len(seq.prompt_token_ids)
        padded_len = _round_preferred(real_len)
        pos_axis = Axis(""position"", padded_len)
        padded_tokens = list(seq.prompt_token_ids) + [self.eos] * (padded_len - real_len)
        tokens = hax.NamedArray(jnp.array(padded_tokens, dtype=jnp.int32), axes=(pos_axis,))
        seq_named = hax.named([seq.seq_id], ""seq"")
        temps = hax.full((), seq.sampling_params.temperature, dtype=jnp.float32)
        key = jrandom.PRNGKey(0)
        tok, page_table, cache = do_prefill(self.model, cache, page_table, tokens, self.sampler, seq_named, temps, key)
        return int(tok.array), cache, page_table
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"    def solve(self, root, is_goal, max_cost=None):
        """""" Returns the shortest path between the root and a given goal, as well as the total cost.
        If the cost exceeds a given max_cost, the function returns None. If you do not give a
        maximum cost the solver will never return for unsolvable instances.""""""

        self.is_goal = is_goal
        self.path = [root]
        self.is_in_path = {root}
        self.path_descrs = []
        self.nodes_evaluated = 0

        bound = self.h(root)

        while True:
            t = self._search(0, bound)
            if t is self.FOUND: return self.path, self.path_descrs, bound, self.nodes_evaluated
            if t is None: return None
            bound = t
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,IDAStar
survived,"    def __init__(self, object_list):
        """"""
        Save a list in a heapq.
        Assume that each object only appears once
        in the list.
        """"""
        self.queue_length = 0
        self.qheap = []
        for e in object_list:
            self.qheap.append((e.fscore,e.tiles))
            self.queue_length += 1
        heapq.heapify(self.qheap)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue
survived,"    def pop(self):
        """""" remove object from heap and return """"""
        if self.queue_length < 1:
            return None
        fscore, tiles = heapq.heappop(self.qheap)
        self.queue_length -= 1
        global all_positions
        pos = all_positions[tiles]
        if pos.fscore == fscore:
            return pos
        else:
            return self.pop()
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,PriorityQueue
survived,"    def test_can_load_grammar(self):
        try:
            tree_sitter.Language(tree_sitter_racket.language())
        except Exception:
            self.fail(""Error loading Racket grammar"")",third_party/tree-sitter-racket/bindings/python/tests/test_binding.py,TestLanguage
survived,"    def _TryToFindUnixSocket(self) -> Optional[str]:

        # This is required to find the socket.
        if self.MoonrakerConfigFilePath is None:
            self.Logger.error(""_TryToFindUnixSocket - No moonraker config file path provided - Is this a companion plugin?"")
            return None

        # First, try to parse the moonraker config to find the klipper socket path, since the moonraker socket should be similar.
        try:
            # Open and read the config.
            # allow_no_value allows keys with no values - strict allows duplicate sections, because sometimes that happens for unknown reasons.
            # Since this is edited by the user, we allow non-strict stuff, since they can make mistakes like multiple sections.
            moonrakerConfig = configparser.ConfigParser(allow_no_value=True, strict=False)
            moonrakerConfig.read(self.MoonrakerConfigFilePath)
            if ""server"" not in moonrakerConfig:
                self.Logger.info(""_TryToFindUnixSocket - No server block found in moonraker config."")
            else:
                if ""klippy_uds_address"" not in moonrakerConfig[""server""]:
                    self.Logger.info(""_TryToFindUnixSocket - klippy_uds_address not found in moonraker config."")
                else:
                    # In most installs, this will be something like `~/printer_data/comms/klippy.sock`
                    klippySocketFilePath = moonrakerConfig[""server""][""klippy_uds_address""]
                    self.Logger.info(""Moonraker klippy unix socket path found in config: ""+klippySocketFilePath)
                    possibleComFolderPath = self._GetParentDirectory(klippySocketFilePath)
                    possibleMoonrakerSocketFilePath = os.path.join(possibleComFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
                    if os.path.exists(possibleMoonrakerSocketFilePath):
                        self.Logger.info(""Moonraker socket path found from moonraker config klippy socket path. :""+possibleMoonrakerSocketFilePath)
                        return possibleMoonrakerSocketFilePath
        except configparser.ParsingError as e:
            if ""Source contains parsing errors"" in str(e):
                self.Logger.error(""_TryToFindUnixSocket failed to handle moonraker config. ""+str(e))
        except Exception as e:
            Sentry.OnException(""_TryToFindUnixSocket failed to handle moonraker config."", e)

        # If that failed, try to find the path by stepping back from the moonraker config a few times.
        moonrakerConfigFolderPath = self._GetParentDirectory(self.MoonrakerConfigFilePath)

        # Test the config folder for the file and file + comms folder
        # This isn't likely, but we might as well try.
        testPath = os.path.join(moonrakerConfigFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker config path. :""+testPath)
            return testPath
        testPath = os.path.join(moonrakerConfigFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileNameWithCommsFolder)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker config path. :""+testPath)
            return testPath

        # Move a folder up and try again. This is where we expect the comms folder to be located, next to the config folder
        moonrakerPrinterFolderPath = self._GetParentDirectory(moonrakerConfigFolderPath)
        testPath = os.path.join(moonrakerPrinterFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileName)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker printer folder path. :""+testPath)
            return testPath
        testPath = os.path.join(moonrakerPrinterFolderPath, MoonrakerCredentialManager.c_MoonrakerUnixSocketFileNameWithCommsFolder)
        if os.path.exists(testPath):
            self.Logger.info(""Moonraker unix socket path found from moonraker printer folder path. :""+testPath)
            return testPath
        return None
",moonraker_octoeverywhere/moonrakercredentialmanager.py,MoonrakerCredentialManager
survived,"def visualize_shardings(tree) -> None:
    """"""Print the sharding for each array-like leaf in ``tree``.

    Both :class:`NamedArray` and regular JAX arrays are supported. NamedArrays
    will show the mapping from logical axis names to physical axes. Plain arrays
    will fall back to :func:`jax.debug.visualize_sharding`.
    """"""

    import haliax.tree_util as htu

    def _show(x):
        if isinstance(x, NamedArray):
            arr = x.array
            axes = x.axes
        else:
            arr = x
            axes = None

        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass

        jax.debug.inspect_array_sharding(arr, callback=cb)
        return x

    htu.tree_map(_show, tree, is_leaf=is_jax_or_hax_array_like)",src/haliax/debug.py,
survived,"    def fn(x):
        visualize_shardings(x)
        return x
",tests/test_visualize_sharding.py,
survived,"            def run_generations(self, *_a):
                pass
",tests/test_agent_aiga_entrypoint.py,TestAgentAIGAEntry.DummyEvolver
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        return await self.tools.list_agents()
",alpha_factory_v1/demos/alpha_asi_world_model/openai_agents_bridge.py,InspectorAgent
survived,"async def evolve(generations: int = 1) -> str:
    EVOLVER.run_generations(generations)
    return EVOLVER.latest_log()
",alpha_factory_v1/demos/aiga_meta_evolution/openai_agents_bridge.py,
survived,"async def detect_yield_curve_alpha_tool() -> Dict[str, str]:
    msg = detect_yield_curve_alpha()
    return {""alpha"": msg}
",alpha_factory_v1/demos/era_of_experience/agent_experience_entrypoint.py,
survived,"def test_cli_help():
    result = subprocess.run([
        sys.executable,
        '-m', 'alpha_factory_v1.demos.muzero_planning',
        '--help'
    ], capture_output=True, text=True)
    assert result.returncode == 0
    assert 'MuZero planning demo' in result.stdout",tests/test_muzero_cli.py,
survived,"def _make_cert(tmp: Path) -> tuple[str, str, bytes]:
    key = rsa.generate_private_key(public_exponent=65537, key_size=2048)
    cert = (
        x509.CertificateBuilder()
        .subject_name(x509.Name([x509.NameAttribute(NameOID.COMMON_NAME, ""localhost"")]))
        .issuer_name(x509.Name([x509.NameAttribute(NameOID.COMMON_NAME, ""localhost"")]))
        .public_key(key.public_key())
        .serial_number(x509.random_serial_number())
        .not_valid_before(datetime.utcnow())
        .not_valid_after(datetime.utcnow() + timedelta(days=1))
        .add_extension(x509.SubjectAlternativeName([x509.DNSName(""localhost"")]), False)
        .sign(key, hashes.SHA256())
    )
    cert_pem = cert.public_bytes(serialization.Encoding.PEM)
    key_pem = key.private_bytes(
        serialization.Encoding.PEM,
        serialization.PrivateFormat.TraditionalOpenSSL,
        serialization.NoEncryption(),
    )
    cert_path = tmp / ""cert.pem""
    key_path = tmp / ""key.pem""
    cert_path.write_bytes(cert_pem)
    key_path.write_bytes(key_pem)
    return str(cert_path), str(key_path), cert_pem
",tests/test_agents.py,
survived,"    def _align_cpu(self, seq1: str, seq2: str) -> tuple[int, list[list[int]]]:
        len1, len2 = len(seq1), len(seq2)
        H = [[0] * (len2 + 1) for _ in range(len1 + 1)]
        max_score = 0
        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                match_score = self.match if seq1[i - 1] == seq2[j - 1] else self.mismatch
                diag = H[i - 1][j - 1] + match_score
                up = H[i - 1][j] + self.gap
                left = H[i][j - 1] + self.gap
                val = diag
                if up > val:
                    val = up
                if left > val:
                    val = left
                if val < 0:
                    val = 0
                H[i][j] = val
                if val > max_score:
                    max_score = val
        return max_score, H
",src/python/gpu_smith_waterman.py,SmithWatermanGPU
survived,"def main():

    gen = HldaDataGenerator(0.01, make_plot=True)

    n_topics = 5
    vocab_size = 25
    document_length = 1000
    n_docs = 100
    df, vocab = gen.generate_input_df(
        n_topics,
        vocab_size,
        document_length,
        n_docs,
    )
",examples/synthetic_data.py,
survived,"    def observe(self, *a, **kw):
        pass
",tests/test_eventbus.py,_M
survived,"    async def _drain_loop(self) -> None:
        assert self._queues is not None
        try:
            while True:
                for q in list(self._queues.values()):
                    while not q.empty():
                        try:
                            q.get_nowait()
                        except asyncio.QueueEmpty:
                            break
                await asyncio.sleep(0.1)
        except asyncio.CancelledError:
            pass
",alpha_factory_v1/backend/agent_runner.py,EventBus
survived,"def _env_float(name: str, default: float) -> float:
    """"""Return ``float`` environment value or ``default`` if conversion fails.""""""

    val = os.getenv(name)
    if val is None:
        return default
    try:
        return float(val)
    except (TypeError, ValueError):
        log.warning(""Invalid %s=%r, using default %s"", name, val, default)
        return default
",alpha_factory_v1/backend/agent_runner.py,
survived,"    def max_seqs(self) -> int:
        return self.page_indices.axis_size(""seq"")
",src/levanter/layers/page_table.py,PageTable
survived,"    def current_num_seqs(self) -> int:
        return hax.sum(self.seq_lens >= 0).scalar()
",src/levanter/layers/page_table.py,PageTable
survived,"        def token_body(i, carry):
            token_dests, seq_cursors = carry
            seq_id = tokens[""position"", i].scalar()

            def assign(carry):
                token_dests, seq_cursors = carry
                page_idx = seq_cursors[seq_id] // self.page_size
                page_offset = seq_cursors[seq_id] % self.page_size
                page = new_table.page_indices[""seq"", seq_id, ""page"", page_idx]
                dest = hax.where(page < 0, -1, page * self.page_size + page_offset)
                token_dests = token_dests.at[""position"", i].set(dest)
                seq_cursors = seq_cursors.at[seq_id].add(1)
                return token_dests, seq_cursors

            token_dests, seq_cursors = jax.lax.cond(seq_id >= 0, assign, lambda c: c, (token_dests, seq_cursors))
            return token_dests, seq_cursors
",src/levanter/layers/page_table.py,PageTable
survived,"    def add_output_guardrail(self, guardrail: Callable[[str], Awaitable[None]]) -> None:
        self.output_guardrails.append(guardrail)
",src/meta_agent/services/guardrail_router.py,GuardrailModelRouter
survived,"    def test_mixed_args_grad_numpy(self):
        self._check_mixed_args_grad(""numpy"")
",tests/test_autograd.py,TestAutograd
survived,"def main() -> None:
    root = Path(__file__).resolve().parent.parent
    base = root / ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1""
    for rel, cid in ASSETS.items():
        dest = base / rel
        print(f""Fetching {rel} from {cid}..."")
        download(cid, dest)
",scripts/fetch_assets.py,
survived,"    def register(self, *a, **k):
        pass
",tests/test_rate_lock.py,DummyRuntime
survived,"        def run(self) -> None:
            pass
",tests/test_alpha_opportunity_stub.py,DummyRuntime
survived,"def test_firejail_used_when_available(monkeypatch) -> None:
    calls: dict[str, list] = {}

    def fake_run(cmd, **kwargs):
        calls[""cmd""] = cmd
        calls[""preexec_fn""] = kwargs.get(""preexec_fn"")

        class P:
            stdout = ""{}""
            stderr = """"

        return P()

    monkeypatch.setattr(codegen_agent.shutil, ""which"", lambda n: ""/usr/bin/firejail"")
    monkeypatch.setattr(codegen_agent.subprocess, ""run"", fake_run)

    agent = _make_agent()
    agent.execute_in_sandbox(""print('hi')"")

    assert calls[""cmd""][0] == ""/usr/bin/firejail""
    assert ""--net=none"" in calls[""cmd""]
    assert calls[""preexec_fn""] is None",tests/test_codegen_agent.py,
survived,"    def close(self) -> None:
        """"""Unsubscribe the agent from the bus.""""""
        self.bus.unsubscribe(self.name, self._handler)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent
survived,"def test_load_dotenv(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    env = tmp_path / ""sample.env""
    env.write_text(""FOO=bar\n"", encoding=""utf-8"")
    monkeypatch.delenv(""FOO"", raising=False)
    cfg._load_dotenv(str(env))
    assert os.environ[""FOO""] == ""bar""
",tests/test_config_utils.py,
survived,"    def _model_dump_json(self: BaseModel, *args: Any, **kwargs: Any) -> str:
        return self.json(*args, **kwargs)
",src/meta_agent/__init__.py,
survived,"    def _worker() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        task = loop.create_task(coro)
        try:
            result.append(asyncio.get_event_loop().run_until_complete(task))
        finally:
            loop.close()
",alpha_factory_v1/backend/agents/energy_agent.py,
survived,"def backtrack_boost(pop: List[Any], archive: List[Any], rate: float) -> Any:
    """"""Return a parent possibly selected from weaker individuals.

    With probability ``rate`` the parent is drawn uniformly from the
    lower half of ``archive`` based on fitness.  Otherwise the regular
    ``select_parent`` mechanism chooses from ``pop``.
    """"""

    if not pop:
        raise ValueError(""population is empty"")
    if rate <= 0.0:
        return select_parent(pop, temp=1.0)
    if random.random() < rate:
        ranked = sorted(archive, key=lambda c: getattr(c, ""fitness"", 0.0))
        bottom = ranked[: max(1, len(ranked) // 2)]
        return random.choice(bottom)
    return select_parent(pop, temp=1.0)",src/simulation/mats_ops.py,
survived,"def _evaluate_patch(patch: Path) -> Dict[str, float]:
    """"""Return baseline and ablation scores for ``patch``.""""""

    scores: Dict[str, float] = {}
    with tempfile.TemporaryDirectory() as tmp:
        repo = Path(tmp)
        _clone_repo(repo)
        apply_patch(patch.read_text(), repo_path=tmp)
        base_flags = {n: True for n in INNOVATIONS}
        baseline = _run_bench(repo, base_flags)
        scores[""baseline""] = baseline
        for name in INNOVATIONS:
            flags = base_flags.copy()
            flags[name] = False
            scores[name] = _run_bench(repo, flags)
    return scores
",src/tools/ablation_runner.py,
survived,"    def evolve(
        self,
        scenario_hash: str,
        fn: Callable[[list[float]], tuple[float, ...]],
        genome_length: int,
        **kwargs: object,
    ) -> mats.Population:
        """"""Run evolution for ``scenario_hash`` using persistent islands.""""""

        pop = mats.run_evolution(
            fn,
            genome_length,
            scenario_hash=scenario_hash,
            populations=self.island_pops,
            **kwargs,
        )
        self.island_pops[scenario_hash] = pop
        return pop
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator
survived,"def _add_path(root: Dict[str, Any], path: Iterable[str]) -> None:
    node = root
    for name in path:
        children = node.setdefault(""children"", [])
        for child in children:
            if child.get(""name"") == name:
                node = child
                break
        else:
            child = {""name"": name}
            children.append(child)
            node = child
",alpha_factory_v1/demos/alpha_agi_insight_v1/tools/export_tree.py,
survived,"    def test_index_negative_column(self):
        """"""Use negative index for last column""""""
        self.assert_eval_cmp('[[1 2 3] [4 5 6]]:@[[] -1]', '[3 6]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior
survived,"def test_business_bridge_offline(monkeypatch, capsys):
    # Stub google_adk so adk_bridge imports succeed without network
    dummy = types.ModuleType(""google_adk"")
    dummy.Agent = object

    class _Router:
        def __init__(self):
            self.app = types.SimpleNamespace(middleware=lambda *_a, **_k: lambda f: f)

        def register_agent(self, _agent):
            pass

    dummy.Router = _Router
    dummy.AgentException = Exception
    monkeypatch.setitem(sys.modules, ""google_adk"", dummy)

    # Ensure OPENAI_API_KEY unset and openai_agents import fails
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    monkeypatch.setattr(
        check_env, ""main"", lambda *_a, **_k: (_ for _ in ()).throw(requests.exceptions.ConnectionError(""offline""))
    )
    sys.modules.pop(""openai_agents"", None)
    orig_import = builtins.__import__

    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)

    bridge = importlib.reload(
        importlib.import_module(""alpha_factory_v1.demos.alpha_agi_business_v1.openai_agents_bridge"")
    )

    assert bridge._require_openai_agents() is False

    bridge.main()
    captured = capsys.readouterr()
    assert ""OpenAI Agents SDK not available; bridge inactive."" in captured.out",tests/test_business_bridge_offline.py,
survived,"def test_build_and_search(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")
    reg.register(_meta(""bar""), ""hello bar"")

    index = TemplateIndex(reg)
    index.rebuild()

    results = index.search(""hello foo"")
    assert results and results[0][""slug""] == ""foo""
",tests/test_template_index.py,
survived,"    def ensure_up_to_date(self) -> None:
        if self.needs_rebuild():
            self.rebuild()
        else:
            self.load()
",src/meta_agent/template_index.py,TemplateIndex
survived,"def _free_port() -> int:
    with socket.socket() as s:
        s.bind((""127.0.0.1"", 0))
        return int(s.getsockname()[1])
",tests/test_evolution_worker.py,
survived,"async def mutate(
    tar: UploadFile | None = File(None),
    repo_url: str | None = Form(None),
) -> MutationResponse:
    """"""Return a mutated child from one evolution step.""""""

    if tar is None and not repo_url:
        raise HTTPException(status_code=400, detail=""tar or repo_url required"")

    tmp = tempfile.mkdtemp(dir=STORAGE_PATH)
    tmp_path = Path(tmp)
    try:
        if tar is not None:
            with tarfile.open(fileobj=tar.file) as tf:
                tf.extractall(tmp_path)
        if repo_url:
            (tmp_path / ""repo.txt"").write_text(repo_url)

        pop = mats.run_evolution(
            lambda g: (g[0] ** 2, g[1] ** 2),
            2,
            population_size=4,
            generations=1,
            seed=42,
        )
        child = pop[0].genome
        return MutationResponse(child=child)
    finally:
        for p in tmp_path.rglob(""*""):
            if p.is_file():
                p.unlink()
        tmp_path.rmdir()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evolution_worker.py,
survived,"    def best_architecture(self) -> str:
        return self.best_genome.to_json() if self.best_genome else """"",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def _ray_eval(self):
        @ray.remote
        def _worker(js: str):
            g = Genome.from_json(js)
            module = import_module(__name__)
            return module.MetaEvolver._simulate(module.MetaEvolver, g)
        futures = [_worker.remote(g.to_json()) for g in self.population]
        results = ray.get(futures)
        return self._post_eval(results)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def load(self, path: pathlib.Path | None = None):
        if path is None:
            latest = max(self.ckpt_dir.glob(""gen_*.json""), default=None)
            if not latest:
                raise FileNotFoundError(""no checkpoint found"")
            path = latest
        js = json.loads(path.read_text())
        self.gen = js[""gen""]
        self.population = [Genome.from_json(j) for j in js[""pop""]]
        self.history = js.get(""hist"", [])
        self._archive = [np.array(a) for a in js.get(""arc"", [])]
        self.rng.seed(js.get(""seed"", 0))
        self._best_fitness = js.get(""best_fitness"", -math.inf)
        bg = js.get(""best_genome"")
        self.best_genome = Genome.from_json(bg) if bg else self.population[0]
        if _fitness_gauge:
            _fitness_gauge.set(self._best_fitness)
        LOG.info(""Loaded checkpoint gen=%d sha=%s"", self.gen, js.get(""sha"", ""?""))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def _close_producer() -> None:  # graceful flush on exit
        try:
            _producer.flush()
            _producer.close()
        except Exception:  # noqa: BLE001
            log.exception(""Kafka producer close failed"")
",alpha_factory_v1/backend/orchestrator.py,
survived,"def test_entropy_js() -> None:
    subprocess.check_call([""npm"", ""test""], cwd=BROWSER_DIR)",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_entropy_js.py,
survived,"def _require_node_20() -> None:
    try:
        out = subprocess.check_output(
            [""node"", ""-e"", ""console.log(process.versions.node)""],
            text=True,
        ).strip()
    except FileNotFoundError:
        sys.exit(""Node.js 20+ is required. 'node' not found."")
    major = int(out.split(""."")[0])
    if major < 20:
        sys.exit(f""Node.js 20+ is required. Current version: {out}"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py,
survived,"    def test_invalid_parameters_raise(self):
        with self.assertRaises(ValueError):
            run_sim(agents=0, rounds=10, delta=0.5, stake=1)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=0, delta=0.5, stake=1)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=1.5, stake=1)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=0.5, stake=-1)
",alpha_factory_v1/tests/test_governance_sim.py,GovernanceSimTest
survived,"        def fake_openai_agent(*_a, **kwargs):
            return types.SimpleNamespace(base_url=kwargs.get(""base_url""))
",tests/test_openai_bridge_runtime.py,TestAIGABridgeRuntime
survived,"        def json(self) -> dict:
            return {""choices"": [{""message"": {""content"": ""ok""}}]}
",tests/test_llm_client_offline.py,DummyResp
survived,"def main() -> int:
    demos = iter_demos()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            for demo in demos:
                page = browser.new_page()
                page.goto((demo / ""index.html"").resolve().as_uri())
                page.wait_for_selector(""body"")
                page.wait_for_selector(""h1"")
                page.close()
            browser.close()
        return 0
    except PlaywrightError as exc:
        print(f""Playwright error: {exc}"", file=sys.stderr)
        return 1
    except Exception as exc:  # noqa: BLE001
        print(f""Demo check failed: {exc}"", file=sys.stderr)
        return 1
",scripts/verify_demo_pages.py,
survived,"def test_dist_has_no_superintelligence() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    text = dist.read_text(encoding=""utf-8"")
    assert ""superintelligence"" not in text",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_no_superintelligence.py,
survived,"            def _decorator(func):
                return func
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"    def test_run_demo_market_data(self) -> None:
        import tempfile

        with tempfile.NamedTemporaryFile(""w"", delete=False) as fh:
            fh.write(""6,6,6"")
            feed_path = fh.name
        result = subprocess.run(
            [
                sys.executable,
                ""-m"",
                ""alpha_factory_v1.demos.meta_agentic_tree_search_v0.run_demo"",
                ""--episodes"",
                ""2"",
                ""--market-data"",
                feed_path,
            ],
            capture_output=True,
            text=True,
        )
        self.assertEqual(result.returncode, 0, result.stderr)
        self.assertIn(""Best agents"", result.stdout)
",tests/test_meta_agentic_tree_search_demo.py,TestMetaAgenticTreeSearchDemo
survived,"    def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai.agents"":
            raise ModuleNotFoundError
        return orig_import(name, globals, locals, fromlist, level)
",tests/test_agents.py,
survived,"def test_python_requires_is_39():
    setup_path = Path(__file__).resolve().parents[1] / ""setup.py""
    with open(setup_path, ""r"", encoding=""utf-8"") as f:
        tree = ast.parse(f.read(), filename=""setup.py"")

    for node in ast.walk(tree):
        if isinstance(node, ast.keyword) and node.arg == ""python_requires"":
            assert isinstance(node.value, ast.Constant)
            value = node.value.value
            assert value == "">=3.9, <4""
            break
    else:
        pytest.fail(""python_requires not found"")",tests/test_setup.py,
survived,"def cytomat_rack_60mm_8(name: str):
  return _cytomat_rack(name=name, site_height=60, num_sites=8, model=""cytomat_rack_60mm_8"")
",pylabrobot/storage/cytomat/racks.py,
survived,"def validate_storage_location_number(storage_location_number: str):
  try:
    int(storage_location_number)
  except ValueError as exc:
    raise ValueError(""Storage location number must be an integer."") from exc
  if len(storage_location_number) != 3:
    raise ValueError(""Storage location number must be a three-digit number."")",pylabrobot/storage/cytomat/utils.py,
survived,"  async def set_temperature(self, temperature: float):
    raise NotImplementedError(""Temperature control not implemented yet"")
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"def cytomat_rack_18mm_26(name: str):
  return _cytomat_rack(name=name, site_height=18, num_sites=26, model=""cytomat_rack_18mm_26"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def open_door(self):
    return await self.backend.open_door()
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def start_shaking(self, frequency: float, shakers: Optional[List[int]] = None):
    if self.model == CytomatType.C5C:
      raise NotImplementedError(""Shaking is not supported on this model"")
    await self.set_shaking_frequency(frequency=int(frequency), shakers=shakers)
    return hex_to_binary(await self.send_command(""ll"", ""va"", """"))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def close_door(self):
    print(""Closing door"")
",pylabrobot/storage/chatterbox.py,IncubatorChatterboxBackend
survived,"def cytomat_rack_69mm_7(name: str):
  return _cytomat_rack(name=name, site_height=69, num_sites=7, model=""cytomat_rack_69mm_7"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def action_read_barcode(
    self,
    site_number_a: str,
    site_number_b: str,
  ) -> OverviewRegisterState:
    # Read barcode of storage locations
    validate_storage_location_number(site_number_a)
    validate_storage_location_number(site_number_b)
    resp = await self.send_command(""mv"", ""sn"", f""{site_number_a} {site_number_b}"")
    return OverviewRegisterState.from_resp(resp)
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def start_shaking(self, frequency: float = 1.0):
    await self.backend.start_shaking(frequency=frequency)
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def fetch_plate_to_loading_tray(self, plate: Plate):
    pass
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def set_temperature(self, temperature: float):
    """"""Set the temperature of the incubator in degrees Celsius.""""""
    return await self.backend.set_temperature(temperature)
",pylabrobot/storage/incubator.py,Incubator
survived,"        def _walk(name: str) -> None:
            if name in visited:
                return
            s, v = _split_name(name)
            source = self.registry.load_template(s, v) or """"
            deps = pattern.findall(source)
            visited[name] = deps
            for dep in deps:
                _walk(dep)
",src/meta_agent/template_mixer.py,TemplateMixer
survived,"def _load_dotenv(path: str = "".env"") -> None:
    """"""Load default variables from ``path`` when available.""""""
    if Path(path).is_file():
        for k, v in _load_env_file(path).items():
            os.environ.setdefault(k, v)
",alpha_factory_v1/utils/config_common.py,
survived,"    def merge_versions(self, slug: str, ours: str, theirs: str) -> str:
        """"""Naively merge two versions preferring ``theirs`` on conflict.""""""
        ours_content = self.registry.load_template(slug, ours) or """"
        theirs_content = self.registry.load_template(slug, theirs) or """"
        ours_lines = ours_content.splitlines()
        theirs_lines = theirs_content.splitlines()
        merged: List[str] = []
        max_len = max(len(ours_lines), len(theirs_lines))
        for i in range(max_len):
            if i < len(theirs_lines):
                merged.append(theirs_lines[i])
            elif i < len(ours_lines):
                merged.append(ours_lines[i])
        return ""\n"".join(merged)",src/meta_agent/template_sharing.py,TemplateSharingManager
survived,"def _load_model() -> None:
    """"""Load a local model if available, otherwise use an echo stub.""""""
    global _MODEL, _CALL
    model_path = os.getenv(
        ""LLAMA_MODEL_PATH"",
        os.path.expanduser(""~/.cache/llama/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf""),
    )

    def _wrap(fn: Callable[[str], str]) -> Callable[[str], str]:
        return fn

    if Llama is not None:
        try:
            _MODEL = Llama(model_path=model_path, n_ctx=int(os.getenv(""LLAMA_N_CTX"", ""2048"")))

            def call_llama(prompt: str) -> str:
                out = cast(Any, _MODEL)(prompt)
                return cast(str, out[""choices""][0][""text""]).strip()

            _CALL = _wrap(call_llama)
            return
        except Exception:  # pragma: no cover - model load failure
            _MODEL = None
    if AutoModelForCausalLM is not None:
        try:
            _MODEL = AutoModelForCausalLM.from_pretrained(model_path, model_type=""llama"")

            def call_ctrans(prompt: str) -> str:
                return cast(str, cast(Any, _MODEL)(prompt))

            _CALL = _wrap(call_ctrans)
            return
        except Exception:  # pragma: no cover - model load failure
            _MODEL = None

    def call_stub(prompt: str) -> str:
        return f""[offline] {prompt}""

    _CALL = _wrap(call_stub)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/local_llm.py,
survived,"    def __exit__(self, exc_type, exc, tb):
        pass
",tests/test_selfheal_import_stubs.py,DummyBlocks
survived,"def _start_server(port: int, env: dict[str, str] | None = None) -> subprocess.Popen[bytes]:
    cmd = [
        sys.executable,
        ""-m"",
        ""src.interface.api_server"",
        ""--host"",
        ""127.0.0.1"",
        ""--port"",
        str(port),
    ]
    return subprocess.Popen(cmd, env=env or os.environ.copy())
",tests/test_api_server_subprocess.py,
survived,"def _wait_running(url: str, headers: dict[str, str]) -> None:
    for _ in range(50):
        try:
            r = httpx.get(f""{url}/runs"", headers=headers)
            if r.status_code == 200:
                return
        except Exception:
            time.sleep(0.1)
    raise AssertionError(""server did not start"")
",tests/test_api_server_subprocess.py,
survived,"def api_server_cmd(host: str, port: int) -> None:
    """"""Launch the FastAPI backend server.""""""

    try:
        import uvicorn
    except Exception as exc:  # pragma: no cover - optional
        raise click.ClickException(""uvicorn is required to run the API server"") from exc

    uvicorn.run(
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface.api_server:app"",
        host=host,
        port=port,
    )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"def test_show_memory_lists_entries(tmp_path: Path) -> None:
    mem_path = tmp_path / ""mem.log""
    mem_path.write_text('{""foo"": ""bar""}\n', encoding=""utf-8"")
    runner = CliRunner()
    from unittest.mock import patch

    with patch.object(cli.config.CFG, ""memory_path"", str(mem_path)):  # type: ignore[attr-defined]
        result = runner.invoke(cli.main, [""show-memory""])
    assert result.exit_code == 0
    assert ""foo"" in result.output
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,
survived,"def test_detect_bottleneck_selects_largest_gap(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    agent = MetaRefinementAgent(repo, tmp_path / ""logs"")
    entries = [
        {""module"": ""a"", ""ts"": 0},
        {""module"": ""b"", ""ts"": 2},
        {""module"": ""c"", ""ts"": 8},
    ]
    assert agent._detect_bottleneck(entries) == ""c""
",tests/test_meta_refinement_agent.py,
survived,"def test_update_settings_thread_safety(config_with_yaml):
    config = config_with_yaml

    exceptions = []

    def update(val):
        try:
            config.update_settings({""int_property"": val})
        except Exception as e:
            exceptions.append(e)

    threads = [threading.Thread(target=update, args=(i,)) for i in range(5)]

    for t in threads:
        t.start()
    for t in threads:
        t.join()

    assert not exceptions
    assert config.int_property in range(5)",libs/core/kiln_ai/utils/test_config.py,
survived,"    def test_discover_alpha_invalid_num(self) -> None:
        with self.assertRaises(ValueError):
            stub.discover_alpha(num=0, ledger=None, model=""gpt-4o-mini"")
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"  def test_low_bits(self):
    self.assertEqual(getbits(0b11010110, 0, 3), 0b0110)
",test/unit/test_helpers.py,TestGetBits
survived,"def _lambda12():
    draw.get(1000)()
    draw.get(4000)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-octal-3.py,
survived,"def _lambda9():
    draw.get(100)()
    draw.get(600)()
",tests/rosetta/transpiler/Python/cistercian-numerals.py,
survived,"def isAlphaNumDot(ch):
    return (ch >= ""A"" and ch <= ""Z"") or (ch >= ""a"" and ch <= ""z"") or (ch >= ""0"" and ch <= ""9"") or ch == ""_"" or ch == "".""
",tests/rosetta/transpiler/Python/function-frequency.py,
survived,"def newFps(fn):
    return Fps(coeffs=[], compute=fn)
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def div(a, b):
    q = newFps(lambda n: 0.0)
    q = dataclasses.replace(q, compute=_lambda2)
    return q
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def fork(hasChild):
    global nextPID
    pid = nextPID
    nextPID = nextPID + 1
    print(""PID: "" + str(pid))
    if not hasChild:
        print(""Done."")
        return
    childPID = nextPID
    print(""Child's PID: "" + str(childPID))
    fork(False)
",tests/rosetta/transpiler/Python/fork-2.py,
survived,"def drawPoint(g, x, y):
    if x >= 0 and x < width and y >= 0 and y < height:
        row = g[y]
        row[x] = ""#""
        g[y] = row
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def show(n, level):
    indent = level * 4
    name = str(n.get(""name""))
    nl = len(name) + indent
    line = spaces(indent) + name
    line = line + spaces(32 - nl) + ""|  ""
    line = line + padLeft(str(int(n.get(""weight""))), 3) + ""   | ""
    line = line + formatFloat(computeCoverage(n), 6) + "" |""
    print(line)
    cs = n.get(""children"")
    for child in cs:
        show(child, level + 1)
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,
survived,"def greLeap(year):
    a = int((year % 4))
    b = int((year % 100))
    c = int((year % 400))
    return a == 0 and (b != 0 or c == 0)
",tests/rosetta/transpiler/Python/french-republican-calendar.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/four-bit-adder-1.py,
survived,"def say(n):
    if n < 20:
        return small[n]
    if n < 100:
        res = tens[n // 10]
        m = n % 10
        if m != 0:
            res = res + ""-"" + small[m]
        return res
    if n < 1000:
        res = say(n // 100) + "" hundred""
        m = n % 100
        if m != 0:
            res = res + "" "" + say(m)
        return res
    if n < 1000000:
        res = say(n // 1000) + "" thousand""
        m = n % 1000
        if m != 0:
            res = res + "" "" + say(m)
        return res
    res = say(n // 1000000) + "" million""
    m = n % 1000000
    if m != 0:
        res = res + "" "" + say(m)
    return res
",tests/rosetta/transpiler/Python/four-is-the-number-of-letters-in-the-....py,
survived,"def mul(a, b):
    return newFps(_lambda1)
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/general-fizzbuzz.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    program = [[17, 91], [78, 85], [19, 51], [23, 38], [29, 33], [77, 29], [95, 23], [77, 19], [1, 17], [11, 13], [13, 11], [15, 14], [15, 2], [55, 1]]
    n = 2
    primes = 0
    count = 0
    limit = 1000000
    two = 2
    line = """"
    while primes < 20 and count < limit:
        res = step(n, program)
        n = res.n
        if not res.ok:
            break
        m = n
        pow = 0
        while m % two == 0:
            m = m // two
            pow = pow + 1
        if m == 1 and pow > 1:
            line = line + str(pow) + "" ""
            primes = primes + 1
        count = count + 1
    if len(line) > 0:
        print(line[0:len(line) - 1])
    else:
        print("""")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fractran.py,
survived,"    def _mc_eval(
        self, env: MiniWorld, policy: Callable[[np.ndarray], int], episodes: int
    ) -> float:
        scores = []
        for _ in range(episodes):
            obs = env.reset()
            total = 0.0
            for _ in range(env.size * env.size * 4):
                a = policy(obs)
                obs, r, done, _ = env.step(a)
                total += r
                if done:
                    break
            scores.append(total)
        return float(np.mean(scores))
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,POETGenerator
survived,"    def test_nested_prefix_handling(self) -> None:
        with tempfile.TemporaryDirectory() as repo:
            os.makedirs(os.path.join(repo, ""alpha""), exist_ok=True)
            file_path = os.path.join(repo, ""alpha"", ""test.py"")
            with open(file_path, ""w"") as fh:
                fh.write(""x = 1\n"")
            patch = """"""--- a/alpha/test.py
+++ b/alpha/test.py
@@
-x = 1
+x = 2
""""""
            # Should not raise for valid nested path
            patcher_core._sanity_check_patch(patch, pathlib.Path(repo))
            patcher_core.apply_patch(patch, repo_path=repo)
            with open(file_path) as fh:
                data = fh.read()
            self.assertIn(""x = 2"", data)
",tests/test_self_healing_patcher.py,TestPatcherCore
survived,"    def test_build_core_agent_stub_when_sdk_missing(self) -> None:
        os.environ.pop(""OPENAI_API_KEY"", None)
        sys.modules.pop(""agents"", None)
        sys.modules.pop(""alpha_factory_v1.backend.agent_factory"", None)
        importlib.invalidate_caches()

        orig_import_module = importlib.import_module

        def _fake_import(name: str, *args: object, **kwargs: object) -> object:
            if name == ""agents"":
                raise ModuleNotFoundError
            return orig_import_module(name, *args, **kwargs)

        with mock.patch(""importlib.import_module"", side_effect=_fake_import):
            af = orig_import_module(""alpha_factory_v1.backend.agent_factory"")
            af = importlib.reload(af)
            agent = af.build_core_agent(name=""t"", instructions=""demo"")

        self.assertTrue(hasattr(agent, ""run""))
        self.assertEqual(agent.run(""hi""), ""[t-stub] echo: hi"")
        self.assertFalse(any(isinstance(t, af.ComputerTool) for t in af.DEFAULT_TOOLS))
",tests/test_agent_factory.py,TestAgentFactory
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/cross_join_triple.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/two-sum.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/map_in_operator.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/python_math.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_items_iteration.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/list_set_ops.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/count_builtin.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/cross_join.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/group_by.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/unary_neg.py,
survived,"def main():
    """"""Run the PostHog example ingest function and print results.""""""
    load_dotenv(override=True)
    df = ingest()
    print(df)
",scripts/posthog_example.py,
survived,"def test_invalid_sim_request_returns_422() -> None:
    port = _free_port()
    proc = _start_demo_server(port)
    url = f""http://127.0.0.1:{port}""
    headers = {""Authorization"": ""Bearer test-token""}
    try:
        _wait_running(url, headers)
        r = httpx.post(
            f""{url}/simulate"",
            json={
                ""horizon"": 0,
                ""pop_size"": 2,
                ""generations"": 1,
                ""mut_rate"": 0.1,
                ""xover_rate"": 0.5,
                ""curve"": ""linear"",
            },
            headers=headers,
        )
        assert r.status_code == 422
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_api_server_subprocess.py,
survived,"def test_transfer_test_runs(monkeypatch: pytest.MonkeyPatch) -> None:
    def fake_run(models: list[str], top_n: int) -> None:
        click.echo(f""models:{','.join(models)} top:{top_n}"")

    monkeypatch.setattr(""src.tools.transfer_test.run_transfer_test"", fake_run)
    runner = CliRunner()
    result = runner.invoke(cli.main, [""transfer-test""])

    assert result.exit_code == 0
    assert ""models:claude-3.7,gpt-4o top:3"" in result.output",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,
survived,"    async def fake_evolve(*args: object, **kwargs: object) -> None:
        called[""ok""] = True
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,
survived,"def test_queue_limit_and_fetch_fallback() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""navigator.sendBeacon=()=>false;""
            ""Object.defineProperty(navigator,'onLine',{get:()=>false,configurable:true});""
            ""localStorage.setItem('telemetryQueue',JSON.stringify(Array.from({length:100},()=>({}))))""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.click(""text=Share"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert (
            page.evaluate(
                ""JSON.parse(localStorage.getItem('telemetryQueue')).length""
            )
            == 100
        )
        page.evaluate(
            ""window.fetch=(...a)=>{window.fetchArgs=a;return Promise.resolve({status:200});};""
            ""Object.defineProperty(navigator,'onLine',{get:()=>true});""
            ""window.dispatchEvent(new Event('online'));""
        )
        page.wait_for_function(""window.fetchArgs !== undefined"")
        assert page.evaluate(""localStorage.getItem('telemetryQueue')"") == ""[]""
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,
survived,"def inc(x: int) -> int:
    return x + k
",tests/human/x/python/pure_global_fold.py,
survived,"def sum_tree(t: Tree) -> int:
    if isinstance(t, Leaf):
        return 0
    elif isinstance(t, Node):
        return sum_tree(t.left) + t.value + sum_tree(t.right)
    else:
        raise TypeError(""Unknown node"")
",tests/human/x/python/tree_sum.py,
survived,"def add(a: int, b: int) -> int:
    return a + b
",tests/human/x/python/partial_application.py,
survived,"    def test_add_valid_relation(self):
        self.g.add(""A"", ""VALID_REL"", ""B"")
        self.assertIn(""B"", self.g.neighbours(""A"", rel=""VALID_REL""))
",tests/test_memory_graph_rel.py,TestGraphMemoryRelationValidation
survived,"    def test_batch_add_valid_relation(self):
        self.g.batch_add([(""A"", ""REL1"", ""B""), (""B"", ""REL2"", ""C"")])
        self.assertIn(""B"", self.g.neighbours(""A"", rel=""REL1""))
        self.assertIn(""C"", self.g.neighbours(""B"", rel=""REL2""))
",tests/test_memory_graph_rel.py,TestGraphMemoryRelationValidation
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_adk_agent.py,DummyLedger
survived,"    async def skill_test(self, payload: dict) -> dict:
        """"""Respond to skill test pings.""""""
        return {""pong"": True}
",alpha_factory_v1/backend/agents/ping_agent.py,PingAgent
survived,"def xor_checksum(address: int, sig: Signal, d: bytearray) -> int:
    checksum = 0
    checksum_byte = sig.start_bit // 8
    for i in range(len(d)):
        if i != checksum_byte:
            checksum ^= d[i]
    return checksum
",opendbc/can/packer.py,
survived,"def chrysler_checksum(address: int, sig: Signal, d: bytearray) -> int:
    checksum = 0xFF
    for j in range(len(d) - 1):
        curr = d[j]
        shift = 0x80
        for _ in range(8):
            bit_sum = curr & shift
            temp_chk = checksum & 0x80
            if bit_sum:
                bit_sum = 0x1C
                if temp_chk:
                    bit_sum = 1
                checksum = (checksum << 1) & 0xFF
                temp_chk = checksum | 1
                bit_sum ^= temp_chk
            else:
                if temp_chk:
                    bit_sum = 0x1D
                checksum = (checksum << 1) & 0xFF
                bit_sum ^= checksum
            checksum = bit_sum & 0xFF
            shift >>= 1
    return (~checksum) & 0xFF
",opendbc/can/packer.py,
survived,"def honda_checksum(address: int, sig: Signal, d: bytearray) -> int:
    s = 0
    extended = address > 0x7FF
    addr = address
    while addr:
        s += addr & 0xF
        addr >>= 4
    for i in range(len(d)):
        x = d[i]
        if i == len(d) - 1:
            x >>= 4
        s += (x & 0xF) + (x >> 4)
    s = 8 - s
    if extended:
        s += 3
    return s & 0xF
",opendbc/can/packer.py,
survived,"    def magnitude(self) -> float:
        """"""Return the Euclidean norm.""""""
        return (self.x ** 2 + self.y ** 2) ** 0.5",runtime/ffi/python/testmod.py,Point
survived,"def test_insight_cli_full_simulation(tmp_path: Path) -> None:
    """"""Run the Insight CLI simulate command end-to-end.""""""
    ledger = tmp_path / ""audit.db""
    env = os.environ.copy()
    env[""AGI_INSIGHT_LEDGER_PATH""] = str(ledger)

    cmd = [
        sys.executable,
        ""-m"",
        ""alpha_factory_v1.demos.alpha_agi_insight_v1.src.interface.cli"",
        ""simulate"",
        ""--horizon"",
        ""1"",
        ""--pop-size"",
        ""1"",
        ""--generations"",
        ""1"",
        ""--offline"",
        ""--no-broadcast"",
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, env=env)
    assert result.returncode == 0, result.stderr
    assert ""year"" in result.stdout.lower()
    assert ledger.exists()",tests/test_insight_cli_e2e.py,
survived,"def test_lineage_detail(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv(""API_RATE_LIMIT"", ""1000"")
    monkeypatch.setenv(""ARCHIVE_PATH"", str(tmp_path / ""a.db""))
    from src.archive import Archive
    arch = Archive(tmp_path / ""a.db"")
    arch.add({""diff"": ""root""}, 0.1)
    arch.add({""parent"": 1, ""diff"": ""child""}, 0.2)

    from src.interface import api_server as mod
    api = importlib.reload(mod)

    client = TestClient(cast(Any, api.app))
    headers = {""Authorization"": ""Bearer test-token""}
    resp = client.get(""/lineage/2"", headers=headers)
    assert resp.status_code == 200
    data = resp.json()
    assert len(data) == 2
    assert data[-1][""id""] == 2",tests/test_api_server_static.py,
survived,"def test_unmapped_directory_returns_default(monkeypatch):
    monkeypatch.setenv('XDG_DOWNLOAD_DIR', '/tmp/downloads')
    devicons = reload_devicons('es')
    file = MockFile('RandomDir', is_directory=True)
    assert devicons.devicon(file) == 'î—¿'
",tests/test_devicons.py,
survived,"    def propose_diff(self, file_path: str, goal: str) -> str:  # noqa: D401
        return generate_diff(file_path, goal)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/codegen_agent.py,CodeGenAgent
survived,"    def cmp(self, op):
        if isinstance(op, ast.Eq):
            return ""==""
        if isinstance(op, ast.NotEq):
            return ""!=""
        if isinstance(op, ast.Lt):
            return ""<""
        if isinstance(op, ast.LtE):
            return ""<=""
        if isinstance(op, ast.Gt):
            return "">""
        if isinstance(op, ast.GtE):
            return "">=""
        return ""?""
",tools/any2mochi/py_simple.py,Conv
survived,"    def visit_If(self, node):
        self.emit(f""if {self.expr(node.test)} {{"")
        self.indent += 1
        for s in node.body:
            self.visit(s)
        self.indent -= 1
        if node.orelse:
            self.emit(""} else {"")
            self.indent += 1
            for s in node.orelse:
                self.visit(s)
            self.indent -= 1
        self.emit(""}"")
",tools/any2mochi/py_simple.py,Conv
survived,"    def visit_Break(self, node):
        self.emit(""break"")
",tools/any2mochi/py_simple.py,Conv
survived,"        async def Stream(self, req_iter, ctx):  # noqa: N802
            async for req in req_iter:
                kind = req.WhichOneof(""payload"")
                if kind == ""trigger"" and req.trigger.name in runners:
                    runners[req.trigger.name].next_ts = 0
                    yield a2a_pb2.StreamReply(ack=a2a_pb2.Ack(id=req.id))
                elif kind == ""status"":
                    stats = [a2a_pb2.AgentStat(name=n, next_run=int(r.next_ts)) for n, r in runners.items()]
                    yield a2a_pb2.StreamReply(status_reply=a2a_pb2.StatusReply(stats=stats))
",alpha_factory_v1/backend/api_server.py,Peer
survived,"    async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> None:
        if credentials.credentials != token:
            raise HTTPException(status_code=403, detail=""Invalid token"")
",alpha_factory_v1/backend/api_server.py,
survived,"    def __init__(
        self,
        enabled: set[str],
        dev_mode: bool,
        kafka_broker: str | None,
        cycle_seconds: int,
        max_cycle_sec: int,
    ) -> None:
        from backend.agents import list_agents, start_background_tasks

        start_background_tasks()
        avail = list_agents()
        names = [n for n in avail if not enabled or n in enabled]
        if not names:
            raise RuntimeError(f""No agents selected â€“ ENABLED={','.join(enabled) if enabled else 'ALL'}"")

        self.bus = EventBus(kafka_broker, dev_mode)
        self.runners: Dict[str, AgentRunner] = {
            n: AgentRunner(n, cycle_seconds, max_cycle_sec, self.bus.publish) for n in names
        }
        self._hb_task: Optional[asyncio.Task] = None
        self._reg_task: Optional[asyncio.Task] = None
",alpha_factory_v1/backend/agent_manager.py,AgentManager
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/inner_join.py,Customer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by.py,Person
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_items_iteration.py,Data
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join_sort.py,Lineitem
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join.py,Supplier
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/sort_stable.py,Item
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/save_jsonl_stdout.py,Person
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/group_by_left_join.py,Order
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/left_join_multi.py,Order
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join.py,Nation
survived,"def dev_orchestrator(monkeypatch: pytest.MonkeyPatch) -> orch_mod.Orchestrator:
    monkeypatch.setenv(""DEV_MODE"", ""true"")
    monkeypatch.setenv(""API_TOKEN"", ""test-token"")
    monkeypatch.setenv(""AGENT_ERR_THRESHOLD"", ""1"")

    from alpha_factory_v1.backend.agents import _HEALTH_Q
    import inspect
    import time

    def list_agents(_detail: bool = False) -> list[str]:  # noqa: D401
        return [""dummy"", ""fail""]

    def get_agent(name: str) -> object:  # noqa: D401
        agent = DummyAgent() if name == ""dummy"" else FailingAgent()

        if hasattr(agent, ""step"") and inspect.iscoroutinefunction(agent.step):
            orig = agent.step

            async def _wrapped(*a: object, **kw: object) -> object:
                t0 = time.perf_counter()
                ok = True
                try:
                    return await orig(*a, **kw)
                except Exception:
                    ok = False
                    raise
                finally:
                    _HEALTH_Q.put((name, (time.perf_counter() - t0) * 1000, ok))

            agent.step = _wrapped
        return agent

    monkeypatch.setattr(""alpha_factory_v1.backend.agents.list_agents"", list_agents)
    monkeypatch.setattr(""alpha_factory_v1.backend.agents.get_agent"", get_agent)
    monkeypatch.setattr(""alpha_factory_v1.backend.agent_runner.get_agent"", get_agent)
    start_background_tasks()

    orch = orch_mod.Orchestrator()
    yield orch
",tests/test_backend_orchestrator_dev.py,
survived,"def _dtm_to_corpus(dtm: Any) -> List[List[int]]:
    """"""Convert a document-term matrix into an integer corpus.""""""
    if sparse.issparse(dtm):
        dtm = dtm.toarray()
    else:
        dtm = np.asarray(dtm)
    corpus: List[List[int]] = []
    for row in dtm:
        doc: List[int] = []
        for idx, count in enumerate(row):
            if count:
                doc.extend([idx] * int(count))
        corpus.append(doc)
    return corpus
",src/hlda/sklearn_wrapper.py,
survived,"def test_execute_in_sandbox_stdout() -> None:
    agent = _make_agent()
    out, err = agent.execute_in_sandbox(""print('x')"")
    assert out == ""x\n""
    assert err == """"
",tests/test_codegen_agent.py,
survived,"def _make_agent() -> codegen_agent.CodeGenAgent:
    cfg = config.Settings(bus_port=0)
    bus = messaging.A2ABus(cfg)
    ledger = Ledger("":memory:"", broadcast=False)
    return codegen_agent.CodeGenAgent(bus, ledger)
",tests/test_codegen_agent.py,
survived,"def _gen_certs(tmp: Path) -> tuple[str, str, bytes, str]:
    root = Path(__file__).resolve().parents[1]
    script = root / ""infrastructure"" / ""gen_bus_certs.sh""
    subprocess.run([""bash"", str(script)], cwd=tmp, check=True, capture_output=True)
    cert = tmp / ""certs"" / ""bus.crt""
    key = tmp / ""certs"" / ""bus.key""
    token = ""change_this_token""
    ca = cert.read_bytes()
    return str(cert), str(key), ca, token
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_bus_secure.py,
survived,"def transform_groups(groups: List[Group], member_map: Dict[str, List[str]]) -> List[Dict[str, Any]]:
    """"""Transform API responses into dictionaries for ingestion.""""""
    result: List[Dict[str, Any]] = []
    for g in groups:
        transformed = {
            ""id"": g.id,
            ""display_name"": g.display_name,
            ""description"": g.description,
            ""mail"": g.mail,
            ""mail_nickname"": g.mail_nickname,
            ""mail_enabled"": g.mail_enabled,
            ""security_enabled"": g.security_enabled,
            ""group_types"": g.group_types,
            ""visibility"": g.visibility,
            ""is_assignable_to_role"": g.is_assignable_to_role,
            ""created_date_time"": g.created_date_time,
            ""deleted_date_time"": g.deleted_date_time,
            ""member_ids"": member_map.get(g.id, []),
        }
        result.append(transformed)
    return result
",cartography/intel/entra/groups.py,
survived,"    def _run(
        self, *args: str, env: dict | None = None
    ) -> subprocess.CompletedProcess[str]:
        if not self.git_available():
            raise RuntimeError(""git executable not found"")
        return subprocess.run(
            [""git"", *args],
            cwd=self.repo_dir,
            text=True,
            check=True,
            capture_output=True,
            env=env,
        )
",src/meta_agent/git_utils.py,GitManager
survived,"        async def run() -> None:
            await orch.bus.start()
            assert orch.bus._server is not None
            try:
                creds = grpc.ssl_channel_credentials(root_certificates=ca)
                async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                    stub = ch.unary_unary(""/bus.Bus/Send"")
                    payload = {
                        ""sender"": ""a"",
                        ""recipient"": ""b"",
                        ""payload"": {},
                        ""ts"": 0.0,
                        ""token"": ""bad"",
                    }
                    with pytest.raises(grpc.aio.AioRpcError):
                        await stub(json.dumps(payload).encode())
            finally:
                await orch.bus.stop()
                await orch.ledger.stop_merkle_task()
                orch.ledger.close()
",tests/test_orchestrator_bus_tls_env.py,
survived,"def test_orchestrator_bus_tls_env(tmp_path: Path) -> None:
    """"""Orchestrator bus requires valid token when TLS is enabled via env vars.""""""
    port = _free_port()
    cert, key, ca = _make_cert(tmp_path)

    env = {
        ""AGI_INSIGHT_BUS_PORT"": str(port),
        ""AGI_INSIGHT_BUS_CERT"": cert,
        ""AGI_INSIGHT_BUS_KEY"": key,
        ""AGI_INSIGHT_BUS_TOKEN"": ""tok"",
        ""AGI_INSIGHT_LEDGER_PATH"": str(tmp_path / ""ledger.db""),
        ""AGI_INSIGHT_OFFLINE"": ""1"",
    }

    with mock.patch.dict(os.environ, env, clear=True):
        importlib.reload(config)
        importlib.reload(orchestrator)
        orch = orchestrator.Orchestrator(config.Settings())

        async def run() -> None:
            await orch.bus.start()
            assert orch.bus._server is not None
            try:
                creds = grpc.ssl_channel_credentials(root_certificates=ca)
                async with grpc.aio.secure_channel(f""localhost:{port}"", creds) as ch:
                    stub = ch.unary_unary(""/bus.Bus/Send"")
                    payload = {
                        ""sender"": ""a"",
                        ""recipient"": ""b"",
                        ""payload"": {},
                        ""ts"": 0.0,
                        ""token"": ""bad"",
                    }
                    with pytest.raises(grpc.aio.AioRpcError):
                        await stub(json.dumps(payload).encode())
            finally:
                await orch.bus.stop()
                await orch.ledger.stop_merkle_task()
                orch.ledger.close()

        asyncio.run(run())",tests/test_orchestrator_bus_tls_env.py,
survived,"    def create(self, *args: Any, **kwargs: Any) -> Any:  # pragma: no cover - stub
        raise NotImplementedError(""OpenAI SDK not available"")
",src/meta_agent/services/openai_stub.py,_ChatCompletions
survived,"def intToChurch(i):
    if i == 0:
        return zero
    return succ(intToChurch(i - 1))
",tests/rosetta/transpiler/Python/church-numerals-1.py,
survived,"def runVM(prog):
    data = []
    i = 0
    while i < prog[""dataSize""]:
        data = data + [0]
        i = i + 1
    stack = []
    pc = 0
    code = prog[""code""]
    addrMap = prog[""addrMap""]
    pool = prog[""strings""]
    while pc < len(code):
        inst = code[pc]
        op = inst[""op""]
        arg = inst[""arg""]
        if op == ""push"":
            stack = stack + [arg]
            pc = pc + 1
            continue
        if op == ""store"":
            data[arg] = stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""fetch"":
            stack = stack + [data[arg]]
            pc = pc + 1
            continue
        if op == ""add"":
            stack[len(stack) - 2] = stack[len(stack) - 2] + stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""lt"":
            v = 0
            if stack[len(stack) - 2] < stack[len(stack) - 1]:
                v = 1
            stack[len(stack) - 2] = v
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""jz"":
            v = stack[len(stack) - 1]
            stack = slice(stack, 0, len(stack) - 1)
            if v == 0:
                pc = addrMap[arg]
            else:
                pc = pc + 1
            continue
        if op == ""jmp"":
            pc = addrMap[arg]
            continue
        if op == ""prts"":
            print(pool[stack[len(stack) - 1]])
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""prti"":
            print(str(stack[len(stack) - 1]))
            stack = slice(stack, 0, len(stack) - 1)
            pc = pc + 1
            continue
        if op == ""halt"":
            break
        pc = pc + 1
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,
survived,"def circles(p1, p2, r):
    if p1.x == p2.x and p1.y == p2.y:
        if r == 0.0:
            return [p1, p1, ""Coincident points with r==0.0 describe a degenerate circle.""]
        return [p1, p2, ""Coincident points describe an infinite number of circles.""]
    if r == 0.0:
        return [p1, p2, ""R==0.0 does not describe circles.""]
    dx = p2.x - p1.x
    dy = p2.y - p1.y
    q = hypot(dx, dy)
    if q > 2.0 * r:
        return [p1, p2, ""Points too far apart to form circles.""]
    m = Point(x=(p1.x + p2.x) / 2.0, y=(p1.y + p2.y) / 2.0)
    if q == 2.0 * r:
        return [m, m, ""Points form a diameter and describe only a single circle.""]
    d = sqrtApprox(r * r - q * q / 4.0)
    ox = d * dx // q
    oy = d * dy // q
    return [Point(x=m.x - oy, y=m.y + ox), Point(x=m.x + oy, y=m.y - ox), ""Two circles.""]
",tests/rosetta/transpiler/Python/circles-of-given-radius-through-two-points.py,
survived,"def bigFromInt(x):
    if x == 0:
        return [0]
    digits = []
    n = x
    while n > 0:
        digits = digits + [n % 10]
        n = n // 10
    return digits
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,
survived,"def commatize(n):
    s = str(n)
    out = """"
    i = len(s) - 1
    c = 0
    while i >= 0:
        out = """".join(s[i:i + 1]) + out
        c = c + 1
        if c % 3 == 0 and i > 0:
            out = "","" + out
        i = i - 1
    return out
",tests/rosetta/transpiler/Python/composite-numbers-k-with-no-single-digit-factors-whose-factors-are-all-substrings-of-k.py,
survived,"def pow2(k):
    r = 1
    i = 0
    while i < k:
        r = r * 2
        i = i + 1
    return r
",tests/rosetta/transpiler/Python/chernicks-carmichael-numbers.py,
survived,"def test_parse_time_spec_variants():
    assert parse_time_spec(""30n"") == {""type"": ""n"", ""value"": 30}
    assert parse_time_spec(15) == {""type"": ""n"", ""value"": 15}
    assert parse_time_spec(""24h"") == {""type"": ""time"", ""value"": timedelta(hours=24)}
    assert parse_time_spec(""45m"") == {""type"": ""time"", ""value"": timedelta(minutes=45)}
    assert parse_time_spec(""7d"") == {""type"": ""time"", ""value"": timedelta(days=7)}
    assert parse_time_spec(None) == {""type"": ""n"", ""value"": DEFAULT_LAST_N}
    with pytest.raises(ValueError):
        parse_time_spec(""abc"")
",tests/test_dashboard.py,
survived,"    def __str__(self) -> str:
        return self.message
",src/meta_agent/ux/error_handler.py,UXError
survived,"    def handle(self, error: UXError) -> None:
        """"""Log the error and display the message via CLI.""""""
        msg = str(error)
        if error.context:
            self.logger.error(""%s | context=%s"", msg, error.context)
        else:
            self.logger.error(msg)
        try:
            self.cli_output.error(msg)
        except Exception:
            self.logger.error(""Failed to output error message via CLI"")",src/meta_agent/ux/error_handler.py,ErrorHandler
survived,"    def fail(*args, **kwargs):
        raise OSError(""boom"")
",tests/ux/test_cli_output.py,
survived,"def test_diagram_generation_error_subclass():
    assert issubclass(DiagramGenerationError, UXError)",tests/ux/test_error_handler.py,
survived,"        def copy(self, text):
            copied[""text""] = text
",tests/ux/test_user_feedback.py,Dummy
survived,"    def notify(self, message: str, severity: NotificationSeverity = NotificationSeverity.INFO) -> None:
        """"""Display a notification with the appropriate style.""""""
        if severity is NotificationSeverity.SUCCESS:
            self.cli_output.success(message)
        elif severity is NotificationSeverity.WARNING:
            self.cli_output.warning(message)
        elif severity in (NotificationSeverity.ERROR, NotificationSeverity.CRITICAL):
            self.cli_output.error(message)
        else:
            self.cli_output.info(message)
",src/meta_agent/ux/user_feedback.py,UserFeedback
survived,"    def metadata(self) -> BundleMetadata:
        if self._metadata is None:
            self.refresh_metadata()
        assert self._metadata is not None
        return self._metadata
",src/meta_agent/bundle.py,Bundle
survived,"def test_bundle_load_and_list_files(tmp_path: Path) -> None:
    gen = BundleGenerator(tmp_path)
    gen.generate(agent_code=""print('hi')"")

    b = Bundle(tmp_path)
    assert b.metadata.schema_version
    files = b.list_files()
    assert ""agent.py"" in files
    assert b.read_text(""agent.py"").strip() == ""print('hi')""
",tests/test_bundle_api.py,
survived,"def test_bundle_generator_hooks(tmp_path: Path) -> None:
    calls: list[str] = []

    def pre(path: Path) -> None:
        calls.append(""pre"")

    def post(path: Path, meta) -> None:
        calls.append(""post"")

    gen = BundleGenerator(tmp_path)
    gen.generate(agent_code=""print('x')"", pre_hook=pre, post_hook=post)

    assert calls == [""pre"", ""post""]",tests/test_bundle_api.py,
survived,"    def __exit__(self, exc_type: object, exc: object, tb: object) -> None:
        """"""Ensure the database connection is closed.""""""
        self.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"    async def invoke_tool(self, name: str, args: dict[str, object] | None = None) -> object:
        """"""Invoke a tool by name using :class:`mcp.ClientSessionGroup`.""""""
        args = args or {}
        return await self._group.call_tool(name, args)",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mcp_adapter.py,MCPAdapter
survived,"    async def patched_handle(self, env):
        text = self.adk.generate_text(env.payload.get(""plan"", """"))
        await self.emit(""strategy"", {""research"": text})
",tests/test_adapters.py,
survived,"        def close(self) -> None:
            pass
",tests/test_adapters.py,DummyLedger
survived,"def test_missing_code_defaults_to_ok(payload: dict[str, object]) -> None:
    bus = DummyBus(config.Settings(bus_port=0))
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(""src"", ""safety"", payload, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""ok""
",tests/test_safety_guardian_property.py,
survived,"        def __init__(self, val: str) -> None:  # pragma: no cover - dummy
            pass
",tests/test_safety_guardian_property.py,DummyPk
survived,"    def __init__(self, *args, **kwargs):
        pass
",openai_agents/__init__.py,AgentRuntime
survived,"    def score(self, genome: str | Iterable[float]) -> float:
        tokens = str(genome).lower().split()
        best = 0.0
        for ex in self.examples:
            sim = self._jaccard(tokens, ex.lower().split())
            if sim > best:
                best = sim
        noise = self.rng.random() * 0.001
        val = best + noise
        return float(min(1.0, max(0.0, val)))",src/evaluators/feasibility_critic.py,FeasibilityCritic
survived,"            def _create_resolver(f_name=field_name, model=sa_model, target=target_model):
                async def func(entity_id: int, ctx: EnrichContext) -> Any | None:
                    session_factory = ctx.request_context.lifespan_context[session_key]
                    async with session_factory() as session:  # type: AsyncSession
                        obj = await session.get(model, entity_id)
                        if not obj:
                            return None
                        await session.refresh(obj, [f_name])
                        value = getattr(obj, f_name)
                        return _sa_to_enrich(value, target) if value else None

                return func
",src/enrichmcp/sqlalchemy/auto.py,
survived,"def test_concurrent_experiments(tmp_path, monkeypatch) -> None:
    monkeypatch.setenv(""ARCHIVE_PATH"", str(tmp_path / ""arch.db""))
    monkeypatch.setenv(""SOLUTION_ARCHIVE_PATH"", str(tmp_path / ""sol.duckdb""))
    settings = config.Settings(bus_port=0)
    with mock.patch.object(orchestrator.Orchestrator, ""_init_agents"", lambda self: []):
        orch = orchestrator.Orchestrator(settings)

    import numpy as np

    monkeypatch.setattr(""src.evaluators.novelty.embed"", lambda _t: np.zeros((1, 1), dtype=""float32""))
    monkeypatch.setattr(""src.simulation.surrogate_fitness.aggregate"", lambda vals, **kw: [0.0 for _ in vals])
    times = []

    def dummy_run(*_a, **_kw):
        times.append(time.perf_counter())
        time.sleep(0.2)
        ind = orchestrator.mats.Individual([0.0])
        ind.score = 0.0
        return [ind]

    monkeypatch.setattr(orchestrator.mats, ""run_evolution"", dummy_run)

    def fn(genome: list[float]) -> tuple[float]:
        time.sleep(0.2)
        return (sum(genome),)

    async def run() -> None:
        await asyncio.gather(
            orch.evolve(""a"", fn, 1, experiment_id=""exp1"", population_size=2, generations=1),
            orch.evolve(""b"", fn, 1, experiment_id=""exp2"", population_size=2, generations=1),
        )

    asyncio.run(run())
    assert len(times) == 2
    assert abs(times[0] - times[1]) < 0.05
    assert ""exp1"" in orch.experiment_pops
    assert ""exp2"" in orch.experiment_pops
    assert orch.experiment_pops[""exp1""] is not orch.experiment_pops[""exp2""]

    specs = [json.loads(row[0])[""experiment_id""] for row in orch.archive.conn.execute(""SELECT spec FROM entries"")]
    assert {""exp1"", ""exp2""} <= set(specs)",tests/test_experiments.py,
survived,"    def recent_memory(self, agent: str, n: int = 5) -> list[Any]:
        """"""Fetch the agent's most recent memory entries.""""""
        url = f""{self.base_url}/memory/{agent}/recent""
        resp = requests.get(url, params={""n"": n})
        resp.raise_for_status()
        return resp.json()
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,MarketplaceClient
survived,"def call_local_model(prompt_messages: list[dict[str, str]]) -> str:
    """"""Return a response from a local model (placeholder implementation).""""""
    return """"
",alpha_factory_v1/demos/self_healing_repo/agent_core/llm_client.py,
survived,"def test_get_tables_trailing_semicolon():
    dialect = ""bigquery""

    query = ""SELECT * FROM table1;""
    expected = {""tables"": [""table1""]}
    assert get_tables(query, dialect) == expected

    query = ""SELECT * FROM table1;;""
    expected = {""tables"": [""table1""]}
    assert get_tables(query, dialect) == expected
",pythonsrc/parser/main_test.py,
survived,"def banner() -> str:
    """"""Return the standard startup banner.""""""
    return (
        ""\N{MILITARY MEDAL} \N{GREEK SMALL LETTER ALPHA}\N{HYPHEN-MINUS}AGI""
        "" Insight \N{EYE}\N{SPARKLES} â€” Beyond Human Foresight""
    )
",alpha_factory_v1/demos/alpha_agi_insight_v0/openai_agents_bridge.py,
survived,"  def supports_active_cooling(self) -> bool:
    return False
",pylabrobot/temperature_controlling/chatterbox.py,TemperatureControllerChatterboxBackend
deleted,"  def supports_active_cooling(self) -> bool:
    return False",pylabrobot/heating_shaking/chatterbox.py,HeaterShakerChatterboxBackend
survived,"    def __getitem__(self, idx):
        return self.data[idx]
",no-ocr-api/tests/test_ingest_search.py,FakeDataset
survived,"    def fake_convert_from_path(*args, **kwargs):
        return [Image.new(""RGB"", (10, 10)), Image.new(""RGB"", (10, 10))]
",no-ocr-api/tests/test_ingest_search.py,
survived,"        def extract_text(self):
            return self.text
",no-ocr-api/tests/test_ingest_search.py,FakePage
survived,"def test_vllm_call_missing_dataset(client):
    response = client.post(
        ""/vllm_call"",
        data={
            ""user_query"": ""foo"",
            ""user_id"": ""user"",
            ""case_name"": ""case"",
            ""pdf_name"": ""x.pdf"",
            ""pdf_page"": 1,
        },
    )
    assert response.status_code == 404
",no-ocr-api/tests/test_ingest_search.py,
survived,"def main() -> None:
    app = EnrichMCP(title=""Hello HTTP API"", description=""A simple HTTP example"")

    @app.retrieve(description=""Say hello over HTTP"")
    async def hello_http() -> dict[str, str]:
        return {""message"": ""Hello over HTTP!""}

    print(""Starting HTTP server on http://localhost:8000 ..."")
    app.run(transport=""streamable-http"")
",examples/hello_world_http/app.py,
survived,"        async def send_and_wait(self, topic: str, data: bytes) -> None:
            return None
",tests/test_bus_large_payloads_property.py,Prod
survived,"def test_adk_generate_text_unreachable(httpx_mock, stub_adk):
    httpx_mock.add_exception(httpx.ConnectError(""offline""), url=""https://adk.example/generate"")
    adapter = ADKAdapter()
    with pytest.raises(httpx.HTTPError):
        adapter.generate_text(""hi"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,
survived,"def test_check_gzip_call_present() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    text = (browser_dir / ""manual_build.py"").read_text()
    assert ""def check_gzip_size"" in text
    pattern = r""write_text\(bundle\).*\n\s*check_gzip_size\(dist_dir / \""insight.bundle.js\""\)""
    assert re.search(pattern, text)
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_manual_build_size_limit.py,
survived,"def get(
    url: str,
    *,
    params: dict | None = None,
    headers: dict | None = None,
    timeout: float | None = None,
) -> Response:
    """"""Perform a simple HTTP GET request.""""""
    return _call(""GET"", url, params=params, headers=headers, timeout=timeout)
",alpha_factory_v1/af_requests.py,
survived,"    def _load_manifest(self) -> Dict[str, Any]:
        try:
            with open(self.manifest_path, ""r"", encoding=""utf-8"") as f:
                return json.load(f)
        except (IOError, json.JSONDecodeError):
            logger.warning(""Failed to read template registry manifest. Recreating."")
            return {}
",src/meta_agent/template_registry.py,TemplateRegistry
survived,"    async def _combined(app: EnrichMCP) -> AsyncIterator[dict[str, Any]]:
        async with AsyncExitStack() as stack:
            merged: dict[str, Any] = {}
            for ls in lifespans:
                ctx = await stack.enter_async_context(ls(app))
                if isinstance(ctx, dict):
                    merged.update(ctx)
            yield merged
",src/enrichmcp/lifespan.py,
survived,"    def _validate(self, t: Triplet) -> bool:
        if (
            len(t.program.splitlines()) > MAX_PROG_LOC
            or len(t.inp) > 256
            or len(t.out) > 256
            or any(b in t.program for b in _BANNED)
        ):
            return False
        stdout, stderr = _exec_trusted(t.program, t.inp)
        return stderr == """" and stdout.strip() == t.out.strip()
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"    def __repr__(self) -> str:
        return f""<AZREngine buf={len(self.buffer)} T={self.temperature:.2f}>""
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"    def learn(self, results: Sequence[TaskResult]) -> None:
        if not results:
            return
        # Multiâ€‘objective scalarisation (simple): reward = unsolved_ratio * 0.7 + novelty * 0.3
        solved_frac = sum(r.solved for r in results) / len(results)
        diff_reward = 1.0 - solved_frac
        novelty = sum(r.complexity for r in results) / len(results)
        novelty_norm = min(1.0, novelty / 15.0)
        reward = 0.7 * diff_reward + 0.3 * novelty_norm

        beta = 0.1
        self._baseline = (1 - beta) * self._baseline + beta * reward
        adv = reward - self._baseline
        # PPOâ€‘lite temperature adjust
        delta = -0.04 if adv > 0 else 0.04
        self.temperature = max(0.1, min(1.0, self.temperature + delta))

        # Buffer maintenance â€“ keep correctly solved tasks
        for r in results:
            if r.solved:
                self._add(r.triplet)

        self.log(f""[AZR] reward={reward:.3f} adv={adv:+.3f} -> T={self.temperature:.2f}"")

        examples = (
            ""\n\n"".join(
                f""```python\n{t.program}```\n```json\n{t.inp}```\n```json\n{t.out}```""
                for t in self._rng.sample(self.buffer, k=min(3, len(self.buffer)))
            )
            or ""(buffer empty)""
        )

        return self._PROMPT.format(
            n=n,  # noqa: F821
            max_loc=MAX_PROG_LOC,
            buf=len(self.buffer),
            examples=examples,
        )
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"    def tearDown(self) -> None:
        asyncio.run(self.orch.bus.stop())
        asyncio.run(self.orch.ledger.stop_merkle_task())
        self.orch.ledger.close()
        self.tmp.cleanup()
",tests/test_insight_orchestrator_features.py,TestInsightOrchestrator
survived,"def test_timeline_df() -> None:
    secs = [sector.Sector(""a""), sector.Sector(""b"")]
    traj = forecast.forecast_disruptions(secs, 2, pop_size=2, generations=1)
    df = web_app.timeline_df(traj)
    assert set(df.columns) == {""year"", ""sector"", ""energy"", ""disrupted""}
    assert len(df) == 4
",tests/test_web_app.py,
survived,"def _disruption_df(traj: list[Any]) -> ""pd.DataFrame"":
    """"""Return the first disruption year per sector.""""""

    import pandas as pd

    years: dict[str, int] = {}
    for point in traj:
        for sec in point.sectors:
            if sec.disrupted and sec.name not in years:
                years[sec.name] = point.year
    return pd.DataFrame({""sector"": list(years.keys()), ""year"": list(years.values())})
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/web_app.py,
survived,"    async def ws_progress(websocket: WebSocket) -> None:
        """"""Stream year-by-year progress updates to the client.""""""
        await websocket.accept()
        _progress_ws.add(websocket)
        try:
            while True:
                await websocket.receive_text()
        except Exception:
            pass
        finally:
            _progress_ws.discard(websocket)
",src/interface/api_server.py,
survived,"        def __init__(self, market_data: list[int] | None = None) -> None:
            super().__init__()
            self.market_data = market_data
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,MATSAgent
survived,"def test_assets_replaced() -> None:
    _assert_no_placeholder(BASE / ""lib"" / ""workbox-sw.js"")
    _assert_no_placeholder(BASE / ""lib"" / ""bundle.esm.min.js"")
    _assert_no_placeholder(BASE / ""dist"" / ""workbox-sw.js"")
    _assert_no_placeholder(BASE / ""dist"" / ""bundle.esm.min.js"")",tests/test_assets_replaced.py,
survived,"def load_documents(data_dir: str):
    """"""Load and preprocess all text files under *data_dir*.""""""
    corpus = []
    for filename in sorted(glob.glob(os.path.join(data_dir, ""*.txt""))):
        with open(filename, ""r"", encoding=""utf-8"", errors=""ignore"") as f:
            text = f.read().lower()
        tokens = [t for t in TOKEN_RE.findall(text) if t not in STOPWORDS]
        corpus.append(tokens)
    return corpus
",scripts/bbc_demo.py,
survived,"def test_ideal_sensor_readings_near_one() -> None:
    """"""Typical ideal metrics should yield ~1.0.""""""
    sensors = {""steps"": 10_000, ""resting_hr"": 60, ""sleep_hours"": 8, ""cal_intake"": 2100}
    value = fr.reward(None, None, sensors)
    assert 0.0 <= value <= 1.0
    assert value == approx(1.0, rel=0, abs=1e-7)
",tests/test_fitness_reward.py,
survived,"def test_llama_paged_decode_matches_full_prefill():
    """"""Ensure llama paged decode matches full forward when prefilling entire sequences.""""""
    Pos = Axis(""position"", 16)
    Embed = Axis(""embed"", 8)
    Vocab = Axis(""vocab"", 64)

    cfg = LlamaConfig(
        seq_len=Pos.size,
        hidden_dim=Embed.size,
        intermediate_dim=16,
        num_layers=2,
        num_heads=2,
        num_kv_heads=2,
        rope=None,
        gradient_checkpointing=False,
        scan_layers=True,
        attn_backend=AttentionBackend.VANILLA,
    )

    model_key, input_key = jrandom.split(jrandom.PRNGKey(0))
    model = LlamaLMHeadModel.init(Vocab=Vocab, config=cfg, key=model_key)

    input_ids = hax.random.randint(input_key, Pos, 0, Vocab.size)

    pt = PageTable.init(max_pages=4, max_seqs=2, page_size=4, max_pages_per_seq=4)
    pt, seq1 = pt.assign_seq_id_to_seq()
    pt, seq2 = pt.assign_seq_id_to_seq()

    seq_ids = hax.named([seq1, seq2, -1, -1, -1, -1, -1, -1], ""seq"")
    new_token_counts = hax.named([4, 3, 0, 0, 0, 0, 0, 0], ""seq"")
    seg_ids = hax.named([0] * 4 + [1] * 3 + [-1] * 9, ""position"")
    pt, binfo = pt.allocate_for_seqs(updated_seqs=seq_ids, new_counts=new_token_counts, tokens=seg_ids)

    mask = AttentionMask.causal().with_segment_ids(seg_ids)
    full_out = model.activations(input_ids, attn_mask=mask, key=jrandom.PRNGKey(1))

    layer_caches = model.transformer.initial_cache(pt, dtype=jnp.float32)
    page_state = KvPageState.from_batch(binfo, layer_caches)
    pos_ids = hax.arange(Pos, dtype=jnp.int32)
    x = model.embeddings.embed(input_ids)
    decode_out, _ = _jit_paged_decode(model.transformer, x, pos_ids, page_state)

    full_out = full_out[""position"", hax.dslice(0, 7)]
    decode_out = decode_out[""position"", hax.dslice(0, 7)]
    assert_trees_all_close(full_out.array, decode_out.array, atol=1e-4, rtol=1e-4)
",tests/test_llama_decode.py,
survived,"def valid_spec_dict():
    return {
        ""task_description"": ""Test agent for CLI"",
        ""inputs"": {""data"": ""string""},
        ""outputs"": {""status"": ""string""},
        ""constraints"": [""Must run quickly""],
        ""technical_requirements"": [""Python 3.10+""],
        ""metadata"": {""test_id"": ""cli-001""},
    }
",tests/integration/test_telemetry_integration.py,
survived,"async def test_build_regex_guardrails_multiple_rules():
    cfg = GuardrailConfig(
        rules=[
            GuardrailRule(name=""a"", pattern=""foo""),
            GuardrailRule(name=""b"", pattern=""bar""),
        ]
    )
    guards = build_regex_guardrails(cfg)
    assert len(guards) == 2

    await guards[0](""nothing here"")
    await guards[1](""nothing here"")

    with pytest.raises(ValueError):
        await guards[0](""foo"")
    with pytest.raises(ValueError):
        await guards[1](""bar"")",tests/test_guardrail_generator.py,
survived,"    def __init__(self, bus: orchestrator.messaging.A2ABus, ledger: orchestrator.Ledger) -> None:  # type: ignore[override]
        super().__init__(""boom"", bus, ledger)
        self.first = True
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,BoomAgent
survived,"    async def loop_no_catch(self: orchestrator.AgentRunner, bus, ledger) -> None:
        await self.agent.run_cycle()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,
survived,"def test_compute_max_widths_strips_colors():
    data = [['1.1.1.1', 'AWS', '\x1b[31mno permission\x1b[0m']]
    widths = compute_max_widths(data)
    assert widths['Permission'] >= len('no permission') + 2",tests/test_whois_perms.py,
survived,"        def close(self) -> None:
            pass
",tests/test_agents.py,DummyLedger
survived,"def test_surrogate_pair_split_digits() -> None:
    chunks = ['{""a"": ""\\u', 'd83d\\u', 'de00""}']
    parsed = _stream_to_dict({}, chunks)
    assert parsed == {""a"": ""ðŸ˜€""}
",api/core/utils/streams_test.py,
survived,"def test_unicode_escape_sequence() -> None:
    chunks = ['{""a"": ""\\', 'u00e9""}']
    parsed = _stream_to_dict({}, chunks)
    assert parsed == {""a"": ""Ã©""}
",api/core/utils/streams_test.py,
survived,"def test_surrogate_pair_split() -> None:
    chunks = ['{""a"": ""\\ud83', 'd\\ude00""}']
    parsed = _stream_to_dict({}, chunks)
    assert parsed == {""a"": ""ðŸ˜€""}
",api/core/utils/streams_test.py,
survived,"def test_Q3_returns_revenue_per_order_with_correct_priority():
    assert order_line_join == [
        {
            ""l_orderkey"": 100,
            ""revenue"": 1000 * 0.95 + 500,
            ""o_orderdate"": ""1995-03-14"",
            ""o_shippriority"": 1,
        }
    ]
",tests/machine/x/python/q3.py,
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Supplier
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q2.py,Part
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q2.py,Supplier
survived,"    def data_model_tool_name(self) -> str:
        """"""Return the name of the built-in data model exploration tool.""""""

        return f""explore_{self.name.lower().replace(' ', '_')}_data_model""
",src/enrichmcp/app.py,EnrichMCP
survived,"    def fake_eval(agent, model):
        return agent.score + 1
",tests/test_transfer_test.py,
survived,"def run() -> None:
    n = 24
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_024.py,
survived,"def run() -> None:
    n = 4
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_004.py,
survived,"def run() -> None:
    n = 3
    total = sum(range(n))
    expected = n*(n-1)//2
    assert total == expected",benchmarks/swe_mini/task_003.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""7""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(7)",benchmarks/poly_mini/task_007.py,
survived,"def test_mixed_test_and_src_patch() -> None:
    diff = (
        ""--- a/src/foo.py\n""
        ""+++ b/src/foo.py\n""
        ""@@\n""
        ""-a\n""
        ""+b\n""
        ""--- a/tests/bar.py\n""
        ""+++ b/tests/bar.py\n""
        ""@@\n""
        ""-x\n""
        ""+y\n""
    )
    assert is_patch_valid(diff)",tests/test_patch_guard.py,
survived,"    def _patched_init_subclass(cls, **kwargs: Any) -> None:  # type: ignore[override]
        _orig_init_subclass(**kwargs)
        _ensure_pydantic_methods(cls)
",src/meta_agent/__init__.py,
survived,"    async def _optimise_async(self, sequence: str) -> Dict[str, Any]:
        """"""Minimal GC-content optimisation when heavy libs are absent.""""""
        gc_orig = sequence.count(""G"") + sequence.count(""C"")
        gc_new_seq = sequence.replace(""A"", ""G"")
        gc_new = gc_new_seq.count(""G"") + gc_new_seq.count(""C"")
        delta = (gc_new - gc_orig) / len(sequence)
        return {
            ""optimised_sequence"": gc_new_seq,
            ""delta_stability"": round(delta, 4),
        }
",alpha_factory_v1/backend/agents/biotech_agent.py,BiotechAgent
survived,"    def optimise(self, sequence: str) -> Dict[str, Any]:  # noqa: D401
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(self._optimise_async(sequence))
",alpha_factory_v1/backend/agents/biotech_agent.py,BiotechAgent
survived,"    def test_momentum(self):
        prices = [1, 2, 3, 4, 5]
        self.assertAlmostEqual(am.momentum(prices, lookback=4), 4.0)
        self.assertEqual(am.momentum(prices, lookback=10), 0.0)
",alpha_factory_v1/tests/test_alpha_model.py,AlphaModelTest
survived,"    def test_ema(self):
        prices = [1] * 5 + [10]
        self.assertGreater(am.ema(prices, span=3), 1)
",alpha_factory_v1/tests/test_alpha_model.py,AlphaModelTest
survived,"    async def __aenter__(self) -> ""SimulatedMarketData"":
        return self
",alpha_factory_v1/backend/market_data.py,SimulatedMarketData
survived,"    async def close(self) -> None:  # pragma: no cover - interface default
        """"""Gracefully close underlying resources.""""""
        await self.__aexit__(None, None, None)
",alpha_factory_v1/backend/market_data.py,BaseMarketData
survived,"    async def __aenter__(self) -> ""PolygonMarketData"":
        await self._client()
        return self
",alpha_factory_v1/backend/market_data.py,PolygonMarketData
survived,"    def close(self) -> None:
        """"""Close any open database connections.""""""
        if self._driver:
            try:
                self._driver.close()
            except Exception as exc:  # pragma: no cover - defensive
                _log.warning(""Neo4j driver close failed (%s)"", exc)
            finally:
                self._driver = None
",alpha_factory_v1/backend/memory_graph.py,GraphMemory
survived,"    async def run_cycle(self):  # noqa: D401
        self.ran = True
",alpha_factory_v1/tests/test_planner_agent.py,DummyAgent
survived,"    def to_json(self) -> str:
        return json.dumps(asdict(self), separators=("","", "":""))
",alpha_factory_v1/backend/portfolio.py,Fill
survived,"    def reset(self):
        self.agent = (0,0)
        return self._obs()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MiniWorld
survived,"    def _obs(self):
        vec = np.zeros(self.size*self.size, dtype=np.float32)
        vec[self.agent[0]*self.size+self.agent[1]] = 1.0
        return vec
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,MiniWorld
survived,"    def _on(self, msg: dict):
        try:
            self.handle(msg)
        except Exception as exc:
            LOG.exception(""[%s] crash: %s"", self.name, exc)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Agent
survived,"def _cfg_from_env() -> dict[str, Any]:
    mapping: dict[str, str] = {
        ""SUCCESS_THRESHOLD"": ""success_thresh"",
        ""MAX_SIM_MINUTES"": ""max_minutes"",
        ""MICRO_CURRICULUM"": ""micro_curr"",
        ""AGIALPHA_SUPPLY"": ""max_supply"",
    }
    cfg: dict[str, Any] = {}
    for k, v in CFG_DEFAULTS.items():
        key = mapping.get(k, k.lower())
        val = os.getenv(f""OMNI_{k}"", v)
        if k.endswith(""PATH""):
            cfg[key] = Path(val)
        else:
            cfg[key] = type(CFG_DEFAULTS[k])(val)
    return cfg
",alpha_factory_v1/demos/omni_factory_demo/omni_factory_demo.py,
survived,"    def test_parse_defaults(self):
        args = self._parse([])
        self.assertEqual(args.port, 8000)
        self.assertIsNone(args.agents)
        self.assertIsNone(args.metrics_port)
        self.assertIsNone(args.a2a_port)
        self.assertIsNone(args.cycle)
        self.assertEqual(args.loglevel, ""INFO"")
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest
survived,"    def test_invalid_port(self):
        with self.assertRaises(SystemExit):
            self._parse([""--port"", ""-1""])
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest
survived,"def on_join():
    domain = domain_var.get().strip()
    user = user_var.get().strip()
    ou = ou_var.get().strip()
    if not domain:
        messagebox.showerror(""Error"", ""Domain is required"")
        return
    if not user:
        messagebox.showerror(""Error"", ""Admin user is required"")
        return
    output, code = join_domain(domain, user, ou)
    if code == 0:
        messagebox.showinfo(""Success"", f""Successfully joined {domain}"")
    else:
        messagebox.showerror(""Join Failed"", output or ""Unknown error"")
",adconnection_gui.py,
survived,"    def root(self):
        return {""ok"": True}
",tests/test_core/test_decorators/test_guard.py,GuardController
survived,"def list_and_download(username: str, amount: int = 10):
    """"""Download recent posts from the specified account.""""""
    cl = login_with_persistence()
    user_id = cl.user_id_from_username(username)
    for media in cl.user_medias(user_id, amount=amount):
        if media.media_type == 1:
            cl.photo_download(media.pk)
        elif media.media_type == 2:
            cl.video_download(media.pk)
        elif media.media_type == 8:
            cl.album_download(media.pk)
",examples/session_login.py,
survived,"def test_error_propagation(monkeypatch, caplog):
    def fail_secho(*args, **kwargs):
        raise OSError(""boom"")

    # Force CLI output to fail so CLIOutputError is raised
    monkeypatch.setattr(click, ""secho"", fail_secho)
    cli = CLIOutput()

    with pytest.raises(CLIOutputError) as exc:
        cli.info(""hi"")

    # Restore working output for error handling path
    monkeypatch.setattr(click, ""secho"", lambda *a, **k: None)
    handler = ErrorHandler(cli_output=cli, log=logging.getLogger(""test""))
    with caplog.at_level(logging.ERROR):
        handler.handle(exc.value)
    assert ""failed to write output"" in caplog.text

    # Diagram generation failure should propagate through ErrorHandler
    generator = DiagramGenerator()
    with caplog.at_level(logging.ERROR):
        try:
            generator.generate(None)  # type: ignore[arg-type]
        except DiagramGenerationError as dg_err:
            handler.handle(dg_err)
    assert ""spec must be a mapping"" in caplog.text

    feedback = UserFeedback(cli_output=cli)
    suggestion = feedback.error_suggestion(""Failed to load file"")
    assert suggestion and ""file path exists"" in suggestion",tests/integration/test_ux_interactions.py,
survived,"    async def readiness() -> str:
        """"""Check orchestrator background task.""""""

        task = getattr(app_f.state, ""task"", None)
        if task and not task.done():
            return ""ok""
        # If the orchestrator failed to start, return OK for local tests.
        return ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    def getExpressionIds(self):
        exprs = list(map(str, self.importer.flux_ids))
        exprs += [str(k) for k in self.importer.symbols.get(SymbolId.EXPRESSION, {}).keys()]
        return exprs
",tests/testSBMLSuiteJax.py,DummyModel
survived,"def result_path_jax() -> Path:
    return Path(__file__).parent / ""amici-semantic-results-jax""
",tests/testSBMLSuiteJax.py,
survived,"    def getParameterIds(self):
        return list(map(str, self.jax_model.parameter_ids))
",tests/testSBMLSuiteJax.py,DummyModel
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q1.py,Auto1
survived,"def test_moe_linear_gmm_matches_ragged_dot_general():
    B, In, Out, E = hax.make_axes(B=3, In=4, Out=5, E=2)
    moe = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(0), use_gmm=True)

    x = hax.random.normal(jrandom.PRNGKey(1), (B, In))
    group_sizes = hax.named(jnp.array([2, 1], dtype=jnp.int32), (E,))

    with jax.sharding.Mesh(jax.devices(), (""data"",)):
        actual = moe(x, group_sizes)

    expected = _expected_moe_linear_output(moe, x, group_sizes)

    assert actual.axes == expected.axes
    assert jnp.allclose(actual.array, expected.array, rtol=1e-5, atol=1e-5)",tests/test_moe_linear.py,
survived,"def main() -> int:
    missing = []
    for pkg in REQUIRED:
        if importlib.util.find_spec(pkg) is None:
            missing.append(pkg)
    if missing:
        print('Missing packages:', ', '.join(missing))
        print('Install with: pip install -r requirements.txt')
        return 1
    print('Environment OK')
    return 0
",check_env.py,
survived,"    async def _run() -> None:
        await mgr.start()
        await mgr.stop()
",tests/test_agent_manager_consumer.py,
survived,"        def publish(self, *_a: object, **_kw: object) -> None:
            pass
",tests/test_agent_manager_consumer.py,DummyBus
survived,"    def tearDown(self) -> None:
        agents.AGENT_REGISTRY.clear()
        agents.AGENT_REGISTRY.update(self._reg_backup)
        discovery.FAILED_AGENTS.clear()
        discovery.FAILED_AGENTS.update(self._fail_backup)
",tests/test_failed_agent_discovery.py,TestFailedAgentDiscovery
survived,"            async def stop(self) -> None:
                events.append(""stop"")
",tests/test_message_bus.py,TestMessageBus.Prod
survived,"            async def send_and_wait(self, topic: str, data: bytes) -> None:
                events.append((topic, data))
",tests/test_message_bus.py,TestMessageBus.Prod
survived,"async def _update_listener(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """"""Handle options update.""""""
    await hass.config_entries.async_reload(entry.entry_id)",custom_components/gree/__init__.py,
survived,"def test_root_disclaimer_plain(client: TestClient) -> None:
    """"""Plain text disclaimer is returned by default.""""""

    r = client.get(""/"")
    assert r.status_code == 200
    assert r.text.strip() == DISCLAIMER
",tests/test_insight_api_server.py,
survived,"    def test_selects_long_bonds(self) -> None:
        signals = {
            ""yield_curve"": ""spread -0.5, consider LONG BONDS soon"",
            ""supply_chain"": ""all clear"",
        }
        self.assertEqual(alpha_report.best_alpha(signals), signals[""yield_curve""])
",tests/test_alpha_report.py,TestBestAlpha
survived,"def lookup_musicbrainz(artist: str, session: requests.Session) -> str | None:
    """"""Query MusicBrainz directly for the artist MBID.""""""

    url = ""https://musicbrainz.org/ws/2/artist/""
    params = {""query"": f'artist:""{artist}""', ""fmt"": ""json""}
    headers = {
        ""User-Agent"": ""ArrTools (https://github.com/sirk123au/ArrTools)"",
    }

    try:
        rsp = session.get(url, headers=headers, params=params, timeout=10)
    except requests.RequestException as exc:
        log.error(f""Error searching MusicBrainz for {artist}: {exc}"")
        return None

    if rsp.status_code != 200:
        log.error(
            f""MusicBrainz search failed for {artist}. Status: {rsp.status_code}""
        )
        return None

    data = rsp.json()
    artists = data.get(""artists"")
    if not artists:
        log.error(f""Sorry. We couldn't find {artist} on MusicBrainz"")
        with open(""not_found.txt"", ""a+"", encoding=""utf-8"") as fo:
            fo.write(f""{artist}\n"")
        return None

    return artists[0].get(""id"")
",lidarr_add_from_list.py,
survived,"def _parse_duration(value: str) -> timedelta:
    """"""Parse duration in the format '14d' or '24h'.""""""
    if value.endswith(""d""):
        return timedelta(days=int(value[:-1]))
    if value.endswith(""h""):
        return timedelta(hours=int(value[:-1]))
    raise ValueError(""Invalid duration format. Use Nd or Nh"")
",scripts/rotate_lmdb.py,
survived,"def _should_delete(record: dict[str, Any], threshold: datetime) -> bool:
    """"""Check if the record is older than the threshold.""""""
    timestamp = record.get(""updated_at"") or record.get(""created_at"")
    if not timestamp:
        return False
    try:
        ts = datetime.fromisoformat(timestamp)
    except ValueError:
        return False
    return ts < threshold
",scripts/rotate_lmdb.py,
survived,"def handle_anthropic_parallel_tools(
    response_model: type[Iterable[T]], new_kwargs: dict[str, Any]
) -> tuple[AnthropicParallelBase, dict[str, Any]]:
    if new_kwargs.get(""stream"", False):
        from instructor.exceptions import ConfigurationError

        raise ConfigurationError(
            ""stream=True is not supported when using ANTHROPIC_PARALLEL_TOOLS mode""
        )

    model_types = list(get_types_array(response_model))
    new_kwargs[""tools""] = [m.anthropic_schema for m in model_types]
    new_kwargs[""tool_choice""] = {""type"": ""auto""}

    system_messages = extract_system_messages(new_kwargs.get(""messages"", []))

    if system_messages:
        new_kwargs[""system""] = combine_system_messages(
            new_kwargs.get(""system""), system_messages
        )

    new_kwargs[""messages""] = [
        m for m in new_kwargs.get(""messages"", []) if m[""role""] != ""system""
    ]

    return AnthropicParallelModel(typehint=response_model), new_kwargs
",instructor/process_response.py,
survived,"def clean_text(text: str) -> str:
    """"""Return *text* with markdown emphasis and HTML tags stripped.""""""
    # Unescape HTML entities first
    text = html.unescape(text)
    # Drop HTML tags
    text = re.sub(r""<[^>]+>"", """", text)
    # Remove emphasis markers such as *, _, ** and backticks
    text = re.sub(r""\*\*|__|[*_`]"", """", text)
    return text.strip()
",scripts/generate_gallery_html.py,
survived,"def test_simulate_runs() -> None:
    runner = CliRunner()
    with patch.object(cli, ""asyncio"") as aio:
        aio.run.return_value = None
        with patch.object(cli.orchestrator, ""Orchestrator""):
            res = runner.invoke(cli.main, [""simulate"", ""--horizon"", ""1"", ""--offline"", ""--pop-size"", ""1"", ""--generations"", ""1""])
        assert res.exit_code == 0
        aio.run.assert_called_once()",tests/test_cli.py,
survived,"def test_gibbs_free_energy() -> None:
    logp = [math.log(0.7), math.log(0.3)]
    value = gibbs.free_energy(logp, temperature=1.0, task_cost=1.0)
    entropy = -sum(p * math.log(p) for p in [0.7, 0.3])
    assert value == pytest.approx(1.0 - entropy)",tests/test_forecast.py,
survived,"def test_agent_registration_and_heartbeat() -> None:
    meta = agents.AgentMetadata(
        name=DummyHB.NAME,
        cls=DummyHB,
        version=""0.1"",
        capabilities=DummyHB.CAPABILITIES,
        compliance_tags=[],
    )
    q: Queue = Queue()
    with patch.object(agents, ""_HEALTH_Q"", q):
        agents.register_agent(meta)
        agent = agents.get_agent(""dummy_hb"")
        asyncio.run(agent.step())
        name, _, ok = q.get(timeout=1)
        assert name == ""dummy_hb""
        assert ok
    agents.AGENT_REGISTRY.pop(""dummy_hb"", None)",tests/test_agents.py,
survived,"    def readlines(self, *args):
        return self.inputs.split(""\n"")
",scripts/utils/lcb_runner.py,MockStdinWithBuffer
survived,"def convert_line_to_decimals(line: str) -> tuple[bool, list[Decimal]]:
    try:
        decimal_line = [Decimal(elem) for elem in line.split()]
    except:
        return False, []
    return True, decimal_line
",scripts/utils/lcb_runner.py,
survived,"def grade_call_based(
    code: str, all_inputs: list, all_outputs: list, fn_name: str, timeout: int
):
    # call-based clean up logic
    # need to wrap in try-catch logic after to catch the correct errors, but for now this is fine.
    code = import_string + ""\n\n"" + code
    compiled_sol = compile_code(code, timeout)

    if compiled_sol is None:
        return

    method = get_function(compiled_sol, fn_name)

    if method is None:
        return

    all_inputs = [
        [json.loads(line) for line in inputs.split(""\n"")] for inputs in all_inputs
    ]

    all_outputs = [json.loads(output) for output in all_outputs]

    total_execution = 0
    all_results = []
    for idx, (gt_inp, gt_out) in enumerate(zip(all_inputs, all_outputs)):
        signal.alarm(timeout)
        faulthandler.enable()
        try:
            # can lock here so time is useful
            start = time.time()
            prediction = method(*gt_inp)
            total_execution += time.time() - start
            signal.alarm(0)

            # don't penalize model if it produces tuples instead of lists
            # ground truth sequences are not tuples
            if isinstance(prediction, tuple):
                prediction = list(prediction)

            tmp_result = prediction == gt_out

            # handle floating point comparisons

            all_results.append(tmp_result)

            if not tmp_result:
                return all_results, {
                    ""output"": truncatefn(prediction),
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                    ""error_code"": -2,
                    ""error_message"": ""Wrong Answer"",
                }
        except Exception as e:
            signal.alarm(0)
            if ""timeoutexception"" in repr(e).lower():
                all_results.append(-3)
                return all_results, {
                    ""error"": repr(e),
                    ""error_code"": -3,
                    ""error_message"": ""Time Limit Exceeded"",
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                }
            else:
                all_results.append(-4)
                return all_results, {
                    ""error"": repr(e),
                    ""error_code"": -4,
                    ""error_message"": ""Runtime Error"",
                    ""inputs"": truncatefn(gt_inp),
                    ""expected"": truncatefn(gt_out),
                }

        finally:
            signal.alarm(0)
            faulthandler.disable()

    return all_results, {""execution time"": total_execution}
",scripts/utils/lcb_runner.py,
survived,"def test_archive_insert_and_load(tmp_path) -> None:
    db = tmp_path / ""a.db""
    arch = Archive(db)
    arch.add({""name"": ""a""}, 0.1)
    arch.add({""name"": ""b""}, 0.2)
    rows = arch.all()
    assert len(rows) == 2
    assert rows[0].meta[""name""] == ""a""
",tests/test_archive.py,
survived,"    def all(self) -> List[Agent]:
        with sqlite3.connect(self.path) as cx:
            rows = list(cx.execute(""SELECT id, meta, score FROM agents ORDER BY id""))
        return [Agent(id=r[0], meta=json.loads(r[1]), score=float(r[2])) for r in rows]
",src/archive.py,Archive
survived,"def test_sample_bias(tmp_path) -> None:
    db = tmp_path / ""a.db""
    arch = Archive(db)
    arch.add({""name"": ""low""}, 0.0)
    arch.add({""name"": ""high""}, 1.0)
    random.seed(0)
    counts = {""low"": 0, ""high"": 0}
    for _ in range(50):
        chosen = arch.sample(1)[0]
        counts[chosen.meta[""name""]] += 1
    assert counts[""high""] > counts[""low""]",tests/test_archive.py,
survived,"    def fake_improve(repo, patch, metric, log):
        called.append(repo)
        return 1.0, Path(repo)
",tests/test_scheduler.py,
survived,"async def test_scheduler_runs_jobs(tmp_path):
    jobs_file = tmp_path / ""jobs.json""
    jobs = [
        {""repo"": ""r1"", ""patch"": ""p1"", ""tokens"": 5},
        {""repo"": ""r2"", ""patch"": ""p2"", ""tokens"": 5},
    ]
    jobs_file.write_text(json.dumps(jobs))

    called = []

    def fake_improve(repo, patch, metric, log):
        called.append(repo)
        return 1.0, Path(repo)

    with patch.object(scheduler.self_improver, ""improve_repo"", side_effect=fake_improve):
        sch = scheduler.SelfImprovementScheduler(
            [scheduler.Job(**j) for j in jobs], tokens_quota=10, time_quota=2, interval=""0.1 second""
        )
        await sch.serve()

    assert called == [""r1"", ""r2""]
    assert sch.tokens_used == 10
",tests/test_scheduler.py,
survived,"def test_str_replace_basic(tmp_path: Path) -> None:
    p = tmp_path / ""f.txt""
    p.write_text(""foo bar foo"")
    n = str_replace(p, ""foo"", ""baz"")
    assert n == 2
    assert p.read_text() == ""baz bar baz""
",tests/test_file_ops.py,
survived,"    async def run():
        await evolve(_op, _eval, arch, max_cost=0.1)
",tests/test_evolve.py,
survived,"    def __init__(self) -> None:
        self._items: list[Candidate] = []
",src/evolve.py,InMemoryArchive
survived,"    def setUp(self) -> None:
        self.orig_pub = agents._WHEEL_PUBKEY
        self.orig_sigs = agents._WHEEL_SIGS.copy()
        agents._WHEEL_PUBKEY = PUB_KEY_B64
        sig = SIG_PATH.read_text().strip()
        agents._WHEEL_SIGS = {WHEEL_PATH.name: sig}
",tests/test_verify_wheel.py,VerifyWheelTests
survived,"async def _pygwalker_router(req: Request) -> Response:
    gid = req.path_params[""gid""]
    comm_obj = reflex_comm_map.get(gid, None)
    if comm_obj is None:
        return JSONResponse({""success"": False, ""message"": f""Unknown gid: {gid}""})
    json_data = await req.json()
    result = comm_obj._receive_msg(json_data[""action""], json_data[""data""])
    result = json.dumps(result, cls=DataFrameEncoder)
    return JSONResponse(json.loads(result))
",pygwalker/communications/reflex_comm.py,
survived,"def test_cache_cleanup_on_activate() -> None:
    repo = Path(__file__).resolve().parents[1]
    dist = repo / ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist""

    server, thread = _start_server(dist)
    host, port = server.server_address
    url = f""http://{host}:{port}""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            context = browser.new_context()
            context.add_init_script(""caches.open('legacy-cache')"")
            page = context.new_page()
            page.goto(url + ""/index.html"")
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""navigator.serviceWorker.controller !== null"")
            names = page.evaluate(""caches.keys()"")
            assert ""legacy-cache"" not in names
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
    finally:
        server.shutdown()
        thread.join()",tests/test_pwa_offline.py,
survived,"    def _read_csv(self, task: Dict, path: str) -> pd.DataFrame:
        csv_str = self.preload_task_data(task, path)
        return pd.read_csv(io.StringIO(csv_str))
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"    def _get_tasks(self, project_id: int) -> List[Dict]:
        ls = label_studio_sdk.Client(self.LABEL_STUDIO_HOST, self.LABEL_STUDIO_API_KEY)
        project = ls.get_project(id=project_id)
        return project.get_labeled_tasks()
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"    def fit(self, event, data, **kwargs):
        if event not in ('ANNOTATION_CREATED', 'ANNOTATION_UPDATED', 'START_TRAINING'):
            logger.info(f""Skip training: event {event} is not supported"")
            return
        project_id = data['annotation']['project']
        tasks = self._get_tasks(project_id)
        if len(tasks) % self.START_TRAINING_EACH_N_UPDATES != 0 and event != 'START_TRAINING':
            logger.info(
                f'Skip training: {len(tasks)} tasks are not multiple of {self.START_TRAINING_EACH_N_UPDATES}')
            return
        params = self._get_labeling_params()
        label2idx = {l: i for i, l in enumerate(params['labels'])}
        X, y = [], []
        for task in tasks:
            df = self._read_csv(task, task['data'][params['value']])
            if df.empty:
                continue
            annotations = [a for a in task['annotations'] if a.get('result')]
            for ann in annotations:
                for r in ann['result']:
                    if r['from_name'] != params['from_name']:
                        continue
                    start = r['value']['start']
                    end = r['value']['end']
                    label = r['value']['timeserieslabels'][0]
                    mask = (df[params['time_col']] >= start) & (df[params['time_col']] <= end)
                    seg = df.loc[mask, params['channels']].values
                    X.extend(seg)
                    y.extend([label2idx[label]] * len(seg))
        if not X:
            logger.warning('No data collected for training')
            return
        model = self._get_model(blank=True)
        model.fit(np.array(X), np.array(y))
        os.makedirs(self.MODEL_DIR, exist_ok=True)
        model_path = os.path.join(self.MODEL_DIR, 'model.pkl')
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        global _model
        _model = None
        self._get_model()
",label_studio_ml/examples/timeseries_segmenter/model.py,TimeSeriesSegmenter
survived,"def is_windsurf_installed(host_system: str) -> bool:
    """"""Check if Windsurf is installed.""""""
    # TODO: Implement actual detection logic
    return False
",skyvern/cli/commands.py,
survived,"def convert_archetype_features(v1_path, out_dir, doc_slug):
    data = load_json(v1_path)
    features = []
    items = []
    for obj in data:
        slug = obj[""pk""]
        parent = f""{doc_slug}_{slug}""
        sections = parse_feature_sections(obj[""fields""].get(""desc"", """"))
        for name, desc in sections:
            feat_slug = f""{parent}_{slugify(name)}""
            features.append({
                ""model"": ""api_v2.classfeature"",
                ""pk"": feat_slug,
                ""fields"": {
                    ""name"": name,
                    ""desc"": desc,
                    ""document"": doc_slug,
                    ""parent"": parent,
                },
            })
            level = extract_level(desc)
            if level:
                items.append({
                    ""model"": ""api_v2.classfeatureitem"",
                    ""pk"": f""{feat_slug}_{level}"",
                    ""fields"": {""parent"": feat_slug, ""level"": level, ""column_value"": None},
                })
    if features:
        save_json(features, os.path.join(out_dir, ""ClassFeature.json""))
    if items:
        save_json(items, os.path.join(out_dir, ""ClassFeatureItem.json""))
",convert_missing.py,
survived,"        def __init__(self) -> None:
            self.instructions: list[Any] = []
",tests/test_ledger.py,DummyTx
survived,"def summarise_shard(shard_path: str, test_dataset: str, training_dataset: str, attr_key: str) -> dict:
    """"""Return basic overlap statistics for a single attribute shard.""""""

    ids_seen: set[str] = set()
    overlap_ids: set[str] = set()
    with fsspec.open(shard_path, ""rt"", compression=""infer"") as f:
        for line in f:
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            doc_id = rec.get(""id"")
            if doc_id is None:
                continue
            ids_seen.add(doc_id)
            attrs = rec.get(""attributes"", {})
            if attrs.get(attr_key):
                overlap_ids.add(doc_id)

    return {
        ""shard_path"": shard_path,
        ""test_dataset"": test_dataset,
        ""training_dataset"": training_dataset,
        ""ids_seen"": list(ids_seen),
        ""overlap_ids"": list(overlap_ids),
    }
",experiments/train_test_overlap/dolma/aggregate_total.py,
survived,"def build_step(name: str, dataset_dir: str, max_in_flight: int) -> ExecutorStep:
    cfg = ShardedDedupeConfig(
        dataset_dir=dataset_dir,
        output_path=this_output_path(),
        max_in_flight=max_in_flight,
    )
    return ExecutorStep(
        name=f""train_test_overlap/dolma/total/{name}"",
        fn=run_all_shards,
        config=cfg,
        description=f""Run dedupe train-test overlap on {name} shards"",
    )
",experiments/train_test_overlap/dolma/dedupe_total.py,
survived,"def env_setup(monkeypatch):
    env = {
        ""COLPALI_TOKEN"": ""test-token"",
        ""VLLM_URL"": ""http://localhost"",
        ""COLPALI_BASE_URL"": ""http://localhost"",
        ""VLLM_API_KEY"": ""dummy"",
    }
    for k, v in env.items():
        monkeypatch.setenv(k, v)
",no-ocr-api/tests/test_utils.py,
survived,"def test_engine_against_reference(tmp_path):
    if not _have_opencl() or os.environ.get(""RUN_ENGINE_TESTS"") != ""1"":
        print(""Skipping engine comparison test due to missing OpenCL"")
        return
    out_dir = tmp_path
    cmd = [
        ""./Release/Sibernetic"",
        ""-no_g"",
        ""-f"",
        ""configuration/test/test_energy"",
        ""-l_to"",
        f""lpath={out_dir}"",
        ""timelimit=0.001"",
        ""logstep=25"",
    ]
    subprocess.run(cmd, check=True)
    gen_path = os.path.join(out_dir, ""position_buffer.txt"")
    assert os.path.exists(gen_path)
    generated = _load_matrix(""position_buffer.txt"", base=out_dir)
    baseline = _load_matrix(""positions_step0.txt"")
    for g_row, b_row in zip(generated, baseline):
        for gv, bv in zip(g_row, b_row):
            assert math.isfinite(gv)
            assert abs(gv - bv) < 1e-3",tests/test_solver_logs.py,
survived,"def export_command(
    db_path: Path,
    fmt: str,
    output_path: Path | None,
    start: str | None,
    end: str | None,
    metrics: tuple[str, ...],
) -> None:
    """"""Export telemetry data from the database.""""""
    db = TelemetryDB(db_path)
    if output_path is None:
        suffix = ""json"" if fmt == ""json"" else ""csv"" if fmt == ""csv"" else fmt
        output_path = Path(f""telemetry_export.{suffix}"")
    if fmt == ""json"":
        db.export_json(output_path, start=start, end=end, metrics=metrics or None)
    elif fmt == ""csv"":
        db.export_csv(output_path, start=start, end=end, metrics=metrics or None)
    else:
        db.export(output_path, fmt=fmt, start=start, end=end, metrics=metrics or None)
    click.echo(f""Exported telemetry to {output_path}"")
    db.close()
",src/meta_agent/cli/main.py,
survived,"    def export_csv(
        self,
        path: str | Path,
        *,
        start: datetime | str | None = None,
        end: datetime | str | None = None,
        metrics: Iterable[str] | None = None,
        compress: bool | None = None,
    ) -> str:
        """"""Export telemetry to a CSV file with optional compression.""""""
        compress = compress or str(path).endswith(("".gz"", "".gzip""))
        data = self.fetch_all(start=start, end=end, metrics=metrics)
        if not metrics:
            metrics = [""tokens"", ""cost"", ""latency"", ""guardrail_hits""]
        header = [""timestamp"", *metrics]
        open_fn = gzip.open if compress else open
        mode = ""wt""
        with open_fn(path, mode, encoding=""utf-8"", newline="""") as f:
            writer = csv.writer(f)
            writer.writerow(header)
            for row in data:
                writer.writerow([row.get(key, """") for key in header])
        return str(path)
",src/meta_agent/telemetry_db.py,TelemetryDB
survived,"def _load_env_file(path: str | os.PathLike[str]) -> Dict[str, str]:
    """"""Return key/value pairs from ``path``.

    Falls back to a minimal parser when :mod:`python_dotenv` is unavailable.
    """"""
    try:  # pragma: no cover - optional dependency
        from dotenv import dotenv_values

        return {k: v for k, v in dotenv_values(path).items() if v is not None}
    except Exception:  # noqa: BLE001 - any import/parsing error falls back
        pass

    data: Dict[str, str] = {}
    for line in Path(path).read_text(encoding=""utf-8"").splitlines():
        line = line.strip()
        if not line or line.startswith(""#"") or ""="" not in line:
            continue
        k, v = line.split(""="", 1)
        data[k.strip()] = v.strip().strip('""')
    return data",alpha_factory_v1/utils/env.py,
survived,"def make_sampling_callback(llm: ChatOpenAI | ChatOllama):
    async def sampling_callback(
        context: ClientSession, params: CreateMessageRequestParams
    ) -> CreateMessageResult | ErrorData:
        lc_messages = []
        if params.system_prompt:
            lc_messages.append(SystemMessage(content=params.system_prompt))
        for msg in params.messages:
            content = msg.content.text
            if msg.role == ""assistant"":
                lc_messages.append(AIMessage(content=content))
            else:
                lc_messages.append(HumanMessage(content=content))

        try:
            result_msg = await llm.ainvoke(
                lc_messages,
                temperature=params.temperature,
                max_tokens=params.max_tokens,
                stop=params.stop_sequences,
            )
        except Exception as exc:  # pragma: no cover - runtime error handling
            return ErrorData(code=400, message=str(exc))

        text = getattr(result_msg, ""content"", str(result_msg))
        model_name = getattr(llm, ""model"", ""llm"")
        return CreateMessageResult(
            content=TextContent(text=text, type=""text""),
            model=model_name,
            role=""assistant"",
        )

    return sampling_callback
",examples/openai_chat_agent/app.py,
survived,"def discover_entrypoints() -> None:
    try:
        eps = imetadata.entry_points(group=""alpha_factory.agents"")  # type: ignore[arg-type]
    except Exception:  # noqa: BLE001
        return
    for ep in eps:
        try:
            obj = ep.load()
        except Exception:  # noqa: BLE001
            logger.exception(""Entry-point load failed: %s"", ep.name)
            continue
        AgentBase = _agent_base()
        if inspect.isclass(obj) and issubclass(obj, AgentBase):
            name = getattr(obj, ""NAME"", ep.name)
            if name not in AGENT_REGISTRY:
                _register(
                    AgentMetadata(
                        name=name,
                        cls=obj,
                        version=getattr(obj, ""__version__"", ""0.1.0""),
                        capabilities=list(getattr(obj, ""CAPABILITIES"", [])),
                        compliance_tags=list(getattr(obj, ""COMPLIANCE_TAGS"", [])),
                        requires_api_key=getattr(obj, ""REQUIRES_API_KEY"", False),
                    )
                )
",alpha_factory_v1/backend/agents/discovery.py,
survived,"def _init_repo(path: Path) -> Any:
    repo = git.Repo.init(path)
    (path / ""metric.txt"").write_text(""1\n"")
    repo.git.add(""metric.txt"")
    repo.index.commit(""init"")
    return repo
",tests/test_self_improver.py,
survived,"    def matches_axes(self, spec: NamedArrayAxesSpec) -> bool:
        """"""Check whether this NamedArray conforms to the given `NamedArray` type.

        Parameters
        ----------
        spec : NamedArrayAxesSpec
            The specification to check against. It can be produced via the
            ``NamedArray[...]`` syntax or passed directly as a string or
            sequence of axis names.
        """"""

        ann = _parse_namedarray_axes(spec)
        names = tuple(ax.name for ax in self.axes)
        if ann.ordered:
            if not ann.subset:
                return names == ann.before
            if len(names) < len(ann.before) + len(ann.after):
                return False
            if names[: len(ann.before)] != ann.before:
                return False
            if ann.after and names[-len(ann.after) :] != ann.after:
                return False
            return True
        else:
            name_set = set(names)
            spec_set = set(ann.before)
            if ann.subset:
                return spec_set.issubset(name_set)
            else:
                return name_set == spec_set
",src/haliax/core.py,NamedArray
survived,"    async def __aenter__(self) -> ""ArchiveService"":
        self.start_merkle_task()
        return self
",src/archive/service.py,ArchiveService
survived,"def test_archive_service_chain_growth(tmp_path) -> None:
    svc = ArchiveService(tmp_path / ""arch.db"", broadcast=False)
    root1 = svc.insert_entry({""id"": 1}, {""score"": 0.1})
    first_hash = svc.last_hash()
    root2 = svc.insert_entry({""id"": 2}, {""score"": 0.2})
    second_hash = svc.last_hash()
    assert first_hash != second_hash
    assert svc.conn.execute(""SELECT COUNT(*) FROM entries"").fetchone()[0] == 2
    parent = svc.conn.execute(""SELECT parent FROM entries WHERE hash=?"", (second_hash,)).fetchone()[0]
    assert parent == first_hash
    assert root1 != """"
    assert root2 != """"
",tests/test_archive.py,
survived,"        async def send_transaction(self, tx: object, *args: object) -> None:
            if raise_err:
                raise RuntimeError(""fail"")
            captured[""root""] = tx.instructions[0].data.decode()
",tests/test_archive.py,DummyClient
survived,"        def __init__(self, url: str) -> None:
            captured[""url""] = url
",tests/test_archive.py,DummyClient
survived,"def test_code_diff_offline(tmp_path: Path, monkeypatch) -> None:
    target = tmp_path / ""demo.py""
    target.write_text(""def demo():\n    return 1\n"", encoding=""utf-8"")
    monkeypatch.setenv(""AGI_INSIGHT_OFFLINE"", ""1"")
    diff = code_diff.propose_diff(str(tmp_path), ""demo.py:extra"")
    patcher_core.apply_patch(diff, repo_path=tmp_path)
    assert ""extra"" in target.read_text(encoding=""utf-8"")
    subprocess.check_call([sys.executable, ""-m"", ""py_compile"", str(target)])
",tests/test_code_diff.py,
survived,"def test_code_diff_online(tmp_path: Path, monkeypatch) -> None:
    target = tmp_path / ""demo.py""
    target.write_text(""def demo():\n    return 1\n"", encoding=""utf-8"")
    from src.tools.diff_mutation import propose_diff

    patch = propose_diff(str(target), ""increase"")
    monkeypatch.setenv(""OPENAI_API_KEY"", ""k"")
    with mock.patch.object(code_diff, ""_sync_chat"", return_value=patch):
        diff = code_diff.propose_diff(str(tmp_path), ""demo.py:increase"")
    patcher_core.apply_patch(diff, repo_path=tmp_path)
    assert ""# TODO: increase"" in target.read_text(encoding=""utf-8"")
    subprocess.check_call([sys.executable, ""-m"", ""py_compile"", str(target)])",tests/test_code_diff.py,
survived,"    def __init__(self, docs: Iterable[str] | None = None) -> None:
        self.db = VectorDB(docs)
        self._server: ""grpc.aio.Server"" | None = None
",src/critics/dual_critic_service.py,DualCriticService
survived,"def test_macro_sentinel_first_cells(tmp_path: Path) -> None:
    """"""Execute the first two code cells of the macro sentinel notebook.""""""
    nb_path = Path(""alpha_factory_v1/demos/macro_sentinel/colab_macro_sentinel.ipynb"")
    assert nb_path.exists(), nb_path

    nb = nbformat.read(nb_path, as_version=4)

    # keep the first two code cells only
    code_cells = [cell for cell in nb.cells if cell.cell_type == ""code""][:2]
    nb.cells = code_cells

    ep = ExecutePreprocessor(timeout=60, kernel_name=""python3"")
    ep.preprocess(nb, {""metadata"": {""path"": str(tmp_path)}})",tests/test_notebooks.py,
survived,"    def test_get_ok(self):
        self.server, self.thread, H, url = start_server()
        resp = requests.get(url)
        self.assertEqual(resp.status_code, 200)
        self.assertEqual(resp.text, ""ok"")
        self.assertIsNone(H.received_body)
        self.assertIn(""Host"", H.received_headers)
",alpha_factory_v1/tests/test_requests_shim.py,RequestsShimTest
survived,"    def decorator(inner_cls):
        from_backend_base = _agent_base()
        if not issubclass(inner_cls, from_backend_base):
            raise TypeError(""register() only allowed on AgentBase subclasses"")

        cond_result = condition() if callable(condition) else bool(condition)
        if cond_result:
            meta = AgentMetadata(
                name=getattr(inner_cls, ""NAME"", inner_cls.__name__),
                cls=inner_cls,
                version=getattr(inner_cls, ""__version__"", ""0.1.0""),
                capabilities=list(getattr(inner_cls, ""CAPABILITIES"", [])),
                compliance_tags=list(getattr(inner_cls, ""COMPLIANCE_TAGS"", [])),
                requires_api_key=getattr(inner_cls, ""REQUIRES_API_KEY"", False),
            )
            _register(meta, overwrite=False)
        else:
            logger.info(
                ""Agent %s not registered (condition=false)"",
                getattr(inner_cls, ""NAME"", inner_cls.__name__),
            )
        return inner_cls
",alpha_factory_v1/backend/agents/__init__.py,
survived,"    def test_discover_alpha_negative_num(self) -> None:
        with self.assertRaises(ValueError):
            stub.discover_alpha(num=-1, ledger=None)
",alpha_factory_v1/tests/test_cross_industry_alpha.py,TestCrossIndustryAlpha
survived,"        def __init__(self, host: str) -> None:  # pragma: no cover - init only
            captured[""adk_host""] = host
",tests/test_alpha_agi_business_3_v1.py,DummyADK
survived,"    def _step(population: Population) -> Population:
        evaluate(population, fn)
        offspring: Population = []
        while len(offspring) < population_size:
            a, b = rng.sample(population, 2)
            cut = rng.randint(1, genome_length - 1)
            child_genome = a.genome[:cut] + b.genome[cut:]
            if rng.random() < mutation_rate:
                idx = rng.randrange(genome_length)
                child_genome[idx] += rng.uniform(-1, 1)
            offspring.append(Individual(child_genome))
        evaluate(offspring, fn)
        union = population + offspring
        fronts = _non_dominated_sort(union)
        new_pop: Population = []
        for front in fronts:
            _crowding(front)
            front.sort(key=lambda x: (-x.rank, -x.crowd))
            for ind in front:
                if len(new_pop) < population_size:
                    new_pop.append(ind)
        return new_pop
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"def kill_long_running_runs(context: SensorEvaluationContext):
    """"""Terminate Dagster runs that exceed a configured runtime.""""""
    kill_after = get_kill_after_minutes()
    instance = context.instance
    cutoff = datetime.now(timezone.utc) - timedelta(minutes=kill_after)
    running_runs = instance.get_runs(
        filters=RunsFilter(statuses=[DagsterRunStatus.STARTED])
    )
    killed = 0
    for run in running_runs:
        run_stats = instance.get_run_stats(run.run_id)
        if run_stats.start_time is None:
            continue
        started_at = datetime.fromtimestamp(run_stats.start_time, tz=timezone.utc)
        duration = datetime.now(timezone.utc) - started_at
        if started_at < cutoff:
            try:
                context.log.info(
                    f""Terminating run {run.run_id} running for {duration}""
                )
                instance.report_run_canceling(run)
                instance.run_launcher.terminate(run.run_id)
                killed += 1
            except DagsterUserCodeUnreachableError as exc:
                context.log.warning(
                    (
                        f""Could not terminate run {run.run_id}: {exc}. ""
                        ""Marking as failed.""
                    )
                )
                instance.report_run_failed(run)
            except Exception as exc:
                context.log.error(
                    (
                        f""Unexpected error terminating run {run.run_id}: {exc}. ""
                        ""Marking as failed.""
                    )
                )
                instance.report_run_failed(run)
    if killed == 0:
        yield SkipReason(""No long running runs found"")
    else:
        yield SkipReason(f""Killed {killed} long running run(s)"")",anomstack/sensors/timeout.py,
survived,"def _load_config_timeout_minutes() -> int:
    env_val = os.getenv(""ANOMSTACK_KILL_RUN_AFTER_MINUTES"")
    if env_val:
        try:
            return int(env_val)
        except ValueError:
            pass

    dagster_home = Path(os.getenv(""DAGSTER_HOME"", """"))
    if not dagster_home:
        dagster_home = Path.cwd()
    config_path = dagster_home / ""dagster.yaml""
    minutes = DEFAULT_MINUTES
    if config_path.exists():
        try:
            with open(config_path, ""r"", encoding=""utf-8"") as f:
                cfg = yaml.safe_load(f)
            minutes = int(cfg.get(""kill_sensor"", {}).get(""kill_after_minutes"", minutes))
        except Exception:
            pass
    return minutes
",anomstack/sensors/timeout.py,
survived,"def test_generate_pkce_pair_invalid():
    st.session_state.clear()
    with pytest.raises(Exception):
        _generate_pkce_pair(""plain"", key=""x"")",tests/test_internal.py,
survived,"def test_refresh_token_expired(monkeypatch):
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)

    monkeypatch.setattr(oauth.client, ""refresh_token"", AsyncMock(return_value={""access_token"": ""new""}))

    token = {""access_token"": ""old"", ""refresh_token"": ""r"", ""expires_at"": time.time() - 1}
    result = oauth.refresh_token(token)

    assert result[""access_token""] == ""new""
",tests/test_oauth_component.py,
survived,"def apply_rotary_pos_emb(x, cos, sin):
    return (x * cos) + (rotate_half(x) * sin)
",src/model/u2tokenizer/rope.py,
survived,"        def observe(self, *_a: Any) -> None:
            ...
",alpha_factory_v1/core/utils/tracing.py,_N
survived,"def forecast_disruptions(
    sectors: Iterable[Sector],
    horizon: int,
    curve: str = ""logistic"",
    *,
    k: float | None = None,
    x0: float | None = None,
    pop_size: int = 6,
    generations: int = 1,
    seed: int | None = None,
    mut_rate: float = 0.1,
    xover_rate: float = 0.5,
) -> List[TrajectoryPoint]:
    """"""Simulate sector trajectories and disruption events.

    Args:
        sectors: Iterable of sectors to simulate.
        horizon: Number of years to simulate.
        curve: Name of the capability growth curve.
        k: Optional curve steepness parameter.
        x0: Optional curve midpoint shift.
        pop_size: Population size for the evolutionary search.
        generations: Number of evolution steps.
        seed: Random seed for deterministic behaviour.
        mut_rate: Probability of mutating a gene.
        xover_rate: Probability of performing crossover.

    Returns:
        List of trajectory points for each simulated year.
    """"""

    secs = list(sectors)
    results: List[TrajectoryPoint] = []
    for year in range(1, horizon + 1):
        t = year / horizon
        cap = capability_growth(t, curve, k=k, x0=x0)
        affected: List[Sector] = []
        for sec in secs:
            if not sec.disrupted:
                sec.energy *= 1.0 + sec.growth
                if thermodynamic_trigger(sec, cap):
                    sec.disrupted = True
                    sec.energy += _innovation_gain(
                        pop_size,
                        generations,
                        seed=seed,
                        mut_rate=mut_rate,
                        xover_rate=xover_rate,
                    )
                    affected.append(sec)
        snapshot = [Sector(s.name, s.energy, s.entropy, s.growth, s.disrupted) for s in secs]
        results.append(TrajectoryPoint(year, cap, snapshot))
    return results
",alpha_factory_v1/core/simulation/forecast.py,
survived,"def _crowding(pop: Population) -> None:
    """"""Compute the crowding distance for a Pareto front.""""""

    if not pop or pop[0].fitness is None:
        return
    m = len(pop[0].fitness)
    for ind in pop:
        ind.crowd = 0.0
    for i in range(m):
        pop.sort(key=lambda x: (x.fitness or (0.0,) * m)[i])
        first_fit = pop[0].fitness
        last_fit = pop[-1].fitness
        assert first_fit is not None and last_fit is not None
        pop[0].crowd = pop[-1].crowd = float(""inf"")
        fmin = first_fit[i]
        fmax = last_fit[i]
        span = fmax - fmin or 1.0
        for j in range(1, len(pop) - 1):
            prev_fit = pop[j - 1].fitness
            next_fit = pop[j + 1].fitness
            assert prev_fit is not None and next_fit is not None
            prev_f = prev_fit[i]
            next_f = next_fit[i]
            pop[j].crowd += (next_f - prev_f) / span
",alpha_factory_v1/core/simulation/mats.py,
survived,"    def __init__(
        self,
        name: str,
        bus: messaging.A2ABus,
        ledger: ""Ledger"",
        *,
        backend: str = ""gpt-4o"",
        island: str = ""default"",
    ) -> None:
        self.island = island
        self.backend = backend
        self.name = f""{name}_{island}"" if island != ""default"" else name
        self.bus = bus
        self.ledger = ledger
        self.llm = None
        if backend.startswith(""gpt"") and AgentContext is not None:
            try:
                self.oai_ctx = AgentContext(
                    model=bus.settings.model_name,
                    temperature=bus.settings.temperature,
                    context_window=bus.settings.context_window,
                )
            except TypeError:
                try:
                    self.oai_ctx = AgentContext(
                        model=bus.settings.model_name,
                        temperature=bus.settings.temperature,
                    )
                except Exception:
                    self.oai_ctx = AgentContext()
        else:
            self.oai_ctx = None
            if LLMProvider is not None:
                try:
                    self.llm = LLMProvider(
                        temperature=bus.settings.temperature,
                        max_tokens=bus.settings.context_window,
                    )
                except Exception:
                    self.llm = LLMProvider() if LLMProvider is not None else None
        self.adk = ADKAdapter() if ADKAdapter.is_available() else None
        self.mcp = MCPAdapter() if MCPAdapter.is_available() else None
        self._handler = self._on_envelope
        self.bus.subscribe(self.name, self._handler)
",alpha_factory_v1/core/agents/base_agent.py,BaseAgent
survived,"def improve_repo(
    repo_url: str,
    patch_file: str,
    metric_file: str,
    log_file: str,
    cleanup: bool = True,
) -> Tuple[float, Path]:
    """"""Clone ``repo_url``, apply ``patch_file`` and log score delta.

    Parameters
    ----------
    repo_url:
        Repository to clone.
    patch_file:
        Unified diff to apply.
    metric_file:
        File containing the numeric metric used for scoring.
    log_file:
        JSON file updated with the score delta.
    cleanup:
        When ``True`` the temporary clone is removed before returning.

    Returns
    -------
    tuple[float, Path]
        Score delta and path to the cloned repository (if ``cleanup`` is
        ``False``).
    """"""
    if git is None:
        raise RuntimeError(""GitPython is required"")
    repo_dir = Path(tempfile.mkdtemp(prefix=""selfimprover-""))
    repo = git.Repo.clone_from(repo_url, repo_dir)
    baseline = _evaluate(repo_dir, metric_file)

    diff = Path(patch_file).read_text()
    if not is_patch_valid(diff):
        raise ValueError(""Invalid or unsafe patch"")

    repo.git.apply(patch_file)
    repo.index.add([metric_file])
    repo.index.commit(""apply patch"")
    # run basic checks before scoring
    run_preflight(repo_dir)
    new_score = _evaluate(repo_dir, metric_file)
    delta = new_score - baseline
    _log_delta(delta, Path(log_file))
    if cleanup:
        shutil.rmtree(repo_dir, ignore_errors=True)
    return delta, repo_dir",alpha_factory_v1/core/self_evolution/self_improver.py,
survived,"async def _main() -> None:  # pragma: no cover - CLI helper
    orch = Orchestrator()
    await orch.run_forever()
",alpha_factory_v1/core/orchestrator.py,
survived,"    def first_cross(seq: Sequence[float]) -> int:
        for i, v in enumerate(seq, 1):
            if v >= thr:
                return i
        return months + 1
",alpha_factory_v1/core/evaluators/lead_time.py,
survived,"def _reset() -> None:
    sc._seen_request_ids.clear()
",tests/test_safety_compliance_reward.py,
survived,"def test_helm_template_renders() -> None:
    subprocess.run(
        [""helm"", ""template"", ""alpha-demo"", str(CHART_DIR), ""-f"", str(VALUES_FILE)],
        check=True,
        cwd=CHART_DIR,
        capture_output=True,
        text=True,
    )",tests/test_helm_template.py,
survived,"async def _devnet_available() -> bool:
    try:
        from solana.rpc.async_api import AsyncClient
    except Exception:
        return False
    try:
        client = AsyncClient(""https://api.devnet.solana.com"")
        await client.get_version()
        await client.close()
        return True
    except Exception:
        return False
",tests/test_devnet_broadcast.py,
survived,"    def close(self) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_property.py,DummyLedger
survived,"def test_contract_addresses_format():
    for path in config_paths():
        cfg = load(path)
        for addr in cfg.get('contracts', {}):
            assert addr.startswith('0x') and len(addr) == 42, f""Bad addr {addr} in {path}""",tests/test_configs.py,
survived,"def test_resolve_dep():
    cfg = {
        'dependencies': {
            '@oz/contracts': {'url': 'u', 'commit': 'c', 'relative_root': ''}
        }
    }
    repo, dep_name = resolve_dep('@oz/contracts/token.sol', cfg)
    assert dep_name == '@oz/contracts'
    assert repo['commit'] == 'c'",tests/test_github_utils.py,
survived,"def test_percent_str_call(state: State):
    s_in = """"""'%s' % str(var)""""""
    s_expected = """"""f'{var!s}'""""""

    s_out, count = code_editor.fstringify_code_by_line(s_in, state)
    assert s_out == s_expected
",test/test_edits.py,
survived,"def test_format_repr_call(state: State):
    s_in = """"""'{}'.format(repr(var))""""""
    s_expected = """"""f'{var!r}'""""""

    s_out, count = code_editor.fstringify_code_by_line(s_in, state)
    assert s_out == s_expected
",test/test_edits.py,
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/supply_chain_agent.py,SupplyChainAgent
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/finance_agent.py,FinanceAgent
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/biotech_agent.py,BiotechAgent
survived,"    def __init__(self, path: str) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Ledger
survived,"    def _init_agents(self) -> List[messaging.Envelope]:  # type: ignore[override]
        agents = [
            planning_agent.PlanningAgent(self.bus, self.ledger),
            research_agent.ResearchAgent(self.bus, self.ledger),
            strategy_agent.StrategyAgent(self.bus, self.ledger),
            market_agent.MarketAgent(self.bus, self.ledger),
            codegen_agent.CodeGenAgent(self.bus, self.ledger),
            safety_agent.SafetyGuardianAgent(self.bus, self.ledger),
            memory_agent.MemoryAgent(self.bus, self.ledger),
        ]
        return agents
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator
survived,"    async def run_cycle(self) -> None:
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/codegen_agent.py,CodeGenAgent
survived,"def _crowding(pop: Population) -> None:
    if not pop or pop[0].fitness is None:
        return
    m = len(pop[0].fitness)
    for ind in pop:
        ind.crowd = 0.0
    for i in range(m):
        pop.sort(key=lambda x: x.fitness[i])  # type: ignore[index]
        pop[0].crowd = pop[-1].crowd = float(""inf"")
        fmin = pop[0].fitness[i]
        fmax = pop[-1].fitness[i]
        span = fmax - fmin or 1.0
        for j in range(1, len(pop) - 1):
            prev_f = pop[j - 1].fitness[i]
            next_f = pop[j + 1].fitness[i]
            pop[j].crowd += (next_f - prev_f) / span
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"        def outer(i, carry):
            page_indices, page_owners = carry
            seq_id = updated_seqs[""seq"", i].scalar()

            def do_alloc(carry):
                return _alloc_pages_for_seq(seq_id, carry)

            cond = jnp.logical_and(seq_id >= 0, seq_id < self.max_seqs)
            page_indices, page_owners = jax.lax.cond(cond, do_alloc, lambda c: c, (page_indices, page_owners))
            return page_indices, page_owners
",src/levanter/layers/page_table.py,PageTable
survived,"    def test_acreate_uses_timeout(self) -> None:
        response = types.SimpleNamespace(choices=[types.SimpleNamespace(message=types.SimpleNamespace(content=""{}""))])
        openai_mock = types.SimpleNamespace(
            ChatCompletion=types.SimpleNamespace(acreate=AsyncMock(return_value=response))
        )
        with patch.dict(os.environ, {""OPENAI_API_KEY"": ""x""}):
            with patch.object(energy_agent, ""openai"", openai_mock):
                agent = EnergyAgent()
                agent.cfg.openai_enabled = True
                asyncio.run(agent._hedge())
        openai_mock.ChatCompletion.acreate.assert_awaited()
        kwargs = openai_mock.ChatCompletion.acreate.call_args.kwargs
        self.assertEqual(kwargs.get(""timeout""), energy_agent.OPENAI_TIMEOUT_SEC)
",tests/test_energy_agent.py,TestOpenAITimeout
survived,"    def is_rune_warning(self):
        '''
        is_rune_warning
        '''
        x0, y0 = self.cfg.rune_warning_top_left
        x1, y1 = self.cfg.rune_warning_bottom_right
        _, score, _ = find_pattern_sqdiff(
                        self.img_frame_gray[y0:y1, x0:x1],
                        self.img_rune_warning)
        if self.status == ""hunting"" and score < self.cfg.rune_warning_diff_thres:
            logger.info(f""[is_rune_warning] Detect rune warning on screen with score({score})"")
            return True
        else:
            return False
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def is_player_stuck(self):
        """"""
        Detect if the player is stuck (not moving).
        If stuck for more than WATCH_DOG_TIMEOUT seconds, performs a random action.
        """"""
        dx = abs(self.loc_player_global[0] - self.loc_watch_dog[0])
        dy = abs(self.loc_player_global[1] - self.loc_watch_dog[1])

        current_time = time.time()
        if dx + dy > self.cfg.watch_dog_range:
            # Player moved, reset watchdog timer
            self.loc_watch_dog = self.loc_player_global
            self.t_watch_dog = current_time
            return False

        dt = current_time - self.t_watch_dog
        if dt > self.cfg.watch_dog_timeout:
            # watch dog idle for too long, player stuck
            self.loc_watch_dog = self.loc_player_global
            self.t_watch_dog = current_time
            logger.warning(f""[is_player_stuck] Player stuck for {dt} seconds."")
            return True
        return False
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def stop(self):
        '''
        Stop capturing thread
        '''
        self.is_terminated = True
        logger.info(""[GameWindowCapturor] Terminated"")
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"    def is_rune_near_player(self):
        '''
        is_rune_near_player
        '''
        # Calculate bounding box
        h, w = self.img_frame.shape[:2]
        x0 = max(0, self.loc_player[0] - self.cfg.rune_detect_box_width // 2)
        y0 = max(0, self.loc_player[1] - self.cfg.rune_detect_box_height)
        x1 = min(w, self.loc_player[0] + self.cfg.rune_detect_box_width // 2)
        y1 = min(h, self.loc_player[1])

        # Debug
        draw_rectangle(
            self.img_frame_debug, (x0, y0), (y1-y0, x1-x0),
            (255, 0, 0), ""Rune Detection Range""
        )

        # Find rune icon near player
        if  (x1 - x0) < self.img_rune.shape[1] or \
            (y1 - y0) < self.img_rune.shape[0]:
            return False # Skip check if box is out of range
        else:
            img_roi = self.img_frame[y0:y1, x0:x1]
            loc_rune, score, _ = find_pattern_sqdiff(
                            img_roi,
                            self.img_rune,
                            mask=get_mask(self.img_rune, (0, 255, 0)))
            # # Draw rectangle for debug
            # draw_rectangle(
            #     self.img_frame_debug,
            #     (x0 + loc_rune[0], y0 + loc_rune[1]),
            #     self.img_rune.shape,
            #     (255, 0, 255),  # purple in BGR
            #     f""Rune,{round(score, 2)}""
            # )
            detect_thres = self.cfg.rune_detect_diff_thres + self.rune_detect_level*self.cfg.rune_detect_level_coef
            if score < detect_thres:
                logger.info(f""[Rune Detect] Found rune near player with score({score}),"" + \
                            f""level({self.rune_detect_level}),threshold({detect_thres})"")
                # Draw rectangle for debug
                draw_rectangle(
                    self.img_frame_debug,
                    (x0 + loc_rune[0], y0 + loc_rune[1]),
                    self.img_rune.shape,
                    (255, 0, 255),  # purple in BGR
                    f""Rune,{round(score, 2)}""
                )
                screenshot(self.img_frame_debug, ""rune_detected"")

                return True
            else:
                return False
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"    def get_frame(self):
        '''
        å®‰å…¨åœ°ç²å–æœ€æ–°çš„èž¢å¹•ç•«é¢
        '''
        with self.lock:
            if self.frame is None:
                return None
            # cv2.imwrite(""debug_frame.png"", self.frame)
            return cv2.cvtColor(self.frame, cv2.COLOR_BGRA2BGR)
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"    def get_hp_mp_exp(self):
        '''
        get_hp_mp_exp
        '''
        # HP crop
        hp_bar = self.img_frame[self.cfg.hp_bar_top_left[1]:self.cfg.hp_bar_bottom_right[1]+1,
                                self.cfg.hp_bar_top_left[0]:self.cfg.hp_bar_bottom_right[0]+1]
        # MP crop
        mp_bar = self.img_frame[self.cfg.mp_bar_top_left[1]:self.cfg.mp_bar_bottom_right[1]+1,
                                self.cfg.mp_bar_top_left[0]:self.cfg.mp_bar_bottom_right[0]+1]
        # EXP crop
        exp_bar = self.img_frame[self.cfg.exp_bar_top_left[1]:self.cfg.exp_bar_bottom_right[1]+1,
                                self.cfg.exp_bar_top_left[0]:self.cfg.exp_bar_bottom_right[0]+1]
        # HP Detection (detect empty part)
        empty_mask_hp = (hp_bar[:,:,0] == hp_bar[:,:,1]) & (hp_bar[:,:,0] == hp_bar[:,:,2])
        empty_pixels_hp = np.count_nonzero(empty_mask_hp)-6 # 6 pixel always be white
        total_pixels_hp = hp_bar.shape[0] * hp_bar.shape[1] - 6
        hp_ratio = 1 - (empty_pixels_hp / total_pixels_hp)

        # MP Detection (detect empty part)
        empty_mask_mp = (mp_bar[:,:,0] == mp_bar[:,:,1]) & (mp_bar[:,:,0] == mp_bar[:,:,2])
        empty_pixels_mp = np.count_nonzero(empty_mask_mp)-6 # 6 pixel always be white
        total_pixels_mp = mp_bar.shape[0] * mp_bar.shape[1] - 6
        mp_ratio = 1 - (empty_pixels_mp / total_pixels_mp)

        # EXP Detection (detect eexpty part)
        empty_mask_exp = (exp_bar[:,:,0] == exp_bar[:,:,1]) & (exp_bar[:,:,0] == exp_bar[:,:,2])
        eexpty_pixels_exp = np.count_nonzero(empty_mask_exp)-6 # 6 pixel always be white
        total_pixels_exp = exp_bar.shape[0] * exp_bar.shape[1] - 6
        exp_ratio = 1 - (eexpty_pixels_exp / total_pixels_exp)

        # Compute original bar dimensions
        hp_h, hp_w = hp_bar.shape[:2]
        mp_h, mp_w = mp_bar.shape[:2]
        exp_h, exp_w = exp_bar.shape[:2]

        # Overlay HP/MP/EXP text
        x_start, y_start = (250, 90)
        cv2.putText(self.img_frame_debug, f""HP: {hp_ratio*100:.1f}%"", (x_start, y_start),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)
        cv2.putText(self.img_frame_debug, f""MP: {mp_ratio*100:.1f}%"", (x_start, y_start+30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)
        cv2.putText(self.img_frame_debug, f""EXP: {exp_ratio*100:.1f}%"", (x_start, y_start+60),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)

        # Paste HP/MP/EXP bar on img_frame_debug
        x_start, y_start = (410, 73)
        self.img_frame_debug[y_start:y_start+hp_h, x_start:x_start+hp_w] = hp_bar
        self.img_frame_debug[y_start+30:y_start+30+mp_h, x_start:x_start+mp_w] = mp_bar
        self.img_frame_debug[y_start+60:y_start+60+exp_h, x_start:x_start+exp_w] = exp_bar

        return hp_ratio, mp_ratio, exp_ratio
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"def main(demo: str, *, print_only: bool = False) -> None:
    print(DISCLAIMER, file=sys.stderr)
    url = _demo_url(demo)
    if _remote_available(url):
        if print_only:
            print(url)
            return
        print(f""Opening {url}"")
        webbrowser.open(url)
        return

    repo_root = Path(__file__).resolve().parents[1]
    site_dir = repo_root / ""site"" / ""alpha_factory_v1"" / ""demos"" / demo
    local_page = site_dir / ""index.html""
    if not local_page.is_file():
        print(""Remote page unavailable. Building local copy..."", file=sys.stderr)
        if not _build_local_site(repo_root) or not local_page.is_file():
            print(
                f""Demo {demo} not found. Build the gallery with ./scripts/build_gallery_site.sh"",
                file=sys.stderr,
            )
            sys.exit(1)

    handler = partial(SimpleHTTPRequestHandler, directory=str(site_dir))
    with ThreadingHTTPServer((""127.0.0.1"", 0), handler) as httpd:
        port = httpd.server_address[1]
        local_url = f""http://127.0.0.1:{port}/index.html""
        print(f""Serving local copy at {local_url}"", file=sys.stderr)
        thread = threading.Thread(target=httpd.serve_forever, daemon=True)
        thread.start()
        try:
            if print_only:
                print(local_url)
            else:
                webbrowser.open(local_url)
            thread.join()
        except KeyboardInterrupt:
            pass
",scripts/open_subdir_demo.py,
survived,"    def log_message(self, *_args: str) -> None:  # pragma: no cover - quiet
        pass
",tests/test_aiga_openai_bridge_offline.py,_Handler
survived,"def test_restart_alert(monkeypatch) -> None:
    sent: dict[str, object] = {}

    def fake_post(url: str, *, json=None, timeout=None):
        sent[""url""] = url
        sent[""payload""] = json
        return type(""R"", (), {""status_code"": 200})()

    monkeypatch.setattr(alerts, ""requests"", type(""M"", (), {""post"": fake_post}))
    monkeypatch.setattr(orchestrator, ""Ledger"", DummyLedger)
    monkeypatch.setattr(orchestrator.Orchestrator, ""_init_agents"", lambda self: [])

    settings = config.Settings(bus_port=0, alert_webhook_url=""http://hook"")
    orch = orchestrator.Orchestrator(settings)
    runner = orchestrator.AgentRunner(DummyAgent(orch.bus, orch.ledger))

    orch._record_restart(runner)

    assert sent[""url""] == ""http://hook""
    assert (
        sent[""payload""].get(""text"") == ""dummy restarted""
        if ""text"" in sent[""payload""]
        else sent[""payload""].get(""content"") == ""dummy restarted""
    )
",tests/test_alert_webhook.py,
survived,"    def __init__(self, *_a, **_kw) -> None:
        self.events = []
",tests/test_alert_webhook.py,DummyLedger
survived,"    def close(self) -> None:  # pragma: no cover - test stub
        pass
",tests/test_alert_webhook.py,DummyLedger
survived,"    def __init__(self, bus: messaging.A2ABus, ledger: DummyLedger) -> None:
        super().__init__(""dummy"", bus, ledger)
",tests/test_alert_webhook.py,DummyAgent
survived,"def _gallery_url() -> str:
    remote = subprocess.check_output([""git"", ""config"", ""--get"", ""remote.origin.url""], text=True).strip()
    repo_path = remote.split(""github.com"")[-1].lstrip("":/"").removesuffix("".git"")
    org, repo = repo_path.split(""/"", 1)
    return f""https://{org}.github.io/{repo}/alpha_factory_v1/demos/""
",scripts/launch_gallery.py,
survived,"    def assimilate_handler(self, wu, results, canonical_result):
        """"""
        Assimilates a canonical result, in this case assimilation
        means dumping the contents of the result to the log.
        Also calls report_errors to log any problems present in the workunit (wu)
        """"""

        # check for valid wu.canonical_result
        if wu.canonical_result:
            # do application specific processing
            self.logNormal(""[%s] Found canonical result\n"", wu.name)
            result = self.get_file_path(canonical_result)
            for line in open(result, 'r').readlines():
                line = line.strip()
                self.logDebug(""  [%s] Answer found %s %s\n"", canonical_result.name, line[-32:], line[:-33])
        else:
            self.logNormal(""[%s] No canonical result\n"", wu.name)

        if self.report_errors(wu):
            # report_errors returns true if error state was present
            # perhaps add some special logic here
            # even if no logic is required, report_errors should be called
            pass
",sched/testasm.py,TestAssimilator
survived,"    def test_delta_must_be_within_range(self) -> None:
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=-0.1, stake=1.0)
        with self.assertRaises(ValueError):
            run_sim(agents=1, rounds=10, delta=1.1, stake=1.0)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"def test_service_worker_registration_failure_toast() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.add_init_script(
            ""navigator.serviceWorker.register = () => Promise.reject('fail')""
        )
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(
            ""document.getElementById('toast').textContent.includes('offline mode disabled')""
        )
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py,
survived,"def check_openai_agents_version(min_version: str = ""0.0.14"") -> bool:
    """"""Verify ``openai_agents`` is new enough when installed.""""""
    import importlib

    spec = importlib.util.find_spec(""openai_agents"")
    if spec is None:  # not installed
        return True

    mod = importlib.import_module(""openai_agents"")
    version = getattr(mod, ""__version__"", ""0"")
    if _version_lt(version, min_version):
        banner(
            f""openai_agents {version} detected; >={min_version} required"",
            ""RED"",
        )
        return False
    banner(f""openai_agents {version} detected"", ""GREEN"")
    return True
",alpha_factory_v1/scripts/preflight.py,
survived,"        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            return orig_find_spec(name, *args, **kwargs)
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion
survived,"    def _version_lt(a: str, b: str) -> bool:
        return Version(a) < Version(b)
",alpha_factory_v1/scripts/preflight.py,
survived,"def ensure_model() -> None:
    """"""Ensure the checkpoint files are available.""""""
    dest = MODEL_DIR / MODEL_NAME
    if dest.exists():
        return
    script = Path(__file__).resolve().parents[2] / ""scripts"" / ""download_openai_gpt2.py""
    subprocess.run([sys.executable, str(script), MODEL_NAME, ""--dest"", str(MODEL_DIR)], check=True)
",alpha_factory_v1/demos/gpt2_small_cli/gpt2_cli.py,
survived,"        def __init__(self, *args: object, **kwargs: object) -> None:
            pass
",tests/test_gpt2_cli_demo.py,FakeTokenizer
survived,"def test_gpt2_cli_help() -> None:
    result = subprocess.run([sys.executable, str(SCRIPT), ""--help""], capture_output=True, text=True)
    assert result.returncode == 0
    assert ""usage"" in result.stdout.lower()
",tests/test_gpt2_cli_demo.py,
survived,"        def from_pretrained(cls, name: str) -> ""FakeModel"":
            return cls()
",tests/test_gpt2_cli_demo.py,FakeModel
survived,"def test_compose_health(compose_stack: None) -> None:
    assert _wait(""http://localhost:8000/healthz""), ""/healthz endpoint not healthy""
    assert _wait(""http://localhost:8000/readiness""), ""/readiness endpoint not healthy""",tests/test_compose_health.py,
survived,"    def test_minimum_demo_count(self) -> None:
        base = validate_demos.DEFAULT_DIR
        demos = [
            d
            for d in os.listdir(base)
            if os.path.isdir(os.path.join(base, d))
            and not d.startswith(""."")
            and not d.startswith(""__"")
        ]
        self.assertGreaterEqual(len(demos), 10)
",tests/test_demo_quality.py,TestDemoDirectoryCount
survived,"    def undo_task(self) -> dict[str, bool]:
        return {""ok"": undo_last_edit()}",src/self_edit/tools.py,FileToolsADK
survived,"def undo_last_edit() -> bool:
    """"""Revert the last edit operation if possible.""""""
    if not _EDIT_HISTORY:
        return False
    p, text = _EDIT_HISTORY.pop()
    p.write_text(text, encoding=""utf-8"")
    return True
",src/self_edit/tools.py,
survived,"    def _compute_root(self, cids: Iterable[str]) -> str:
        hashes = [hashlib.sha256(c.encode()).digest() for c in sorted(cids)]
        if not hashes:
            return """"
        while len(hashes) > 1:
            if len(hashes) % 2 == 1:
                hashes.append(hashes[-1])
            hashes = [hashlib.sha256(hashes[i] + hashes[i + 1]).digest() for i in range(0, len(hashes), 2)]
        return hashes[0].hex()
",src/archive/hash_archive.py,HashArchive
survived,"    def _ipfs_add(self, tarball: Path) -> str:
        cmd = shutil.which(""ipfs"")
        if cmd:
            try:
                proc = subprocess.run([cmd, ""add"", ""-Q"", str(tarball)], capture_output=True, text=True, check=True)
                return proc.stdout.strip()
            except Exception:
                pass
        return hashlib.sha256(tarball.read_bytes()).hexdigest()
",src/archive/hash_archive.py,HashArchive
survived,"def parse_score(text: str) -> Sequence[float]:
    return [float(x) for x in text.split("","")]
",scripts/verify_snark.py,
survived,"def _ipfs_add(path: Path) -> str:
    cmd = shutil.which(""ipfs"")
    if cmd:
        try:
            proc = subprocess.run([cmd, ""add"", ""-Q"", str(path)], capture_output=True, text=True, check=True)
            return proc.stdout.strip()
        except Exception:
            pass
    return hashlib.sha256(path.read_bytes()).hexdigest()
",src/utils/snark.py,
survived,"def _parse_simple_yaml(lines: List[str], start: int, indent: int) -> tuple[Any, int]:
    """"""Parse a simple block of YAML starting at ``start`` with ``indent``.""""""

    # If the first relevant line starts with a list indicator, parse a list
    i = start
    while i < len(lines) and not lines[i].strip():
        i += 1
    if (
        i < len(lines)
        and lines[i].lstrip().startswith(""- "")
        and (len(lines[i]) - len(lines[i].lstrip("" "")) == indent)
    ):
        lst: List[Any] = []
        while i < len(lines):
            line = lines[i]
            if not line.strip():
                i += 1
                continue
            current_indent = len(line) - len(line.lstrip("" ""))
            if current_indent < indent:
                break
            if not line.lstrip().startswith(""- ""):
                break
            item_content = line.lstrip()[2:].strip()
            i += 1
            if item_content:
                if "":"" in item_content:
                    k, v = item_content.split("":"", 1)
                    item: Any = {k.strip(): _convert_scalar(v.strip())}
                else:
                    item = _convert_scalar(item_content)
            else:
                item = {}
            # parse subfields
            while i < len(lines):
                sub_line = lines[i]
                sub_indent = len(sub_line) - len(sub_line.lstrip("" ""))
                if sub_indent <= current_indent:
                    break
                sub_key, sub_val = sub_line.strip().split("":"", 1)
                item[sub_key.strip()] = _convert_scalar(sub_val.strip())
                i += 1
            lst.append(item)
        return lst, i

    result: dict[str, Any] = {}
    while i < len(lines):
        line = lines[i]
        if not line.strip():
            i += 1
            continue
        current_indent = len(line) - len(line.lstrip("" ""))
        if current_indent < indent:
            break
        if current_indent > indent:
            raise YAMLError(""Invalid indentation"")
        stripped = line.strip()
        if "":"" not in stripped:
            raise YAMLError(f""Invalid line: {line}"")
        key, rest = stripped.split("":"", 1)
        key = key.strip()
        rest = rest.strip()
        i += 1
        if rest == """":
            # Determine if next block is a list or nested mapping
            if i < len(lines) and lines[i].lstrip().startswith(""- ""):
                lst: List[Any] = []
                while i < len(lines):
                    item_line = lines[i]
                    if len(item_line) - len(item_line.lstrip("" "")) < indent + 2:
                        break
                    if not item_line.lstrip().startswith(""- ""):
                        break
                    item_content = item_line.lstrip()[2:].strip()
                    i += 1
                    item: Any
                    if item_content == """":
                        item = {}
                    elif "":"" in item_content:
                        k, v = item_content.split("":"", 1)
                        item = {k.strip(): _convert_scalar(v.strip())}
                    else:
                        item = _convert_scalar(item_content)
                    # Parse additional properties for the list item
                    while i < len(lines):
                        sub_line = lines[i]
                        sub_indent = len(sub_line) - len(sub_line.lstrip("" ""))
                        if sub_indent <= indent + 2:
                            break
                        sub_key, sub_val = sub_line.strip().split("":"", 1)
                        item[sub_key.strip()] = _convert_scalar(sub_val.strip())
                        i += 1
                    lst.append(item)
                result[key] = lst
            else:
                sub_obj, i = _parse_simple_yaml(lines, i, indent + 2)
                result[key] = sub_obj
        else:
            result[key] = _convert_scalar(rest)
    return result, i
",src/yaml/__init__.py,
survived,"def toyota_checksum(address: int, sig, d: bytearray) -> int:
  s = len(d)
  addr = address
  while addr:
    s += addr & 0xFF
    addr >>= 8
  for i in range(len(d) - 1):
    s += d[i]
  return s & 0xFF",opendbc/car/toyota/toyotacan.py,
survived,"def body_checksum(address: int, sig, d: bytearray) -> int:
  crc = 0xFF
  poly = 0xD5
  for i in range(len(d) - 2, -1, -1):
    crc ^= d[i]
    for _ in range(8):
      if crc & 0x80:
        crc = ((crc << 1) ^ poly) & 0xFF
      else:
        crc = (crc << 1) & 0xFF
  return crc",opendbc/car/body/bodycan.py,
survived,"async def plan_trip(
    preferences: Annotated[str, Field(description=""Your travel preferences"")],
    ctx: EnrichContext,
) -> list[Destination]:
    """"""Return three destinations that best match the given preferences.""""""

    bullet_list = ""\n"".join(f""- {d.name}: {d.summary}"" for d in DESTINATIONS)
    prompt = (
        ""Select the three best destinations from the list below based on the ""
        ""given preferences. Reply with a JSON list of names only.\nPreferences: ""
        f""{preferences}\n\n{bullet_list}""
    )
    result = await ctx.sampling(
        prompt,
        model_preferences=prefer_fast_model(),
        max_tokens=50,
    )
    try:
        names = json.loads(result.content.text)
    except Exception:
        return []
    return [d for d in DESTINATIONS if d.name in names]
",examples/server_side_llm_travel_planner/app.py,
survived,"def test_js_serializer_malformed_json(tmp_path: Path) -> None:
    script = tmp_path / ""run.mjs""
    script.write_text(
        f""import {{load}} from '{SERIALIZER.resolve().as_posix()}';\n""
        ""try {\n""
        ""  load(process.argv[2]);\n""
        ""} catch (err) {\n""
        ""  console.error(err.message);\n""
        ""  process.exit(1);\n""
        ""}\n""
    )
    result = subprocess.run([""node"", script, ""{invalid""], capture_output=True, text=True)
    assert result.returncode == 1",tests/test_serializer.py,
survived,"def _run_client() -> None:
    with TestClient(api_server.app):
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_token_required.py,
survived,"def test_named_param_annotation():
    def foo(x: Named[""batch embed""]):
        pass

    axes = typing.get_args(foo.__annotations__[""x""])[1]
    assert axes.before == (""batch"", ""embed"")
",tests/test_namedarray_typing.py,
survived,"    def foo(x: Named[""batch embed""]):
        pass
",tests/test_namedarray_typing.py,
survived,"def tpus_per_node(tpu_type: str) -> int:
    """"""Return the number of TPU chips per node for a given TPU type.""""""
    if tpu_type in {""v4-8"", ""v5p-8""}:
        return 4
    match = re.search(r""-(\d+)$"", tpu_type)
    if not match:
        raise ValueError(f""Cannot parse TPU type: {tpu_type}"")
    chips = int(match.group(1))
    if chips > 8:
        raise ValueError(""Only single tpu nodes are supported with the CLI"")
    return chips
",marin/run/ray_run.py,
survived,"def test_tpus_per_node():
    assert tpus_per_node(""v4-8"") == 4
    assert tpus_per_node(""v5p-8"") == 4
    assert tpus_per_node(""v5e-4"") == 4
    assert tpus_per_node(""v5e-2"") == 2
    with pytest.raises(ValueError):
        tpus_per_node(""v5e-16"")",tests/test_ray_run.py,
survived,"    def to_json(self, report: SummaryReport) -> str:
        """"""Return a JSON representation of the report.""""""
        return json.dumps(asdict(report), indent=2)
",src/meta_agent/evaluation/reporting.py,ReportingModule
survived,"    def summarize(self, result: CollectionResult) -> SummaryReport:
        """"""Create a :class:`SummaryReport` from a collection result.""""""
        return SummaryReport(
            exit_code=result.exit_code,
            passed=result.exit_code == 0,
            duration=result.duration,
            stdout=result.stdout,
            stderr=result.stderr,
        )
",src/meta_agent/evaluation/reporting.py,ReportingModule
survived,"def egyptianDivide(dividend, divisor):
    if dividend < 0 or divisor <= 0:
        panic(""Invalid argument(s)"")
    if dividend < divisor:
        return DivResult(q=0, r=dividend)
    powers = [1]
    doublings = [divisor]
    doubling = divisor * 2
    while doubling <= dividend:
        powers = powers + [powers[len(powers) - 1] * 2]
        doublings = doublings + [doubling]
        doubling = doubling * 2
    ans = 0
    accum = 0
    i = len(doublings) - 1
    while i >= 0:
        if accum + doublings[i] <= dividend:
            accum = accum + doublings[i]
            ans = ans + powers[i]
            if accum == dividend:
                break
        i = i - 1
    return DivResult(q=ans, r=dividend - accum)
",tests/rosetta/transpiler/Python/egyptian-division.py,
survived,"def elementWiseMM(m1, m2, f):
    z = []
    r = 0
    while r < len(m1):
        row = []
        c = 0
        while c < len(m1[r]):
            row = row + [f(m1[r][c], m2[r][c])]
            c = c + 1
        z = z + [row]
        r = r + 1
    return z
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def singleInit(cells):
    s = """"
    i = 0
    while i < cells:
        if i == (cells // 2):
            s = s + ""1""
        else:
            s = s + ""0""
        i = i + 1
    return s
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/empty-string-1.py,
survived,"def revInt(n):
    r = 0
    t = n
    while t > 0:
        r = r * 10 + t % 10
        t = int((t // 10))
    return r
",tests/rosetta/transpiler/Python/emirp-primes.py,
survived,"def powf(base, exp):
    if exp == 0.5:
        guess = base
        i = 0
        while i < 20:
            guess = (guess + base // guess) / 2.0
            i = i + 1
        return guess
    result = 1.0
    n = int(exp)
    i = 0
    while i < n:
        result = result * base
        i = i + 1
    return result
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def sub(a, b):
    return a - b
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    m1 = [[3.0, 1.0, 4.0], [1.0, 5.0, 9.0]]
    m2 = [[2.0, 7.0, 1.0], [8.0, 2.0, 8.0]]
    printMatrix(""m1:"", m1)
    printMatrix(""m2:"", m2)
    print("""")
    printMatrix(""m1 + m2:"", elementWiseMM(m1, m2, add))
    printMatrix(""m1 - m2:"", elementWiseMM(m1, m2, sub))
    printMatrix(""m1 * m2:"", elementWiseMM(m1, m2, mul))
    printMatrix(""m1 / m2:"", elementWiseMM(m1, m2, div))
    printMatrix(""m1 ^ m2:"", elementWiseMM(m1, m2, exp))
    print("""")
    s = 0.5
    print(""s: "" + str(s))
    printMatrix(""m1 + s:"", elementWiseMS(m1, s, add))
    printMatrix(""m1 - s:"", elementWiseMS(m1, s, sub))
    printMatrix(""m1 * s:"", elementWiseMS(m1, s, mul))
    printMatrix(""m1 / s:"", elementWiseMS(m1, s, div))
    printMatrix(""m1 ^ s:"", elementWiseMS(m1, s, exp))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def main() -> None:
    parser = argparse.ArgumentParser(description=""Backup Lidarr artists to CSV"")
    parser.add_argument(
        ""-c"",
        ""--config"",
        default=""./config.ini"",
        help=""Path to config.ini"",
    )
    parser.add_argument(
        ""-o"",
        ""--output"",
        default=""./lidarr_backup.csv"",
        help=""Output CSV file"",
    )

    args = parser.parse_args()

    try:
        backup_lidarr(args.config, args.output)
        print(""Done..."")
    except Exception as exc:  # keep CLI simple
        print(f""Error: {exc}"")
",backup_lidarr_2csv.py,
survived,"def docker_compose_cmd() -> list[str]:
    if subprocess.run([""docker"", ""compose"", ""version""], capture_output=True).returncode == 0:
        return [""docker"", ""compose""]
    if subprocess.run([""docker-compose"", ""--version""], capture_output=True).returncode == 0:
        return [""docker-compose""]
    sys.exit(""docker compose plugin not found"")
",alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py,
survived,"def main(argv: list[str] | None = None) -> None:  # pragma: no cover - CLI wrapper
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument(""--alpha"", default=""Generic opportunity"", help=""text description of the opportunity"")
    p.add_argument(""--ledger"", help=""path to ledger JSON file"")
    p.add_argument(""--model"", default=os.getenv(""ALPHA_CONVERSION_MODEL"", ""gpt-4o-mini""), help=""OpenAI model when API key present"")
    p.add_argument(""--no-log"", action=""store_true"", help=""do not write ledger file"")
    args = p.parse_args(argv)

    ledger = _ledger_path(args.ledger)
    plan = convert_alpha(args.alpha, ledger=None if args.no_log else ledger, model=args.model)
    if args.no_log:
        ledger.unlink(missing_ok=True)
    print(json.dumps(plan, indent=2))
    if not args.no_log:
        print(f""Logged to {ledger}"")
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py,
survived,"def convert_alpha(alpha: str, *, ledger: Path | None = None, model: str = ""gpt-4o-mini"") -> Dict[str, object]:
    """"""Return a plan dictionary and log to *ledger*.""""""
    plan: Dict[str, object] = SAMPLE_PLAN
    if ""openai"" in globals() and os.getenv(""OPENAI_API_KEY""):
        prompt = (
            f""Given the opportunity: {alpha}\n""
            ""Provide a short JSON plan with three concise steps to realise value.""
        )
        try:
            resp = openai.ChatCompletion.create(
                model=model,
                messages=[{""role"": ""user"", ""content"": prompt}],
            )
            plan = json.loads(resp.choices[0].message.content)  # type: ignore[index]
            if not isinstance(plan, dict):
                plan = SAMPLE_PLAN
        except Exception:
            plan = SAMPLE_PLAN
    (_ledger_path(ledger)).write_text(json.dumps(plan, indent=2))
    return plan
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_conversion_stub.py,
survived,"            def add(self, instr: Any) -> ""DummyTx"":
                self.instructions.append(instr)
                return self
",tests/test_ledger_broadcast.py,DummyTx
survived,"            async def send_transaction(self, tx: Any, *args: Any) -> None:
                captured[""root""] = tx.instructions[0].data.decode()
                raise RuntimeError(""fail"")
",tests/test_ledger_broadcast.py,DummyClient
survived,"        def __bool__(self):
            return False
",tests/test_statemachine.py,FalseyModel
survived,"    def visit_For(self, node):
        self.emit(f""for {self.expr(node.target)} in {self.expr(node.iter)} {{"")
        self.indent += 1
        for s in node.body:
            self.visit(s)
        self.indent -= 1
        self.emit(""}"")
",tools/any2mochi/py_simple.py,Conv
survived,"async def test_as_proxy_with_server(fastmcp_server):
    """"""FastMCP.as_proxy should accept a FastMCP instance.""""""
    proxy = FastMCP.as_proxy(fastmcp_server)
    result = await proxy._mcp_call_tool(""greet"", {""name"": ""Test""})
    assert isinstance(result[0], mcp.types.TextContent)
    assert result[0].text == ""Hello, Test!""
",tests/server/test_proxy.py,
survived,"    def _scan_shell_commands(self, content: str) -> List[str]:
        issues: List[str] = []
        for pattern, desc in self._SHELL_PATTERNS:
            if re.search(pattern, content):
                issues.append(f""shell command detected: {desc}"")
        return issues
",src/meta_agent/template_validator.py,TemplateValidator
survived,"def f1_score(truth: list[bool], pred: list[bool]) -> float:
    """"""Return the F1 score for ``pred`` against ``truth``.""""""
    tp = sum(t and p for t, p in zip(truth, pred))
    fp = sum((not t) and p for t, p in zip(truth, pred))
    fn = sum(t and (not p) for t, p in zip(truth, pred))
    denom = 2 * tp + fp + fn
    if denom == 0:
        # perfect prediction with no positive samples
        return 1.0
    return 2 * tp / denom
",src/simulation/replay.py,
survived,"            def labels(self, *_a: Any, **_kw: Any) -> ""_N"":
                return self
",src/interface/api_server.py,_N
survived,"            def observe(self, *_a: Any) -> None: ...
",src/interface/api_server.py,_N
survived,"def test_safety_agent_emits_status() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = safety_agent.SafetyGuardianAgent(bus, led)
    env = messaging.Envelope(""codegen"", ""safety"", {""code"": ""import os""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][1].payload[""status""] == ""blocked""
",tests/test_agent_handle_methods.py,
survived,"def test_log_and_tail(tmp_path):
    ledger = Ledger(str(tmp_path / ""ledger.db""), broadcast=False)
    e1 = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    e2 = messaging.Envelope(""b"", ""c"", {""v"": 2}, 1.0)
    ledger.log(e1)
    ledger.log(e2)
    tail = ledger.tail(2)
    assert tail[0][""payload""][""v""] == 1
    assert tail[1][""payload""][""v""] == 2",tests/test_ledger_basic.py,
survived,"def test_planning_agent_handle_logs() -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = planning_agent.PlanningAgent(bus, led)
    env = messaging.Envelope(""a"", ""planning"", {""plan"": ""x""}, 0.0)
    asyncio.run(agent.handle(env))
    assert led.logged and led.logged[0] is env
",tests/test_agent_handle_methods.py,
survived,"def local_gpt2_tokenizer(tmp_path_factory):
    """"""Load a GPT2 tokenizer from a local JSON file to avoid network downloads.""""""

    config_src = Path(__file__).parent / ""gpt2_tokenizer_config.json""
    tmpdir = tmp_path_factory.mktemp(""gpt2_tok"")
    shutil.copy(config_src, tmpdir / ""tokenizer.json"")
    shutil.copy(config_src, tmpdir / ""tokenizer_config.json"")
    (tmpdir / ""config.json"").write_text(json.dumps({""model_type"": ""gpt2"", ""vocab_size"": 5027}))
    return AutoTokenizer.from_pretrained(str(tmpdir))",tests/conftest.py,
survived,"    def _append_memory_leak_analysis(self, snapshot_file):
        """"""
        åˆ†æžå†…å­˜æ³„æ¼å’Œå¢žé•¿è¶‹åŠ¿
        """"""
        with open(snapshot_file, 'a', encoding='utf-8') as f:
            f.write(""\n"" + ""="" * 80 + ""\n"")
            f.write(""å†…å­˜æ³„æ¼åˆ†æž:\n"")
            f.write(""-"" * 80 + ""\n"")
            
            # èŽ·å–tracemallocç»Ÿè®¡
            current, peak = tracemalloc.get_traced_memory()
            f.write(f""å½“å‰tracemallocå†…å­˜: {current / 1024 / 1024:.2f} MB\n"")
            f.write(f""tracemallocå³°å€¼å†…å­˜: {peak / 1024 / 1024:.2f} MB\n"")
            
            # èŽ·å–å†…å­˜åˆ†é…ç»Ÿè®¡
            try:
                stats = tracemalloc.get_traced_memory()
                f.write(f""å†…å­˜åˆ†é…ç»Ÿè®¡: {stats}\n"")
                
                # èŽ·å–å‰10ä¸ªå†…å­˜åˆ†é…æœ€å¤šçš„ä½ç½®
                snapshot = tracemalloc.take_snapshot()
                top_stats = snapshot.statistics('lineno')
                
                f.write(""\nå†…å­˜åˆ†é…æœ€å¤šçš„ä½ç½® (å‰10ä¸ª):\n"")
                f.write(""-"" * 80 + ""\n"")
                for i, stat in enumerate(top_stats[:10], 1):
                    f.write(f""{i:2d}. {stat.count:>8} ä¸ªå¯¹è±¡, {stat.size / 1024 / 1024:>8.2f} MB\n"")
                    f.write(f""    {stat.traceback.format()}\n"")
                    
            except Exception as e:
                f.write(f""èŽ·å–å†…å­˜åˆ†é…ç»Ÿè®¡å¤±è´¥: {e}\n"")
            
            # åžƒåœ¾å›žæ”¶ç»Ÿè®¡
            f.write(""\nåžƒåœ¾å›žæ”¶ç»Ÿè®¡:\n"")
            f.write(""-"" * 80 + ""\n"")
            for i in range(3):
                count = gc.get_count()[i]
                f.write(f""GCä»£ {i}: {count} æ¬¡\n"")
            
            # èŽ·å–ä¸å¯è¾¾å¯¹è±¡æ•°é‡
            unreachable = len(gc.garbage)
            f.write(f""ä¸å¯è¾¾å¯¹è±¡æ•°é‡: {unreachable}\n"")
            
            f.flush()
        
        logger.debug(""å†…å­˜æ³„æ¼åˆ†æžå·²å®Œæˆå¹¶å†™å…¥"")
",app/helper/memory.py,MemoryHelper
survived,"    async def rollout(self, client, model, prompt, answer, task=""default"", info={}, sampling_args={}, **kwargs):
        """"""Simple test rollout implementation.""""""
        response = await self.get_model_response(
            prompt=prompt,
            client=client,
            model=model,
            sampling_args=sampling_args
        )
        if self.message_type == 'chat':
            return [{'role': 'assistant', 'content': response}], {}
        return response, {}
",tests/test_environment.py,SimpleEnvironment
deleted,"    def _process_complex_signal_partition(self, signal_type: type) -> list[Column]:
        """"""Process complex signal types (DataModel subclasses) for partition_by.
        
        Args:
            signal_type: The DataModel type to process (e.g., File, Image)
            
        Returns:
            List of Column objects representing the unique identifier columns
            for the complex signal type.
        """"""
        if not (isinstance(signal_type, type) and issubclass(signal_type, DataModel)):
            raise ValueError(
                f""Complex signal type {signal_type} must be a DataModel subclass""
            )
        
        # Find the signal name in the schema that matches this type
        signal_name = None
        for name, schema_type in self.signals_schema.values.items():
            if schema_type == signal_type:
                signal_name = name
                break
        
        if signal_name is None:
            raise ValueError(
                f""Signal type {signal_type} not found in the current schema""
            )
        
        # Get the unique ID keys for this DataModel type
        unique_keys = getattr(signal_type, '_unique_id_keys', None)
        if unique_keys is None:
            # Fall back to using all columns of the signal if no unique keys defined
            unique_keys = list(signal_type._datachain_column_types.keys())
        
        # Generate column objects for each unique key
        partition_columns = []
        for key in unique_keys:
            col_name = f""{signal_name}.{key}""
            col_db_name = ColumnMeta.to_db_name(col_name)
            try:
                col_type = self.signals_schema.get_column_type(col_db_name)
                column = Column(col_db_name, python_to_sql(col_type))
                partition_columns.append(column)
            except Exception:
                # Skip columns that don't exist in the schema
                continue
        
        if not partition_columns:
            raise ValueError(
                f""No valid partition columns found for signal type {signal_type}""
            )
        
        return partition_columns
",src/datachain/lib/dc/datachain.py,DataChain
survived,"def ask_pkgx(import_id: str) -> str | None:
    """"""
    ask max's scraping work for the homepage of a package
    Homepage comes from the pkgxdev/www repo
    The API https://pkgx.dev/pkgs/{name}.json returns a blob which may contain
    the homepage field
    """"""
    response: Response = get(HOMEPAGE_URL.format(name=import_id))
    if response.status_code == 200:
        data: dict[str, str] = response.json()
        if ""homepage"" in data:
            return data[""homepage""]
",package_managers/pkgx/url.py,
survived,"    def test_sort_by_speed_index_asc(self):
        """"""Test sorting by speed index (lowest first).""""""
        models: list[ConciseModelResponse | ConciseLatestModelResponse] = [
            create_test_model(""model1"", speed_index=300),
            create_test_model(""model2"", speed_index=800),
            create_test_model(""model3"", speed_index=600),
        ]

        sorted_models = sort_models(models, ""speed_index"", ""asc"")

        assert [m.id for m in sorted_models] == [""model1"", ""model3"", ""model2""]
",api/api/routers/mcp/_utils/model_sorting_test.py,TestSortModels
survived,"    def test_perplexity_reasoning_models_support_reasoning(self):
        """"""
        Test that Perplexity Sonar reasoning models are correctly identified as supporting reasoning
        """"""
        from litellm.utils import supports_reasoning
        
        # Set up local model cost map
        os.environ[""LITELLM_LOCAL_MODEL_COST_MAP""] = ""True""
        litellm.model_cost = litellm.get_model_cost_map(url="""")
        
        reasoning_models = [
            ""perplexity/sonar-reasoning"",
            ""perplexity/sonar-reasoning-pro"",
        ]
        
        for model in reasoning_models:
            assert supports_reasoning(model, None), f""{model} should support reasoning""
",tests/llm_translation/test_perplexity_reasoning.py,TestPerplexityReasoning
survived,"def test_perplexity_config():
    """"""Test Perplexity config and supported parameters""""""
    print(""Testing Perplexity configuration..."")
    
    from litellm.llms.perplexity.chat.transformation import PerplexityChatConfig
    
    config = PerplexityChatConfig()
    
    # Test API base configuration
    api_base, api_key = config._get_openai_compatible_provider_info(
        api_base=None, api_key=""test-key""
    )
    
    expected_api_base = ""https://api.perplexity.ai""
    print(f""API Base: {api_base}"")
    assert api_base == expected_api_base, f""API base should be {expected_api_base}""
    
    # Test supported parameters
    supported_params = config.get_supported_openai_params(model=""perplexity/sonar-reasoning"")
    print(f""Supported params: {supported_params}"")
    
    assert ""reasoning_effort"" in supported_params, ""reasoning_effort should be in supported params""
    
    print(""âœ“ Perplexity configuration test passed!\n"")
",verify_perplexity_reasoning.py,
survived,"def main():
    """"""Run all verification tests""""""
    print(""=== Perplexity Reasoning Effort Verification ===\n"")
    
    try:
        test_perplexity_reasoning_models_in_model_cost()
        test_reasoning_effort_parameter_mapping()
        test_perplexity_reasoning_support()
        test_perplexity_config()
        
        print(""ðŸŽ‰ All tests passed! Perplexity reasoning effort functionality is working correctly."")
        
    except Exception as e:
        print(f""âŒ Test failed with error: {e}"")
        import traceback
        traceback.print_exc()
        sys.exit(1)
",verify_perplexity_reasoning.py,
survived,"def update_preset_column_config(
    preset_id: uuid.UUID,
    body: ColumnConfigurationDto,
    authenticated_entity: AuthenticatedEntity = Depends(
        IdentityManagerFactory.get_auth_verifier([""write:presets""])
    ),
    session: Session = Depends(get_session),
) -> PresetDto:
    tenant_id = authenticated_entity.tenant_id
    logger.info(""Updating preset column configuration"", extra={""preset_id"": preset_id})
    
    statement = (
        select(Preset)
        .where(Preset.tenant_id == tenant_id)
        .where(Preset.id == preset_id)
    )
    preset = session.exec(statement).first()
    if not preset:
        raise HTTPException(404, ""Preset not found"")

    # Get current options and remove any existing column config options
    current_options = [
        option for option in preset.options 
        if option.get(""label"", """").lower() not in [
            ""column_visibility"", 
            ""column_order"", 
            ""column_rename_mapping"", 
            ""column_time_formats"", 
            ""column_list_formats""
        ]
    ]

    # Add new column configuration options
    if body.column_visibility:
        current_options.append({
            ""label"": ""column_visibility"",
            ""value"": body.column_visibility
        })
    
    if body.column_order:
        current_options.append({
            ""label"": ""column_order"", 
            ""value"": body.column_order
        })
    
    if body.column_rename_mapping:
        current_options.append({
            ""label"": ""column_rename_mapping"",
            ""value"": body.column_rename_mapping
        })
    
    if body.column_time_formats:
        current_options.append({
            ""label"": ""column_time_formats"",
            ""value"": body.column_time_formats
        })
    
    if body.column_list_formats:
        current_options.append({
            ""label"": ""column_list_formats"",
            ""value"": body.column_list_formats
        })

    # Update the preset options
    preset.options = current_options
    session.commit()
    session.refresh(preset)
    
    logger.info(""Updated preset column configuration"", extra={""preset_id"": preset_id})
    return PresetDto(**preset.to_dict())
",keep/api/routes/preset.py,
survived,"    def column_time_formats(self) -> Dict[str, str]:
        """"""Get column time formats from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_time_formats""
        ]
        if not config:
            return {}
        return config[0].get(""value"", {})
",keep/api/models/db/preset.py,PresetDto
survived,"    def column_order(self) -> List[str]:
        """"""Get column order configuration from preset options""""""
        config = [
            option
            for option in self.options
            if option.get(""label"", """").lower() == ""column_order""
        ]
        if not config:
            return []
        return config[0].get(""value"", [])
",keep/api/models/db/preset.py,PresetDto
survived,"def test_chained_split_gather_workflow():
    """"""Test a realistic workflow combining split and gather operations.""""""
    # Start with a document
    df = pd.DataFrame({
        ""document"": [
            ""This is the first paragraph of a long document. "" * 5 + 
            ""This is the second paragraph with different content. "" * 5 +
            ""This is the third and final paragraph. "" * 5
        ],
        ""doc_id"": [""doc1""]
    })
    
    # Split the document into chunks
    split_result = df.semantic.split(
        split_key=""document"",
        method=""token_count"",
        method_kwargs={""num_tokens"": 20}
    )
    
    # Gather context for each chunk
    gather_result = split_result.semantic.gather(
        content_key=""document_chunk"",
        doc_id_key=""semantic_split_0_id"",
        order_key=""semantic_split_0_chunk_num"",
        peripheral_chunks={
            ""previous"": {""head"": {""count"": 1}},
            ""next"": {""head"": {""count"": 1}}
        }
    )
    
    assert len(gather_result) >= len(df)  # Should have multiple chunks
    assert ""document_chunk_rendered"" in gather_result.columns
    assert len(gather_result.semantic.history) == 2
    assert gather_result.semantic.history[0].op_type == ""split""
    assert gather_result.semantic.history[1].op_type == ""gather"" ",tests/test_pandas_accessors.py,
survived,"    def test_private_fields_accepts_task_input_and_task_output(self):
        """"""Test that private_fields accepts both 'task_input' and 'task_output' as valid literal values.

        This test ensures that the private_fields type annotation correctly includes both
        'task_input' and 'task_output' (not a duplicate 'task_input').
        """"""
        # This should work - task_input is valid
        request_with_task_input = RunRequest.model_validate(
            {
                ""task_input"": {""test"": ""data""},
                ""version"": 1,
                ""private_fields"": [""task_input""],
            }
        )
        assert ""task_input"" in request_with_task_input.private_fields

        # This should also work - task_output should be valid
        request_with_task_output = RunRequest.model_validate(
            {
                ""task_input"": {""test"": ""data""},
                ""version"": 1,
                ""private_fields"": [""task_output""],
            }
        )
        assert ""task_output"" in request_with_task_output.private_fields

        # Both should work together
        request_with_both = RunRequest.model_validate(
            {
                ""task_input"": {""test"": ""data""},
                ""version"": 1,
                ""private_fields"": [""task_input"", ""task_output""],
            }
        )
        assert ""task_input"" in request_with_both.private_fields
        assert ""task_output"" in request_with_both.private_fields

        # Custom string fields should also work
        request_with_custom = RunRequest.model_validate(
            {
                ""task_input"": {""test"": ""data""},
                ""version"": 1,
                ""private_fields"": [""task_input.image"", ""metadata.secret""],
            }
        )
        assert ""task_input.image"" in request_with_custom.private_fields
        assert ""metadata.secret"" in request_with_custom.private_fields",api/api/routers/run_test.py,TestRunRequestValidation
survived,"def test_severity_preprocessing_cel_utils_integration(
    db_session, workflow_manager, create_workflow, create_alert
):
    """"""
    Test that the cel_utils.preprocess_cel_expression function is properly integrated
    into the workflow manager to fix the lexicographic comparison bug
    """"""
    
    # This test specifically validates that lexicographic issues are resolved
    # Before fix: 'high' < 'info' lexicographically (h comes before i in alphabet)
    # After fix: high (4) > info (2) numerically
    
    workflow = create_workflow(
        ""test-preprocessing-integration"", 
        ""severity > 'info'""
    )

    # Create a 'high' severity alert - this is the key test case
    # that would fail with lexicographic comparison but should pass with numeric
    high_alert = create_alert(
        severity=AlertSeverity.HIGH,
        source=[""test""], 
        fingerprint=""fp-high-severity""
    )

    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [high_alert])
    
    # This assertion would fail before the fix, but should pass after
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1, \
        ""HIGH severity alert should match 'severity > info' expression after preprocessing fix""
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id",tests/test_workflow_severity_comparisons.py,
survived,"def parse_packages_file(file_path: str) -> dict[str, str | None]:
    """"""
    Parse the packages file and return a mapping of package_name -> source_name.

    Args:
        file_path: Path to the packages file

    Returns:
        Dictionary mapping package names to their source package names (None if not specified)
    """"""
    package_source_map = {}

    with open(file_path, encoding=""utf-8"") as f:
        current_package = None
        current_source = None

        for line in f:
            line = line.strip()

            if line.startswith(""Package: ""):
                # Save previous package if exists
                if current_package:
                    package_source_map[current_package] = current_source

                # Start new package
                current_package = line[9:].strip()
                current_source = None

            elif line.startswith(""Source: ""):
                # Extract source name (may include version info in parentheses)
                source_str = line[8:].strip()
                # Remove version info if present: ""source (version)"" -> ""source""
                if ""("" in source_str:
                    current_source = source_str.split(""("")[0].strip()
                else:
                    current_source = source_str

            elif line == """" and current_package:
                # End of current package entry
                package_source_map[current_package] = current_source
                current_package = None
                current_source = None

        # Handle last package if file doesn't end with blank line
        if current_package:
            package_source_map[current_package] = current_source

    return package_source_map
",package_managers/debian/scripts/investigate_sources.py,
survived,"def create_debian_package(
    package: str = ""test-package"",
    description: str = ""Test package"",
    homepage: str = """",
    vcs_git: str = """",
    vcs_browser: str = """",
    directory: str = """",
    filename: str = """",
    depends: list[str] | None = None,
    build_depends: list[str] | None = None,
    recommends: list[str] | None = None,
    suggests: list[str] | None = None,
) -> DebianData:
    """"""Helper to create DebianData instances for testing""""""

    debian_data = DebianData()
    debian_data.package = package
    debian_data.description = description
    debian_data.homepage = homepage
    debian_data.vcs_git = vcs_git
    debian_data.vcs_browser = vcs_browser
    debian_data.directory = directory
    debian_data.filename = filename

    # Convert string dependencies to Depends objects
    if depends:
        debian_data.depends = [Depends(package=dep, semver=""*"") for dep in depends]
    if build_depends:
        # build_depends is now list[Depends] like other dependency fields
        debian_data.build_depends = [
            Depends(package=dep, semver=""*"") for dep in build_depends
        ]
    if recommends:
        debian_data.recommends = [
            Depends(package=dep, semver=""*"") for dep in recommends
        ]
    if suggests:
        debian_data.suggests = [Depends(package=dep, semver=""*"") for dep in suggests]

    return debian_data",tests/package_managers/debian/conftest.py,
survived,"    def test_dependency_type_change_build_to_runtime(
        self, mock_config, mock_logger, mock_db
    ):
        """"""
        Scenario:
          - p1 has build dependency to p2 in cache
          - p1 has runtime dependency to p2 in parsed data.

        Expect removed build dependency and new runtime dependency
        """"""

        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""debian/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""debian/p2"", name=""p2"", import_id=""p2"")

        # Existing build dependency
        existing_build_dep = LegacyDependency(
            package_id=p1_id,
            dependency_id=p2_id,
            dependency_type_id=mock_config.dependency_types.build,
        )

        cache = Cache(
            package_map={""debian/p1"": p1_pkg, ""debian/p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={p1_id: {existing_build_dep}},
        )

        # Parsed data only has runtime dependency
        new_pkg_data = create_debian_package(
            package=""p1"",
            depends=[""p2""],  # runtime
            build_depends=[],  # no build deps
        )

        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""debian/p1"", new_pkg_data)

        # Should remove build and add runtime
        assert len(removed_deps) == 1
        assert removed_deps[0].dependency_id == p2_id
        assert removed_deps[0].dependency_type_id == mock_config.dependency_types.build

        assert len(new_deps) == 1
        assert new_deps[0].dependency_id == p2_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.runtime
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading
survived,"    def diff_pkg_url(
        self, pkg_id: UUID, resolved_urls: dict[UUID, UUID]
    ) -> tuple[list[PackageURL], list[dict]]:
        """"""Takes in a package_id and resolved URLs from diff_url, and generates
        new PackageURL objects as well as a list of changes to existing ones""""""

        new_links: list[PackageURL] = []
        updates: list[dict] = []

        # what are the existing links?
        existing: set[UUID] = {
            pu.url_id for pu in self.caches.package_urls.get(pkg_id, set())
        }

        # for each URL type/URL for this package:
        for _url_type, url_id in resolved_urls.items():
            if url_id not in existing:
                # new link!
                new_links.append(
                    PackageURL(
                        id=uuid4(),
                        package_id=pkg_id,
                        url_id=url_id,
                        created_at=self.now,
                        updated_at=self.now,
                    )
                )
            else:
                # existing link - update timestamp
                existing_pu = next(
                    pu for pu in self.caches.package_urls[pkg_id] if pu.url_id == url_id
                )
                existing_pu.updated_at = self.now
                updates.append({""id"": existing_pu.id, ""updated_at"": self.now})

        return new_links, updates
",package_managers/debian/diff.py,DebianDiff
survived,"def test_binutils(binutils):
    m = mock_open(read_data=binutils)

    with patch(""builtins.open"", m):
        result = parse_sources_file(""dummy"")

    assert result == {
        ""binutils"": {
            ""binutils-for-host"",
            ""binutils-for-build"",
            ""binutils-ia64-linux-gnu-dbg"",
            ""binutils-m68k-linux-gnu"",
            ""binutils-mips64el-linux-gnuabin32-dbg"",
            ""binutils-mipsisa64r6-linux-gnuabin32"",
            ""binutils-mipsisa64r6el-linux-gnuabi64-dbg"",
        }
    }
",package_managers/debian/scripts/test_investigate_sources.py,
survived,"def get_task_status(task_id):
    """"""Get the status of a specific task""""""
    if task_id not in tasks:
        logger.warning(f""ðŸ” Frontend polling for unknown task: {task_id}"")
        return jsonify({'error': 'Task not found'}), 404
    
    task = tasks[task_id]
    logger.info(f""ðŸ“Š Frontend polling task {task_id}: status={task['status']}"")
    
    return jsonify({
        'status': 'success',
        'task': {
            'id': task['id'],
            'status': task['status'],
            'prompt': task['prompt'],
            'repo_url': task['repo_url'],
            'branch': task['branch'],
            'model': task.get('model', 'claude'),  # Include model in response
            'commit_hash': task.get('commit_hash'),
            'changed_files': task.get('changed_files', []),
            'error': task.get('error'),
            'created_at': task['created_at']
        }
    })
",server/tasks.py,
survived,"def home():
    """"""Root endpoint""""""
    return jsonify({
        'status': 'success',
        'message': 'Claude Code Automation API',
        'endpoints': ['/ping', '/start-task', '/task-status', '/git-diff', '/create-pr']
    })",server/health.py,
survived,"def get_git_diff(task_id):
    """"""Get the git diff for a completed task""""""
    if task_id not in tasks:
        return jsonify({'error': 'Task not found'}), 404
    
    task = tasks[task_id]
    logger.info(f""ðŸ“‹ Frontend requesting git diff for task {task_id} (status: {task['status']})"")
    
    if task['status'] != TaskStatus.COMPLETED:
        logger.warning(f""âš ï¸ Git diff requested for incomplete task {task_id}"")
        return jsonify({'error': 'Task not completed yet'}), 400
    
    diff_length = len(task.get('git_diff', ''))
    logger.info(f""ðŸ“„ Returning git diff: {diff_length} characters"")
    
    return jsonify({
        'status': 'success',
        'git_diff': task.get('git_diff', ''),
        'commit_hash': task.get('commit_hash')
    })",server/git_operations.py,
survived,"def migrate_legacy_tasks():
    """"""Migrate tasks from legacy JSON storage to Supabase""""""
    try:
        user_id = request.headers.get('X-User-ID')
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        # This would be called manually to migrate existing tasks
        # Load legacy tasks from file if it exists
        import json
        import os
        
        legacy_file = 'tasks_backup.json'
        if not os.path.exists(legacy_file):
            return jsonify({
                'status': 'success',
                'message': 'No legacy tasks file found',
                'migrated': 0
            })
        
        with open(legacy_file, 'r') as f:
            legacy_tasks = json.load(f)
        
        migrated_count = 0
        for task_id, task_data in legacy_tasks.items():
            try:
                # Check if already migrated
                existing = DatabaseOperations.get_task_by_legacy_id(task_id)
                if existing:
                    continue
                
                # Migrate task
                DatabaseOperations.migrate_legacy_task(task_data, user_id)
                migrated_count += 1
            except Exception as e:
                logger.warning(f""Failed to migrate task {task_id}: {e}"")
        
        return jsonify({
            'status': 'success',
            'message': f'Migrated {migrated_count} tasks',
            'migrated': migrated_count
        })
        
    except Exception as e:
        logger.error(f""Error migrating legacy tasks: {str(e)}"")
        return jsonify({'error': str(e)}), 500",server/tasks.py,
survived,"def validate_github_token():
    """"""Validate GitHub token and check permissions""""""
    try:
        data = request.get_json()
        github_token = data.get('github_token')
        repo_url = data.get('repo_url', '')
        
        if not github_token:
            return jsonify({'error': 'github_token is required'}), 400
        
        # Create GitHub client
        g = Github(github_token)
        
        # Test basic authentication
        user = g.get_user()
        logger.info(f""ðŸ” Token belongs to user: {user.login}"")
        
        # Test token scopes
        rate_limit = g.get_rate_limit()
        logger.info(f""ðŸ“Š Rate limit info: {rate_limit.core.remaining}/{rate_limit.core.limit}"")
        
        # If repo URL provided, test repo access
        repo_info = {}
        if repo_url:
            try:
                repo_parts = repo_url.replace('https://github.com/', '').replace('.git', '')
                repo = g.get_repo(repo_parts)
                
                # Test various permissions
                permissions = {
                    'read': True,  # If we got here, we can read
                    'write': False,
                    'admin': False
                }
                
                try:
                    # Test if we can read branches
                    branches = list(repo.get_branches())
                    permissions['read_branches'] = True
                    logger.info(f""âœ… Can read branches ({len(branches)} found)"")
                    
                    # Test if we can create branches
                    test_branch_name = f""test-permissions-{int(time.time())}""
                    try:
                        # Try to create a test branch
                        main_branch = repo.get_branch(repo.default_branch)
                        test_ref = repo.create_git_ref(f""refs/heads/{test_branch_name}"", main_branch.commit.sha)
                        permissions['create_branches'] = True
                        logger.info(f""âœ… Can create branches - test successful"")
                        
                        # Clean up test branch immediately
                        test_ref.delete()
                        logger.info(f""ðŸ§¹ Cleaned up test branch"")
                        
                    except Exception as branch_error:
                        permissions['create_branches'] = False
                        logger.warning(f""âŒ Cannot create branches: {branch_error}"")
                        
                except Exception as e:
                    permissions['read_branches'] = False
                    permissions['create_branches'] = False
                    logger.warning(f""âŒ Cannot read branches: {e}"")
                
                try:
                    # Check if we can write (without actually writing)
                    repo_perms = repo.permissions
                    permissions['write'] = repo_perms.push
                    permissions['admin'] = repo_perms.admin
                    logger.info(f""ðŸ“‹ Repo permissions: push={repo_perms.push}, admin={repo_perms.admin}"")
                except Exception as e:
                    logger.warning(f""âš ï¸ Could not check repo permissions: {e}"")
                
                repo_info = {
                    'name': repo.full_name,
                    'private': repo.private,
                    'permissions': permissions,
                    'default_branch': repo.default_branch
                }
                
            except Exception as repo_error:
                return jsonify({
                    'error': f'Cannot access repository: {str(repo_error)}',
                    'user': user.login
                }), 403
        
        return jsonify({
            'status': 'success',
            'user': user.login,
            'repo': repo_info,
            'message': 'Token is valid and has repository access'
        })
        
    except Exception as e:
        logger.error(f""Token validation error: {str(e)}"")
        return jsonify({'error': f'Token validation failed: {str(e)}'}), 401
",server/tasks.py,
survived,"def create_pull_request(task_id):
    """"""Create a pull request by applying the saved patch to a fresh repo clone""""""
    try:
        user_id = request.headers.get('X-User-ID')
        if not user_id:
            return jsonify({'error': 'User ID required'}), 400
        
        logger.info(f""ðŸ” PR creation requested for task: {task_id}"")
        
        task = DatabaseOperations.get_task_by_id(task_id, user_id)
        if not task:
            logger.error(f""âŒ Task {task_id} not found"")
            return jsonify({'error': 'Task not found'}), 404
        
        if task['status'] != 'completed':
            return jsonify({'error': 'Task not completed yet'}), 400
            
        if not task.get('git_patch'):
            return jsonify({'error': 'No patch data available for this task'}), 400
        
        data = request.get_json() or {}
        
        # Get prompt from chat messages
        prompt = """"
        if task.get('chat_messages'):
            for msg in task['chat_messages']:
                if msg.get('role') == 'user':
                    prompt = msg.get('content', '')
                    break
        
        pr_title = data.get('title', f""Claude Code: {prompt[:50]}..."")
        pr_body = data.get('body', f""Automated changes generated by Claude Code.\n\nPrompt: {prompt}\n\nChanged files:\n"" + '\n'.join(f""- {f}"" for f in task.get('changed_files', [])))
        github_token = data.get('github_token')
        
        if not github_token:
            return jsonify({'error': 'github_token is required'}), 400
        
        logger.info(f""ðŸš€ Creating PR for task {task_id}"")
        
        # Extract repo info from URL
        repo_parts = task['repo_url'].replace('https://github.com/', '').replace('.git', '')
        
        # Create GitHub client
        g = Github(github_token)
        repo = g.get_repo(repo_parts)
        
        # Determine branch strategy
        base_branch = task['target_branch']
        pr_branch = f""claude-code-{task_id}""
        
        logger.info(f""ðŸ“‹ Creating PR branch '{pr_branch}' from base '{base_branch}'"")
        
        # Get the latest commit from the base branch
        base_branch_obj = repo.get_branch(base_branch)
        base_sha = base_branch_obj.commit.sha
        
        # Create new branch for the PR
        try:
            # Check if branch already exists
            try:
                existing_branch = repo.get_branch(pr_branch)
                logger.warning(f""âš ï¸ Branch '{pr_branch}' already exists, deleting it first..."")
                repo.get_git_ref(f""heads/{pr_branch}"").delete()
                logger.info(f""ðŸ—‘ï¸ Deleted existing branch '{pr_branch}'"")
            except:
                pass  # Branch doesn't exist, which is what we want
            
            # Create the new branch
            new_ref = repo.create_git_ref(f""refs/heads/{pr_branch}"", base_sha)
            logger.info(f""âœ… Created branch '{pr_branch}' from {base_sha[:8]}"")
            
        except Exception as branch_error:
            logger.error(f""âŒ Failed to create branch '{pr_branch}': {str(branch_error)}"")
            
            # Provide specific error messages based on the error
            error_msg = str(branch_error).lower()
            if ""resource not accessible"" in error_msg:
                detailed_error = (
                    f""GitHub token lacks permission to create branches. ""
                    f""Please ensure your token has 'repo' scope (not just 'public_repo'). ""
                    f""Error: {branch_error}""
                )
            elif ""already exists"" in error_msg:
                detailed_error = f""Branch '{pr_branch}' already exists. Please try again or use a different task.""
            else:
                detailed_error = f""Failed to create branch '{pr_branch}': {branch_error}""
                
            return jsonify({'error': detailed_error}), 403
        
        # Apply the patch by creating/updating files
        logger.info(f""ðŸ“¦ Applying patch with {len(task.get('changed_files', []))} changed files..."")
        
        # For now, we'll use a simple approach to apply changes
        # In a real implementation, you'd want a more sophisticated patch parser
        patch_content = task['git_patch']
        files_updated = []
        
        # Simple file update based on changed_files list
        for file_path in task.get('changed_files', []):
            try:
                # This is a simplified approach - in reality you'd parse the patch properly
                logger.info(f""ðŸ“ Updating file: {file_path}"")
                files_updated.append(file_path)
            except Exception as e:
                logger.warning(f""Failed to update {file_path}: {e}"")
        
        # Create pull request
        pr = repo.create_pull(
            title=pr_title,
            body=pr_body,
            head=pr_branch,
            base=base_branch
        )
        
        # Update task with PR information
        DatabaseOperations.update_task(task_id, user_id, {
            'pr_branch': pr_branch,
            'pr_number': pr.number,
            'pr_url': pr.html_url
        })
        
        logger.info(f""ðŸŽ‰ Created PR #{pr.number}: {pr.html_url}"")
        
        return jsonify({
            'status': 'success',
            'pr_url': pr.html_url,
            'pr_number': pr.number,
            'branch': pr_branch,
            'files_updated': len(files_updated)
        })
        
    except Exception as e:
        logger.error(f""Error creating PR: {str(e)}"")
        return jsonify({'error': str(e)}), 500
",server/tasks.py,
survived,"def find_metadata_files(decrypted_dir: str) -> Dict[str, Any]:
    """"""
    Find all metadata.json files and categorize them by type.
    
    Args:
        decrypted_dir: Path to the decrypted_overrides directory
        
    Returns:
        Dictionary with categories as keys and lists of file paths as values
    """"""
    files = {
        ""global"": [],
        ""regions"": defaultdict(list),
        ""locales"": defaultdict(list)
    }
    
    # Walk through all directories
    for root, dirs, filenames in os.walk(decrypted_dir):
        if ""metadata.json"" in filenames:
            metadata_path = os.path.join(root, ""metadata.json"")
            
            # Check if this is a region-specific file
            if ""/region/"" in metadata_path:
                region_match = re.search(r'/region/([^/]+)/', metadata_path)
                if region_match:
                    region = region_match.group(1)
                    files[""regions""][region].append(metadata_path)
            
            # Check if this is a locale-specific file  
            elif ""/locale/"" in metadata_path:
                locale_match = re.search(r'/locale/([^/]+)/', metadata_path)
                if locale_match:
                    locale = locale_match.group(1)
                    files[""locales""][locale].append(metadata_path)
            
            # Check if this is a global file (directly in AssetData/)
            elif metadata_path.endswith(""/AssetData/metadata.json""):
                files[""global""].append(metadata_path)
    
    return files
",combine_metadata.py,
deleted,"        def create_flow_tool(graph_obj: Graph, flow_name: str, flow_desc: str):
            """"""Create a tool function for a specific flow.""""""
            
            @mcp.tool()
            def flow_tool(input_data: FlowInput) -> FlowOutput:
                f""""""Execute the {flow_name} flow.
                
                {flow_desc}
                """"""
                try:
                    # Import here to avoid circular imports
                    import time
                    
                    start_time = time.time()
                    
                    # Execute the flow
                    # Note: This follows the same pattern as the REST API execution
                    result = graph_obj.run(
                        inputs={""input_value"": input_data.input_value},
                        tweaks=input_data.tweaks or {}
                    )
                    
                    execution_time = time.time() - start_time
                    
                    return FlowOutput(
                        result=result,
                        execution_time=execution_time
                    )
                    
                except Exception as e:
                    return FlowOutput(
                        result=None,
                        error=str(e)
                    )
            
            # Dynamically set the function name to match the flow
            flow_tool.__name__ = f""execute_{flow_name.replace(' ', '_').replace('-', '_').lower()}""
            return flow_tool
",src/backend/base/langflow/cli/mcp_server.py,
survived,"    async def test_transfer_traces_fails_with_non_existent_project_id(
        self,
        gql_client: AsyncGraphQLClient,
        trace_transfer_fixture: dict[str, int],
        db: DbSessionFactory,
    ) -> None:
        trace1_id = trace_transfer_fixture[""trace1_id""]
        trace2_id = trace_transfer_fixture[""trace2_id""]

        async with db() as session:
            traces = (
                await session.scalars(
                    select(models.Trace).where(models.Trace.id.in_([trace1_id, trace2_id]))
                )
            ).all()
            assert len(traces) == 2

        result = await gql_client.execute(
            self.TRANSFER_TRACES_MUTATION,
            variables={
                ""traceIds"": [
                    str(GlobalID(""Trace"", str(trace1_id))),
                    str(GlobalID(""Trace"", str(trace2_id))),
                ],
                ""projectId"": str(GlobalID(""Project"", ""99999"")),
            },
        )
        assert result.errors

        async with db() as session:
            traces = (
                await session.scalars(
                    select(models.Trace).where(models.Trace.id.in_([trace1_id, trace2_id]))
                )
            ).all()
            assert len(traces) == 2
            source_project_id = trace_transfer_fixture[""source_project_id""]
            assert all(trace.project_rowid == source_project_id for trace in traces)
",tests/unit/server/api/mutations/test_trace_transfer_mutations.py,TestTraceTransferMutationMixin
survived,"    def test_env_group_mismatched_names_fails(self, mock_openai_client):
        """"""Test that EnvGroup fails when env_names length doesn't match envs.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        with pytest.raises(ValueError, match=""Number of env_names must match number of envs""):
            EnvGroup(envs=[env1], env_names=[""math"", ""code""])
",tests/test_env_group.py,TestEnvGroup
survived,"        def mock_encode(text, **kwargs):
            # Return tokens based on the text content
            if ""assistant: Hi there!"" in text:
                # Prompt + completion: return extended tokens
                return [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
            elif ""user: Hello"" in text:
                # Just prompt: return base tokens
                return [1, 2, 3, 4, 5]
            else:
                # Default case
                return [1, 2, 3]
",tests/test_environment.py,TestEnvironmentBase
survived,"    async def test_rollout_state_structure(self, mock_singleturn_env):
        """"""Test that rollout creates proper state structure.""""""
        prompt = [{""role"": ""user"", ""content"": ""Hello""}]
        answer = ""Hi""
        task = ""greeting""
        info = {""context"": ""test""}
        
        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model=""test-model"",
            prompt=prompt,
            answer=answer,
            task=task,
            info=info
        )
        
        # Check all expected state fields
        assert state[""prompt""] == prompt
        # state[""completion""] is initialized to [] but not updated during rollout
        assert state[""completion""] == []
        assert state[""answer""] == answer
        assert state[""task""] == task
        assert state[""info""] == info
        assert ""responses"" in state
        assert isinstance(state[""responses""], list)
        assert len(state[""responses""]) == 1
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"            def is_completed(self, messages, state, **kwargs):
                return ""DONE"" in messages
",tests/test_multiturn_env.py,TestMultiTurnEnv.CompletionMultiTurnEnv
survived,"    async def test_completion_format_multiturn(self, mock_openai_client):
        """"""Test MultiTurnEnv with completion format.""""""
        class CompletionMultiTurnEnv(MultiTurnEnv):
            def __init__(self, **kwargs):
                super().__init__(message_type=""completion"", **kwargs)
                
            def is_completed(self, messages, state, **kwargs):
                return ""DONE"" in messages
            
            def env_response(self, messages, state, **kwargs):
                return "" Continue."", state
        
        completion_dataset = Dataset.from_dict({
            ""prompt"": [""Start:""],
            ""answer"": [""Done""]
        })
        
        env = CompletionMultiTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=completion_dataset,
            max_turns=3
        )
        
        mock_openai_client.add_text_response(""Start:"", ""First response"")
        mock_openai_client.add_text_response(""Start:First response Continue."", ""Final DONE"")
        
        prompt = ""Start:""
        completion, state = await env.rollout(
            client=mock_openai_client,
            model=""test-model"",
            prompt=prompt,
            answer=""Done""
        )
        
        assert isinstance(completion, str)
        assert ""First response"" in completion
        assert ""DONE"" in completion
        assert len(state[""responses""]) == 2
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def test_mcp_json_saas_mode(self):
        with override_settings(SENTRY_MODE=""saas""):
            response = self.client.get(""/.well-known/mcp.json"")

        assert response.status_code == 200
        assert response[""Content-Type""] == ""application/json""
        
        data = json.loads(response.content)
        assert data[""name""] == ""Sentry""
        assert data[""description""] == ""Connect to Sentry, debug faster.""
        assert data[""endpoint""] == ""https://mcp.sentry.dev/mcp""
",tests/sentry/web/test_api.py,McpJsonTest
survived,"    def __init__(self, runner, console: Console):
        self.runner = runner
        self.console = console
        self.default_lm_api_base = runner.config.get(""default_lm_api_base"", None)
",docetl/operations/utils/api.py,LLMCallHandler
deleted,"    def _handle_model_error(self, model: str, error: Exception) -> None:
        """"""Handle model-specific errors.""""""
        if model not in BASIC_MODELS and ""/"" not in model:
            raise ValueError(
                f""Note: You may also need to prefix your model name with the provider, ""
                f""e.g. 'openai/gpt-4o-mini' or 'gemini/gemini-1.5-flash' to conform to ""
                f""LiteLLM API standards. Original error: {error}""
            )
        raise error
",docetl/operations/utils/api.py,LLMCallHandler
deleted,"    def _build_scratchpad_instructions(self) -> str:
        """"""Build instructions for scratchpad usage.""""""
        return """"""

You are incrementally processing data across multiple batches. You will see:
1. The current batch of data to process
2. The intermediate output so far (what you returned last time)
3. A scratchpad for tracking additional state

IMPORTANT: Only use the scratchpad if your task specifically requires tracking items that appear multiple times across batches. If you only need to track distinct/unique items, leave the scratchpad empty and set updated_scratchpad to null.

The intermediate output contains the result that directly answers the user's task, for **all** the data processed so far, including the current batch. You must return this via the send_output function.

Example task that NEEDS scratchpad - counting words that appear >2 times:
- Call send_output with: {""frequent_words"": [""the"", ""and""]} # Words seen 3+ times - this is your actual result
- Set updated_scratchpad to: {""pending"": {""cat"": 2, ""dog"": 1}} # Must track words seen 1-2 times

Example task that does NOT need scratchpad - collecting unique locations:
- Call send_output with: {""locations"": [""New York"", ""Paris""]} # Just the unique items
- Set updated_scratchpad to: null # No need to track counts since we only want distinct items

As you process each batch:
1. Use both the previous output and scratchpad (if needed) to inform your processing
2. Call send_output with your result that combines the current batch with previous output
3. Set updated_scratchpad only if you need to track counts/frequencies between batches

If you use the scratchpad, keep it concise (~500 chars) and easily parsable using:
- Key-value pairs
- JSON-like format
- Simple counters/tallies

Your main result must be sent via send_output. The updated_scratchpad is only for tracking state between batches, and should be null unless you specifically need to track frequencies.""""""
",docetl/operations/utils/api.py,LLMCallHandler
survived,"        def func1(completion, **kwargs):
            return 1.0
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_format_reward_function(self, xml_parser):
        """"""Test the format reward function.""""""
        reward_func = xml_parser.get_format_reward_func()
        
        # Well-formatted completion
        good_completion = [
            {""role"": ""assistant"", ""content"": ""<reasoning>Good reasoning</reasoning><answer>42</answer>""}
        ]
        good_reward = reward_func(good_completion)
        assert 0.0 <= good_reward <= 1.0
        
        # Poorly formatted completion - gets partial credit for proper spacing
        bad_completion = [
            {""role"": ""assistant"", ""content"": ""Just plain text without XML""}
        ]
        bad_reward = reward_func(bad_completion)
        assert bad_reward == 0.2  # Gets 0.2 for proper spacing (no XML tags to mess up)",tests/test_xml_parser.py,TestXMLParser
survived,"    async def test_prompt_copying(self, mock_multiturn_env):
        """"""Test that original prompt is not modified.""""""
        original_prompt = [{""role"": ""user"", ""content"": ""Original message""}]
        prompt_copy = [{""role"": ""user"", ""content"": ""Original message""}]
        
        mock_multiturn_env.client.add_chat_response(
            messages=[{""role"": ""user"", ""content"": ""Original message""}],
            response=""Response DONE""
        )
        
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model=""test-model"",
            prompt=original_prompt,
            answer=""test_answer""
        )
        
        # Original prompt should be unchanged
        assert original_prompt == prompt_copy
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def test_get_format_str(self, xml_parser):
        """"""Test format string generation.""""""
        format_str = xml_parser.get_format_str()
        assert ""<reasoning>"" in format_str
        assert ""</reasoning>"" in format_str
        assert ""<answer>"" in format_str
        assert ""</answer>"" in format_str
",tests/test_xml_parser.py,TestXMLParser
survived,"def think_parser():
    """"""Return a ThinkParser instance.""""""
    return ThinkParser()
",tests/conftest.py,
survived,"def mock_openai_client():
    """"""Return a mocked AsyncOpenAI client with input-output mapping.""""""
    return MockAsyncOpenAI()
",tests/conftest.py,
survived,"    def test_rubric_group_get_reward_func_names(self):
        """"""Test getting aggregated reward function names from all rubrics.""""""
        def func1(completion, **kwargs):
            return 1.0
        
        def func2(completion, **kwargs):
            return 0.5
        
        def func3(completion, **kwargs):
            return 0.3
        
        rubric1 = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])
        rubric2 = Rubric(funcs=[func3], weights=[0.8])
        
        group = RubricGroup(rubrics=[rubric1, rubric2])
        names = group.get_reward_func_names()
        
        assert names == [""func1"", ""func2"", ""func3""]
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def set_default_responses(self, chat_response=None, text_response=None):
        """"""Set default responses when no mapping found.""""""
        if chat_response:
            self.default_chat_response = chat_response
        if text_response:
            self.default_text_response = text_response
",tests/conftest.py,MockAsyncOpenAI
survived,"    async def test_run_rollouts(self, mock_openai_client):
        """"""Test running multiple rollouts.""""""
        env = TestEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            eval_dataset=Dataset.from_dict({""question"": [""test""], ""answer"": [""test""]}),
            parser=Parser(),
            rubric=Rubric()
        )
        
        prompts = [
            [{""role"": ""user"", ""content"": ""Hello""}],
            [{""role"": ""user"", ""content"": ""Hi""}]
        ]
        answers = [""response1"", ""response2""]
        tasks = [""default"", ""default""]
        infos = [{}, {}]
        
        # Mock the rollout method calls
        results = await env.run_rollouts(
            client=mock_openai_client,
            model=""test-model"",
            prompts=prompts,
            answers=answers,
            tasks=tasks,
            infos=infos
        )
        
        assert len(results) == 2
        assert all(len(result) == 2 for result in results)  # Each result is (completion, state)",tests/test_environment.py,TestEnvironmentBase
survived,"        def comprehensive_func(prompt, completion, answer, state, task, info, **kwargs):
            return len(completion) + len(answer) + len(task)
",tests/test_rubric.py,TestRubric
survived,"    def test_parse_with_custom_extractor(self, think_parser_with_extractor):
        """"""Test parsing with custom extraction function.""""""
        text = """"""<think>
        I need to solve this step by step.
        </think>
        The answer is \\boxed{42}.""""""
        
        result = think_parser_with_extractor.parse(text)
        assert result == ""42""
",tests/test_think_parser.py,TestThinkParser
survived,"        def accuracy_func(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0
",tests/test_rubric.py,TestRubric
survived,"        def length_func(completion, **kwargs):
            return len(str(completion))
",tests/test_rubric.py,TestRubric
survived,"    def test_parse_whitespace_handling(self, think_parser):
        """"""Test that whitespace is properly stripped.""""""
        text = """"""<think>
        Thinking process here.
        </think>
        
        Answer with spaces around it.
        
        """"""
        result = think_parser.parse(text)
        assert result == ""Answer with spaces around it.""
",tests/test_think_parser.py,TestThinkParser
survived,"    def test_go_simple(self):
        # should match function name exactly or struct.functionName
        group_id_1 = [
            self._create_event(
                function_names=[""handler.planet"", ""service.blue""],
                filenames=[""baz.go"", ""foo.go""],
                user_id=str(i),
            )
            for i in range(7)
        ][0].group.id
        group_id_2 = [
            self._create_event(
                function_names=[""service.blue"", ""world""],
                filenames=[""foo.go"", ""baz.go""],
                user_id=str(i),
            )
            for i in range(6)
        ][0].group.id
        top_5_issues = self.open_pr_comment_workflow.get_top_5_issues_by_count_for_file(
            projects=[self.project], sentry_filenames=[""baz.go""], function_names=[""world"", ""planet""]
        )
        top_5_issue_ids = [issue[""group_id""] for issue in top_5_issues]
        function_names = [issue[""function_name""] for issue in top_5_issues]
        assert top_5_issue_ids == [group_id_1, group_id_2]
        assert function_names == [""handler.planet"", ""world""]
",tests/sentry/integrations/github/tasks/test_open_pr_comment.py,TestGetCommentIssues
survived,"    def test_go_methods_with_receivers(self):
        patch = """"""
@@ -152,10 +152,6 @@ func (s *Server) Start() error

@@ -152,10 +152,6 @@ func (s Server) Stop()

@@ -152,10 +152,6 @@ func (h *Handler) ServeHTTP(w ResponseWriter, r *Request)

@@ -152,10 +152,6 @@ func (db *Database) Query(query string) (*Result, error)

@@ -152,10 +152,6 @@ func (u User) String() string

@@ -152,10 +152,6 @@ func (p *Point) Distance(q *Point) float64

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""Start"",
            ""Stop"",
            ""ServeHTTP"",
            ""Query"",
            ""String"",
            ""Distance"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase
survived,"    def test_send_plain_text_email(self, mock_smtp_class, smtp_provider):
        """"""Test sending a plain text email.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send plain text email
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            body=""This is a plain text email"",
        )

        # Verify SMTP was called correctly
        mock_smtp_class.assert_called_once_with(""smtp.example.com"", 587)
        mock_smtp.starttls.assert_called_once()
        mock_smtp.login.assert_called_once_with(""test@example.com"", ""testpassword"")
        
        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        assert call_args[0][0] == ""sender@example.com""
        assert call_args[0][1] == ""recipient@example.com""
        
        # Verify the email content contains plain text
        email_content = call_args[0][2]
        assert ""Content-Type: text/plain"" in email_content
        assert ""This is a plain text email"" in email_content
        
        # Verify return value
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test Subject"",
            ""body"": ""This is a plain text email"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def test_validate_scopes_success(self, smtp_provider):
        """"""Test successful scope validation.""""""
        with patch.object(smtp_provider, ""generate_smtp_client"") as mock_generate:
            mock_smtp = MagicMock()
            mock_generate.return_value = mock_smtp
            
            result = smtp_provider.validate_scopes()
            
            assert result == {""send_email"": True}
            mock_smtp.quit.assert_called_once()
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"def start_session(
    session_mode=""application"",  # type: str
):
    # type: (...) -> None
    return get_isolation_scope().start_session(session_mode=session_mode)
",sentry_sdk/api.py,
deleted,"    def __test_connection_via_rest_api(self):
        """"""
        Test connection to OpenShift using REST API instead of CLI.
        This is more reliable as it doesn't depend on oc CLI being installed.
        """"""
        try:
            # Suppress SSL warnings if insecure is True
            if self.authentication_config.insecure:
                # Suppress SSL verification warnings
                warnings.filterwarnings('ignore', message='Unverified HTTPS request')
            
            # Test API connectivity by hitting the /version endpoint
            headers = {
                'Authorization': f'Bearer {self.authentication_config.token}',
                'Accept': 'application/json'
            }
            
            verify_ssl = not self.authentication_config.insecure
            
            # Try to get cluster version info
            response = requests.get(
                f""{self.authentication_config.api_server}/version"",
                headers=headers,
                verify=verify_ssl,
                timeout=30
            )
            
            if response.status_code == 200:
                self.logger.info(""Successfully connected to OpenShift cluster via REST API"")
                return True, None
            else:
                error_msg = f""API returned status code {response.status_code}: {response.text}""
                self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
                return False, error_msg
                
        except requests.exceptions.RequestException as e:
            error_msg = f""Connection error: {str(e)}""
            self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
            return False, error_msg
        except Exception as e:
            error_msg = f""Unexpected error: {str(e)}""
            self.logger.error(f""Failed to connect to OpenShift cluster: {error_msg}"")
            return False, error_msg
",keep/providers/openshift_provider/openshift_provider.py,OpenshiftProvider
survived,"    async def test_delete_api_key_with_api_key_auth_success(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test that deleting an API key using API key authentication works correctly.

        This test verifies the fix for the bug where API key authentication
        would fail when deleting keys. Now it should work properly.
        """"""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock API key authentication (user will be None)
        mock_user_dep.return_value = None

        # Mock successful deletion
        mock_api_keys_service.delete_key.return_value = True

        # This should now work correctly with the fix
        response = await test_api_client.delete(""/_/api/keys/test_key_id"")

        # The fix: operation succeeds when using API key authentication
        assert response.status_code == 204
        mock_api_keys_service.delete_key.assert_called_once_with(""test_key_id"")
",api/api/routers/api_keys_test.py,TestDeleteAPIKey
survived,"    def __init__(self, name: str, concurrency: int, requests: int, duration: float,
                 latencies: List[float], errors: int):
        self.name = name
        self.concurrency = concurrency
        self.requests = requests
        self.duration = duration
        self.latencies = latencies
        self.errors = errors
        
        self.rps = requests / duration
        self.avg_latency = mean(latencies) if latencies else 0
        self.median_latency = median(latencies) if latencies else 0
        self.min_latency = min(latencies) if latencies else 0
        self.max_latency = max(latencies) if latencies else 0
        self.stdev_latency = stdev(latencies) if len(latencies) > 1 else 0
",benchmarks/benchmark.py,BenchmarkResult
survived,"    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = ""https://api.opensea.io/api/v2""
",python/src/plugins/opensea/goat_plugins/opensea/service.py,OpenSeaService
survived,"    async def get_nft_trades(self, parameters: dict):
        """"""Get trades for a specific NFT collection or token from Nansen""""""
        url = f""{self.base_url}/nft/trades""
        params = {
            ""token_address"": parameters[""token_address""],
            ""nft_id"": parameters[""nft_id""],
            ""start_date"": parameters[""start_date""],
            ""end_date"": parameters[""end_date""]
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"def nansen(options: NansenPluginOptions) -> NansenPlugin:
    return NansenPlugin(options)",python/src/plugins/nansen/goat_plugins/nansen/__init__.py,
survived,"    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = ""https://api.nansen.ai/v1""
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"    def supports_chain(self, chain) -> bool:
        return True
",python/src/plugins/nansen/goat_plugins/nansen/__init__.py,NansenPlugin
deleted,"def version_increment_check():
    return VersionIncrementCheck()
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,
deleted,"    def _get_github_master_metadata_url(self, connector: Connector) -> str:
        return f""{GITHUB_URL_PREFIX_FOR_CONNECTORS}/{connector.technical_name}/{METADATA_FILE_NAME}""
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck
survived,"    def _have_same_major_minor_patch(self, master_version: semver.Version, current_version: semver.Version) -> bool:
        """"""Check if both versions have the same major, minor, and patch versions.""""""
        return (
            master_version.major == current_version.major
            and master_version.minor == current_version.minor
            and master_version.patch == current_version.patch
        )
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionIncrementCheck
deleted,"    def test_are_both_versions_release_candidates(self, version_increment_check):
        assert version_increment_check._are_both_versions_release_candidates(
            semver.Version.parse(""1.0.0-rc.1""),
            semver.Version.parse(""1.0.0-rc.2"")
        )
        
        assert not version_increment_check._are_both_versions_release_candidates(
            semver.Version.parse(""1.0.0-rc.1""),
            semver.Version.parse(""1.0.0"")
        )
        
        assert not version_increment_check._are_both_versions_release_candidates(
            semver.Version.parse(""1.0.0""),
            semver.Version.parse(""1.0.0-rc.1"")
        )
        
        assert not version_increment_check._are_both_versions_release_candidates(
            semver.Version.parse(""1.0.0""),
            semver.Version.parse(""1.1.0"")
        )
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,TestVersionIncrementCheck
deleted,"    def name(self) -> str:
        return ""Version Check""
",airbyte-ci/connectors/connectors_qa/src/connectors_qa/checks/version.py,VersionCheck
survived,"def sample_csv(reasoning: str, csv_path: str, row_count: int) -> str:
    """"""Returns a sample of rows from the CSV file.

    The agent uses this to understand actual data content and patterns.
    This helps validate data types and identify any potential data quality issues.

    Args:
        reasoning: Explanation of why we're sampling this data
        csv_path: Path to the CSV file
        row_count: Number of rows to sample (aim for 3-5 rows)

    Returns:
        String containing sample rows in readable format

    Example:
        sample = sample_csv(""Check age values and formats"", ""data.csv"", 3)
        # Returns formatted string with 3 rows of data
    """"""
    try:
        df = pl.scan_csv(csv_path).limit(row_count).collect()
        # Convert to string representation
        output = df.select(pl.all()).write_csv(None)
        console.log(
            f""[blue]Sample CSV Tool[/blue] - Rows: {row_count} - Reasoning: {reasoning}""
        )
        console.log(f""[dim]Sample:\n{output}[/dim]"")
        return output
    except Exception as e:
        console.log(f""[red]Error sampling CSV: {str(e)}[/red]"")
        return """"
",sfa_polars_csv_agent_openai_v2.py,
survived,"    async def completion(
        self,
        messages: List[common.Message],
        max_tokens: int,
        model: str | None = None,
        temperature: float = 1.0,
        tools: List[common.Tool] | None = None,
        tool_choice: str | None = None,
        system_prompt: str | None = None,
        *args,
        **kwargs,
    ) -> common.Completion:
        chosen_model = model or self.default_model
        ollama_messages = self._messages_into(messages)
        
        if system_prompt:
            ollama_messages.insert(0, {""role"": ""system"", ""content"": system_prompt})
        
        request_params = {
            ""model"": chosen_model,
            ""messages"": ollama_messages,
            ""options"": {
                ""temperature"": temperature,
                ""num_predict"": max_tokens,
            }
        }
        
        ollama_tools = self._tools_into(tools)
        if ollama_tools:
            request_params[""tools""] = ollama_tools
        
        try:
            response = await self.client.chat(**request_params)
            return self._completion_into(response, input_tokens=0)
        except Exception as e:
            logger.error(f""Ollama API error: {e}"")
            raise",agent/llm/ollama_client.py,OllamaLLM
survived,"def post_process(
    loc,
    conf,
    landms,
    prior_data,
    cfg,
    scale,
    scale1,
    resize,
    confidence_threshold,
    top_k,
    nms_threshold,
    keep_top_k,
):

    boxes = decode(loc, prior_data, cfg[""variance""])
    boxes = boxes * scale / resize
    boxes = boxes
    scores = conf[:, 1]

    landms_copy = decode_landm(landms, prior_data, cfg[""variance""])

    landms_copy = landms_copy * scale1 / resize
    landms_copy = landms_copy

    inds = np.where(scores > confidence_threshold)[0]
    boxes = boxes[inds]
    landms_copy = landms_copy[inds]
    scores = scores[inds]

    order = scores.argsort()[::-1][:top_k]
    boxes = boxes[order]
    landms_copy = landms_copy[order]
    scores = scores[order]

    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
    keep = py_cpu_nms(dets, nms_threshold)
    dets = dets[keep, :]
    landms_copy = landms_copy[keep]

    dets = dets[:keep_top_k, :]
    landms_copy = landms_copy[:keep_top_k, :]

    dets = np.concatenate((dets, landms_copy), axis=1)
    dets = sorted(dets, key=lambda x: x[4], reverse=True)
    dets = [parse_det(x) for x in dets]

    return dets
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,
survived,"def batch_detect(net, images):
    confidence_threshold = 0.02
    cfg = cfg_mnet
    top_k = 5000
    nms_threshold = 0.4
    keep_top_k = 750
    resize = 1
    img = np.float32(images)
    mean = np.array([[[[104, 117, 123]]]], dtype=img.dtype)
    img -= mean
    img = img.transpose(0, 3, 1, 2)
    batch_size, _, im_height, im_width, = img.shape
    scale = np.array(
        [im_width, im_height, im_width, im_height],
        dtype=img.dtype
    )
    loc, conf, landms = net.run(img)
    priorbox = PriorBox(cfg, image_size=(im_height, im_width))
    prior_data = priorbox.forward()
    scale1 = np.array(
        [
            img.shape[3],
            img.shape[2],
            img.shape[3],
            img.shape[2],
            img.shape[3],
            img.shape[2],
            img.shape[3],
            img.shape[2],
            img.shape[3],
            img.shape[2],
        ],
        dtype=img.dtype
    )

    all_dets = [
        post_process(
            loc_i,
            conf_i,
            landms_i,
            prior_data,
            cfg,
            scale,
            scale1,
            resize,
            confidence_threshold,
            top_k,
            nms_threshold,
            keep_top_k,
        )
        for loc_i, conf_i, landms_i in zip(loc, conf, landms)
    ]

    return all_dets
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,
survived,"def main():
    check_and_download_models(WEIGHT_PATH_6DRepNet360, MODEL_PATH_6DRepNet360, REMOTE_PATH_6DRepNet360)
    check_and_download_models(WEIGHT_PATH_FACE, MODEL_PATH_FACE, REMOTE_PATH_FACE)

    if args.video is not None:
        recognize_from_video()
    else:
        recognize_from_image()
",face_recognition/6d_repnet_360/6d_repnet_360.py,
survived,"                def __init__(self, provider, session, init_timestamp, kwargs):
                    self.provider = provider
                    self.session = session
                    self.init_timestamp = init_timestamp
                    self.kwargs = kwargs
                    self.stream = None
                    # Create a new LLM event for this stream
                    self.llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                    if session is not None:
                        self.llm_event.session_id = session.session_id
                        self.llm_event.agent_id = check_call_stack_for_agent_id()
                        self.llm_event.model = kwargs.get(""model"", ""command-r-plus"")
                        self.llm_event.prompt = kwargs.get(""message"", """")
                        self.llm_event.completion = """"  # Initialize empty completion
                        logger.info(f""Initialized async stream LLM event with session_id: {session.session_id}"")
",agentops/llms/providers/cohere.py,CohereProvider.AsyncStreamWrapper
deleted,"    def _override_chat_stream_async(self):
        import cohere

        original_method = cohere.AsyncClient.chat_stream

        async def patched_function(self_client, *args, **kwargs):
            init_timestamp = get_ISO_time()
            session = kwargs.pop(""session"", None) if ""session"" in kwargs else None
            kwargs_copy = kwargs.copy()
            if ""session"" in kwargs_copy: 
                del kwargs_copy[""session""]

            # Create an async generator class that wraps the original method
            class AsyncStreamWrapper:
                def __init__(self, provider, session, init_timestamp, kwargs):
                    self.provider = provider
                    self.session = session
                    self.init_timestamp = init_timestamp
                    self.kwargs = kwargs
                    self.stream = None
                    # Create a new LLM event for this stream
                    self.llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                    if session is not None:
                        self.llm_event.session_id = session.session_id
                        self.llm_event.agent_id = check_call_stack_for_agent_id()
                        self.llm_event.model = kwargs.get(""model"", ""command-r-plus"")
                        self.llm_event.prompt = kwargs.get(""message"", """")
                        self.llm_event.completion = """"  # Initialize empty completion
                        logger.info(f""Initialized async stream LLM event with session_id: {session.session_id}"")

                def __aiter__(self):
                    return self

                async def __anext__(self):
                    if self.stream is None:
                        # Get the stream from the original method - it's already an async generator
                        response = original_method(self_client, *args, **kwargs_copy)
                        self.stream = aiter(response)

                    try:
                        # Get the next chunk
                        chunk = await anext(self.stream)
                        # Handle the chunk and track events
                        self.provider.handle_stream_chunk(chunk, self.session, self.llm_event, self.kwargs)
                        return chunk
                    except StopAsyncIteration:
                        # Record the LLM event when the stream completes
                        if self.session is not None:
                            self.llm_event.end_timestamp = get_ISO_time()
                            if not isinstance(self.llm_event.completion, dict):
                                self.llm_event.completion = {
                                    ""role"": ""assistant"",
                                    ""content"": self.llm_event.completion if isinstance(self.llm_event.completion, str) else """"
                                }
                            logger.info(f""Stream completed. Recording LLM event with completion: {self.llm_event.completion}"")
                            self.provider._safe_record(self.session, self.llm_event)
                            logger.info(""Successfully recorded async stream LLM event"")
                        raise
                    except Exception as e:
                        print(f""Error in AsyncStreamWrapper: {str(e)}"")
                        if not isinstance(self.llm_event, str):
                            self.provider._safe_record(self.session, ErrorEvent(trigger_event=self.llm_event, exception=e))
                        raise

            # Return an instance of the async generator wrapper
            return AsyncStreamWrapper(self, session, init_timestamp, kwargs)

        # Store original method and override
        self.original_create_stream_async = original_method
        cohere.AsyncClient.chat_stream = patched_function",agentops/llms/providers/cohere.py,CohereProvider
survived,"def test_litellm_integration():
    """"""Integration test demonstrating all four LiteLLM call patterns:
    1. Sync (non-streaming)
    2. Sync (streaming)
    3. Async (non-streaming)
    4. Async (streaming)

    Verifies that AgentOps correctly tracks all LLM calls via analytics.
    """"""
    # Initialize AgentOps without auto-starting session
    agentops.init(auto_start_session=False)
    session = agentops.start_session()
    
    # Initialize LiteLLM provider
    from agentops.llms.providers.litellm import LiteLLMProvider
    provider = LiteLLMProvider(None)  # LiteLLM doesn't need a client
    provider.override()
    
    # Pass session to provider
    provider.client = session

    def sync_no_stream():
        litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync no stream"", ""role"": ""user""}],
            session=session
        )

    def sync_stream():
        stream_response = litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass

    async def async_no_stream():
        await litellm.acompletion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from async no stream"", ""role"": ""user""}],
            session=session
        )

    async def async_stream():
        async_stream_response = await litellm.acompletion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from async streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        # Handle streaming response
        if isinstance(async_stream_response, str):
            _ = async_stream_response
        else:
            async for chunk in async_stream_response:
                _ = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta, 'content') else ''

    async def run_async_tests():
        await async_no_stream()
        await async_stream()

    # Call each function with proper error handling
    try:
        sync_no_stream()
        sync_stream()
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f""Error during LiteLLM test: {str(e)}"")
        raise

    session.end_session(""Success"")
    analytics = session.get_analytics()
    print(analytics)
    # Verify that all LLM calls were tracked
    assert analytics[""LLM calls""] >= 4, f""Expected at least 4 LLM calls, but got {analytics['LLM calls']}""
",tests/core_manual_tests/providers/litellm_canary.py,
survived,"    def sync_no_stream():
        anthropic_client.messages.create(
            max_tokens=1024,
            model=""claude-3-5-sonnet-20240620"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""Hello from sync no stream"",
                }
            ],
            session=session
        )
",tests/core_manual_tests/providers/anthropic_canary.py,
survived,"    def sync_no_stream():
        try:
            print(""\nExecuting sync_no_stream..."")
            response = co.chat(message=""Hello from sync no stream"", model=""command"", session=session)
            print(f""sync_no_stream completed successfully with response: {response.text}"")
        except Exception as e:
            print(f""Error in sync_no_stream: {str(e)}"")
            raise
",tests/core_manual_tests/providers/cohere_canary.py,
survived,"    def sync_stream():
        stream_response = litellm.completion(
            model=""gpt-3.5-turbo"",
            messages=[{""content"": ""Hello from sync streaming"", ""role"": ""user""}],
            stream=True,
            session=session
        )
        for _ in stream_response:
            pass
",tests/core_manual_tests/providers/litellm_canary.py,
deleted,"                def __next__(self):
                    try:
                        chunk = next(self.stream)
                        # Handle the chunk and track events
                        self.provider.handle_stream_chunk(chunk, self.session, self.llm_event, self.kwargs)
                        return chunk
                    except StopIteration:
                        raise
                    except Exception as e:
                        print(f""Error in StreamWrapper: {str(e)}"")
                        if not isinstance(self.llm_event, str):
                            self.provider._safe_record(self.session, ErrorEvent(trigger_event=self.llm_event, exception=e))
                        raise
",agentops/llms/providers/cohere.py,CohereProvider.StreamWrapper
survived,"    async def run_async_tests():
        await async_no_stream()
        await async_stream()
",tests/core_manual_tests/providers/ai21_canary.py,
survived,"    def sync_stream():
        try:
            print(""\nExecuting sync_stream..."")
            stream = co.chat_stream(message=""Hello from sync streaming"", model=""command"", session=session)
            completion = """"
            for chunk in stream:
                if hasattr(chunk, 'text'):
                    completion += chunk.text
                print(f""Received sync chunk: {chunk}"")
            print(f""sync_stream completed successfully with completion: {completion}"")
        except Exception as e:
            print(f""Error in sync_stream: {str(e)}"")
            raise
",tests/core_manual_tests/providers/cohere_canary.py,
survived,"    def sync_stream():
        stream_response = chat_client.create(
            model=""jamba-instruct"",
            system=""You are a helpful AI assistant"",
            messages=sync_stream_messages,
            maxTokens=10,
            stream=True
        )
        for chunk in stream_response:
            _ = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta, 'content') else ''
",tests/core_manual_tests/providers/ai21_canary.py,
survived,"    async def async_stream():
        stream_response = await client.chat_stream(
            model=""mistral-tiny"",
            messages=[ChatMessage(role=""user"", content=""Hello from async streaming"")]
        )
        async for chunk in stream_response:
            _ = chunk.delta.content if hasattr(chunk.delta, 'content') else ''
",tests/core_manual_tests/providers/mistral_canary.py,
survived,"def test_get_sample_values_returns_primitives() -> None:
    """"""Test that get_sample_values always returns primitive types.""""""
    import polars as pl

    def is_primitive(value: Any) -> bool:
        return isinstance(
            value,
            (
                str,
                int,
                float,
                bool,
                type(None),
                datetime.datetime,
                datetime.date,
            ),
        )

    class Enum:
        A = ""a""
        B = ""b""
        C = ""c""

    # Create a DataFrame with various types including categorical/enum-like columns
    df = pl.DataFrame(
        {
            ""category"": pl.Series([""A"", ""B"", ""C""], dtype=pl.Categorical),
            ""mixed"": pl.Series([""str"", ""123"", ""45.67""]),
            ""list"": pl.Series([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
            ""dict"": pl.Series(
                [
                    {""a"": 1, ""b"": Enum.A},
                    {""c"": 3, ""d"": Enum.B},
                    {""e"": 5, ""f"": Enum.C},
                ]
            ),
            ""enum"": pl.Series([Enum.A, Enum.B, Enum.C]),
            ""dates"": [
                datetime.datetime(2021, 1, 1),
                datetime.datetime(2021, 1, 2),
                datetime.datetime(2021, 1, 3),
            ],
        },
    )

    manager: NarwhalsTableManager[Any] = NarwhalsTableManager.from_dataframe(
        df
    )

    # Verify all values are primitives
    for column in df.columns:
        values = manager.get_sample_values(column)
        for val in values:
            assert is_primitive(val), (
                f""Column {column} returned non-primitive or non-datetime value: {val} of type {type(val)}""
            )
",tests/_plugins/ui/_impl/tables/test_narwhals.py,
survived,"    async def _run(self) -> StepResult:
        if self.original_manifest:
            self.manifest_path.write_text(self.original_manifest)

        if self.backup_schema_path:
            copy_directory(self.backup_schema_path, self.schemas_path)

        return StepResult(
            step=self,
            status=StepStatus.SUCCESS,
        )
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RestoreInlineState
survived,"async def migrate_to_inline_schemas(ctx: click.Context, report: bool) -> bool:
    verify_formatters()
    return await run_connector_pipeline(
        ctx,
        ""Migrate to inline schemas"",
        report,
        run_connector_migrate_to_inline_schemas_pipeline,
    )",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/commands.py,
survived,"    def __init__(self, context: PipelineContext) -> None:
        super().__init__(context)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,CheckIsInlineCandidate
survived,"    def __init__(self, context: ConnectorContext) -> None:
        super().__init__(context)
        self.manifest_path = context.connector.manifest_path
        self.original_manifest = None
        if self.manifest_path.is_file():
            self.original_manifest = self.manifest_path.read_text()

        self.schemas_path = context.connector.python_source_dir_path / SCHEMAS_DIR_NAME
        self.backup_schema_path = None
        if self.schemas_path.is_dir():
            self.backup_schema_path = Path(tempfile.mkdtemp())
            copy_directory(self.schemas_path, self.backup_schema_path)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RestoreInlineState
survived,"    async def _run(self) -> StepResult:
        connector = self.context.connector
        python_path = connector.python_source_dir_path
        schemas_path = python_path / SCHEMAS_DIR_NAME
        logger = self.logger

        manifest = connector.manifest_path.read_text()

        if manifest.find(""JsonFileSchemaLoader"") != -1:
            return StepResult(
                step=self,
                status=StepStatus.SKIPPED,
                stderr=""Skipping: the manifest is still using JSON Schema loader."",
            )

        if schemas_path.exists():
            logger.info(f""    Removing schemnas dir: {schemas_path}"")
            shutil.rmtree(schemas_path)

        return StepResult(step=self, status=StepStatus.SUCCESS)
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,RemoveUnusedJsonSchamas
survived,"def _has_subdirectory(directory: Path) -> bool:
    # Iterate through all items in the directory
    for entry in directory.iterdir():
        # Check if this entry is a directory
        if entry.is_dir():
            return True

    return False
",airbyte-ci/connectors/pipelines/pipelines/airbyte_ci/connectors/migrate_to_inline_schemas/pipeline.py,
survived,"    def test_pause_live_updates_with_active_session(self):
        """"""Test pausing when Live session is active.""""""
        formatter = ConsoleFormatter()
        
        mock_live = MagicMock(spec=Live)
        formatter._live = mock_live
        formatter._live_paused = False
        
        formatter.pause_live_updates()
        
        mock_live.stop.assert_called_once()
        assert formatter._live_paused
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume
survived,"            def track_pause():
                pause_calls.append(True)
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration
survived,"    def test_polars_groupby_alias() -> None:
        """"""Test that group by operations use original column names correctly.""""""
        import polars as pl

        import marimo as mo

        # Create a test dataframe with age and group columns
        df = pl.DataFrame({
            ""group"": [""a"", ""a"", ""b"", ""b""],
            ""age"": [10, 20, 30, 40],
        })
        # Test the transformation directly using TransformsContainer
        from marimo._plugins.ui._impl.dataframes.transforms.apply import (
            TransformsContainer,
            get_handler_for_dataframe,
        )
        from marimo._plugins.ui._impl.dataframes.transforms.types import (
            GroupByTransform,
            TransformType,
            Transformations,
        )

        handler = get_handler_for_dataframe(df)
        transform_container = TransformsContainer(df, handler)
        
        # Create and apply the transformation
        transform = GroupByTransform(
            type=TransformType.GROUP_BY,
            column_ids=[""group""],
            drop_na=True,
            aggregation=""max"",
        )
        transformations = Transformations([transform])
        transformed_df = transform_container.apply(transformations)

        # Verify the transformed DataFrame
        assert isinstance(transformed_df, pl.DataFrame)
        assert ""group"" in transformed_df.columns
        assert ""age_max"" in transformed_df.columns
        assert transformed_df.shape == (2, 2)
        assert transformed_df[""age_max""].to_list() == [20, 40]  # max age for each group

        # The resulting frame should have correct column names and values
        # Convert to dict and verify values
        result_dict = {
            col: transformed_df[col].to_list()
            for col in transformed_df.columns
        }
        assert result_dict == {
            ""group"": [""a"", ""b""],
            ""age_max"": [20, 40],
        }

        # Verify the generated code uses original column names
        from marimo._plugins.ui._impl.dataframes.transforms.print_code import (
            python_print_polars,
        )
        code = python_print_polars(
            ""df"",
            [""group"", ""age""],
            transform,
        )
        # Code should reference original ""age"" column, not ""age_max""
        assert 'pl.col(""age"")' in code
        assert 'alias(""age_max"")' in code
        assert 'pl.col(""group"")' in code  # Original column name in group by
",tests/_plugins/ui/_impl/dataframes/test_dataframe.py,TestDataframes
survived,"    def _generate_uuid(cls, seed: str) -> str:
        """"""
        Generate a deterministic UUID based on a seed string.

        Args:
            seed (str): The seed string to use for UUID generation

        Returns:
            str: A string representation of the UUID consistently generated from the seed
        """"""
        if not isinstance(seed, str):
            raise ValueError(""Seed must be a string"")
        
        if not seed.strip():
            raise ValueError(""Seed cannot be empty or whitespace"")
            
        # Create a deterministic UUID using v5 (SHA-1)
        # Custom namespace for CrewAI to enhance security

        # Using a unique namespace specific to CrewAI to reduce collision risks
        CREW_AI_NAMESPACE = uuid.UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479')
        return str(uuid.uuid5(CREW_AI_NAMESPACE, seed))
",src/crewai/security/fingerprint.py,Fingerprint
survived,"def consume(config):
    connection = create_sink_connection(config=config)

    while True:
        # Consume transformed event from the pipeline
        res = connection.consume()

        if res.status_code == 200:
            record = res.event()
            assert record[""data""] == TEST_MESSAGE
            assert record[""stream""] == TEST_STREAM
            assert record[""namespace""] == TEST_NAMESPACE
            assert ""emitted_at"" in record
            break
",airbyte-integrations/connectors/destination-glassflow/integration_tests/integration_test.py,
survived,"def test_check_fails():
    f = open(
        ""integration_tests/invalid_config.json"",
    )
    config = json.load(f)
    destination = DestinationGlassflow()
    status = destination.check(logger=Mock(), config=config)
    assert status.status == Status.FAILED
",airbyte-integrations/connectors/destination-glassflow/integration_tests/integration_test.py,
survived,"    def check(self, logger: Logger, config: Mapping[str, Any]) -> AirbyteConnectionStatus:
        """"""
        Tests if the input configuration can be used to successfully connect to the destination with the needed permissions
            e.g: if a provided API token or password can be used to connect and write to the destination.

        :param logger: Logging object to display debug/info/error to the logs
            (logs will not be accessible via airbyte UI if they are not passed to this logger)
        :param config: Json object containing the configuration of this destination, content of this json is as specified in
        the properties of the spec.json file

        :return: AirbyteConnectionStatus indicating a Success or Failure
        """"""
        try:
            connection = create_source_connection(config)
            try:
                connection.validate_credentials()
                return AirbyteConnectionStatus(status=Status.SUCCEEDED)
            except errors.PipelineAccessTokenInvalidError:
                return AirbyteConnectionStatus(status=Status.FAILED, message=f""The pipeline access token is not valid"")
        except Exception as e:
            logger.error(f""Failed to create connection. Error: {e}"")
            return AirbyteConnectionStatus(status=Status.FAILED, message=f""An exception occurred: {repr(e)}"")",airbyte-integrations/connectors/destination-glassflow/destination_glassflow/destination.py,DestinationGlassflow
survived,"def test_check_fails(client):
    pipeline = _init_mocks(client)
    pipeline.validate_credentials.side_effect = errors.PipelineAccessTokenInvalidError(mock.Mock())
    destination = DestinationGlassflow()
    status = destination.check(logger=Mock(), config=config)
    assert status.status == Status.FAILED
",airbyte-integrations/connectors/destination-glassflow/unit_tests/unit_test.py,
survived,"def test_crew_output_import():
    """"""Test that CrewOutput can be imported from crewai.""""""
    from crewai import CrewOutput
    
    assert CrewOutput is not None",tests/imports_test.py,
survived,"def test_get_configured_catalog_parametrized(
    mock_catalog,
    mock_stream,
    cursor_override,
    pk_override,
    expected_cursor,
    expected_pk,
):
    """"""Test various combinations of cursor and primary key overrides.""""""
    cursor_key_overrides = {""test_stream"": cursor_override} if cursor_override else None
    primary_key_overrides = {""test_stream"": pk_override} if pk_override else None

    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(
            executor=Mock(),
            name=""test-source"",
            cursor_key_overrides=cursor_key_overrides,
            primary_key_overrides=primary_key_overrides,
        )

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.cursor_field == expected_cursor
        assert configured_stream.primary_key == expected_pk",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"def mock_stream():
    """"""Create a mock AirbyteStream for testing.""""""
    stream = Mock(spec=AirbyteStream)
    stream.name = ""test_stream""
    stream.source_defined_primary_key = [[""original_pk""]]
    stream.source_defined_cursor = [""original_cursor""]
    return stream
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"def test_set_primary_keys(input_keys, expected_output):
    """"""Test that set_primary_keys properly converts and updates the primary key overrides.""""""
    with patch.object(Source, ""_discover"", return_value=Mock()):
        source = Source(executor=Mock(), name=""test-source"")

        source.set_primary_keys(kwargs=input_keys)

        assert source._primary_key_overrides == expected_output

        update_keys = {""stream3"": ""pk3""}
        expected_after_update = expected_output.copy()
        expected_after_update[""stream3""] = [""pk3""]

        source.set_primary_keys(kwargs=update_keys)
        assert source._primary_key_overrides == expected_after_update
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"    def set_cursor_key(
        self,
        stream_name: str,
        cursor_key: str,
    ) -> None:
        """"""Set the cursor for a single stream.""""""
        self._cursor_key_overrides[stream_name] = cursor_key
",airbyte/sources/base.py,Source
survived,"    def set_primary_keys(
        self,
        *,
        kwargs: dict[str, str | list[str]],
    ) -> None:
        """"""Override the primary keys for one or more streams.

        This does not unset previously set primary keys.

        The primary key can be a single column name, or a list of fields which should comprise
        the composite primary key.

        Args:
            kwargs: A dictionary mapping stream names to either a primary key column name, or a
            list of fields which should comprise the composite primary key.
        """"""
        self._primary_key_overrides.update(
            {k: v if isinstance(v, list) else [v] for k, v in kwargs.items()}
        )
",airbyte/sources/base.py,Source
survived,"    def test_load_persistent_cache(self) -> None:
        """"""Test loading a persistent cache.""""""
        loader = JsonLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        
        # Create a valid JSON cache
        cache_dict = {
            ""defs"": {""var1"": ""value1""},
            ""hash"": ""hash1"",
            ""stateful_refs"": [],
            ""cache_type"": ""Pure"",
            ""hit"": True,
            ""meta"": {}
        }
        
        with open(cache_path, ""w"") as f:
            json.dump(cache_dict, f)
        
        # Load the cache
        loaded_cache = loader.load_persistent_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        assert isinstance(loaded_cache.stateful_refs, set)
        
        # Should raise for non-existent cache
        with pytest.raises(FileNotFoundError):
            loader.load_persistent_cache(""nonexistent"", ""Pure"")
        
        # Test with invalid JSON
        invalid_path = loader.build_path(""invalid"", ""Pure"")
        with open(invalid_path, ""w"") as f:
            f.write(""not valid json"")
        
        with pytest.raises(json.JSONDecodeError):
            loader.load_persistent_cache(""invalid"", ""Pure"")
        
        # Test with missing required fields
        missing_fields_path = loader.build_path(""missing"", ""Pure"")
        with open(missing_fields_path, ""w"") as f:
            json.dump({""hash"": ""missing"", ""stateful_refs"": []}, f)
        
        with pytest.raises(LoaderError, match=""Invalid json object""):
            loader.load_persistent_cache(""missing"", ""Pure"")
",tests/_save/loaders/test_json_loader.py,TestJsonLoader
survived,"    def test_load_persistent_cache(self) -> None:
        """"""Test loading a persistent cache.""""""
        loader = PickleLoader(""test"", self.save_path)
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        # Use string directly instead of Name constructor
        original_cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        
        with open(cache_path, ""wb"") as f:
            pickle.dump(original_cache, f)
        
        # Load the cache
        loaded_cache = loader.load_persistent_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        
        # Should raise for non-existent cache
        with pytest.raises(FileNotFoundError):
            loader.load_persistent_cache(""nonexistent"", ""Pure"")
        
        # Test with invalid cache object
        invalid_path = loader.build_path(""invalid"", ""Pure"")
        with open(invalid_path, ""wb"") as f:
            pickle.dump(""not a cache"", f)
        
        with pytest.raises(LoaderError, match=""Excepted cache object""):
            loader.load_persistent_cache(""invalid"", ""Pure"")
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def test_save_cache(self) -> None:
        """"""Test saving a cache.""""""
        loader = JsonLoader(""test"", self.save_path)
        
        # Create a cache with a stateful reference
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            {""stateful""},
            ""Pure"",
            True,
            {""version"": 1}
        )
        
        # Save the cache
        loader.save_cache(cache)
        
        # Verify the file was created
        cache_path = loader.build_path(""hash1"", ""Pure"")
        assert os.path.exists(cache_path)
        
        # Load the JSON and verify contents
        with open(cache_path, ""r"") as f:
            loaded_json = json.load(f)
        
        assert loaded_json[""hash""] == ""hash1""
        assert loaded_json[""cache_type""] == ""Pure""
        assert loaded_json[""hit""] is True
        assert isinstance(loaded_json[""stateful_refs""], list)  # Should be converted to list
        assert loaded_json[""meta""] == {""version"": 1}
        
        # Save another cache with different type
        cache2 = Cache(
            {""var2"": ""value2""}, 
            ""hash2"", 
            set(),
            ""Deferred"",
            True,
            {}
        )
        
        loader.save_cache(cache2)
        
        # Verify the second file was created
        cache2_path = loader.build_path(""hash2"", ""Deferred"")
        assert os.path.exists(cache2_path)
        
        # Load the second JSON and verify contents
        with open(cache2_path, ""r"") as f:
            loaded_json2 = json.load(f)
        
        assert loaded_json2[""hash""] == ""hash2""
        assert loaded_json2[""cache_type""] == ""Deferred""",tests/_save/loaders/test_json_loader.py,TestJsonLoader
survived,"    def test_init_with_custom_cache(self) -> None:
        """"""Test initialization with a custom cache.""""""
        custom_cache = OrderedDict()
        loader = MemoryLoader(""test"", cache=custom_cache)
        assert id(loader._cache) != id(custom_cache)  # Should be a copy, not the same instance
        assert len(loader._cache) == 0
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader
deleted,"def test_crew_train_with_memory():
    """"""Test that training a crew with memory enabled does not raise validation errors.""""""
    agent = Agent(role=""Test Agent"", goal=""Test Goal"", backstory=""Test Backstory"")
    task = Task(description=""Test Task"", expected_output=""Test Output"", agent=agent)
    crew = Crew(agents=[agent], tasks=[task], memory=True)

    with tempfile.TemporaryDirectory() as tmpdir:
        filename = os.path.join(tmpdir, ""training_data.pkl"")
        try:
            crew.train(n_iterations=1, filename=filename)
        except pydantic_core.ValidationError as e:
             if ""Input should be an instance of"" in str(e) and (""Memory"" in str(e)):
                  pytest.fail(f""Training with memory raised Pydantic ValidationError, likely due to incorrect memory copy: {e}"")
             else:
                  raise e
        except Exception as e:
            print(f""Warning: Training raised an unexpected exception: {e}"")",tests/crew_test.py,
survived,"def test_invalid_schema(tmp_path):
    """"""Test that an invalid schema raises RuntimeError with mismatch message.""""""
    db_path = tmp_path / ""metadata.db""
    # Manually create a runs table with incorrect columns
    with sqlite3.connect(str(db_path)) as conn:
        conn.execute(""CREATE TABLE runs (wrong_col TEXT)"")
    
    db = MetadataDB(str(db_path))
    with pytest.raises(RuntimeError, match=""mismatch""):
        db.store_metadata({
            ""run_hash"": ""test2"",
            ""dataset_hash"": ""hash2"",
            ""prompt_func"": ""def prompt_func(): pass"",
            ""model_name"": ""test-model-2"",
            ""response_format"": ""{}"",
            ""batch_mode"": True,
            ""timestamp"": ""2023-01-01T01:00:00Z"",
        })",tests/test_db_schema.py,
deleted,"    def test_sanitize_collection_name_short_name(self):
        """"""Test sanitizing a very short name.""""""
        short_name = ""A""
        sanitized = sanitize_collection_name(short_name)
        self.assertGreaterEqual(len(sanitized), 3)
        self.assertTrue(sanitized[0].isalnum())
        self.assertTrue(sanitized[-1].isalnum())
",tests/utilities/test_string_utils.py,TestStringUtils
deleted,"    def _get_backend_shutdown_handler(self):
        """"""Get the backend shutdown handler.

        Returns:
            A lambda function that does nothing for compatibility.
        """"""
        return lambda: None
",reflex/testing.py,AppHarnessProd
survived,"    def test_require_api_key_config(self, mock_get_context: MagicMock) -> None:
        """"""Test _require_api_key with config.""""""
        mock_context = MagicMock()
        mock_context.marimo_config = {""ai"": {""google"": {""api_key"": ""config-key""}}}
        mock_get_context.return_value = mock_context

        model = google(""gemini-pro"")
        assert model._require_api_key == ""config-key""
",tests/_ai/llm/_impl.py,TestGoogle
survived,"def test_groq_require() -> None:
    """"""Test that groq.require raises ModuleNotFoundError.""""""
    model = groq(""llama3-70b-8192"")
    messages = [ChatMessage(role=""user"", content=""Test prompt"")]
    config = ChatModelConfig()
    with pytest.raises(ModuleNotFoundError):
        model(messages, config)
",tests/_ai/llm/_impl.py,
survived,"    def test_require_api_key_missing(self) -> None:
        """"""Test _require_api_key with missing key.""""""
        model = groq(""llama3-70b-8192"")
        with pytest.raises(ValueError):
            _ = model._require_api_key
",tests/_ai/llm/_impl.py,TestGroq
survived,"    def test_call_azure(
        self, mock_azure_openai_class: MagicMock, mock_require_api_key: MagicMock
    ) -> None:
        """"""Test calling the openai class with Azure OpenAI.""""""
        mock_require_api_key.return_value = ""test-key""
        mock_client = MagicMock()
        mock_azure_openai_class.return_value = mock_client
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = ""Test response""
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response

        model = openai(
            ""gpt-4"",
            base_url=""https://example.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2023-05-15"",
        )
        # Patch the _require_api_key property to return the test key directly
        with patch.object(model, ""_require_api_key"", ""test-key""):
            messages = [ChatMessage(role=""user"", content=""Test prompt"")]
            config = ChatModelConfig()

            result = model(messages, config)
            assert result == ""Test response""

            mock_azure_openai_class.assert_called_once_with(
                api_key=""test-key"",
                api_version=""2023-05-15"",
                azure_endpoint=""https://example.openai.azure.com"",
            )
        mock_client.chat.completions.create.assert_called_once()
        call_args = mock_client.chat.completions.create.call_args[1]
        assert call_args[""model""] == ""gpt-4""
",tests/_ai/llm/_impl.py,TestOpenAI
survived,"    def test_is_file_path_with_nonexistent_file(self) -> None:
        # Test with nonexistent file
        with pytest.raises(click.BadParameter) as excinfo:
            is_file_path(None, None, ""nonexistent_file.txt"")
        assert ""File does not exist: nonexistent_file.txt"" in str(excinfo.value)
",tests/_cli/test_cli_validators.py,TestIsFilePath
survived,"    def test_base_url_with_none_or_empty(self) -> None:
        # Test with None or empty string
        assert base_url(None, None, None) == """"
        assert base_url(None, None, """") == """"
",tests/_cli/test_cli_validators.py,TestBaseUrl
survived,"    def test_highlight_traceback(self) -> None:
        # Test that _highlight_traceback adds HTML formatting
        traceback = ""Traceback (most recent call last):\n  File \""<stdin>\"", line 1, in <module>\nValueError: invalid value""

        highlighted = _highlight_traceback(traceback)

        # Should contain HTML formatting
        assert ""<span class=\""codehilite\"">"" in highlighted
        assert ""</span>"" in highlighted

        # Should contain the original traceback text
        assert ""Traceback"" in highlighted
        # The ValueError text is present but with HTML tags around it
        assert ""ValueError"" in highlighted
        assert ""invalid value"" in highlighted
",tests/_messaging/test_tracebacks.py,TestTracebacks
deleted,"    def test_marimo_exception_raised_error(self) -> None:
        error = MarimoExceptionRaisedError(
            msg=""ValueError: invalid value"",
            exception_type=""ValueError"",
            raising_cell=""cell1"",
        )

        # Test properties
        assert error.type == ""exception""
        assert error.describe() == ""ValueError: invalid value""
        assert error.raising_cell == ""cell1""
        assert error.exception_type == ""ValueError""
",tests/_messaging/test_errors.py,TestErrorClasses
survived,"    def test_print_override_normal(self) -> None:
        # Test print_override when not in a marimo thread
        with patch(""marimo._messaging.print_override._original_print"") as mock_print:
            print_override(""Hello, world!"")
            mock_print.assert_called_once_with(""Hello, world!"")
",tests/_messaging/test_print_override.py,TestPrintOverride
survived,"    async def invoke_llm(
        self,
        query: core_entities.Query,
        model: requester.RuntimeLLMModel,
        messages: typing.List[llm_entities.Message],
        funcs: typing.List[tools_entities.LLMFunction] = None,
        extra_args: dict[str, typing.Any] = {},
    ) -> llm_entities.Message:
        genai.configure(api_key=model.token_mgr.get_token())

        generation_model = genai.GenerativeModel(model.model_entity.name)

        gemini_messages = []
        
        system_content = None
        for i, m in enumerate(messages):
            if m.role == 'system':
                system_content = m.content
                break
        
        for m in messages:
            if m.role == 'system':
                continue  # Skip system message as it's handled separately
            
            if m.role == 'user':
                role = 'user'
            elif m.role == 'assistant':
                role = 'model'
            else:
                continue  # Skip other roles for now
            
            content = m.content
            if isinstance(content, list):
                parts = []
                for part in content:
                    if part.get('type') == 'text':
                        parts.append(part.get('text', ''))
                content = '\n'.join(parts)
            
            gemini_messages.append({'role': role, 'parts': [content]})
        
        try:
            chat_params = extra_args.copy()
            if system_content:
                chat_params['system_instruction'] = system_content
            
            chat = generation_model.start_chat(history=gemini_messages)
            
            response = await chat.send_message_async(
                content="""",  # Empty content to get response based on history
                **chat_params
            )
            
            content = response.text
            
            return llm_entities.Message(
                role='assistant',
                content=content
            )
        except Exception as e:
            if 'invalid api key' in str(e).lower():
                raise errors.RequesterError(f'æ— æ•ˆçš„ api-key: {str(e)}')
            elif 'not found' in str(e).lower():
                raise errors.RequesterError(f'è¯·æ±‚è·¯å¾„é”™è¯¯æˆ–æ¨¡åž‹æ— æ•ˆ: {str(e)}')
            elif 'rate limit' in str(e).lower() or 'quota' in str(e).lower():
                raise errors.RequesterError(f'è¯·æ±‚è¿‡äºŽé¢‘ç¹æˆ–ä½™é¢ä¸è¶³: {str(e)}')
            else:
                raise errors.RequesterError(f'è¯·æ±‚é”™è¯¯: {str(e)}')",pkg/provider/modelmgr/requesters/geminichatcmpl.py,GeminiChatCompletions
survived,"    def condition_func(task_output: TaskOutput) -> bool:
        return ""success"" in task_output.raw.lower()
",tests/crew_test.py,
survived,"def test_smart_wallet_with_email(smart_api, test_email, test_wallet_options):
    """"""Test smart wallet creation with email.""""""
    options = {
        **test_wallet_options,
        ""linkedUser"": {""email"": test_email}
    }
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        options[""chain""],
        test_keypair,
        options[""provider""],
        options[""options""][""ensProvider""]
    )
    assert client.get_address() == wallet[""address""]
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def test_custodial_wallet_balance(custodial_api, test_email, solana_connection):
    """"""Test getting wallet balance.""""""
    # Create wallet and client
    wallet = custodial_api.create_custodial_wallet(test_email)
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    
    # Get balance
    balance = client.balance_of(wallet[""address""])
    assert ""value"" in balance
    assert ""symbol"" in balance
    assert balance[""symbol""] == ""SOL""
    assert ""decimals"" in balance
    assert balance[""decimals""] == 9
    assert ""name"" in balance
    assert balance[""name""] == ""Solana""
    assert ""in_base_units"" in balance
",python/src/wallets/crossmint/tests/test_custodial_wallet.py,
survived,"def test_email():
    """"""Fixture providing test email for wallet creation.""""""
    return ""test@example.com""
",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_custodial_wallet_invalid_transaction(custodial_api, test_email, solana_connection):
    """"""Test error handling with invalid transaction.""""""
    # Create wallet and client
    wallet = custodial_api.create_custodial_wallet(test_email)
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    
    # Try to send invalid transaction
    with pytest.raises(Exception) as exc:
        client.send_raw_transaction(""invalid-transaction"")
    assert ""error"" in str(exc.value).lower() or ""invalid"" in str(exc.value).lower()",python/src/wallets/crossmint/tests/test_custodial_wallet.py,
survived,"def test_smart_wallet_message_signing(smart_api, test_wallet_options, test_message, test_keypair):
    """"""Test message signing with smart wallet.""""""
    # Create wallet and client
    wallet = smart_api.create_smart_wallet()
    client = SmartWalletClient(
        wallet[""address""],
        smart_api,
        test_wallet_options[""chain""],
        test_keypair,
        test_wallet_options[""provider""],
        test_wallet_options[""options""][""ensProvider""]
    )
    
    # Sign message
    signature = client.sign_message(test_message)
    assert signature[""signature""].startswith(""0x"")
    
    # Verify signature format
    assert len(signature[""signature""]) > 130  # Valid EVM signature length
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"    def __init__(self, client: BoxClient, folder_id: str, is_recursive: bool = False):
        self.client = client
        self.folder_id = folder_id
        self.is_recursive = is_recursive
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamTextRepresentationFolder
survived,"    def __init__(self, client: BoxClient, folder_id: str, fields_json_str: str, is_recursive: bool = False):
        self.client = client
        self.folder_id = folder_id
        self.is_recursive = is_recursive
        self.fields_json_str = fields_json_str
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder
survived,"def get_generic_json_schema() -> Dict:
    generic_schema = """"""
        {
        ""$schema"": ""http://json-schema.org/draft-07/schema#"",
        ""type"": ""object"",
        ""properties"": {
            ""type"": {
            ""type"": ""string""
            },
            ""id"": {
            ""type"": ""string""
            },
            ""file_version"": {
            ""type"": ""object"",
            ""properties"": {
                ""type"": {
                ""type"": ""string""
                },
                ""id"": {
                ""type"": ""string""
                },
                ""sha1"": {
                ""type"": ""string""
                }
            },
            ""required"": [""type"", ""id"", ""sha1""]
            },
            ""sequence_id"": {
            ""type"": ""string""
            },
            ""etag"": {
            ""type"": ""string""
            },
            ""sha1"": {
            ""type"": ""string""
            },
            ""name"": {
            ""type"": ""string""
            },
            ""description"": {
            ""type"": ""string""
            },
            ""size"": {
            ""type"": ""integer""
            },
            ""path_collection"": {
            ""type"": ""object"",
            ""properties"": {
                ""total_count"": {
                ""type"": ""integer""
                },
                ""entries"": {
                ""type"": ""array"",
                ""items"": [
                    {
                    ""type"": ""object"",
                    ""properties"": {
                        ""type"": {
                        ""type"": ""string""
                        },
                        ""id"": {
                        ""type"": ""string""
                        },
                        ""sequence_id"": {
                        ""type"": ""null""
                        },
                        ""etag"": {
                        ""type"": ""null""
                        },
                        ""name"": {
                        ""type"": ""string""
                        }
                    },
                    ""required"": [""type"", ""id"", ""sequence_id"", ""etag"", ""name""]
                    },
                    {
                    ""type"": ""object"",
                    ""properties"": {
                        ""type"": {
                        ""type"": ""string""
                        },
                        ""id"": {
                        ""type"": ""string""
                        },
                        ""sequence_id"": {
                        ""type"": ""string""
                        },
                        ""etag"": {
                        ""type"": ""string""
                        },
                        ""name"": {
                        ""type"": ""string""
                        }
                    },
                    ""required"": [""type"", ""id"", ""sequence_id"", ""etag"", ""name""]
                    },
                    {
                    ""type"": ""object"",
                    ""properties"": {
                        ""type"": {
                        ""type"": ""string""
                        },
                        ""id"": {
                        ""type"": ""string""
                        },
                        ""sequence_id"": {
                        ""type"": ""string""
                        },
                        ""etag"": {
                        ""type"": ""string""
                        },
                        ""name"": {
                        ""type"": ""string""
                        }
                    },
                    ""required"": [""type"", ""id"", ""sequence_id"", ""etag"", ""name""]
                    }
                ]
                }
            },
            ""required"": [""total_count"", ""entries""]
            },
            ""created_at"": {
            ""type"": ""string""
            },
            ""modified_at"": {
            ""type"": ""string""
            },
            ""trashed_at"": {
            ""type"": ""null""
            },
            ""purged_at"": {
            ""type"": ""null""
            },
            ""content_created_at"": {
            ""type"": ""string""
            },
            ""content_modified_at"": {
            ""type"": ""string""
            },
            ""created_by"": {
            ""type"": ""object"",
            ""properties"": {
                ""type"": {
                ""type"": ""string""
                },
                ""id"": {
                ""type"": ""string""
                },
                ""name"": {
                ""type"": ""string""
                },
                ""login"": {
                ""type"": ""string""
                }
            },
            ""required"": [""type"", ""id"", ""name"", ""login""]
            },
            ""modified_by"": {
            ""type"": ""object"",
            ""properties"": {
                ""type"": {
                ""type"": ""string""
                },
                ""id"": {
                ""type"": ""string""
                },
                ""name"": {
                ""type"": ""string""
                },
                ""login"": {
                ""type"": ""string""
                }
            },
            ""required"": [""type"", ""id"", ""name"", ""login""]
            },
            ""owned_by"": {
            ""type"": ""object"",
            ""properties"": {
                ""type"": {
                ""type"": ""string""
                },
                ""id"": {
                ""type"": ""string""
                },
                ""name"": {
                ""type"": ""string""
                },
                ""login"": {
                ""type"": ""string""
                }
            },
            ""required"": [""type"", ""id"", ""name"", ""login""]
            },
            ""shared_link"": {
            ""type"": ""null""
            },
            ""parent"": {
            ""type"": ""object"",
            ""properties"": {
                ""type"": {
                ""type"": ""string""
                },
                ""id"": {
                ""type"": ""string""
                },
                ""sequence_id"": {
                ""type"": ""string""
                },
                ""etag"": {
                ""type"": ""string""
                },
                ""name"": {
                ""type"": ""string""
                }
            },
            ""required"": [""type"", ""id"", ""sequence_id"", ""etag"", ""name""]
            },
            ""item_status"": {
            ""type"": ""string""
            },
            ""text_representation"": {
            ""type"": ""string""
            }
        }
        }

        """"""
    return json.loads(generic_schema)",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/schemas.py,
survived,"    def read_records(
        self,
        sync_mode: SyncMode,
        cursor_field: Optional[List[str]] = None,
        stream_slice: Optional[Mapping[str, Any]] = None,
        stream_state: Optional[Mapping[str, Any]] = None,
    ) -> Iterable[StreamData]:
        logger.info(f""Extracting text representation for files in folder {self.folder_id} {'recursively' if self.is_recursive else ''}"")
        items = box_folder_text_representation(self.client, self.folder_id, is_recursive=self.is_recursive)
        for item in items:
            airbyte_item: StreamData = item.file.to_dict()
            airbyte_item[""text_representation""] = item.text_representation
            logger.info(f""Reading file {item.file.id} - {item.file.name}"")
            yield airbyte_item
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamTextRepresentationFolder
survived,"def box_file_get_by_id(client: BoxClient, file_id: str) -> File:
    return client.files.get_file_by_id(file_id=file_id)
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"def create_manager_agent(specialists: List[Agent]) -> Agent:
    """"""
    Create a manager agent that coordinates the work of specialist agents.
    
    Args:
        specialists: List of specialist agents to coordinate
        
    Returns:
        An Agent instance that manages the content creation process
    """"""
    instructions = """"""
    You are a content manager who coordinates the work of specialist agents to create high-quality content.
    Your task is to:
    1. Understand the content request
    2. Delegate research to the Research Specialist
    3. Have the Outline Specialist create a structure based on the research
    4. Have the Content Specialist write content based on the outline and research
    5. Have the Editing Specialist refine and polish the content
    6. Deliver the final polished content
    
    Manage the workflow efficiently and ensure each specialist has the information they need.
    """"""
    
    # Create handoffs to specialist agents
    handoffs = [handoff(agent) for agent in specialists]
    
    return Agent(
        name=""ContentManager"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoffs=handoffs
    )
",openai-agents-examples/11_agent_orchestration.py,
survived,"def get_stock_price(params: StockPriceInput) -> str:
    """"""
    Get the current price of a stock.
    
    Args:
        params: The stock price parameters
        
    Returns:
        A string containing the stock price information
    """"""
    # This is a mock implementation - in a real application, you would call a stock API
    stock_prices = {
        ""AAPL"": 175.34,
        ""MSFT"": 410.34,
        ""GOOGL"": 147.68,
        ""AMZN"": 178.75,
        ""META"": 474.99,
    }
    
    symbol = params.symbol.upper()
    
    # Check if stock is supported
    if symbol not in stock_prices:
        return f""Sorry, stock information for {symbol} is not available.""
    
    price = stock_prices[symbol]
    
    return f""The current price of {symbol} is ${price:.2f}.""
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def test_custom_tools():
    """"""Test that the custom tools work correctly.""""""
    # Test currency conversion
    currency_result = convert_currency(CurrencyConversionInput(
        amount=100,
        from_currency=""USD"",
        to_currency=""EUR""
    ))
    assert ""USD"" in currency_result
    assert ""EUR"" in currency_result
    
    # Test stock price
    stock_result = get_stock_price(StockPriceInput(symbol=""AAPL""))
    assert ""AAPL"" in stock_result
    assert ""$"" in stock_result
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"def main():
    """"""Main function to parse arguments and run the agent synchronously.""""""
    parser = argparse.ArgumentParser(description=""Synchronous Agent Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the agent synchronously and get response
        response = run_sync_agent(args.prompt)
        
        # Display the response
        console.print(Panel(response, title=""Synchronous Agent Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/03_sync_agent.py,
survived,"def test_run_multi_agent_system():
    """"""Test that the multi-agent system can run and produce a response.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a simple test query that should go to the tech specialist
    response = asyncio.run(run_multi_agent_system(""What is machine learning?""))
    
    # Verify we got a non-empty response
    assert response
    assert len(response) > 0
",openai-agents-examples/02_multi_agent.py,
survived,"def test_create_financial_assistant():
    """"""Test that the financial assistant agent is created with the correct configuration.""""""
    agent = create_financial_assistant()
    assert agent.name == ""FinancialAssistant""
    assert ""financial assistant"" in agent.instructions.lower()
    assert len(agent.tools) == 2
    assert any(tool.name == ""convert_currency"" for tool in agent.tools)
    assert any(tool.name == ""get_stock_price"" for tool in agent.tools)
",openai-agents-examples/06_agent_with_custom_tools.py,
survived,"async def run_blog_writer_system(prompt: str) -> str:
    """"""
    Run the blog writer system with the given prompt.
    
    Args:
        prompt: The topic or request for a blog post
        
    Returns:
        The blog post content
    """"""
    # Create the research agent
    research_agent = create_research_agent()
    
    # Create the blog writer agent with the research agent as a tool
    blog_writer = create_blog_writer_agent(research_agent)
    
    # Run the blog writer agent with the prompt
    result = await Runner.run(blog_writer, prompt)
    
    # Return the blog post
    return result.final_output
",openai-agents-examples/08_agent_with_agent_as_tool.py,
survived,"async def test_delete_all_endpoint():
    payload = {
        ""iat"": datetime.datetime.utcnow(),
        ""exp"": datetime.datetime.utcnow() + datetime.timedelta(days=1),
        ""subdomain"": ""abcd1234"",
    }
    token = jwt.encode(payload, ""test-secret"", algorithm=""HS256"")
    
    mock_redis.delete.return_value = True
    mock_redis.keys.return_value = [""requests:abcd1234"", ""request:abcd1234:id1"", ""request:abcd1234:id2""]
    
    with patch(""backend.app.config.jwt_secret"", ""test-secret""):
        response = client.post(""/api/delete_all"", params={""token"": token})
        
        assert response.status_code == 200
        assert response.json() == {""msg"": ""Deleted all requests""}
        
        mock_redis.delete.assert_called()",backend/tests/test_endpoints.py,
survived,"        async def get_messages(session_type: str) -> str:
            """"""èŽ·å–è°ƒè¯•æ¶ˆæ¯åŽ†å²""""""
            try:
                if session_type not in ['person', 'group']:
                    return self.http_status(400, -1, 'session_type must be person or group')
                
                webchat_adapter = None
                for bot in self.ap.platform_mgr.bots:
                    if hasattr(bot.adapter, '__class__') and bot.adapter.__class__.__name__ == 'WebChatAdapter':
                        webchat_adapter = bot.adapter
                        break
                
                if not webchat_adapter:
                    return self.http_status(404, -1, 'WebChat adapter not found')
                
                messages = webchat_adapter.get_debug_messages(session_type)
                
                return self.success(data={'messages': messages})
                
            except Exception as e:
                return self.http_status(500, -1, f'Internal server error: {str(e)}')
",pkg/api/http/controller/groups/debug/webchat.py,WebChatDebugRouterGroup
survived,"    async def send_debug_message(self, session_type: str, content: str) -> dict:
        """"""å‘é€è°ƒè¯•æ¶ˆæ¯åˆ°æµæ°´çº¿""""""
        session_key = f'webchat{session_type}'
        
        if session_key not in self.debug_messages:
            self.debug_messages[session_key] = []
            
        message_chain = platform_message.MessageChain([
            platform_message.Plain(content)
        ])
        
        user_message = {
            'id': len(self.debug_messages[session_key]) + 1,
            'type': 'user',
            'content': content,
            'timestamp': datetime.now().isoformat(),
            'message_chain': [{'type': 'Plain', 'text': content}]
        }
        
        self.debug_messages[session_key].append(user_message)
        
        if session_type == 'person':
            sender = platform_entities.Friend(id='webchatperson', nickname='è°ƒè¯•ç”¨æˆ·')
            event = platform_events.FriendMessage(
                sender=sender,
                message_chain=message_chain,
                time=datetime.now().timestamp()
            )
            launcher_type = core_entities.LauncherTypes.PERSON
            launcher_id = 'webchatperson'
        else:
            group = platform_entities.Group(id='webchatgroup', name='è°ƒè¯•ç¾¤èŠ')
            sender = platform_entities.GroupMember(id='webchatperson', nickname='è°ƒè¯•ç”¨æˆ·', group=group)
            event = platform_events.GroupMessage(
                sender=sender,
                message_chain=message_chain,
                time=datetime.now().timestamp()
            )
            launcher_type = core_entities.LauncherTypes.GROUP
            launcher_id = 'webchatgroup'
        
        await self.ap.query_pool.add_query(
            bot_uuid='webchat-debug',
            launcher_type=launcher_type,
            launcher_id=launcher_id,
            sender_id='webchatperson',
            message_event=event,
            message_chain=message_chain,
            adapter=self,
        )
        
        return {'success': True, 'message_id': user_message['id']}
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"    def navigate_to_result(self, url: str):
        """"""Navigate to a search result.""""""
        self.show_results = False
        return rx.redirect(url)
",pcweb/components/docpage/navbar/typesense.py,TypesenseSearchState
survived,"    def recreate_collection(self):
        """"""Recreate the docs collection.""""""
        try:
            self.client.collections['docs'].delete()
            logger.info(""Deleted existing 'docs' collection"")
        except Exception as e:
            logger.info(f""Collection 'docs' doesn't exist or couldn't be deleted: {e}"")
        
        self.client.collections.create(COLLECTION_SCHEMA)
        logger.info(""Created new 'docs' collection"")
",scripts/typesense_indexer.py,TypesenseIndexer
survived,"def test_solana_smart_wallet_transaction(smart_api, test_solana_transaction):
    """"""Test transaction sending with Solana smart wallet.""""""
    # Create a wallet first
    wallet = smart_api.create_wallet(
        wallet_type=WalletType.SOLANA_SMART_WALLET,
        linked_user=""email:test@example.com""
    )
    
    # Test transaction submission
    try:
        # Create transaction parameters
        tx_params = SolanaSmartWalletTransactionParams(
            transaction=test_solana_transaction,
            required_signers=[]  # No required signers for basic test
        )
        
        # Create transaction
        tx = smart_api.create_transaction_for_smart_wallet(
            wallet[""address""],
            tx_params,
            ""solana""
        )
        # If successful, verify response format
        assert ""id"" in tx
        assert tx[""type""] == ""solana-smart-wallet""
        assert tx[""status""] in [""pending"", ""awaiting_signatures"", ""success""]
    except Exception as e:
        error_msg = str(e).lower()
        # Check for expected error cases
        assert any(msg in error_msg for msg in [
            ""invalid transaction"",
            ""transaction verification failed"",
            ""invalid serialized"",
            ""parsing error"",
            ""signatures that would be ignored"",
            ""submit signatures separately""
        ]), f""Unexpected error: {error_msg}""
",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,
survived,"def main():
    """"""Run the pipeline architecture example.""""""
    print(""\n===== Pipeline Architecture Example ====="")
    
    # Create sample data
    data_file = create_sample_data()
    
    # Create pipeline stages
    input_stage = InputStage()
    processing_stage = ProcessingStage()
    output_stage = OutputStage()
    
    # Create and configure pipeline
    pipeline = DataProcessingPipeline(""Sales Data Analysis Pipeline"")
    
    # Add stages
    pipeline.add_stage(""input"", input_stage)
    pipeline.add_stage(""processing"", processing_stage)
    pipeline.add_stage(""output"", output_stage)
    
    # Configure input
    pipeline.configure_input(
        source=data_file,
        source_type=""json"",
        required_fields=[""id"", ""product"", ""price"", ""quantity""]
    )
    
    # Configure processing
    pipeline.configure_processing({
        ""calculate_statistics"": True,
        ""numeric_fields"": [""price"", ""quantity"", ""discount""],
        ""filters"": [
            {
                ""filter_func"": lambda item: item[""price""] * item[""quantity""] > 1000,
                ""description"": ""High-value sales (>$1000)""
            }
        ],
        ""transformations"": {
            ""price"": lambda price: format_currency(price),
            ""discount"": lambda discount: format_percentage(discount)
        },
        ""transformation_description"": ""Format price as currency and discount as percentage""
    })
    
    # Configure output
    pipeline.configure_output({
        ""format_summary"": True,
        ""format_detailed"": True,
        ""print_results"": True,
        ""print_output_type"": ""summary"",
        ""save_to_file"": [
            {
                ""format"": ""json"",
                ""dir"": ""./output"",
                ""filename"": ""sales_analysis.json""
            }
        ]
    })
    
    # Run the pipeline
    result = pipeline.run()
    
    print(""\n===== Pipeline Execution Complete ====="")
    print(f""Pipeline status: {result['metadata']['status']}"")
    print(f""Execution time: {result['metadata']['execution_time_seconds']:.2f} seconds"")
    
    # Show output file location if saved
    if ""stages"" in result and len(result[""stages""]) > 0:
        output_stage_name = result[""stages""][-1][""name""]
        if output_stage_name in pipeline.results:
            output_result = pipeline.results[output_stage_name]
            if ""metadata"" in output_result and ""output_files"" in output_result[""metadata""]:
                print(""\nOutput files:"")
                for output_file in output_result[""metadata""][""output_files""]:
                    print(f""- {output_file['path']} ({output_file['format']})"")
",codebase-architectures/pipeline-architecture/main.py,
survived,"    def get_category(category_id):
        """"""Get a category by ID.""""""
        try:
            category = CategoryService.get_category(category_id)
            if not category:
                return {
                    ""success"": False,
                    ""message"": f""Category with ID {category_id} not found""
                }
            return {
                ""success"": True,
                ""data"": category
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in get_category: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while retrieving the category""
            }
",codebase-architectures/layered-architecture/api/category_api.py,CategoryAPI
survived,"    def change_password(token: str, current_password: str, new_password: str) -> Dict:
        """"""
        Change a user's password.
        
        Args:
            token: Authentication token
            current_password: The current password
            new_password: The new password
            
        Returns:
            Response with success status or error message
        """"""
        # Validate token
        success, user_data = validate_user_token(token)
        if not success:
            return {
                ""status"": ""error"",
                ""message"": ""Invalid or expired token"",
                ""data"": None
            }
        
        # Change password
        success, result = change_password(user_data[""id""], current_password, new_password)
        
        if success:
            return {
                ""status"": ""success"",
                ""message"": ""Password changed successfully"",
                ""data"": None
            }
        else:
            return {
                ""status"": ""error"",
                ""message"": result.get(""error"", ""Password change failed""),
                ""data"": None
            }",codebase-architectures/atomic-composable-architecture/endpoints/user_api.py,UserAPI
survived,"    def create_task(title, description=None, user_id=None):
        """"""Create a new task.""""""
        task_data = {
            ""title"": title,
            ""description"": description,
            ""user_id"": user_id
        }
        return TaskService.create_task(task_data)
",codebase-architectures/vertical-slice-architecture/features/tasks/api.py,TaskAPI
survived,"    def update_task(task_id, task_data):
        """"""Update a task.""""""
        existing_task = db.get(""tasks"", task_id)
        if not existing_task:
            return None
        
        # Update fields
        for key, value in task_data.items():
            if key not in [""id"", ""created_at""]:
                existing_task[key] = value
        
        # Update timestamp
        existing_task[""updated_at""] = get_timestamp()
        
        # Save to database
        db.update(""tasks"", task_id, existing_task)
        return existing_task
",codebase-architectures/vertical-slice-architecture/features/tasks/service.py,TaskService
survived,"def display_header(text):
    """"""Display a header with the given text.""""""
    print(""\n"" + ""="" * 50)
    print(f"" {text}"")
    print(""="" * 50)
",codebase-architectures/vertical-slice-architecture/main.py,
survived,"    def __init__(self):
        self.data = {}
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"    def get_all_categories():
        """"""Get all categories.""""""
        try:
            categories = db.get_all(""categories"")
            Logger.info(app_logger, f""Retrieved {len(categories)} categories"")
            return categories
        except Exception as e:
            Logger.error(app_logger, f""Error getting all categories: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/category_service.py,CategoryService
survived,"def validate_string_length(value: str, min_length: int = 0, max_length: Optional[int] = None) -> bool:
    """"""
    Validate that a string's length is within the specified range.
    
    Args:
        value: The string to validate
        min_length: Minimum allowed length
        max_length: Maximum allowed length, or None for no maximum
        
    Returns:
        True if the string length is valid, False otherwise
    """"""
    if not isinstance(value, str):
        return False
    
    if len(value) < min_length:
        return False
    
    if max_length is not None and len(value) > max_length:
        return False
    
    return True
",codebase-architectures/atomic-composable-architecture/modules/validation.py,
survived,"def generate_report_filename(prefix=""report"", extension=""json""):
    """"""Generate a filename for a report with timestamp.""""""
    timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
    return f""{prefix}_{timestamp}.{extension}""",codebase-architectures/pipeline-architecture/shared/utilities.py,
survived,"    def run(self):
        """"""
        Run the pipeline by executing all stages in sequence.
        
        Returns:
            dict: Pipeline results including data and metadata
        """"""
        self.metadata[""started_at""] = datetime.now().isoformat()
        self.metadata[""status""] = ""running""
        
        print(f""\n=== Starting Pipeline: {self.name} ==="")
        
        # Execute each stage
        for i, stage in enumerate(self.stages):
            stage_name = stage[""name""]
            stage_instance = stage[""instance""]
            
            print(f""\n--- Stage {i+1}: {stage_name} ---"")
            
            try:
                # Execute the stage
                if i == 0:
                    # First stage doesn't take input from previous stage
                    result = self._execute_first_stage(stage_instance)
                else:
                    # Pass result from previous stage
                    previous_result = self.results[self.stages[i-1][""name""]]
                    result = self._execute_stage(stage_instance, previous_result)
                
                # Store the result
                self.results[stage_name] = result
                
                # Update stage status
                stage[""status""] = result[""metadata""][""status""]
                
                # Check for errors
                if result[""metadata""][""status""] in [""error"", ""skipped""]:
                    print(f""Stage {stage_name} {result['metadata']['status']}"")
                    for error in result[""metadata""].get(""errors"", []):
                        print(f""  Error: {error}"")
                    
                    # Add errors to pipeline metadata
                    self.metadata[""errors""].append({
                        ""stage"": stage_name,
                        ""errors"": result[""metadata""].get(""errors"", [])
                    })
                else:
                    print(f""Stage {stage_name} completed successfully"")
            
            except Exception as e:
                # Handle unexpected errors
                error_message = f""Unexpected error in stage {stage_name}: {str(e)}""
                print(f""  Error: {error_message}"")
                
                # Update stage status
                stage[""status""] = ""error""
                
                # Add error to pipeline metadata
                self.metadata[""errors""].append({
                    ""stage"": stage_name,
                    ""errors"": [error_message]
                })
        
        # Update pipeline status
        self.metadata[""completed_at""] = datetime.now().isoformat()
        if self.metadata[""errors""]:
            self.metadata[""status""] = ""completed_with_errors""
        else:
            self.metadata[""status""] = ""completed""
        
        # Calculate total execution time
        start_time = datetime.fromisoformat(self.metadata[""started_at""])
        end_time = datetime.fromisoformat(self.metadata[""completed_at""])
        execution_time = (end_time - start_time).total_seconds()
        self.metadata[""execution_time_seconds""] = execution_time
        
        print(f""\n=== Pipeline {self.name} {self.metadata['status']} ==="")
        print(f""Total execution time: {execution_time:.2f} seconds"")
        
        return self._create_pipeline_result()
",codebase-architectures/pipeline-architecture/pipeline/pipeline_manager.py,PipelineManager
survived,"    def delete_user(user_id):
        """"""Delete a user.""""""
        success = UserService.delete_user(user_id)
        if not success:
            return {""error"": f""User with ID {user_id} not found""}
        return {""message"": f""User with ID {user_id} deleted successfully""}",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI
survived,"    def get_all_products():
        """"""Get all products.""""""
        try:
            products = db.get_all(""products"")
            Logger.info(app_logger, f""Retrieved {len(products)} products"")
            return products
        except Exception as e:
            Logger.error(app_logger, f""Error getting all products: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/product_service.py,ProductService
survived,"    def get_by_sku(sku):
        """"""Get a product by SKU.""""""
        try:
            product = ProductService.get_by_sku(sku)
            if not product:
                return {
                    ""success"": False,
                    ""message"": f""Product with SKU '{sku}' not found""
                }
            return {
                ""success"": True,
                ""data"": product
            }
        except Exception as e:
            Logger.error(app_logger, f""Error in get_by_sku: {str(e)}"", exc_info=True)
            return {
                ""success"": False,
                ""message"": ""An error occurred while retrieving the product""
            }
",codebase-architectures/layered-architecture/api/product_api.py,ProductAPI
survived,"    def error(logger_name: str, message: str, exc_info: bool = False) -> None:
        """"""
        Log an error message.
        
        Args:
            logger_name: Name of the logger
            message: Message to log
            exc_info: Whether to include exception info
        """"""
        console.log(f""[{logger_name}] [error] {message}"")
        
        if exc_info:
            import traceback
            console.log(traceback.format_exc())",example-agent-codebase-arch/layered-architecture/utils/logger.py,Logger
survived,"    def create_file(path: str, file_text: str) -> FileOperationResult:
        """"""
        Create a new file with specified content.

        Args:
            path: The path where the new file should be created
            file_text: The content to write to the new file

        Returns:
            FileOperationResult with result or error message
        """"""
        try:
            # Check if the path is empty or invalid
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                console.log(f""[create_file] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            # Check if the directory exists
            directory = os.path.dirname(path)
            if directory and not os.path.exists(directory):
                console.log(f""[create_file] Creating directory: {directory}"")
                os.makedirs(directory)

            with open(path, ""w"") as f:
                f.write(file_text or """")

            console.print(f""[green]Successfully created file {path}[/green]"")
            console.log(f""[create_file] Successfully created file {path}"")
            return FileOperationResult(True, f""Successfully created file {path}"")
        except Exception as e:
            error_msg = f""Error creating file: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[create_file] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/service.py,FileOperationService
survived,"    def __init__(self, success: bool, message: str, data: Any = None):
        """"""
        Initialize a file operation result.
        
        Args:
            success: Whether the operation was successful
            message: A message describing the result
            data: Optional data returned by the operation
        """"""
        self.success = success
        self.message = message
        self.data = data
",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/model.py,FileOperationResult
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the result to a dictionary.
        
        Returns:
            Dictionary representation of the result
        """"""
        return {
            ""success"": self.success,
            ""message"": self.message,
            ""data"": self.data
        }
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,FileOperationResult
survived,"def test_create_folder_structure_strips_multiple_trailing_slashes():
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        folder_path, folder_name, class_name = create_folder_structure(""hello///"")
        
        assert folder_name == ""hello""
        assert class_name == ""Hello""
        assert folder_path.name == ""hello""
        assert folder_path.exists()
",tests/cli/test_create_crew.py,
survived,"def test_create_crew_with_multiple_trailing_slashes(mock_load_env, mock_write_env, mock_copy_template, temp_dir):
    mock_load_env.return_value = {}
    
    with tempfile.TemporaryDirectory() as work_dir:
        os.chdir(work_dir)
        create_crew(""test-project///"", skip_provider=True)
        
        project_path = Path(work_dir) / ""test_project""
        assert project_path.exists()
        assert (project_path / ""src"" / ""test_project"").exists()
",tests/cli/test_create_crew.py,
survived,"    def load_state(self, flow_uuid: str) -> Optional[Dict[str, Any]]:
        """"""Load the most recent state for a given flow UUID.
        
        Args:
            flow_uuid: Unique identifier for the flow instance
            
        Returns:
            The most recent state as a dictionary, or None if no state exists
        """"""
        pass",src/crewai/flow/persistence/base.py,FlowPersistence
survived,"def test_flow_state_restoration(tmp_path):
    """"""Test restoring flow state from persistence.""""""
    db_path = os.path.join(tmp_path, ""test_flows.db"")
    persistence = SQLiteFlowPersistence(db_path)
    
    # First flow execution to create initial state
    class RestorableFlow(Flow[TestState]):
        initial_state = TestState
        
        @start()
        @persist(persistence)
        def set_message(self):
            self.state.message = ""Original message""
            self.state.counter = 42
    
    flow1 = RestorableFlow(persistence=persistence)
    flow1.kickoff()
    original_uuid = flow1.state.id
    
    # Create new flow instance with restored state
    flow2 = RestorableFlow(
        persistence=persistence,
        restore_uuid=original_uuid,
        counter=43,  # Override counter
    )
    
    # Verify state restoration and merging
    assert flow2.state.id == original_uuid
    assert flow2.state.message == ""Original message""
    assert flow2.state.counter == 43  # Verify override worked
",tests/test_flow_persistence.py,
survived,"        def set_message(self):
            self.state.message = ""Original message""
            self.state.counter = 42
",tests/test_flow_persistence.py,RestorableFlow
survived,"    def init_db(self) -> None:
        """"""Create the necessary tables if they don't exist.""""""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(""""""
            CREATE TABLE IF NOT EXISTS flow_states (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                flow_uuid TEXT NOT NULL,
                method_name TEXT NOT NULL,
                timestamp DATETIME NOT NULL,
                state_json TEXT NOT NULL
            )
            """""")
            # Add index for faster UUID lookups
            conn.execute(""""""
            CREATE INDEX IF NOT EXISTS idx_flow_states_uuid 
            ON flow_states(flow_uuid)
            """""")
",src/crewai/flow/persistence/sqlite.py,SQLiteFlowPersistence
survived,"        def count_up(self):
            self.state.counter += 1
            self.state.message = f""Count is {self.state.counter}""
",tests/test_flow_persistence.py,StructuredFlow
survived,"def persist(persistence: FlowPersistence):
    """"""Decorator to persist flow state after method execution.
    
    This decorator supports both synchronous and asynchronous methods. It will
    persist the flow state after the method completes successfully. For async
    methods, it ensures the state is persisted before returning the result.
    
    Args:
        persistence: FlowPersistence implementation to use for storing state
    
    Returns:
        A decorator function that wraps flow methods and handles state persistence
    
    Raises:
        ValueError: If the flow state doesn't have an 'id' field
        RuntimeError: If state persistence fails
    """"""
    def _persist_state(flow_instance: Any, method_name: str) -> None:
        """"""Helper to persist state with error handling.""""""
        try:
            # Get flow UUID from state
            state = getattr(flow_instance, 'state', None)
            if state is None:
                raise ValueError(""Flow instance has no state"")
                
            flow_uuid: Optional[str] = None
            if isinstance(state, dict):
                flow_uuid = state.get('id')
            elif isinstance(state, BaseModel):
                flow_uuid = getattr(state, 'id', None)
                
            if not flow_uuid:
                raise ValueError(
                    ""Flow state must have an 'id' field for persistence""
                )
                
            # Persist the state
            persistence.save_state(
                flow_uuid=flow_uuid,
                method_name=method_name,
                state_data=state,
            )
        except Exception as e:
            logger.error(
                f""Failed to persist state for method {method_name}: {str(e)}""
            )
            raise RuntimeError(f""State persistence failed: {str(e)}"") from e
    
    def decorator(method: Callable[..., T]) -> Callable[..., T]:
        """"""Decorator that handles both sync and async methods.""""""
        if asyncio.iscoroutinefunction(method):
            @functools.wraps(method)
            async def async_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original async method
                result = await method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
            return cast(Callable[..., T], async_wrapper)
        else:
            @functools.wraps(method)
            def sync_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original sync method
                result = method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
            return cast(Callable[..., T], sync_wrapper)
            
    return decorator",src/crewai/flow/persistence/decorators.py,
survived,"            def sync_wrapper(flow_instance: Any, *args: Any, **kwargs: Any) -> T:
                # Execute the original sync method
                result = method(flow_instance, *args, **kwargs)
                # Persist state after method completion
                _persist_state(flow_instance, method.__name__)
                return result
",src/crewai/flow/persistence/decorators.py,
survived,"    def supports_chain(self, chain) -> bool:
        return True
",python/src/plugins/jsonrpc/goat_plugins/jsonrpc/__init__.py,JSONRpcPlugin
survived,"    def supports_stop_words(self) -> bool:
        """"""Return True to indicate that stop words are supported.""""""
        return True
",tests/custom_llm_test.py,CustomLLM
