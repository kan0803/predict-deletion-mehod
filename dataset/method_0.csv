status,method,filepath,class_name
survived,"def test_keyword_only_param_removed():
    old_code = ""def func(*, a, b): pass""
    new_code = ""def func(*, b): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""Keyword-only param 'a' was removed.""
    assert errors[0].param_name == ""a""
",tests/dev/test_check_function_signatures.py,
survived,"def test_optional_keyword_only_became_required():
    old_code = ""def func(*, a=1): pass""
    new_code = ""def func(*, a): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 1
    assert errors[0].message == ""Keyword-only param 'a' became required.""
    assert errors[0].param_name == ""a""
",tests/dev/test_check_function_signatures.py,
survived,"def get_changed_python_files(base_branch: str = ""master"") -> list[Path]:
    # In GitHub Actions PR context, we need to fetch the base branch first
    if is_github_actions():
        # Fetch the base branch to ensure we have it locally
        subprocess.check_call(
            [""git"", ""fetch"", ""origin"", f""{base_branch}:{base_branch}""],
        )

    result = subprocess.check_output(
        [""git"", ""diff"", ""--name-only"", f""{base_branch}...HEAD""], text=True
    )
    files = [s.strip() for s in result.splitlines()]
    return [Path(f) for f in files if f]
",dev/check_function_signatures.py,
survived,"def test_complex_mixed_violations():
    old_code = ""def func(a, b=1, *, c, d=2): pass""
    new_code = ""def func(x, b, *, c=3, e): pass""

    old_tree = ast.parse(old_code)
    new_tree = ast.parse(new_code)
    errors = check_signature_compatibility(old_tree.body[0], new_tree.body[0])

    assert len(errors) == 3
    error_messages = [e.message for e in errors]
    assert any(""Positional param order/name changed: 'a' -> 'x'."" in msg for msg in error_messages)
    assert any(""Keyword-only param 'd' was removed."" in msg for msg in error_messages)
    assert any(""New required keyword-only param 'e' added."" in msg for msg in error_messages)
",tests/dev/test_check_function_signatures.py,
survived,"    def test_broadcasting_higher_dims(self, func):
        """"""Test that gufunc broadcasting works correctly for higher dimensional arrays.""""""
        np.random.seed(42)

        # 3D array: (2, 4, 10) -> broadcast dims (2,) + core dims (4, 10)
        data_3d = np.random.randn(2, 4, 10)
        result_3d = func(data_3d)
        assert result_3d.shape == (2, 4, 4)

        # 4D array: (2, 3, 4, 10) -> broadcast dims (2, 3) + core dims (4, 10)
        data_4d = np.random.randn(2, 3, 4, 10)
        result_4d = func(data_4d)
        assert result_4d.shape == (2, 3, 4, 4)

        # Check each broadcast element is valid
        for i in range(2):
            for j in range(3):
                matrix = result_4d[i, j]
                # Check symmetry
                assert_allclose(matrix, matrix.T, rtol=1e-10)

                if func == nancorrmatrix:
                    # Check diagonal is 1
                    assert_allclose(np.diag(matrix), np.ones(4), rtol=1e-10)
                    # Check bounds
                    assert np.all((matrix >= -1) & (matrix <= 1))
                else:
                    # Check diagonal (variance) is non-negative
                    assert np.all(np.diag(matrix) >= 0)

        # Verify correctness - each slice should match individual computation
        for i in range(2):
            single_result = func(data_3d[i])
            assert_allclose(result_3d[i], single_result, rtol=1e-10)
",numbagg/test/test_matrix_functions.py,TestMatrixFunctions
survived,"    def test_different_dtypes(self):
        """"""Test consistency between float32 and float64.""""""
        data_f64 = np.array([[1, 2, 3, 4], [2, 4, 6, 8]], dtype=np.float64)
        data_f32 = data_f64.astype(np.float32)

        alpha = 0.5

        result_f64 = move_exp_nancorrmatrix(data_f64, alpha=alpha)
        result_f32 = move_exp_nancorrmatrix(data_f32, alpha=alpha)

        # Results should be close (within float32 precision)
        assert_allclose(result_f64, result_f32, rtol=1e-6)
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"    def test_numerical_stability_near_singular(self):
        """"""Test numerical stability with nearly linearly dependent variables.""""""
        np.random.seed(456)
        n_obs = 100
        a1 = np.random.randn(n_obs)
        # Create a2 that's almost identical to a1 but with detectable noise
        a2 = a1 + 1e-8 * np.random.randn(
            n_obs
        )  # Slightly larger noise for detectability

        data = np.array([a1, a2])

        corr_result = move_exp_nancorrmatrix(data, alpha=0.3)
        cov_result = move_exp_nancovmatrix(data, alpha=0.3)

        # Should not produce NaN or inf values
        assert np.all(np.isfinite(corr_result[-1]))
        assert np.all(np.isfinite(cov_result[-1]))

        # Correlation should be very close to 1
        final_corr = corr_result[-1]
        assert_allclose(final_corr[0, 1], 1.0, rtol=1e-4)

        # With the added noise, correlation should be slightly less than 1.0
        # But we'll be more lenient since exponential weighting might make it exactly 1.0
        assert final_corr[0, 1] <= 1.0
",numbagg/test/test_move_exp_matrix_advanced.py,TestMoveExpMatrixAdvanced
survived,"    def test_covariance_consistency(self, alpha):
        """"""Test that move_exp_nancovmatrix matches move_exp_nancov for pairs.""""""
        np.random.seed(42)

        # Create two time series
        n_obs = 50
        a1 = np.random.randn(n_obs)
        a2 = np.random.randn(n_obs) * 2 + 1

        # Compute using non-matrix function
        cov_nonmatrix = move_exp_nancov(a1, a2, alpha=alpha)

        # Compute using matrix function
        data_matrix = np.array([a1, a2])
        cov_matrix_result = move_exp_nancovmatrix(data_matrix, alpha=alpha)

        # Extract the off-diagonal element (covariance between a1 and a2)
        cov_from_matrix = cov_matrix_result[:, 0, 1]

        # They should match
        assert_allclose(cov_nonmatrix, cov_from_matrix, rtol=1e-10)

        # Also check symmetry - (0,1) should equal (1,0)
        assert_allclose(
            cov_matrix_result[:, 0, 1], cov_matrix_result[:, 1, 0], rtol=1e-10
        )
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency
survived,"    def test_min_weight_consistency(self):
        """"""Test consistency with different min_weight values.""""""
        np.random.seed(111)

        # Create two time series
        n_obs = 25
        a1 = np.random.randn(n_obs)
        a2 = np.random.randn(n_obs) * 1.3

        alpha = 0.2  # Low alpha to test min_weight effects
        min_weight = 0.5

        # Compute using non-matrix functions
        cov_nonmatrix = move_exp_nancov(a1, a2, alpha=alpha, min_weight=min_weight)
        corr_nonmatrix = move_exp_nancorr(a1, a2, alpha=alpha, min_weight=min_weight)

        # Compute using matrix functions
        data_matrix = np.array([a1, a2])
        cov_matrix_result = move_exp_nancovmatrix(
            data_matrix, alpha=alpha, min_weight=min_weight
        )
        corr_matrix_result = move_exp_nancorrmatrix(
            data_matrix, alpha=alpha, min_weight=min_weight
        )

        # Extract off-diagonal elements
        cov_from_matrix = cov_matrix_result[:, 0, 1]
        corr_from_matrix = corr_matrix_result[:, 0, 1]

        # They should match
        assert_allclose(cov_nonmatrix, cov_from_matrix, rtol=1e-10)
        assert_allclose(corr_nonmatrix, corr_from_matrix, rtol=1e-10)
",numbagg/test/test_move_exp_matrix_consistency.py,TestMoveExpMatrixConsistency
survived,"    def test_different_alphas(self, func):
        """"""Test behavior with different alpha values.""""""
        data = np.array([[1, 2, 3, 4, 5], [1, 4, 9, 16, 25]], dtype=np.float64)

        # High alpha (fast decay) vs low alpha (slow decay)
        result_high = func(data, alpha=0.9)
        result_low = func(data, alpha=0.1)

        # Both should have same shape
        assert result_high.shape == result_low.shape == (5, 2, 2)

        # Results should be different
        assert not np.allclose(result_high[-1], result_low[-1], rtol=1e-3)
",numbagg/test/test_move_exp_matrix.py,TestMoveExpMatrixFunctions
survived,"    def __init__(
        self,
        func: Callable,
        signature: tuple[list[tuple], str],
        **kwargs,
    ):
        self.signature = signature
        super().__init__(func, **kwargs)
",numbagg/decorators.py,ndmoveexpmatrix
survived,"    def test_scan_content_with_context(self, runner, temp_model_dir):
        """"""Test content search with context lines""""""
        result = runner.invoke(
            scan_app,
            [""content"", ""Hello"", ""--path"", str(temp_model_dir), ""--context"", ""5"", ""--format"", ""json""]
        )
        
        assert result.exit_code == 0
        output = json.loads(result.stdout)
        assert 'matches' in output
        if output['matches']:
            assert 'context' in output['matches'][0]
",tests/test_scan/test_cli.py,TestScanCLI
survived,"    def test_get_action_prompt(self):
        """"""Test action prompt retrieval""""""
        # Known action
        prompt = self.generator._get_action_prompt('add_type_hints')
        assert 'type hints' in prompt
        
        # Unknown action
        prompt = self.generator._get_action_prompt('unknown_action')
        assert prompt == 'Perform unknown_action on this file following best practices'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _compare_metadata(self, model_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """"""Compare model metadata""""""
        metadata = {}
        
        for model, data in model_data.items():
            model_meta = {
                'has_config': data.get('config') is not None,
                'file_count': len(data.get('files', [])),
                'config_keys': list(data['config'].keys()) if data.get('config') else []
            }
            
            # Extract specific metadata if available
            if data.get('config'):
                config = data['config']
                important_keys = [
                    'model_type', 'architecture', 'license', 
                    'training_data', 'created_by', 'version'
                ]
                
                for key in important_keys:
                    if key in config:
                        model_meta[key] = config[key]
            
            metadata[model] = model_meta
        
        return metadata
",src/haconiwa/scan/comparator.py,ModelComparator
survived,"    def generate_project_wide(self,
                            action: str,
                            file_pattern: str = ""*.py"",
                            exclude_patterns: Optional[List[str]] = None) -> Dict[str, Any]:
        """"""Generate YAML for project-wide changes""""""
        
        from .scanner import ModelScanner
        
        scanner = ModelScanner(self.base_path)
        files = []
        
        # Find all matching files
        for file_path in scanner._iter_files([file_pattern]):
            if exclude_patterns:
                skip = False
                for pattern in exclude_patterns:
                    if pattern in str(file_path):
                        skip = True
                        break
                if skip:
                    continue
            
            files.append(str(file_path.relative_to(self.base_path)))
        
        # Generate tasks
        tasks = []
        for file_path in files[:50]:  # Limit to 50 files for safety
            prompt = self._get_action_prompt(action)
            tasks.append({
                'file': file_path,
                'prompt': prompt
            })
        
        config = {
            'provider': 'claude',
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'source': 'haconiwa scan generate-parallel-config',
                'action': action,
                'file_pattern': file_pattern,
                'total_tasks': len(tasks)
            },
            'tasks': tasks,
            'options': {
                'max_concurrent': 5,
                'timeout': 120,
                'allowed_tools': ['Read', 'Write', 'Edit', 'MultiEdit'],
                'permission_mode': 'confirmEach',
                'output_dir': f'./project-wide-{action}'
            }
        }
        
        return config
",src/haconiwa/scan/generate_parallel.py,ParallelYAMLGenerator
survived,"    def _is_model_directory(self, path: Path) -> bool:
        """"""Check if a directory likely contains model-related files""""""
        model_indicators = [
            'model', 'models', 'checkpoint', 'weights',
            'config.json', 'model.json', 'tokenizer',
            '.pt', '.pth', '.onnx', '.pb', '.h5'
        ]
        
        path_str = str(path).lower()
        return any(indicator in path_str for indicator in model_indicators)
",src/haconiwa/scan/scanner.py,ModelScanner
survived,"    def _compare_formats(self, model_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """"""Compare available model formats""""""
        formats = {}
        
        format_extensions = {
            '.pt': 'PyTorch',
            '.pth': 'PyTorch',
            '.onnx': 'ONNX',
            '.pb': 'TensorFlow',
            '.h5': 'Keras',
            '.tflite': 'TensorFlow Lite',
            '.safetensors': 'SafeTensors',
            '.gguf': 'GGUF',
            '.bin': 'Binary'
        }
        
        for model, data in model_data.items():
            model_formats = set()
            
            for file_info in data.get('files', []):
                file_path = Path(file_info['path'])
                if file_path.suffix in format_extensions:
                    model_formats.add(format_extensions[file_path.suffix])
            
            formats[model] = list(model_formats)
        
        return formats
",src/haconiwa/scan/comparator.py,ModelComparator
survived,"    def test_generate_prompt_for_file_fallback(self):
        """"""Test prompt generation fallback for unknown categories""""""
        prompt = self.generator._generate_prompt_for_file(
            'src/unknown/file.py',
            'add_tests',
            None
        )
        
        assert prompt == 'Create unit tests with pytest covering edge cases'
",tests/test_scan/test_generate_parallel.py,TestParallelYAMLGenerator
survived,"    def _should_ignore(self, path: Path) -> bool:
        """"""Check if path should be ignored""""""
        path_str = str(path)
        
        # Check whitelist first
        if self.whitelist:
            whitelisted = any(
                fnmatch.fnmatch(path_str, pattern) or 
                pattern in path_str 
                for pattern in self.whitelist
            )
            if not whitelisted:
                return True
        
        # Check ignore patterns
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(path.name, pattern):
                return True
            if pattern in path_str:
                return True
        
        return False
",src/haconiwa/scan/scanner.py,ModelScanner
survived,"    def test_different_alphas(self, func):
        """"""Test behavior with different alpha values.""""""
        # Exponential moving functions expect (obs, vars) format
        data = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25]], dtype=np.float64)

        # High alpha (fast decay) vs low alpha (slow decay)
        result_high = func(data, alpha=0.9)
        result_low = func(data, alpha=0.1)

        # Both should have same shape
        assert result_high.shape == result_low.shape == (5, 2, 2)

        # Results should be different
        assert not np.allclose(result_high[-1], result_low[-1], rtol=1e-3)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    def test_fixed_dimensional_conventions(self):
        """"""Test fixed dimensional conventions: (..., obs, vars) -> (..., obs, vars, vars).""""""
        np.random.seed(42)

        # Basic test: (obs, vars) -> (obs, vars, vars)
        data_moving = np.random.randn(100, 3)  # (obs, vars)
        corr_moving = move_nancorrmatrix(data_moving, window=10)
        cov_moving = move_nancovmatrix(data_moving, window=10)

        assert corr_moving.shape == (100, 3, 3)
        assert cov_moving.shape == (100, 3, 3)

        # Broadcasting test: (batch, obs, vars) -> (batch, obs, vars, vars)
        data_moving_3d = np.random.randn(2, 100, 3)  # (batch, obs, vars)
        corr_moving_3d = move_nancorrmatrix(data_moving_3d, window=10)

        assert corr_moving_3d.shape == (2, 100, 3, 3)
",numbagg/test/test_matrix_functions.py,TestMovingMatrices
survived,"    def test_broadcasting_higher_dims(self, func):
        """"""Test that exponential matrix functions broadcast correctly for higher dimensional arrays.""""""
        np.random.seed(42)

        # 3D array: (2, 20, 4) -> broadcast dims (2,) + core dims (20, 4) = (obs, vars)
        data_3d = np.random.randn(2, 20, 4)
        result_3d = func(data_3d, alpha=0.3)
        assert result_3d.shape == (2, 20, 4, 4)

        # 4D array: (2, 3, 15, 4) -> broadcast dims (2, 3) + core dims (15, 4) = (obs, vars)
        data_4d = np.random.randn(2, 3, 15, 4)
        result_4d = func(data_4d, alpha=0.3)
        assert result_4d.shape == (2, 3, 15, 4, 4)

        # Verify correctness - each slice should match individual computation
        for i in range(2):
            single_result = func(data_3d[i], alpha=0.3)
            assert_allclose(result_3d[i], single_result, rtol=1e-10, equal_nan=True)
",numbagg/test/test_matrix_functions.py,TestExponentialMatrices
survived,"    async def test_bash_tool_windows_execution(
        self, mock_subprocess, mock_which, mock_platform
    ):
        """"""Test Windows command execution.""""""
        # Mock process
        mock_process = AsyncMock()
        mock_process.pid = 1234
        mock_process.returncode = 0
        mock_process.communicate.return_value = (b""Hello Windows"", b"""")
        mock_subprocess.return_value = mock_process

        bash_tool = BashTool()
        result = await bash_tool.execute(command=""echo Hello Windows"")

        assert result.success
        assert ""Hello Windows"" in result.output
        mock_subprocess.assert_called_once()
",tests/unit/test_windows_compatibility.py,TestWindowsCompatibility
survived,"    def test_git_detection(self, mock_which):
        """"""Test Git executable detection.""""""
        # Test when Git is available
        mock_which.return_value = ""/usr/bin/git""
        assert shutil.which(""git"") is not None

        # Test when Git is not available
        mock_which.return_value = None
        assert shutil.which(""git"") is None
",tests/unit/test_windows_compatibility.py,TestCrossPlatformDetection
survived,"    def __del__(self) -> None:
        """"""Destructor to ensure database connections are closed.

        Automatically called when the ContextManager object is garbage collected.
        This provides a safety net to ensure SQLite connections are closed even
        if explicit cleanup is not performed.
        """"""
        try:
            self.close_all_connections()
        except Exception:  # nosec B110
            # Ignore errors during destructor
            pass
",ocode_python/core/context_manager.py,ContextManager
survived,"    def test_project_option(self):
        """"""Test --project option for all install commands.""""""
        commands_to_test = [
            [""claude-code"", ""server.py"", ""--project"", ""/path/to/project""],
            [""claude-desktop"", ""server.py"", ""--project"", ""/path/to/project""],
            [""cursor"", ""server.py"", ""--project"", ""/path/to/project""],
            [""mcp-json"", ""server.py"", ""--project"", ""/path/to/project""],
        ]

        for cmd_args in commands_to_test:
            command, bound, _ = install_app.parse_args(cmd_args)
            assert command is not None
            assert str(bound.arguments[""project""]) == ""/path/to/project""",tests/cli/test_install.py,TestInstallCommandParsing
survived,"async def summarise_conversations(
    conversations: List[Conversation],
    *,
    model: BaseSummaryModel,
    checkpoint_manager: Optional[CheckpointManager] = None
) -> List[ConversationSummary]:
    """"""Generate summaries for a list of conversations.
    
    This is a pure function that takes conversations and a summary model,
    and returns conversation summaries. Optionally uses checkpointing.
    
    The function works with any model that implements BaseSummaryModel,
    supporting heterogeneous backends (OpenAI, vLLM, Hugging Face, etc.)
    through polymorphism.
    
    Args:
        conversations: List of conversations to summarize
        model: Model to use for summarization (OpenAI, vLLM, local, etc.)
        checkpoint_manager: Optional checkpoint manager for caching
        
    Returns:
        List of conversation summaries
        
    Example:
        >>> openai_model = OpenAISummaryModel(api_key=""sk-..."")
        >>> checkpoint_mgr = CheckpointManager(""./checkpoints"")
        >>> summaries = await summarise_conversations(
        ...     conversations=my_conversations,
        ...     model=openai_model,
        ...     checkpoint_manager=checkpoint_mgr
        ... )
    """"""
    logger.info(f""Starting summarization of {len(conversations)} conversations using {type(model).__name__}"")
    
    # Try to load from checkpoint
    if checkpoint_manager:
        cached = checkpoint_manager.load_checkpoint(
            model.checkpoint_filename, 
            ConversationSummary
        )
        if cached:
            logger.info(f""Loaded {len(cached)} summaries from checkpoint"")
            return cached
    
    # Generate summaries
    logger.info(""Generating new summaries..."")
    summaries = await model.summarise(conversations)
    logger.info(f""Generated {len(summaries)} summaries"")
    
    # Save to checkpoint
    if checkpoint_manager:
        logger.info(f""Saving summaries to checkpoint: {model.checkpoint_filename}"")
        checkpoint_manager.save_checkpoint(model.checkpoint_filename, summaries)
    
    return summaries
",kura/v1/kura.py,
survived,"    def load_checkpoint(self, filename: str, model_class: type[T]) -> Optional[List[T]]:
        """"""Load data from a checkpoint file if it exists.
        
        Args:
            filename: Name of the checkpoint file
            model_class: Pydantic model class for deserializing the data
            
        Returns:
            List of model instances if checkpoint exists, None otherwise
        """"""
        if not self.enabled:
            return None
            
        checkpoint_path = self.get_checkpoint_path(filename)
        if os.path.exists(checkpoint_path):
            logger.info(f""Loading checkpoint from {checkpoint_path} for {model_class.__name__}"")
            with open(checkpoint_path, ""r"") as f:
                return [model_class.model_validate_json(line) for line in f]
        return None
",kura/v1/kura.py,CheckpointManager
survived,"def _build_cluster_tree(clusters: List[Cluster]) -> dict[str, ClusterTreeNode]:
    """"""Build a tree structure from a list of clusters.
    
    Args:
        clusters: List of clusters to build tree from
        
    Returns:
        Dictionary mapping cluster IDs to tree nodes
    """"""
    node_id_to_cluster = {}

    # Create tree nodes
    for cluster in clusters:
        node_id_to_cluster[cluster.id] = ClusterTreeNode(
            id=cluster.id,
            name=cluster.name,
            description=cluster.description,
            count=len(cluster.chat_ids),
            children=[],
        )

    # Link parent-child relationships
    for cluster in clusters:
        if cluster.parent_id:
            node_id_to_cluster[cluster.parent_id].children.append(cluster.id)

    return node_id_to_cluster
",kura/v1/visualization.py,
survived,"def visualise_pipeline_results(
    clusters: List[Cluster],
    *,
    style: str = ""enhanced"",
    console: Optional[Console] = None
) -> None:
    """"""Visualize clusters that are the result of a pipeline execution.
    
    Convenience function for visualizing clusters directly from pipeline results.
    
    Args:
        clusters: List of clusters from pipeline execution
        style: Visualization style (""basic"", ""enhanced"", or ""rich"")
        console: Rich Console instance (for rich style)
        
    Raises:
        ValueError: If invalid style is provided
    """"""
    if style == ""basic"":
        visualise_clusters(clusters)
    elif style == ""enhanced"":
        visualise_clusters_enhanced(clusters)
    elif style == ""rich"":
        visualise_clusters_rich(clusters, console=console)
    else:
        raise ValueError(f""Invalid style '{style}'. Must be one of: basic, enhanced, rich"") ",kura/v1/visualization.py,
survived,"    def test_update_with_sample_weight(self):
        """"""Test update method with sample weights.""""""
        arms = make_arms(range(3))
        agent = ContextualAgent(arms, ThompsonSampling(), random_seed=42)
        steps = [(""identity"", FunctionTransformer())]

        pipeline = ContextualAgentPipeline(steps, agent)

        X = np.array([[1.0], [2.0]])
        y = np.array([1.0, 2.0])
        sample_weight = np.array([1.0, 0.1])

        # Pull to set arm_to_update
        pipeline.pull(X)

        # Should not raise
        pipeline.update(X, y, sample_weight=sample_weight)
",tests/test_agent_pipeline.py,TestContextualAgentPipeline
survived,"    def test_stateless_transformers(self):
        """"""Test stateless transformers work correctly.""""""

        def double_transform(X):
            return X * 2

        mock_learner = MockLearner()
        pipeline = LearnerPipeline(steps=[(""double"", FunctionTransformer(double_transform))], learner=mock_learner)

        X = np.array([[1, 2], [3, 4]])
        y = np.array([1, 2])

        # Should transform without any fitting
        pipeline.partial_fit(X, y)

        # Check that data was doubled before reaching learner
        received_X, received_y, _ = mock_learner.partial_fit_calls[0]
        np.testing.assert_array_equal(received_X, X * 2)
        np.testing.assert_array_equal(received_y, y)
",tests/test_learner_pipeline.py,TestLearnerPipelineTransformers
survived,"    def _apply_transformers(self, X: X_contra) -> Any:
        """"""Apply transformers to input data.

        Transformers must be either stateless or already fitted.
        """"""
        if not self.steps:
            return X

        # Apply each transformer in sequence
        result = cast(Any, X)  # Cast to Any for sklearn calls
        for name, transformer in self.steps:
            try:
                result = transformer.transform(result)
            except Exception as e:
                # Provide helpful error for common case
                if hasattr(e, ""args"") and ""not fitted"" in str(e).lower():
                    raise RuntimeError(
                        f""Transformer '{name}' is not fitted. In online learning, ""
                        f""all transformers must be either stateless or pre-fitted ""
                        f""before use. Common stateless transformers include ""
                        f""FunctionTransformer. Stateful transformers like ""
                        f""StandardScaler must be fit on historical data before ""
                        f""creating the pipeline.""
                    ) from e
                raise
        return result
",bayesianbandits/pipelines/_learner.py,LearnerPipeline
survived,"    def learner(self) -> Learner[Any]:
        """"""Access the final learner.""""""
        return self._learner
",bayesianbandits/pipelines/_learner.py,LearnerPipeline
survived,"    def __repr__(self) -> str:
        """"""String representation.""""""
        steps_repr = [
            f""('{name}', {transformer.__class__.__name__})""
            for name, transformer in self.steps
        ]
        return f""ContextualAgentPipeline(steps=[{', '.join(steps_repr)}], final_agent={self._agent!r})""
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def arms(self):
        """"""Get the arms from the wrapped agent.""""""
        return self._agent.arms
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def test_transform_multiple_steps(self):
        """"""Test transformation with multiple steps.""""""
        steps = [
            (""double"", FunctionTransformer(lambda x: x * 2)),
            (""add_one"", FunctionTransformer(lambda x: x + 1)),
        ]
        X = np.array([[1], [2]])
        result = _transform_data(X, steps)
        expected = np.array([[3], [5]])  # (x * 2) + 1
        np.testing.assert_array_equal(result, expected)
",tests/test_agent_pipeline.py,TestTransformData
survived,"        def failing_transform(X):
            raise ValueError(""Custom transformation error"")
",tests/test_agent_pipeline.py,TestErrorHandling
survived,"    def test_empty_steps_allowed(self):
        """"""Test empty steps are allowed for non-contextual.""""""
        arms = make_arms(range(3))
        agent = Agent(arms, ThompsonSampling())

        # Should not raise
        pipeline = NonContextualAgentPipeline([], agent)
        assert len(pipeline) == 0
",tests/test_agent_pipeline.py,TestNonContextualAgentPipeline
survived,"    def __init__(self):
        self.partial_fit_calls = []
        self.sample_calls = []
        self.predict_calls = []
        self.decay_calls = []
        self.random_state = None
",tests/test_learner_pipeline.py,MockLearner
survived,"    def arm_to_update(self):
        """"""Get the arm to update from the wrapped agent.""""""
        return self._agent.arm_to_update
",bayesianbandits/pipelines/_agent.py,ContextualAgentPipeline
survived,"    def test_repr_method(self):
        """"""Test __repr__ method.""""""
        pipeline = LearnerPipeline(steps=[(""scale"", StandardScaler())], learner=MockLearner())

        repr_str = repr(pipeline)
        assert ""LearnerPipeline"" in repr_str
        assert ""StandardScaler"" in repr_str
        assert ""MockLearner"" in repr_str",tests/test_learner_pipeline.py,TestLearnerPipelineProperties
survived,"def handle_reask_kwargs(
    kwargs: dict[str, Any],
    mode: Mode,
    response: Any,
    exception: Exception,
) -> dict[str, Any]:
    """"""Handle validation errors by reformatting the request for retry (reask).

    When a response fails validation (e.g., missing required fields, wrong types),
    this function prepares a new request that includes information about the error.
    This allows the LLM to understand what went wrong and correct its response.

    The reask logic is provider-specific because each provider has different ways
    of handling function/tool calls and different message formats.

    Args:
        kwargs (dict[str, Any]): The original request parameters that resulted in
            a validation error. Includes messages, tools, temperature, etc.
        mode (Mode): The provider/format mode that determines which reask handler
            to use. Each mode has a specific strategy for formatting error feedback.
        response (Any): The raw response from the LLM that failed validation.
            Type varies by provider:
            - OpenAI: ChatCompletion with tool_calls
            - Anthropic: Message with tool_use blocks
            - Google: GenerateContentResponse with function calls
        exception (Exception): The validation error that occurred. Usually a
            Pydantic ValidationError with details about which fields failed.

    Returns:
        dict[str, Any]: Modified kwargs for the retry request, typically including:
            - Updated messages with error context
            - Same tool/function definitions
            - Preserved generation parameters
            - Provider-specific formatting

    Reask Strategies by Provider:
        Each provider has a specific strategy for handling retries:

        **JSON Modes:**
        - Adds assistant message with failed attempt
        - Adds user message with error details

        **Tool Calls:**
        - Preserves tool definitions
        - Formats the errors as tool calls responses

    Note:
        This function is typically called internally by the retry logic when
        max_retries > 1. It ensures that each retry attempt includes context
        about previous failures, helping the LLM learn from its mistakes.
    """"""
    # Create a shallow copy of kwargs to avoid modifying the original
    kwargs_copy = kwargs.copy()

    # Organized by provider (matching process_response.py structure)
    REASK_HANDLERS = {
        # OpenAI modes
        Mode.FUNCTIONS: reask_default,
        Mode.TOOLS_STRICT: reask_tools,
        Mode.TOOLS: reask_tools,
        Mode.JSON_O1: reask_default,
        Mode.JSON: reask_md_json,
        Mode.MD_JSON: reask_md_json,
        Mode.JSON_SCHEMA: reask_md_json,
        Mode.PARALLEL_TOOLS: reask_tools,
        Mode.RESPONSES_TOOLS: reask_responses_tools,
        Mode.RESPONSES_TOOLS_WITH_INBUILT_TOOLS: reask_responses_tools,
        # Mistral modes
        Mode.MISTRAL_TOOLS: reask_mistral_tools,
        Mode.MISTRAL_STRUCTURED_OUTPUTS: reask_mistral_structured_outputs,
        # Anthropic modes
        Mode.ANTHROPIC_TOOLS: reask_anthropic_tools,
        Mode.ANTHROPIC_REASONING_TOOLS: reask_anthropic_tools,
        Mode.ANTHROPIC_JSON: reask_anthropic_json,
        Mode.ANTHROPIC_PARALLEL_TOOLS: reask_anthropic_tools,
        # Cohere modes
        Mode.COHERE_TOOLS: reask_cohere_tools,
        Mode.COHERE_JSON_SCHEMA: reask_cohere_tools,
        # Gemini/Google modes
        Mode.GEMINI_TOOLS: reask_gemini_tools,
        Mode.GEMINI_JSON: reask_gemini_json,
        Mode.GENAI_TOOLS: reask_genai_tools,
        Mode.GENAI_STRUCTURED_OUTPUTS: reask_genai_structured_outputs,
        # VertexAI modes
        Mode.VERTEXAI_TOOLS: reask_vertexai_tools,
        Mode.VERTEXAI_JSON: reask_vertexai_json,
        Mode.VERTEXAI_PARALLEL_TOOLS: reask_vertexai_tools,
        # Cerebras modes
        Mode.CEREBRAS_TOOLS: reask_cerebras_tools,
        Mode.CEREBRAS_JSON: reask_default,
        # Fireworks modes
        Mode.FIREWORKS_TOOLS: reask_fireworks_tools,
        Mode.FIREWORKS_JSON: reask_fireworks_json,
        # Writer modes
        Mode.WRITER_TOOLS: reask_writer_tools,
        Mode.WRITER_JSON: reask_writer_json,
        # Bedrock modes
        Mode.BEDROCK_TOOLS: reask_bedrock_tools,
        Mode.BEDROCK_JSON: reask_bedrock_json,
        # Perplexity modes
        Mode.PERPLEXITY_JSON: reask_perplexity_json,
        # OpenRouter modes
        Mode.OPENROUTER_STRUCTURED_OUTPUTS: reask_default,
        # XAI modes
        Mode.XAI_JSON: reask_xai_json,
        Mode.XAI_TOOLS: reask_xai_tools,
    }

    if mode in REASK_HANDLERS:
        return REASK_HANDLERS[mode](kwargs_copy, response, exception)
    else:
        return reask_default(kwargs_copy, response, exception)",instructor/process_response.py,
survived,"    def _identify_new_files(self, feature_request: str, analysis: Dict[str, Any]) -> List[str]:
        """"""Identify new files that need to be created.""""""
        return [""new_feature.py""]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _identify_dependencies(self, feature_request: str, analysis: Dict[str, Any]) -> List[str]:
        """"""Identify new dependencies required.""""""
        return []
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
deleted,"    def _analyze_naming_conventions(self, project_path: str, file_patterns: List[str]) -> Dict[str, Any]:
        """"""Analyze naming conventions used in the project.""""""
        conventions = {""style"": ""snake_case"", ""patterns"": [], ""exceptions"": []}
        # Implementation would analyze actual naming patterns
        return conventions
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def test_agent_inheritance():
    """"""Test that ContextAgent properly inherits from Agent.""""""
    print(""\nðŸ§ª Testing Agent Inheritance..."")
    
    try:
        from praisonaiagents import ContextAgent, Agent
        
        context_agent = ContextAgent()
        
        # Test inheritance
        assert isinstance(context_agent, Agent), ""ContextAgent should inherit from Agent""
        print(""âœ… ContextAgent properly inherits from Agent class"")
        
        # Test that base Agent properties exist
        assert hasattr(context_agent, 'name'), ""Should have name attribute""
        assert hasattr(context_agent, 'role'), ""Should have role attribute""
        assert hasattr(context_agent, 'goal'), ""Should have goal attribute""
        print(""âœ… ContextAgent has all required Agent attributes"")
        
        return True
        
    except Exception as e:
        print(f""âŒ Inheritance test failed: {e}"")
        return False
",test_context_agent.py,
survived,"    def _identify_integration_points(self, feature_request: str, analysis: Dict[str, Any]) -> List[str]:
        """"""Identify integration points with existing code.""""""
        return [""main_api"", ""data_layer""]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"def test_syntax_validation():
    """"""Test that all Python files have valid syntax.""""""
    print(""\nðŸ§ª Testing Syntax Validation..."")
    
    try:
        import ast
        
        # Test the main ContextAgent file
        context_agent_file = project_root / ""praisonaiagents"" / ""agent"" / ""context_agent.py""
        
        with open(context_agent_file, 'r') as f:
            content = f.read()
        
        # Parse the file to check for syntax errors
        ast.parse(content)
        print(""âœ… context_agent.py has valid Python syntax"")
        
        # Test the examples
        example_files = [
            Path(__file__).parent / ""examples"" / ""python"" / ""agents"" / ""context-agent.py"",
            Path(__file__).parent / ""examples"" / ""python"" / ""concepts"" / ""context-engineering-workflow.py""
        ]
        
        for example_file in example_files:
            if example_file.exists():
                with open(example_file, 'r') as f:
                    content = f.read()
                ast.parse(content)
                print(f""âœ… {example_file.name} has valid Python syntax"")
        
        return True
        
    except SyntaxError as e:
        print(f""âŒ Syntax error found: {e}"")
        return False
    except Exception as e:
        print(f""âŒ Syntax validation failed: {e}"")
        return False
",test_context_agent.py,
survived,"    def generate_prp(self, feature_request: str, context_analysis: Dict[str, Any], documentation_links: List[str] = None) -> str:
        """"""
        Generate a Product Requirements Prompt (PRP) with comprehensive context.
        
        Args:
            feature_request (str): The feature to be implemented
            context_analysis (Dict[str, Any]): Analysis of the codebase context
            documentation_links (List[str]): Optional links to relevant documentation
            
        Returns:
            str: Complete PRP with rich context for implementation
        """"""
        if documentation_links is None:
            documentation_links = []
        
        prp = f""""""# Product Requirements Prompt (PRP)
## Context Engineering Enhanced Implementation Guide

### Feature Request
{feature_request}

### Comprehensive Context

#### Codebase Analysis
{self._format_prp_codebase_analysis(context_analysis)}

#### Implementation Blueprint
{self._generate_prp_implementation_blueprint(feature_request, context_analysis)}

#### Validation Framework
{self._generate_prp_validation_framework(feature_request)}

#### Documentation References
{self._format_prp_documentation(documentation_links)}

#### Success Criteria
{self._generate_prp_success_criteria(feature_request, context_analysis)}

### Implementation Instructions

This PRP provides comprehensive context for implementing: {feature_request}

**Context Engineering Principle**: This document contains all necessary context to enable 
first-try implementation success. Follow the patterns, respect the architecture, and 
use the validation framework to ensure quality.

#### Next Steps
1. Review the complete context above
2. Follow the implementation blueprint
3. Adhere to identified patterns and conventions
4. Execute validation framework to verify success
5. Integrate seamlessly with existing architecture

**Confidence Score**: 9/10 (High confidence due to comprehensive context analysis)
""""""
        
        return prp
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    def _get_default_context_tools(self) -> List[Any]:
        """"""Get default tools for Context Engineering operations.""""""
        return [
            self.analyze_codebase_patterns,
            self.generate_context_document,
            self.create_validation_loop,
            self.enhance_prompt_with_context,
            self.generate_prp,
            self.extract_documentation_patterns,
            self.analyze_test_patterns,
            self.create_implementation_blueprint
        ]
",src/praisonai-agents/praisonaiagents/agent/context_agent.py,ContextAgent
survived,"    async def test_ai_text_summarizer_multiple_chunks(self):
        """"""Test that AITextSummarizerBlock correctly accumulates stats across multiple chunks.""""""
        import backend.blocks.llm as llm

        block = llm.AITextSummarizerBlock()

        # Track calls to simulate multiple chunks
        call_count = 0

        async def mock_llm_call(input_data, credentials):
            nonlocal call_count
            call_count += 1

            # Create a mock block with stats to merge from
            mock_structured_block = llm.AIStructuredResponseGeneratorBlock()
            mock_structured_block.execution_stats = NodeExecutionStats(
                input_token_count=25,
                output_token_count=15,
                llm_call_count=1,
            )

            # Simulate merge_llm_stats behavior
            block.merge_llm_stats(mock_structured_block)

            if ""final_summary"" in input_data.expected_format:
                return {""final_summary"": ""Final combined summary""}
            else:
                return {""summary"": f""Summary of chunk {call_count}""}

        block.llm_call = mock_llm_call  # type: ignore

        # Create long text that will be split into chunks
        long_text = "" "".join([""word""] * 1000)  # Moderate size to force ~2-3 chunks

        input_data = llm.AITextSummarizerBlock.Input(
            text=long_text,
            model=llm.LlmModel.GPT4O,
            credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore
            max_tokens=100,  # Small chunks
            chunk_overlap=10,
        )

        # Run the block
        outputs = {}
        async for output_name, output_data in block.run(
            input_data, credentials=llm.TEST_CREDENTIALS
        ):
            outputs[output_name] = output_data

        # Block finished - now grab and assert stats
        assert block.execution_stats is not None
        assert call_count > 1  # Should have made multiple calls
        assert block.execution_stats.llm_call_count > 0
        assert block.execution_stats.input_token_count > 0
        assert block.execution_stats.output_token_count > 0

        # Check output
        assert ""summary"" in outputs
        assert outputs[""summary""] == ""Final combined summary""
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking
survived,"    async def test_stats_accumulation_with_retries(self):
        """"""Test that stats correctly accumulate across retries.""""""
        import backend.blocks.llm as llm

        block = llm.AIStructuredResponseGeneratorBlock()

        # Counter to track calls
        call_count = 0

        async def mock_llm_call(*args, **kwargs):
            nonlocal call_count
            call_count += 1

            # First call returns invalid format
            if call_count == 1:
                return llm.LLMResponse(
                    raw_response="""",
                    prompt=[],
                    response='{""wrong"": ""format""}',
                    tool_calls=None,
                    prompt_tokens=10,
                    completion_tokens=15,
                    reasoning=None,
                )
            # Second call returns correct format
            else:
                return llm.LLMResponse(
                    raw_response="""",
                    prompt=[],
                    response='{""key1"": ""value1"", ""key2"": ""value2""}',
                    tool_calls=None,
                    prompt_tokens=20,
                    completion_tokens=25,
                    reasoning=None,
                )

        block.llm_call = mock_llm_call  # type: ignore

        # Run the block with retry
        input_data = llm.AIStructuredResponseGeneratorBlock.Input(
            prompt=""Test prompt"",
            expected_format={""key1"": ""desc1"", ""key2"": ""desc2""},
            model=llm.LlmModel.GPT4O,
            credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore
            retry=2,
        )

        outputs = {}
        async for output_name, output_data in block.run(
            input_data, credentials=llm.TEST_CREDENTIALS
        ):
            outputs[output_name] = output_data

        # Check stats - should accumulate both calls
        # For 2 attempts: attempt 1 (failed) + attempt 2 (success) = 2 total
        # but llm_call_count is only set on success, so it shows 1 for the final successful attempt
        assert block.execution_stats.input_token_count == 30  # 10 + 20
        assert block.execution_stats.output_token_count == 40  # 15 + 25
        assert block.execution_stats.llm_call_count == 2  # retry_count + 1 = 1 + 1 = 2
        assert block.execution_stats.llm_retry_count == 1
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking
survived,"    async def test_ai_structured_response_block_tracks_stats(self):
        """"""Test that AIStructuredResponseGeneratorBlock correctly tracks stats.""""""
        import backend.blocks.llm as llm

        block = llm.AIStructuredResponseGeneratorBlock()

        # Mock the llm_call method
        async def mock_llm_call(*args, **kwargs):
            return llm.LLMResponse(
                raw_response="""",
                prompt=[],
                response='{""key1"": ""value1"", ""key2"": ""value2""}',
                tool_calls=None,
                prompt_tokens=15,
                completion_tokens=25,
                reasoning=None,
            )

        block.llm_call = mock_llm_call  # type: ignore

        # Run the block
        input_data = llm.AIStructuredResponseGeneratorBlock.Input(
            prompt=""Test prompt"",
            expected_format={""key1"": ""desc1"", ""key2"": ""desc2""},
            model=llm.LlmModel.GPT4O,
            credentials=llm.TEST_CREDENTIALS_INPUT,  # type: ignore  # type: ignore
        )

        outputs = {}
        async for output_name, output_data in block.run(
            input_data, credentials=llm.TEST_CREDENTIALS
        ):
            outputs[output_name] = output_data

        # Check stats
        assert block.execution_stats.input_token_count == 15
        assert block.execution_stats.output_token_count == 25
        assert block.execution_stats.llm_call_count == 1
        assert block.execution_stats.llm_retry_count == 0

        # Check output
        assert ""response"" in outputs
        assert outputs[""response""] == {""key1"": ""value1"", ""key2"": ""value2""}
",autogpt_platform/backend/backend/blocks/test/test_llm.py,TestLLMStatsTracking
survived,"    def _message(self) -> str:
        return (
            ""Usage of `set_active_model` is not allowed in mlflow, use `_set_active_model` instead.""
        )
",dev/clint/src/clint/rules/forbidden_set_active_model_usage.py,ForbiddenSetActiveModelUsage
survived,"    def _find_artifact_path_index(index: ""SymbolIndex"", function_name: str) -> int | None:
        """"""
        Finds the index of the `artifact_path` argument in the function signature of `log_model`
        using the SymbolIndex.
        """"""
        if f := index.resolve(function_name):
            try:
                return f.all_args.index(""artifact_path"")
            except ValueError:
                return None
        return None",dev/clint/src/clint/rules/log_model_artifact_path.py,LogModelArtifactPath
survived,"    def _message(self) -> str:
        return f""Unordered parameters in docstring: {self.params}""",dev/clint/src/clint/rules/docstring_param_order.py,DocstringParamOrder
survived,"    def _message(self) -> str:
        return (
            ""Abstract method should only contain a single statement/expression, ""
            ""and it must be `pass`, `...`, or a docstring.""
        )
",dev/clint/src/clint/rules/invalid_abstract_method.py,InvalidAbstractMethod
survived,"def _test_webhook(webhook_id: str):
    request_message = _get_request_message(TestWebhook())
    event = (
        WebhookEvent.from_proto(request_message.event)
        if request_message.HasField(""event"")
        else None
    )
    store = _get_model_registry_store()
    webhook = store.get_webhook(webhook_id=webhook_id)
    test_result = test_webhook(webhook=webhook, event=event)
    response_message = TestWebhook.Response(result=test_result.to_proto())
    return _wrap_response(response_message)
",mlflow/server/handlers.py,
survived,"    def w_spy_new(vm: 'SPyVM', w_cls: W_Type,
                 w_color: W_Object, w_static_type: W_Type,
                 w_val: W_Object) -> 'W_OpArg':
        """"""
        Create a new OpArg from SPy code:
        - color: 'red' or 'blue'
        - static_type: the static type of the argument
        - val: the value (optional for red OpArg, required for blue)
        """"""
        from spy.vm.str import W_Str
        # Check that w_color is a string
        w_type = vm.dynamic_type(w_color)
        if w_type is not B.w_str:
            raise SPyTypeError(f""OpArg color must be a string, got {w_type.fqn.human_name}"")

        color: Color = vm.unwrap_str(w_color)  # type: ignore
        if color not in ('red', 'blue'):
            raise SPyTypeError(f""OpArg color must be 'red' or 'blue', got '{color}'"")

        # Convert B.w_None to Python None
        if w_val is B.w_None:
            w_val2 = None
        else:
            w_val2 = w_val

        if color == 'blue' and w_val is None:
            raise SPyTypeError(""Blue OpArg requires a value"")

        loc = Loc.here(-2)  # approximate source location
        return W_OpArg(vm, color, w_static_type, w_val2, loc)
",spy/vm/opimpl.py,W_OpArg
survived,"    def definitions(
        self,
        curies: Iterable[CURIE],
        include_metadata=False,
        include_missing=False,
        lang: Optional[LANGUAGE_TAG] = None,
    ) -> Iterator[Tuple[CURIE, Optional[str], Dict]]:
        """"""
        Fetch definitions for multiple CURIEs from OLS.
        
        :param curies: The CURIEs to fetch definitions for
        :param include_metadata: Whether to include metadata (currently not supported)
        :param include_missing: Whether to include CURIEs with no definition
        :param lang: Optional language tag (not currently supported by this implementation)
        :return: Iterator of (CURIE, definition, metadata) tuples
        """"""
        for curie in curies:
            definition = self.definition(curie, lang)
            if definition is None and not include_missing:
                continue
            # Currently OLS doesn't provide metadata for definitions through the API
            # So we're just returning an empty dict
            yield curie, definition, {}
",src/oaklib/implementations/ols/ols_implementation.py,BaseOlsImplementation
survived,"def test_api_key_parameter_not_passed_when_none():
    """"""Test that api_key parameter is handled correctly when None.""""""
    from unittest.mock import patch, MagicMock

    # Mock the openai module
    with patch(""openai.OpenAI"") as mock_openai_class:
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client

        # Mock the from_openai import
        with patch(""instructor.from_openai"") as mock_from_openai:
            mock_instructor = MagicMock()
            mock_from_openai.return_value = mock_instructor

            # Test with None api_key
            from_provider(""openai/gpt-4"", api_key=None)

            # Verify OpenAI was called with None api_key
            mock_openai_class.assert_called_once()
            _, kwargs = mock_openai_class.call_args
            assert kwargs[""api_key""] is None
",tests/test_auto_client.py,
survived,"    def test_large_matrix(self):
        # Test performance with larger matrix
        np.random.seed(42)
        data = np.random.randn(50, 1000)

        # Add some NaNs
        mask = np.random.rand(50, 1000) < 0.1
        data[mask] = np.nan

        result = nancorrmatrix(data)

        # Check basic properties
        assert result.shape == (50, 50)
        assert_allclose(np.diag(result), np.ones(50), rtol=1e-10)
        assert_allclose(result, result.T, rtol=1e-10)
        assert np.all((result >= -1) & (result <= 1) | np.isnan(result))",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix
survived,"    def test_single_observation(self):
        # Test with only one observation per variable
        data = np.array([[1], [2], [3]], dtype=np.float64)
        result = nancorrmatrix(data)

        # Should be NaN except diagonal
        expected = np.full((3, 3), np.nan)
        np.fill_diagonal(expected, 1.0)
        assert_array_equal(result, expected)
",numbagg/test/test_nancorrmatrix.py,TestNanCorrMatrix
survived,"def nancovmatrix(a, out):
    """"""
    Compute covariance matrix treating NaN as missing values.

    For 2D input, computes covariance between variables (rows) across observations (columns).
    Uses pairwise complete observations (like pandas.DataFrame.cov).
    """"""
    n_vars, n_obs = a.shape

    # Compute covariance matrix
    for i in range(n_vars):
        for j in range(i, n_vars):  # Only compute upper triangle
            # Find pairwise complete observations and compute sums in one pass
            sum_i = 0.0
            sum_j = 0.0
            count = 0

            for k in range(n_obs):
                val_i = a[i, k]
                val_j = a[j, k]
                if not np.isnan(val_i) and not np.isnan(val_j):
                    sum_i += val_i
                    sum_j += val_j
                    count += 1

            if count > 1:
                # Compute means using only pairwise complete observations
                mean_i = sum_i / count
                mean_j = sum_j / count

                # Compute covariance in second pass
                cov_sum = 0.0
                for k in range(n_obs):
                    val_i = a[i, k]
                    val_j = a[j, k]
                    if not np.isnan(val_i) and not np.isnan(val_j):
                        cov_sum += (val_i - mean_i) * (val_j - mean_j)

                # Use count - 1 for sample covariance
                out[i, j] = cov_sum / (count - 1)
                out[j, i] = out[i, j]  # Symmetric
            else:
                out[i, j] = np.nan
                out[j, i] = np.nan",numbagg/funcs.py,
survived,"    def __call__(
        self,
        a: np.ndarray,
        axis: int | tuple[int, ...] | None = None,
        **kwargs,
    ):
        if axis is None:
            axis = -1

        if isinstance(axis, tuple):
            if len(axis) != 1:
                raise ValueError(
                    f""Matrix function requires exactly one axis, got {len(axis)}""
                )
            axis = axis[0]

        # Handle 1D input by treating it as a single variable
        if a.ndim == 1:
            a = a.reshape(1, -1)

        # Move the correlation axis to the last position
        a = np.moveaxis(a, axis, -1)

        gufunc = self.gufunc(target=self.target)
        # axes specifies which axes contain the core dimensions
        # For our signature ""(n,m)->(n,n)"":
        # - Input has 2 core dims: second-to-last (n) and last (m)
        # - Output has 2 core dims: last two dimensions (n,n)
        result = gufunc(a, axes=[(-2, -1), (-2, -1)], **kwargs)

        # Return result as-is, let numba handle the output shape
        return result
",numbagg/decorators.py,ndmatrix
survived,"    async def test_list_tools_on_nested_server(
        self,
        mcp_server: FastMCP,
        nested_mcp_server: FastMCP,
        recording_middleware: RecordingMiddleware,
        nested_middleware: RecordingMiddleware,
    ):
        mcp_server.mount(nested_mcp_server, prefix=""nested"")

        async with Client(mcp_server) as client:
            await client.list_tools()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""tools/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_tools"", times=1)

        assert nested_middleware.assert_called(times=3)
        assert nested_middleware.assert_called(method=""tools/list"", times=3)
        assert nested_middleware.assert_called(hook=""on_message"", times=1)
        assert nested_middleware.assert_called(hook=""on_request"", times=1)
        assert nested_middleware.assert_called(hook=""on_list_tools"", times=1)
",tests/server/middleware/test_middleware.py,TestNestedMiddlewareHooks
survived,"    def add_middleware(self, middleware: MCPMiddleware) -> None:
        self.middleware.append(middleware)
",src/fastmcp/server/server.py,FastMCP
deleted,"    async def _middleware_read_resource(
        self,
        uri: AnyUrl | str,
    ) -> list[ReadResourceContents]:
        """"""
        Read a resource with middleware.
        """"""

        async def _handler(
            context: MiddlewareContext[mcp.types.ReadResourceRequestParams],
        ) -> list[ReadResourceContents]:
            return await self._read_resource(
                uri=context.message.uri,
            )

        # Convert string URI to AnyUrl if needed
        if isinstance(uri, str):
            from pydantic import AnyUrl

            uri_param = AnyUrl(uri)
        else:
            uri_param = uri

        mw_context = MiddlewareContext(
            message=mcp.types.ReadResourceRequestParams(uri=uri_param),
            source=""client"",
            type=""request"",
            method=""resources/read"",
            fastmcp_context=fastmcp.server.dependencies.get_context(),
        )
        return await self._apply_middleware(mw_context, _handler)
",src/fastmcp/server/server.py,FastMCP
survived,"    async def test_list_resources(
        self, mcp_server: FastMCP, recording_middleware: RecordingMiddleware
    ):
        async with Client(mcp_server) as client:
            await client.list_resources()

        assert recording_middleware.assert_called(times=3)
        assert recording_middleware.assert_called(method=""resources/list"", times=3)
        assert recording_middleware.assert_called(hook=""on_message"", times=1)
        assert recording_middleware.assert_called(hook=""on_request"", times=1)
        assert recording_middleware.assert_called(hook=""on_list_resources"", times=1)
",tests/server/middleware/test_middleware.py,TestMiddlewareHooks
survived,"    async def sample_tool(context: Context) -> None:
        await context.sample(""hello"")
",tests/server/middleware/test_middleware.py,
deleted,"    async def _middleware_list_tools(self) -> list[Tool]:
        """"""
        List all available tools, in the format expected by the low-level MCP
        server.

        """"""

        async def _handler(
            context: MiddlewareContext[mcp.types.ListToolsRequest],
        ) -> list[Tool]:
            tools = await self._list_tools()

            mcp_tools: list[Tool] = []
            for tool in tools:
                if self._should_enable_component(tool):
                    mcp_tools.append(tool)

            return mcp_tools

        with fastmcp.server.context.Context(fastmcp=self) as fastmcp_ctx:
            # Create the middleware context.
            mw_context = MiddlewareContext(
                message=mcp.types.ListToolsRequest(method=""tools/list""),
                source=""client"",
                type=""request"",
                method=""tools/list"",
                fastmcp_context=fastmcp_ctx,
            )

            # Apply the middleware chain.
            return await self._apply_middleware(mw_context, _handler)
",src/fastmcp/server/server.py,FastMCP
survived,"    def reset(self):
        """"""Clear all recorded calls.""""""
        self.calls.clear()
",tests/server/middleware/test_middleware.py,RecordingMiddleware
survived,"def test_export_datasets_preserve_experiment_structure():
    """"""Test that experiment structure is preserved in the target database""""""
    with tempfile.TemporaryDirectory() as temp_dir:
        source_db_path = Path(temp_dir) / ""source.db""
        target_db_path = Path(temp_dir) / ""target.db""
        export_path = Path(temp_dir) / ""exports""
        
        # Create source database with multiple experiments
        source_conn = connect(source_db_path)
        
        # Create first experiment
        exp1 = load_or_create_experiment(
            experiment_name=""exp1"",
            sample_name=""sample1"",
            conn=source_conn
        )
        
        # Create second experiment
        exp2 = load_or_create_experiment(
            experiment_name=""exp2"",
            sample_name=""sample2"",
            conn=source_conn
        )
        
        # Create interdependencies
        x = ParamSpec(""x"", ""numeric"", unit=""V"")
        y = ParamSpec(""y"", ""numeric"", unit=""A"")
        interdeps = InterDependencies_(dependencies={y: (x,)})
        
        # Create datasets in both experiments
        datasets = []
        for exp in [exp1, exp2]:
            for i in range(2):  # 2 datasets per experiment
                dataset = DataSet(conn=source_conn, exp_id=exp.exp_id)
                dataset.set_interdependencies(interdeps)
                dataset.mark_started()
                
                # Add some data
                for j in range(5):
                    dataset.add_results([{""x"": j, ""y"": j * (i + 1)}])
                
                dataset.mark_completed()
                datasets.append(dataset)
        
        source_conn.close()
        
        # Run the export function
        result = export_datasets_and_create_metadata_db(
            source_db_path=source_db_path,
            target_db_path=target_db_path,
            export_path=export_path,
        )
        
        # Check that all datasets were processed
        assert len(result) == 4
        
        # Check that target database has all runs
        target_conn = connect(target_db_path)
        target_runs = get_runs(target_conn)
        assert len(target_runs) == 4
        target_conn.close()
",tests/dataset/test_export_datasets_and_create_metadata_db.py,
survived,"def _simple_dataset():
    """"""Create a simple dataset for testing""""""
    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = Path(temp_dir) / ""test.db""
        
        # Create experiment and dataset
        exp = load_or_create_experiment(
            experiment_name=""test_exp"",
            sample_name=""test_sample"",
            conn=connect(db_path)
        )
        
        # Create interdependencies
        x = ParamSpec(""x"", ""numeric"", unit=""V"")
        y = ParamSpec(""y"", ""numeric"", unit=""A"")
        interdeps = InterDependencies_(dependencies={y: (x,)})
        
        # Create dataset
        dataset = DataSet(conn=exp.conn, exp_id=exp.exp_id)
        dataset.set_interdependencies(interdeps)
        dataset.mark_started()
        
        # Add some data
        for i in range(10):
            dataset.add_results([{""x"": i, ""y"": i**2}])
        
        dataset.mark_completed()
        
        yield db_path, dataset.run_id
",tests/dataset/test_export_datasets_and_create_metadata_db.py,
survived,"async def test_mcp_client_autogen_pagination(tmp_path: Path) -> None:
    script = tmp_path / ""app.py""
    script.write_text(
        textwrap.dedent(
            """"""
            from sqlalchemy import ForeignKey
            from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
            from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship

            from enrichmcp import EnrichMCP
            from enrichmcp.sqlalchemy import (
                EnrichSQLAlchemyMixin,
                include_sqlalchemy_models,
                sqlalchemy_lifespan,
            )

            class Base(DeclarativeBase, EnrichSQLAlchemyMixin):
                pass

            class User(Base):
                __tablename__ = ""users""
                id: Mapped[int] = mapped_column(primary_key=True, info={""description"": ""ID""})
                name: Mapped[str] = mapped_column(info={""description"": ""Name""})
                orders: Mapped[list[""Order""]] = relationship(
                    back_populates=""user"", info={""description"": ""Orders""}
                )

            class Order(Base):
                __tablename__ = ""orders""
                id: Mapped[int] = mapped_column(primary_key=True, info={""description"": ""ID""})
                user_id: Mapped[int] = mapped_column(ForeignKey(""users.id""))
                user: Mapped[User] = relationship(
                    back_populates=""orders"", info={""description"": ""User""}
                )

            async def seed(session: AsyncSession) -> None:
                user = User(id=1, name=""Alice"")
                orders = [Order(id=i, user=user) for i in range(1, 4)]
                session.add_all([user, *orders])

            engine = create_async_engine(""sqlite+aiosqlite:///:memory:"")
            lifespan = sqlalchemy_lifespan(Base, engine, seed=seed)
            app = EnrichMCP(""Test"", ""Desc"", lifespan=lifespan)
            include_sqlalchemy_models(app, Base)

            if __name__ == ""__main__"":
                app.run()
            """"""
        )
    )

    config = {""mcpServers"": {""app"": {""command"": sys.executable, ""args"": [str(script)]}}}
    client = MCPClient(config=config)
    session = await client.create_session(""app"")

    result = await session.connector.call_tool(
        ""get_userenrichmodel_orders"",
        {""page"": 1, ""page_size"": 2, ""kwargs"": {""user_id"": 1}},
    )
    data = json.loads(result.content[0].text)
    assert len(data[""items""]) == 2
    assert data[""has_next""]

    result2 = await session.connector.call_tool(
        ""get_userenrichmodel_orders"",
        {""page"": 2, ""page_size"": 2, ""kwargs"": {""user_id"": 1}},
    )
    data2 = json.loads(result2.content[0].text)
    assert len(data2[""items""]) == 1
    assert not data2[""has_next""]

    await client.close_all_sessions()",tests/test_sqlalchemy_mcp_use.py,
survived,"    async def register(self, *_, **__):
        self.calls += 1
        if self.calls < 3:
            raise biotech_agent.AdkClientError(""boom"")
",tests/test_register_mesh_backoff.py,StubClient
survived,"        async def run(self, *_: Any, **__: Any) -> Dict[str, Any]:
            return {""status"": ""error"", ""error"": ""agents SDK unavailable""}
",src/meta_agent/agents/guardrail_designer_agent.py,Agent
survived,"def test_settings_repr_masks_secret(monkeypatch):
    monkeypatch.setenv(""OPENAI_API_KEY"", ""shh"")
    import src.utils.config as cfg
    importlib.reload(cfg)
    settings = cfg.Settings()
    rep = repr(settings)
    assert ""shh"" not in rep
    assert ""***"" in rep",tests/test_root_config.py,
survived,"        def read_secret_version(self, path):
            return {""data"": {""data"": {""OPENAI_API_KEY"": ""vault""}}}
",tests/test_root_config.py,FakeKV
survived,"    def __init__(self) -> None:
        self.shutdown = asyncio.Event()
",test/windows/test_shutdown.py,DummyManager
survived,"    def close(self) -> None:
        pass
",tests/test_self_improver.py,DummyLedger
survived,"    def save(
        self,
        response: List[bytes],
        name: str = None,
        dir: str = os.getcwd(),
        filenames_prefix: str = """",
    ) -> List[str]:
        """"""Save your amazing images! ðŸ’¾

        Args:
            response (List[bytes]): List of image data
            name (str, optional): Base name for saved files
            dir (str, optional): Where to save the images
            filenames_prefix (str, optional): Prefix for filenames

        Returns:
            List[str]: List of saved filenames
        """"""
        assert isinstance(response, list), f""Response should be of {list} not {type(response)}""
        name = self.prompt if name is None else name

        if not os.path.exists(dir):
            os.makedirs(dir)

        filenames = []
        count = 0
        for image in response:
            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)

            while os.path.isfile(complete_path()):
                count += 1

            absolute_path_to_file = complete_path()
            filenames.append(filenames_prefix + os.path.split(absolute_path_to_file)[1])

            with open(absolute_path_to_file, ""wb"") as fh:
                fh.write(image)
        return filenames
",webscout/Provider/TTI/pixelmuse.py,PixelMuseImager
survived,"            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(save_dir, filenames_prefix + name + count_value + ""."" + self.image_extension)
",webscout/Provider/TTI/magicstudio.py,MagicStudioImager
survived,"            def complete_path():
                count_value = """" if count == 0 else f""_{count}""
                return os.path.join(dir, name + count_value + ""."" + self.image_extension)
",webscout/Provider/TTI/freeaiplayground.py,FreeAIImager
survived,"    def test_array_grad(self):
        klong = KlongInterpreter()
        klong('x::Ë™!5')
        klong('loss::{+/x*x}')
        r = klong('x âˆ‡ loss')
        self.assertTrue(np.allclose(r, np.array([0,2,4,6,8]), atol=1e-3))
",tests/test_autograd.py,TestAutograd
survived,"def test_insight_invalid_token() -> None:
    _setup_simulations()
    client = _make_client()
    resp = client.post(""/insight"", json={}, headers={""Authorization"": ""Bearer bad""})
    assert resp.status_code == 403",tests/test_insight_endpoint.py,
survived,"async def test_send_http_error():
    with patch(""aiohttp.ClientSession"") as mock_session:
        resp = AsyncMock()
        resp.status = 500
        resp.text = AsyncMock(return_value=""bad"")
        cm = AsyncMock()
        cm.__aenter__.return_value = resp
        mock_session.return_value.post.return_value = cm
        client = TelemetryAPIClient({""trace"": EndpointConfig(""http://example.com"")})
        with pytest.raises(ValueError):
            await client.send(""trace"", {""d"": 1})
        await client.close()
",tests/unit/test_telemetry_client.py,
survived,"    def __post_init__(self) -> None:
        if self.auth_token:
            self.headers[""Authorization""] = f""Bearer {self.auth_token}""
        self.headers.setdefault(""Content-Type"", ""application/json"")
",src/meta_agent/services/telemetry_client.py,EndpointConfig
survived,"async def test_send_success(telemetry_client):
    result = await telemetry_client.send(""trace"", {""data"": 1})
    assert result == {""ok"": True}
",tests/unit/test_telemetry_client.py,
survived,"            async def close(self) -> None:
                pass
",src/meta_agent/services/llm_service.py,AiohttpPlaceholder.ClientSession
survived,"        def __init__(self, *_, **__):
            pass
",src/meta_agent/services/telemetry_client.py,ClientSession
survived,"    def menu(self, prompt: str, options: Sequence[str]) -> str:
        """"""Display a numbered menu and return the selected option.""""""
        if not options:
            raise ValueError(""options must not be empty"")

        while True:
            print(prompt)
            for idx, opt in enumerate(options, 1):
                print(f""{idx}. {opt}"")
            choice = self.ask(""Choose an option:"")
            if choice.isdigit():
                selected = int(choice) - 1
                if 0 <= selected < len(options):
                    return options[selected]
            print(""Invalid choice, try again."")
",src/meta_agent/ux/interactive.py,Interactive
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestAgentRegistryFunctions.BAgent
survived,"    def test_maxdd(self):
        returns = [0.1, -0.2, 0.05, -0.1]
        self.assertAlmostEqual(finance_agent._maxdd(returns), 0.244)
",tests/test_finance_utils.py,TestFinanceUtils
survived,"    def test_wrap_mcp_digest(self):
        payload = {""a"": 1}
        mcp = self.agent._wrap_mcp(payload)
        self.assertEqual(mcp[""payload""], payload)
        raw = json.dumps(payload, separators=("","", "":""))
        import hashlib
        self.assertEqual(mcp[""digest""], hashlib.sha256(raw.encode()).hexdigest())
",tests/test_supply_chain_agent.py,TestSupplyChainAgent
survived,"async def get_order_items(order_id: int, ctx: EnrichContext) -> list[""OrderItemEnrichModel""]:
    """"""Get all items in a specific order.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        result = await session.execute(select(OrderItem).where(OrderItem.order_id == order_id))
        items = result.scalars().all()

        return [
            OrderItemEnrichModel(
                id=item.id,
                order_id=item.order_id,
                product_id=item.product_id,
                quantity=item.quantity,
                unit_price=item.unit_price,
                total_price=item.total_price,
            )
            for item in items
        ]
",examples/sqlalchemy_shop/app.py,
survived,"    def test_model_inheritance(self):
        """"""Test that the EnrichModel properly inherits from EnrichModel base.""""""

        class Base(DeclarativeBase):
            pass

        class Product(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""products""
            id: Mapped[int] = mapped_column(primary_key=True)
            name: Mapped[str] = mapped_column()

        ProductEnrichModel = Product.__enrich_model__()

        # Should be a proper EnrichModel with all its methods
        assert hasattr(ProductEnrichModel, ""model_dump"")
        assert hasattr(ProductEnrichModel, ""model_dump_json"")
        assert hasattr(ProductEnrichModel, ""relationship_fields"")
        assert hasattr(ProductEnrichModel, ""describe"")
",tests/test_sqlalchemy_integration.py,TestEdgeCases
survived,"async def get_order_item_product(
    order_item_id: int, ctx: EnrichContext
) -> Optional[""ProductEnrichModel""]:
    """"""Get the product for a specific order item.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        item = await session.get(OrderItem, order_item_id)
        if not item:
            return None

        # Load the product
        await session.refresh(item, [""product""])
        product = item.product

        return ProductEnrichModel(
            id=product.id,
            name=product.name,
            description=product.description,
            price=product.price,
            stock_quantity=product.stock_quantity,
            category=product.category,
            created_at=product.created_at,
        )
",examples/sqlalchemy_shop/app.py,
survived,"async def list_orders(
    ctx: EnrichContext, status: str | None = None, cursor: str | None = None, limit: int = 10
) -> CursorResult[OrderEnrichModel]:
    """"""List orders with cursor-based pagination.""""""
    session_factory = ctx.request_context.lifespan_context[""session_factory""]
    async with session_factory() as session:
        # Build query
        query = select(Order)

        # Apply status filter
        if status:
            query = query.where(Order.status == status)

        # Apply cursor (assuming cursor is the last order ID seen)
        if cursor:
            query = query.where(Order.id > int(cursor))

        # Order by ID for consistent cursor pagination
        query = query.order_by(Order.id).limit(limit + 1)

        result = await session.execute(query)
        orders = result.scalars().all()

        # Check if there are more results
        has_next = len(orders) > limit
        if has_next:
            orders = orders[:-1]  # Remove the extra item

        items = [
            OrderEnrichModel(
                id=order.id,
                order_number=order.order_number,
                user_id=order.user_id,
                status=order.status,
                total_amount=order.total_amount,
                created_at=order.created_at,
                updated_at=order.updated_at,
                shipping_address=order.shipping_address,
                notes=order.notes,
            )
            for order in orders
        ]

        next_cursor = str(orders[-1].id) if orders else None

        return CursorResult(
            items=items, next_cursor=next_cursor, page_size=limit, has_next=has_next
        )
",examples/sqlalchemy_shop/app.py,
survived,"    def test_nullable_columns(self):
        """"""Test that nullable columns are converted to Optional types.""""""

        class Base(DeclarativeBase):
            pass

        class Product(Base, EnrichSQLAlchemyMixin):
            __tablename__ = ""products""

            id: Mapped[int] = mapped_column(primary_key=True)
            name: Mapped[str] = mapped_column(nullable=False)
            description: Mapped[str | None] = mapped_column(
                nullable=True, info={""description"": ""Product description""}
            )
            price: Mapped[float | None] = mapped_column(nullable=True)

        ProductEnrichModel = Product.__enrich_model__()
        fields = ProductEnrichModel.model_fields

        # Non-nullable fields should not be Optional
        assert fields[""id""].annotation == int
        assert fields[""name""].annotation == str

        # Nullable fields should be Optional
        # Check if it's Optional by looking at the annotation
        desc_type = fields[""description""].annotation
        price_type = fields[""price""].annotation

        # In Python 3.10+, Optional[X] is Union[X, None]
        assert get_origin(desc_type) in {Union, types.UnionType}
        assert type(None) in get_args(desc_type)
        assert str in get_args(desc_type)

        assert get_origin(price_type) in {Union, types.UnionType}
        assert type(None) in get_args(price_type)
        assert float in get_args(price_type)
",tests/test_sqlalchemy_integration.py,TestBasicModel
survived,"    def __init__(self, ledger_path: str | pathlib.Path):
        self.path = pathlib.Path(ledger_path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
",alpha_factory_v1/demos/meta_agentic_agi/agents/agent_base.py,LineageTracer
survived,"def mcts_policy(net: MuZeroTiny, obs: np.ndarray, simulations: int = 16) -> int:
    """"""Very small UCBâ€‘based MCTS on top of MuZeroTiny.""""""
    act_dim = 4
    with torch.no_grad():
        h, v0, p0 = net.initial(torch.tensor(obs, device=CFG.device, dtype=torch.float32))
    N = np.zeros(act_dim); W = np.zeros(act_dim)
    P = p0.exp().cpu().numpy()
    for _ in range(simulations):
        a = np.argmax(P * (np.sqrt(N.sum()+1e-8)/(1+N)))
        a_one = F.one_hot(torch.tensor(a), num_classes=act_dim).float().to(CFG.device)
        h2, r, v, p = net.recurrent(h, a_one)
        q = (r+v).item()
        N[a] += 1; W[a] += q
    best = int(np.argmax(W / (N+1e-8)))
    return best
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def __call__(self, prompt:str, **kw):
        return self.run(prompt, **kw)
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,Agent
survived,"    def loop(self):
        obs=[e.reset() for e in self.envs]
        for t in range(CFG.max_steps):
            if self.stop: break
            for i,(env,learner) in enumerate(zip(self.envs,self.learners)):
                a=learner.act(obs[i])
                nxt,r,done,_=env.step(a)
                learner.remember(obs[i],r)
                loss=learner.train_once()
                obs[i]=env.reset() if done else nxt
                if t%CFG.ui_tick==0 and i==0:
                    A2ABus.publish(""ui"",{""t"":t,""r"":r,""loss"":loss})
        LOG.info(""Orchestrator loop exit at t=%d"", t)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Orchestrator
survived,"async def _startup():
    global orch
    orch=Orchestrator()
    threading.Thread(target=orch.loop,daemon=True).start()
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def __init__(self, hidden: int, act_dim: int):
        super().__init__(); self.r = nn.Linear(hidden+act_dim, 1); self.h = nn.Linear(hidden+act_dim, hidden)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,Dyn
survived,"def emit_helm(dir_:Path=Path(""helm_chart"")):
    dir_.mkdir(exist_ok=True)
    (dir_/""values.yaml"").write_text(HELM_VALUES)
    (dir_/""Chart.yaml"").write_text(""apiVersion: v2\nname: alpha-asi-demo\nversion: 0.1.0\n"")
    print(""Helm chart â†’"",dir_)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo_v4.py,
survived,"    def run(self, code: str, func_name: str, *args, **kw):
        loc: Dict[str,Any] = {}
        with self:
            exec(code, {}, loc)
        if func_name not in loc:
            raise AttributeError(f""{func_name} not found"")
        return loc[func_name](*args, **kw)
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,SafeExec
survived,"    def chat(self, msgs: List[Dict[str,str]], **kw) -> str:
        merged = dict(temperature=self.temperature, max_tokens=self.max_tokens, **kw)
        attempts = 0
        while True:
            GLOBAL_LIMITER.acquire(_str_tkn(json.dumps(msgs)))
            try:
                if self._backend == ""openai"":
                    rsp = self._client.chat.completions.create(model=self._model, messages=msgs, stream=False, **merged)
                    return rsp.choices[0].message.content
                if self._backend == ""anthropic"":
                    rsp = self._client.messages.create(model=self._model, messages=msgs, **merged)
                    return rsp.content[0].text
                if self._backend == ""gemini"":
                    return self._client.generate_content(msgs[-1][""content""], **merged).text
                if self._backend in (""mistral"",""llama""):
                    prompt = """".join(f""<{m['role']}> {m['content']}"" for m in msgs)+""\n<assistant> ""
                    out = self._client(prompt, max_tokens=self.max_tokens, temperature=self.temperature, stop=[""</assistant>""])
                    return out[""choices""][0][""text""].strip()
            except Exception as e:
                attempts += 1
                wait = min(60, 2**attempts)
                LOGGER.warning(""LM error %s; retry in %.1fs"", e, wait)
                time.sleep(wait)
",alpha_factory_v1/demos/meta_agentic_agi_v2/agents/agent_base.py,LMClient
survived,"    def __init__(self,
                 name: str,
                 role: str = ""autonomousâ€‘agent"",
                 provider: str | None = None,
                 objectives: Optional[ObjectiveWeights] = None,
                 lineage_dir: str | pathlib.Path = ""./lineage"",
                 rate_limit_tps: float = 3.0):
        self.name = name
        self.role = role
        self.id = f""{name}-{_sha(uuid.uuid4().hex)}""
        self.objectives = objectives or ObjectiveWeights()
        self.lm = LMClient(provider or os.getenv(""ALPHA_PROVIDER"", ""openai:gpt-4o""))
        self.tracer = LineageTracer(pathlib.Path(lineage_dir)/f""{self.id}.jsonl"")
        self.tracer.log(""init"", role=role, provider=self.lm.endpoint)
        GLOBAL_LIMITER._tps = rate_limit_tps
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,Agent
survived,"def _sha(text: str) -> str:
    return hashlib.sha256(text.encode()).hexdigest()[:10]
",alpha_factory_v1/demos/meta_agentic_agi_v3/agents/agent_base.py,
survived,"    def __init__(self, name):
        super().__init__(""127.0.0.1"", 0)
        self.name = name
",tests/test_multi_contributor.py,FakeComm
survived,"        def __init__(self, *args, **kwargs) -> None:  # pragma: no cover - args ignored
            pass
",tests/test_alpha_agi_business_3_v1.py,DummyAgent
survived,"def accordion(data=None, class_name=None, key=None):
    props = {
        ""data"": data,
        ""className"": class_name,
    }
    component_value = _component_func(comp=""accordion"", props=props, key=key)
    return component_value",streamlit_shadcn_ui/py_components/accordion.py,
survived,"def test_load_vocab(tmp_path):
    vocab_file = tmp_path / ""vocab.csv""
    with open(vocab_file, ""w"", encoding=""utf-8"", newline="""") as f:
        writer = csv.writer(f)
        writer.writerow([0, "" hello""])
        writer.writerow([1, "" world ""])

    vocab = sampler.load_vocab(str(vocab_file))

    assert vocab == [""hello"", ""world""]
",tests/test_sampler_io.py,
survived,"def test_devicon_various_examples(name, expected):
    devicons = reload_devicons('es')
    assert devicons.devicon(MockFile(name)) == expected",tests/test_devicons.py,
survived,"    def close(self) -> None:  # pragma: no cover - dummy
        pass
",tests/test_codegen_safety.py,DummyLedger
survived,"    def test_list_agents_flag(self):
        args = self._parse([""--list-agents""])
        self.assertTrue(args.list_agents)
",alpha_factory_v1/tests/test_edge_runner.py,EdgeRunnerParseTest
survived,"    def test_venv_python_posix(self):
        with mock.patch.object(os, 'name', 'posix'):
            self.assertEqual(
                quickstart._venv_python(Path('/tmp/venv')),
                Path('/tmp/venv/bin/python')
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest
survived,"    def test_venv_python_windows(self):
        with mock.patch.object(os, 'name', 'nt'):
            path = PureWindowsPath('C:/v')
            self.assertEqual(
                quickstart._venv_python(path),
                path / 'Scripts' / 'python.exe'
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest
survived,"    def test_venv_pip_windows(self):
        with mock.patch.object(os, 'name', 'nt'):
            path = PureWindowsPath('C:/v')
            self.assertEqual(
                quickstart._venv_pip(path),
                path / 'Scripts' / 'pip.exe'
            )
",alpha_factory_v1/tests/test_quickstart.py,QuickstartUtilsTest
survived,"    def __init__(self, env_id: str = ""CartPole-v1"") -> None:
        self.env = gym.make(env_id, render_mode=""rgb_array"")
        obs_dim = math.prod(self.env.observation_space.shape)
        self.action_dim = self.env.action_space.n
        self.net = MiniMuNet(obs_dim, self.action_dim)
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,MiniMu
survived,"    def test_reset_produces_valid_grid(self):
        env = ce.CurriculumEnv(genome=ce.EnvGenome(max_steps=10), size=6)
        obs, info = env.reset()
        self.assertEqual(obs.shape[0], env.observation_space.shape[0])
        self.assertIn(""genome_id"", info)
        self.assertLessEqual(info[""difficulty""], 10)
",alpha_factory_v1/tests/test_aiga_meta_evolution.py,CurriculumEnvTest
survived,"    def queue_job(self, job: Mapping[str, Any]) -> requests.Response:
        """"""POST the job to the orchestrator and return the HTTP response.""""""
        agent = job.get(""agent"")
        if not agent:
            raise ValueError(""Job must specify 'agent'"")
        url = f""{self.base_url}/agent/{agent}/trigger""
        resp = requests.post(url, json=job)
        resp.raise_for_status()
        return resp
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,MarketplaceClient
survived,"            def observe(self, *a, **k):
                self.calls.append(""observe"")
",alpha_factory_v1/tests/test_ping_agent.py,PingAgentTest.DummyMetric
survived,"    def tearDown(self):
        sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest
survived,"    def test_fallback_when_package_missing(self):
        original = im.distribution
        def fake_distribution(name):
            raise im.PackageNotFoundError
        im.distribution = fake_distribution
        try:
            mod = importlib.import_module(""requests"")
            from alpha_factory_v1 import requests as shim
            self.assertIs(mod.get, shim.get)
            self.assertIs(mod.post, shim.post)
        finally:
            im.distribution = original
            sys.modules.pop(""requests"", None)
",alpha_factory_v1/tests/test_requests_import.py,RequestsImportTest
survived,"    def _download_vosk(args):
        download_vosk_model(args.url, args.dir)
",speech_recognition/cli.py,
survived,"def test_ws_progress_token_param() -> None:
    port = _free_port()
    proc = _start_server(port)
    url = f""http://127.0.0.1:{port}""
    headers = {""Authorization"": ""Bearer test-token""}
    try:
        _wait_running(url, headers)
        ws_url = f""ws://127.0.0.1:{port}/ws/progress?token=test-token""
        with websockets.connect(ws_url) as ws:
            pass
    finally:
        proc.terminate()
        proc.wait(timeout=5)",tests/test_api_server_subprocess.py,
survived,"    def test_missing_optional_packages_ok(self) -> None:
        def fake_check_pkg(name: str) -> bool:
            # Required packages are always present
            if name in {""pytest"", ""prometheus_client""}:
                return True
            # Simulate all optional deps missing
            if name in preflight.OPTIONAL_DEPS:
                return False
            return True

        patches = [
            mock.patch.object(preflight, ""check_python"", return_value=True),
            mock.patch.object(preflight, ""check_cmd"", return_value=True),
            mock.patch.object(preflight, ""check_node"", return_value=True),
            mock.patch.object(preflight, ""check_docker_daemon"", return_value=True),
            mock.patch.object(preflight, ""check_docker_compose"", return_value=True),
            mock.patch.object(preflight, ""check_patch_in_sandbox"", return_value=True),
            mock.patch.object(preflight, ""check_pkg"", side_effect=fake_check_pkg),
            mock.patch.object(preflight, ""check_openai_agents_version"", return_value=True),
            mock.patch.object(preflight, ""ensure_dir"", return_value=None),
        ]
        with mock.patch.dict(os.environ, {""OPENAI_API_KEY"": """", ""ANTHROPIC_API_KEY"": """"}, clear=False):
            with contextlib.ExitStack() as stack:
                for p in patches:
                    stack.enter_context(p)
                # Should not raise SystemExit
                preflight.main([""--offline""])
",tests/test_preflight_optional_missing.py,TestPreflightOptionalMissing
survived,"def test_trace_call_wb_run_step_query(client):
    full_wb_run_id = f""{client.entity}/{client.project}/test-run""
    from weave.trace import weave_client

    step_counter = iter(range(100))
    with (
        mock.patch.object(
            weave_client, ""safe_current_wb_run_id"", lambda: full_wb_run_id
        ),
        mock.patch.object(
            weave_client, ""safe_current_wb_run_step"", lambda: next(step_counter)
        ),
    ):
        call_spec = simple_line_call_bootstrap()

    server = get_client_trace_server(client)
    res = server.calls_query(
        tsi.CallsQueryReq(project_id=get_client_project_id(client))
    )
    steps = [c.wb_run_step for c in res.calls]
    assert set(steps) == set(range(call_spec.total_calls))

    query = tsi.Query(
        **{""$expr"": {""$eq"": [{""$getField"": ""wb_run_step""}, {""$literal"": 0}]}}
    )
    res = server.calls_query(
        tsi.CallsQueryReq(project_id=get_client_project_id(client), query=query)
    )
    assert len(res.calls) == 1

    max_step = call_spec.total_calls - 2
    range_query = tsi.Query(
        **{""$expr"": {""$gte"": [{""$getField"": ""wb_run_step""}, {""$literal"": max_step}]}}
    )
    res = server.calls_query(
        tsi.CallsQueryReq(project_id=get_client_project_id(client), query=range_query)
    )
    assert len(res.calls) == 2
",tests/trace/test_client_trace.py,
survived,"def primesUpTo(n):
    sieve = []
    i = 0
    while i <= n:
        sieve = sieve + [True]
        i = i + 1
    p = 2
    while p * p <= n:
        if sieve[p]:
            m = p * p
            while m <= n:
                sieve[m] = False
                m = m + p
        p = p + 1
    res = []
    x = 2
    while x <= n:
        if sieve[x]:
            res = res + [x]
        x = x + 1
    return res
",tests/rosetta/transpiler/Python/consecutive-primes-with-ascending-or-descending-differences.py,
survived,"def main():
    fb = Foodbox(items=[PeelFirst(value=""banana""), PeelFirst(value=""mango"")])
    f0 = fb.items[0]
    peelFirstEat(f0)
",tests/rosetta/transpiler/Python/constrained-genericity-4.py,
survived,"def main():
    example10()
",tests/rosetta/transpiler/Python/conditional-structures-10.py,
survived,"def doPos(x):
    pass
",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"    def __str__(self) -> str:
        lines = [f""## {self.name}"", self.description, """"]
        if self.fields:
            lines.append(""### Fields"")
            lines.extend(str(f) for f in self.fields)
            lines.append("""")
        if self.relationships:
            lines.append(""### Relationships"")
            lines.extend(str(r) for r in self.relationships)
            lines.append("""")
        return ""\n"".join(lines)
",src/enrichmcp/datamodel.py,EntityDescription
survived,"def test_evonet_no_relu_layers() -> None:
    g = me.Genome(layers=(4,), activation=""tanh"")
    net = me.EvoNet(2, 1, g)
    assert all(not isinstance(m, torch.nn.ReLU) for m in net.model)
",tests/test_evo_net_activation.py,
survived,"def main() -> None:
    repo_root = Path(__file__).resolve().parent.parent
    manifest_path = repo_root / (""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build_assets.json"")
    manifest = json.loads(manifest_path.read_text())
    checksums = manifest.get(""checksums"", {})
    checksums.update(fa.CHECKSUMS)
    manifest[""checksums""] = {k: checksums[k] for k in sorted(checksums)}
    manifest_path.write_text(json.dumps(manifest, indent=2) + ""\n"")
    print(f""Updated {manifest_path}"")
",scripts/generate_build_manifest.py,
survived,"    async def collect_trajectories(self, item):
        user_content = dict(item[0][0])[""content""]
        messages = []
        if self.config.system_prompt:
            messages.append({""role"": ""system"", ""content"": self.config.system_prompt})
        messages.append({""role"": ""user"", ""content"": user_content})
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        completions = await self.server.completion(
            prompt=prompt,
            n=self.config.group_size,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
        )
        trajectories = []
        for completion in completions.choices:
            completion_text = (
                completion.text if hasattr(completion, ""text"") else completion.message.content
            )
            msg_seq = []
            if self.config.system_prompt:
                msg_seq.append({""role"": ""system"", ""content"": self.config.system_prompt})
            msg_seq.append({""role"": ""user"", ""content"": user_content})
            msg_seq.append({""role"": ""assistant"", ""content"": completion_text})
            trajectories.append(msg_seq)
        return trajectories, []
",environments/sanskrit_poetry_env.py,SanskritPoetryEnv
survived,"    def _score_text(self, text: str) -> float:
        if not self.classifier:
            return 0.0
        try:
            slp_text = iast_to_slp1(text)
            result = self.classifier.classify(slp_text)
            if not result:
                return 0.0
            predicted = getattr(result, ""name"", str(result)).lower()
            raw_score = getattr(result, ""score"", 1.0 if predicted == self.meter.lower() else 0.0)
            if predicted != self.meter.lower():
                raw_score = 1.0 - raw_score if raw_score <= 1.0 else 0.0
            return max(0.0, min(1.0, float(raw_score)))
        except Exception as e:  # pragma: no cover - runtime safeguard
            logger.error(""Error scoring text with chandas: %s"", e)
            return 0.0
",atroposlib/envs/reward_fns/chandas_meter_reward.py,ChandasMeterReward
survived,"    def __init__(self, meter: str = ""tristubh"", weight: float = 1.0, **kwargs):
        super().__init__(weight=weight, **kwargs)
        self.meter = meter
        try:
            from chandas import Classifier  # type: ignore

            if resource_filename is not None:
                data_path = resource_filename(""chandas"", ""data/data.json"")
                self.classifier = Classifier.from_json_file(data_path)
            else:
                self.classifier = Classifier.from_default_location()
        except Exception as e:  # pragma: no cover - optional dependency
            logger.error(""Failed to load chandas Classifier: %s"", e)
            self.classifier = None
",atroposlib/envs/reward_fns/chandas_meter_reward.py,ChandasMeterReward
survived,"    def delete_stale_entries(self, stale_after: timedelta) -> None:
        """"""Delete stale entries from the SQL cache.""""""
        threshold = datetime.now() - stale_after
        with self._lock, self._Session() as session:
            session.execute(
                delete(CacheTable).where(
                    and_(
                        CacheTable.function_id == self._func_str,
                        CacheTable.timestamp < threshold,
                    )
                )
            )
            session.commit()",src/cachier/cores/sql.py,_SQLCore
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""Return order as plain dictionary.""""""
        return asdict(self)
",alpha_factory_v1/backend/broker.py,Order
survived,"    def span(self, agent_name: str, phase: str, **payload: Any) -> Generator[None, None, None]:
        """"""Context manager that records duration in ``payload['duration_ms']``.""""""
        start = _dt.datetime.utcnow()
        try:
            yield
        finally:
            duration = (_dt.datetime.utcnow() - start).total_seconds() * 1000
            payload[""duration_ms""] = round(duration, 3)
            self.record(agent_name, phase, payload)
",alpha_factory_v1/backend/tracer.py,Tracer
survived,"        def _decorator(func):
            return func
",alpha_factory_v1/demos/self_healing_repo/agent_selfheal_entrypoint.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_multi_join_sort.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/group_by_conditional_sum.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/outer_join.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/inner_join.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/machine/x/python/join_multi.py,Customer
survived,"def load_gitignore_as_context_rules(file_path: Path) -> List[str]:
    """"""Load .gitignore rules and convert to Jinni-style context rules.""""""
    raw_lines = load_rules_from_file(file_path)
    converted: List[str] = []
    for line in raw_lines:
        stripped = line.strip()
        if not stripped or stripped.startswith('#'):
            continue
        if stripped.startswith('!'):
            # Git's negation means include; keep without '!'
            converted.append(stripped[1:])
        else:
            # Regular gitignore entry is an exclusion -> prefix '!'
            converted.append('!' + stripped)
    return converted
",jinni/config_system.py,
survived,"def test_length_limit():
    long_title = 'A' * 300
    assert len(remove_chars(long_title)) == 245",tests/test_remove_chars.py,
survived,"def test_trim_invalid_characters():
    assert remove_chars('???Title???') == 'Title'
    assert remove_chars('!@#My Book!!!') == 'My Book'
    assert remove_chars('Book (Edition)') == 'Book - Edition'
",tests/test_remove_chars.py,
survived,"def test_parse_clippings_creates_files(tmp_path):
    clippings = Path(__file__).with_name(""My Clippings.txt"")
    out_dir = tmp_path / ""out""
    parse_clippings(str(clippings), str(out_dir), format=""txt"")

    expected_files = [
        remove_chars(""Example Title: The Beginning (John Doe)"") + "".txt"",
        remove_chars(""Another Book? & Something & Else (Jane Smith)"") + "".txt"",
    ]
    assert sorted(os.listdir(out_dir)) == sorted(expected_files)

    contents1 = (out_dir / expected_files[0]).read_text(encoding=""utf8"")
    assert ""This is a highlight text."" in contents1

    contents2 = (out_dir / expected_files[1]).read_text(encoding=""utf8"")
    assert ""Interesting highlight & note?"" in contents2",tests/test_parse_clippings.py,
survived,"    def visit_Pass(self, node: ast.Pass) -> None:
        pass
",tools/any2mochi/py/py2mochi.py,Converter
survived,"    async def seed(session: AsyncSession) -> None:
        nonlocal seed_called
        session.add(User(id=1))
        seed_called = True
",tests/test_lifespan.py,
survived,"        def observe(self, *_a) -> None:
            pass
",tests/test_agent_experience_entrypoint.py,DummyAgent
survived,"    def Dataframe(self, *a, **k):
        pass
",tests/test_agent_experience_entrypoint.py,DummyBlocks
survived,"    def __init__(self, *a, **k):
        pass
",tests/test_selfheal_env.py,DummyMarkdown
survived,"def test_run_tests_respects_config(tmp_path, monkeypatch):
    repo = tmp_path / ""repo""
    repo.mkdir()
    (repo / ""calc.py"").write_text(
        ""def add(a, b):\n    return a + b\n"",
        encoding=""utf-8"",
    )
    (repo / ""test_calc.py"").write_text(
        ""import calc\n\n"" ""def test_add():\n    assert calc.add(1, 2) == 3\n"",
        encoding=""utf-8"",
    )

    env_file = tmp_path / ""config.env""
    env_file.write_text(
        f""OPENAI_MODEL=my-model\nMODEL_NAME=my-model\nCLONE_DIR={repo}\n"",
        encoding=""utf-8"",
    )
    cfg.init_config(str(env_file))

    monkeypatch.setitem(
        sys.modules,
        ""gradio"",
        types.SimpleNamespace(Blocks=DummyBlocks, Markdown=DummyMarkdown, Button=DummyButton),
    )

    agent_args = {}

    class FakeAgent:
        def __init__(self, *a, **kw):
            agent_args.update(kw)

        def __call__(self, *_a, **_k):
            return ""ok""

    stub = types.SimpleNamespace(
        Agent=lambda *a, **k: object(),
        OpenAIAgent=FakeAgent,
        Tool=lambda *a, **k: (lambda f: f),
    )
    monkeypatch.setitem(sys.modules, ""openai_agents"", stub)
    monkeypatch.setitem(
        sys.modules,
        ""patcher_core"",
        types.SimpleNamespace(
            generate_patch=lambda *_a, **_k: """",
            apply_patch=lambda *_a, **_k: None,
        ),
    )

    sys.modules.pop(
        ""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint"",
        None,
    )
    path = Path(__file__).resolve().parents[1] / ""alpha_factory_v1/demos/self_healing_repo/agent_selfheal_entrypoint.py""
    spec = importlib.util.spec_from_file_location(
        ""alpha_factory_v1.demos.self_healing_repo.agent_selfheal_entrypoint"", path
    )
    entrypoint = importlib.util.module_from_spec(spec)
    entrypoint.apply_patch_and_retst = lambda *_a, **_k: None
    assert spec.loader
    sys.modules[spec.name] = entrypoint
    spec.loader.exec_module(entrypoint)
    monkeypatch.setattr(entrypoint, ""CLONE_DIR"", str(repo))

    result = asyncio.run(entrypoint.run_tests())
    assert result[""rc""] == 0
    assert agent_args.get(""model"") == ""my-model""",tests/test_selfheal_env.py,
survived,"    def click(self, *a, **kw):
        pass
",tests/test_selfheal_env.py,DummyButton
survived,"    def _safe_open(*_: Any, **__: Any) -> None:
        raise PermissionError(""File operations are not permitted"")
",backend/tools/analysis_tools.py,
survived,"def get_metric(factory: Callable[..., Any], name: str, desc: str, labels: list[str] | None = None) -> Any:
    """"""Return an existing metric or create a new one.

    This avoids depending on ``REGISTRY._names_to_collectors`` which may
    change between ``prometheus_client`` versions.
    """"""
    metric = METRICS.get(name)
    if metric is not None:
        return metric
    metric = factory(name, desc, labels) if labels is not None else factory(name, desc)
    METRICS[name] = metric
    return metric",alpha_factory_v1/backend/metrics_registry.py,
survived,"def test_workbox_hash_mismatch(tmp_path: Path) -> None:
    repo = Path(__file__).resolve().parents[1]
    src = repo / ""alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist""
    dist = tmp_path / ""dist""
    shutil.copytree(src, dist)
    sw_file = dist / ""service-worker.js""
    text = sw_file.read_text()
    text = re.sub(r""(WORKBOX_SW_HASH = '\)[^']+(\')"", r""\1sha384-invalid\2"", text)
    sw_file.write_text(text)

    server, thread = _start_server(dist)
    host, port = server.server_address
    url = f""http://{host}:{port}/index.html""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            context = browser.new_context()
            page = context.new_page()
            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_function(
                ""document.getElementById('toast').textContent.includes('offline mode disabled')""
            )
            assert page.evaluate(""navigator.serviceWorker.controller"") is None
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")
    finally:
        server.shutdown()
        thread.join()",tests/test_workbox_integrity.py,
survived,"def _make_tar(member: tarfile.TarInfo) -> bytes:
    buf = io.BytesIO()
    with tarfile.open(mode=""w"", fileobj=buf) as tf:
        tf.addfile(member)
    return buf.getvalue()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_evolution_worker.py,
survived,"    def update(self, *args: Any, **kwargs: Any) -> None:  # type: ignore[override]
        if self.__dict__.get(""_frozen"", False):
            raise TypeError(""Cannot modify attributes after call start"")
        for k, v in dict(*args, **kwargs).items():
            self[k] = v
",weave/trace/weave_client.py,AttributesDict
survived,"def _check_ollama(url: str) -> None:
    """"""Verify an Ollama server is reachable.""""""
    base = url.rstrip(""/"")
    if base.endswith(""/v1""):
        base = base[:-3]
    try:
        request.urlopen(f""{base}/api/tags"", timeout=3)
    except Exception as exc:  # pragma: no cover - network check
        raise RuntimeError(
            f""Ollama not reachable at {base}. ""
            ""Install it from https://ollama.com and run 'ollama serve'.""
        ) from exc
",alpha_factory_v1/demos/macro_sentinel/agent_macro_entrypoint.py,
survived,"def test_get_macros():
    context = Context(paths=[""examples/sushi""])
    lsp_context = LSPContext(context)

    file_path = next(key for key in lsp_context.map.keys() if key.name == ""active_customers.sql"")
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        file_content = f.read()

    file_uri = URI.from_path(file_path)
    completions = LSPContext.get_completions(lsp_context, file_uri, file_content)

    assert ""each"" in completions.macros
    assert ""add_one"" in completions.macros
",tests/lsp/test_completions.py,
survived,"    def add_path(self):
        ''' Adds a document to a path starting from this node '''
        node = self
        node.customers += 1
        for level in range(1, self.num_levels):
            node = node.parent
            node.customers += 1
",src/hlda/sampler.py,NCRPNode
survived,"    def calculate_word_likelihood(self, node_weights, node, weight, level_word_counts, new_topic_weights, level):

        # first calculate the likelihood of the words at this level, given this topic
        node_weight = 0.0
        word_counts = level_word_counts[level]
        total_words = 0

        for w in word_counts:
            count = word_counts[w]
            for i in range(count): # why ?????????
                node_weight += log( (self.eta + node.word_counts[w] + i) /
                                    (self.eta_sum + node.total_words + total_words) )
                total_words += 1

        # propagate that weight to the child nodes
        for child in node.children:
            self.calculate_word_likelihood(node_weights, child, weight + node_weight,
                                           level_word_counts, new_topic_weights, level+1)

        # finally if this is an internal node, add the weight of a new path
        level += 1
        while level < self.num_levels:
            node_weight += new_topic_weights[level]
            level += 1

        node_weights[node] += node_weight
",src/hlda/sampler.py,HierarchicalLDA
survived,"    def calculate_doc_likelihood(self, node_weights, level_word_counts):

        # calculate the weight for a new path at a given level
        new_topic_weights = np.zeros(self.num_levels)
        for level in range(1, self.num_levels):  # skip the root

            word_counts = level_word_counts[level]
            total_tokens = 0

            for w in word_counts:
                count = word_counts[w]
                for i in range(count):  # why ?????????
                    new_topic_weights[level] += log((self.eta + i) / (self.eta_sum + total_tokens))
                    total_tokens += 1

        self.calculate_word_likelihood(node_weights, self.root_node, 0.0, level_word_counts, new_topic_weights, 0)
",src/hlda/sampler.py,HierarchicalLDA
survived,"    def print_nodes(self, n_words, with_weights):
        self.print_node(self.root_node, 0, n_words, with_weights)
",src/hlda/sampler.py,HierarchicalLDA
survived,"    def estimate(self, num_samples, display_topics=50, n_words=5, with_weights=True):

        print('HierarchicalLDA sampling\n')
        for s in range(num_samples):

            sys.stdout.write('.')

            for d in range(len(self.corpus)):
                self.sample_path(d)

            for d in range(len(self.corpus)):
                self.sample_topics(d)

            if (s > 0) and ((s+1) % display_topics == 0):
                print(f"" {s+1}"")
                self.print_nodes(n_words, with_weights)
",src/hlda/sampler.py,HierarchicalLDA
survived,"def test_notebook_ignored_without_flag(tmp_path):
    """"""Without the notebook flag ipynb files were previously skipped.""""""
    nb = tmp_path / ""t.ipynb""
    _write_notebook(nb)
    result = _fstringify_file(str(nb), State())
    assert result is None
    with open(nb) as fh:
        data = json.load(fh)
    assert ""format(1)"" in """".join(data[""cells""][0][""source""])
",test/integration/test_api.py,
survived,"def test_attention_paged_decode_ragged_fill_in_chunks():
    B = Axis(""batch"", 2)
    Pos = Axis(""position"", 8)
    Embed = Axis(""embed"", 16)

    cfg = AttentionConfig(Embed=Embed, num_heads=2, num_kv_heads=2, attn_backend=AttentionBackend.VANILLA)
    attn_key, x_key = jrandom.split(jrandom.PRNGKey(0))
    attn = Attention.init(cfg, key=attn_key)
    x = hax.random.normal(x_key, (B, Pos, Embed)) * 0.2
    full_out = attn(x, AttentionMask.causal(), key=jrandom.PRNGKey(1))

    def padded(start, stop):
        pos = hax.arange(Pos, dtype=jnp.int32, start=start)
        return hax.where(pos >= stop, -1, pos)

    cache = _build_page_cache(cfg, B, Pos)

    chunk_sizes = [[4, 2], [0, 1], [0, 1], [2, 1], [1, 2], [1, 1]]
    off0 = off1 = 0
    outputs0 = []
    outputs1 = []
    for step0, step1 in chunk_sizes:
        pos_ids = hax.stack(""batch"", [padded(off0, off0 + step0), padded(off1, off1 + step1)])

        x0 = x[B, 0, ""position"", off0 : off0 + step0]
        x1 = x[B, 1, ""position"", off1 : off1 + step1]

        x_q = hax.full((B, Pos, Embed), 100, dtype=x.dtype)
        x_q = x_q.at[B, 0, ""position"", 0:step0].set(x0)
        x_q = x_q.at[B, 1, ""position"", 0:step1].set(x1)

        output, cache = _jit_paged_decode(attn, x_q, pos_ids=pos_ids, cache=cache)
        outputs0.append(output[B, 0, ""position"", hax.dslice(0, step0)])
        outputs1.append(output[B, 1, ""position"", hax.dslice(0, step1)])
        off0 += step0
        off1 += step1

    outputs0_cat = hax.concatenate(""position"", outputs0)
    outputs1_cat = hax.concatenate(""position"", outputs1)

    assert_trees_all_close(full_out[B, 0].array, outputs0_cat.array, atol=1e-4, rtol=1e-4)
    assert_trees_all_close(full_out[B, 1].array, outputs1_cat.array, atol=1e-4, rtol=1e-4)

    decoded_arr = hax.stack(""batch"", [outputs0_cat, outputs1_cat])
    assert_trees_all_close(full_out.array, decoded_arr.array, atol=1e-4, rtol=1e-4)",tests/test_attention.py,
survived,"def test_attention_paged_decode_matches_full_ar():
    B = Axis(""batch"", 1)
    Pos = Axis(""position"", 4)
    Embed = Axis(""embed"", 8)

    cfg = AttentionConfig(Embed=Embed, num_heads=2, num_kv_heads=2, rope=None, attn_backend=AttentionBackend.VANILLA)
    attn_key, x_key = jrandom.split(jrandom.PRNGKey(0))
    attn = Attention.init(cfg, key=attn_key)

    x = hax.random.normal(x_key, (B, Pos, Embed)) * 0.2
    full_out = attn(x, AttentionMask.causal(), key=jrandom.PRNGKey(1))

    cache = _build_page_cache(cfg, B, Pos)
    out_chunks = []
    for i in range(Pos.size):
        x_tok = x[Pos, hax.dslice(i, 1)]
        sub_pos = x_tok.resolve_axis(""position"")
        pos_ids_tok = hax.arange(sub_pos, start=i)
        out_tok, cache = _jit_paged_decode(attn, x_tok, pos_ids_tok, cache)
        out_chunks.append(out_tok.array)

    decoded_arr = jnp.concatenate(out_chunks, axis=1)
    assert_trees_all_close(full_out.array, decoded_arr, atol=1e-4, rtol=1e-4)
",tests/test_attention.py,
survived,"def test_no_pep585_generics():
    viols = []
    for root, _, files in os.walk(os.path.dirname(os.path.dirname(__file__))):
        for file in files:
            if file.endswith('.py') and not file.startswith('test_'):
                path = os.path.join(root, file)
                with open(path, 'r', encoding='utf-8') as f:
                    source = f.read()
                try:
                    tree = ast.parse(source)
                except SyntaxError:
                    continue
                for node in ast.walk(tree):
                    if isinstance(node, ast.Subscript) and isinstance(node.value, ast.Name):
                        if node.value.id in ALLOWED_BUILTINS:
                            viols.append(f""{path}:{node.lineno}"")
    assert not viols, ""PEP585 generics found: "" + "", "".join(viols)",tests/test_no_pep585.py,
survived,"    def test_broadcast_success(self) -> None:
        led = self._ledger()
        env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
        led.log(env)
        root = led.compute_merkle_root()
        captured, DummyClient, DummyTx, DummyInstr, DummyPk = self._dummy_classes()
        with (
            mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
            mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
            mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
        ):
            asyncio.run(led.broadcast_merkle_root())
        self.assertEqual(captured[""url""], ""http://rpc.test"")
        self.assertEqual(captured[""root""], root)
",tests/test_merkle_broadcast.py,TestMerkleBroadcast
survived,"    def _ledger(self):
        tmp = tempfile.TemporaryDirectory()
        led = orchestrator.Ledger(os.path.join(tmp.name, ""l.db""), rpc_url=""http://rpc.test"", broadcast=True)
        self.addCleanup(tmp.cleanup)
        return led
",tests/test_merkle_broadcast.py,TestMerkleBroadcast
deleted,"def create_app(graphdb_filename, config_filename):
    handler_definitions = yaml.safe_load(open(config_filename))

    edge_events = list(get_handler_instances(handler_definitions, ""edge_handlers""))
    vertex_events = list(get_handler_instances(handler_definitions, ""vertex_handlers""))
    vertex_reverse_geocoders = list(
        get_handler_instances(handler_definitions, ""vertex_reverse_geocoders"")
    )

    print(""edge event handlers:"")
    for e in edge_events:
        print(f""   {e}"")
    print(""vertex event handlers:"")
    for v in vertex_events:
        print(f""   {v}"")
    print(""vertex reverse geocoders:"")
    for g in vertex_reverse_geocoders:
        print(f""   {g}"")

    rs = RouteServer(graphdb_filename, vertex_events, edge_events, vertex_reverse_geocoders)
    app = Flask(__name__)

    @app.route(""/bounds"")
    def bounds():
        cb = request.args.get(""callback"")
        data = rs.bounds(jsoncallback=cb)
        mimetype = ""application/javascript"" if cb else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/vertices"")
    def vertices():
        return Response(rs.vertices(), mimetype=""text/plain"")

    @app.route(""/get_vertex_id"")
    def get_vertex_id():
        lat = float(request.args[""lat""])
        lon = float(request.args[""lon""])
        return Response(rs.get_vertex_id(lat, lon), mimetype=""application/json"")

    @app.route(""/path"")
    def path_route():
        args = request.args
        data = rs.path(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/geompath"")
    def geompath_route():
        args = request.args
        data = rs.geompath(
            lat1=float(args[""lat1""]),
            lon1=float(args[""lon1""]),
            lat2=float(args[""lat2""]),
            lon2=float(args[""lon2""]),
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)

    @app.route(""/path_retro"")
    def path_retro_route():
        args = request.args
        data = rs.path_retro(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
        )
        return Response(data, mimetype=""application/json"")

    @app.route(""/path_raw"")
    def path_raw_route():
        args = request.args
        currtime = int(args.get(""currtime"")) if args.get(""currtime"") else None
        data = rs.path_raw(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")

    @app.route(""/path_raw_retro"")
    def path_raw_retro_route():
        args = request.args
        currtime = int(args[""currtime""])
        data = rs.path_raw_retro(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")

    return app
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"    def bounds():
        cb = request.args.get(""callback"")
        data = rs.bounds(jsoncallback=cb)
        mimetype = ""application/javascript"" if cb else ""application/json""
        return Response(data, mimetype=mimetype)
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"    def path_raw_route():
        args = request.args
        currtime = int(args.get(""currtime"")) if args.get(""currtime"") else None
        data = rs.path_raw(args[""origin""], args[""dest""], currtime)
        return Response(data, mimetype=""text/plain"")
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"    def path_route():
        args = request.args
        data = rs.path(
            origin=args[""origin""],
            dest=args[""dest""],
            currtime=int(args.get(""currtime"")) if args.get(""currtime"") else None,
            time_offset=int(args.get(""time_offset"")) if args.get(""time_offset"") else None,
            transfer_penalty=int(args.get(""transfer_penalty"", 0)),
            walking_speed=float(args.get(""walking_speed"", 1.0)),
            hill_reluctance=float(args.get(""hill_reluctance"", 1.5)),
            turn_penalty=float(args.get(""turn_penalty"")) if args.get(""turn_penalty"") else None,
            walking_reluctance=float(args.get(""walking_reluctance"")) if args.get(""walking_reluctance"") else None,
            max_walk=float(args.get(""max_walk"")) if args.get(""max_walk"") else None,
            jsoncallback=args.get(""callback""),
        )
        mimetype = ""application/javascript"" if args.get(""callback"") else ""application/json""
        return Response(data, mimetype=mimetype)
",pygs/graphserver/ext/routeserver/routeserver.py,
survived,"def _get_model() -> SentenceTransformer:
    if SentenceTransformer is None:
        raise ImportError(""sentence-transformers missing"")
    global _MODEL
    if _MODEL is None:
        _MODEL = SentenceTransformer(""all-MiniLM-L6-v2"")
    return _MODEL
",src/evaluators/novelty.py,
survived,"def _non_dominated_sort(values: Sequence[Sequence[float]]) -> tuple[list[int], list[list[int]]]:
    n = len(values)
    ranks = [0] * n
    S = [set() for _ in range(n)]
    dominated = [0] * n
    for i, a in enumerate(values):
        for j, b in enumerate(values):
            if i == j:
                continue
            if all(ai <= bj for ai, bj in zip(a, b)) and any(ai < bj for ai, bj in zip(a, b)):
                S[i].add(j)
            elif all(bj <= ai for ai, bj in zip(a, b)) and any(bj < ai for ai, bj in zip(a, b)):
                dominated[i] += 1
        if dominated[i] == 0:
            ranks[i] = 0
    fronts = [[i for i, d in enumerate(dominated) if d == 0]]
    i = 0
    while i < len(fronts):
        nxt: list[int] = []
        for p in fronts[i]:
            for q in S[p]:
                dominated[q] -= 1
                if dominated[q] == 0:
                    ranks[q] = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return ranks, fronts
",src/simulation/surrogate_fitness.py,
survived,"def load_capsule_facts(base: str | Path | None = None) -> MutableMapping[str, CapsuleFacts]:
    """"""Return mapping of sector name to :class:`CapsuleFacts`.""""""

    base_path = Path(base or Path(__file__).parent)
    facts: MutableMapping[str, CapsuleFacts] = {}
    for entry in base_path.iterdir():
        if not entry.is_dir():
            continue
        yaml_path = entry / ""facts.yml""
        if not yaml_path.exists():
            continue
        try:
            data = yaml.safe_load(yaml_path.read_text(encoding=""utf-8"")) or {}
        except Exception:
            continue
        facts[entry.name] = CapsuleFacts(
            market_size=float(data.get(""market_size"", 0.0)),
            efficiency_gain=float(data.get(""efficiency_gain"", 0.0)),
            llm_score=(float(data.get(""llm_score"")) if data.get(""llm_score"") is not None else None),
        )
    return facts
",src/capsules/__init__.py,
survived,"    def score(self, facts: CapsuleFacts, efficiency_gain: float) -> float:
        """"""Return the impact score.""""""
        base = facts.market_size * efficiency_gain
        if facts.llm_score is not None:
            base *= 1.0 + self.llm_weight * facts.llm_score
        return base
",src/capsules/__init__.py,ImpactScorer
survived,"def test_cli_inline_requires_filename(capsys):
    """"""cli() should exit with an error when --inline is passed without a filename.""""""
    with pytest.raises(SystemExit) as exc:
        cli(inline_args=[""--inline""])
    captured = capsys.readouterr()
    assert captured.err.strip() == ""Error: Inline mode requires a filename""
    assert exc.value.code != 0
",tests/test_json_repair.py,
survived,"    def __init__(self) -> None:
        self.inst = object()
        self.next_ts = 0
        self.period = 1
        self.last_beat = time.time()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_backend_rest_auth.py,DummyRunner
survived,"def test_mutate_rejects_traversal(server: str) -> None:
    """"""Tarball members must not escape the extraction directory.""""""
    import io
    import tarfile

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""../evil.txt"")
        data = b""bad""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""bad.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 400",tests/test_evolution_worker.py,
survived,"            def run(self) -> None:
                pass
",tests/test_alpha_opportunity_stub.py,TestAlphaOpportunityStub.DummyRuntime
survived,"def test_env_validation_fails(env_var: str, value: str, message: str) -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    script = browser_dir / ""build"" / ""env_validate.js""
    env = os.environ.copy()
    env[env_var] = value
    res = subprocess.run(
        [""node"", str(script)],
        cwd=browser_dir,
        env=env,
        capture_output=True,
        text=True,
    )
    assert res.returncode == 1
    assert message in res.stderr",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_env_validate.py,
survived,"def _meta() -> TemplateMetadata:
    return TemplateMetadata(
        slug=""demo"",
        title=""Demo Template"",
        description=""Simple demo"",
        category=TemplateCategory.CONVERSATION,
        complexity=TemplateComplexity.BASIC,
        tags=[""demo""],
    )
",tests/test_template_creator.py,
survived,"def test_creator_register(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    creator = TemplateCreator(reg)
    path = creator.create(_meta(), ""hi {{name}}"", version=""0.1.0"")
    assert path
    assert reg.load_template(""demo"") == ""hi {{name}}""",tests/test_template_creator.py,
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = int(s.getsockname()[1])
    s.close()
    return port
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_ledger_local_validator.py,
survived,"def validator() -> str:
    port = _free_port()
    cid = subprocess.check_output(
        [
            ""docker"",
            ""run"",
            ""-d"",
            ""-p"",
            f""{port}:8899"",
            ""solanalabs/solana:edge"",
            ""solana-test-validator"",
            ""--quiet"",
        ]
    ).decode().strip()
    url = f""http://localhost:{port}""
    try:
        if not _wait_rpc(url):
            subprocess.run([""docker"", ""logs"", cid], check=False)
            raise RuntimeError(""validator not ready"")
        yield url
    finally:
        subprocess.run([""docker"", ""rm"", ""-f"", cid], check=False)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_ledger_local_validator.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q15.py,Supplier
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q8.py,Nation
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q15.py,Lineitem
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q14.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q7.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Supplier
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q2.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q20.py,Partsupp
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q4.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q18.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q17.py,Part
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q12.py,Order
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-h/compiler/py/q5.py,Region
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto5
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q27.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto5
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q28.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q1.py,Auto7
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q15.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto13
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto11
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto11
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q18.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q33.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q23.py,Auto7
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q9.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q20.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q17.py,Auto5
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto9
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto6
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q4.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q22.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q25.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q6.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q28.py,Auto10
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q24.py,Auto9
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q5.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto4
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q16.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q3.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q31.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q19.py,Auto6
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q29.py,Auto4
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q30.py,Auto8
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q21.py,Auto8
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q15.py,Auto7
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q13.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q12.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q11.py,Auto10
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/job/compiler/py/q2.py,Auto5
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/job/compiler/py/q26.py,Auto6
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q22.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Auto1
survived,"def _q4():
    _src = customer
    _rows = _query(
        _src,
        [
            {
                ""items"": web_sales,
                ""on"": lambda c, ws: c.c_customer_sk == ws.ws_bill_customer_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda c, ws, d: ws.ws_sold_date_sk == d.d_date_sk,
            },
        ],
        {""select"": lambda c, ws, d: (c, ws, d)},
    )
    _groups = _group_by(
        _rows,
        lambda c, ws, d: Auto3(
            id=c.c_customer_id,
            first=c.c_first_name,
            last=c.c_last_name,
            login=c.c_login,
            year=d.d_year,
        ),
    )
    _items5 = _groups
    return [
        Auto2(
            customer_id=g.key[""id""],
            customer_first_name=g.key[""first""],
            customer_last_name=g.key[""last""],
            customer_login=g.key[""login""],
            dyear=g.key[""year""],
            year_total=_sum(
                [
                    (
                        x[1].ws_ext_list_price
                        - x[1].ws_ext_wholesale_cost
                        - x[1].ws_ext_discount_amt
                        + x[1].ws_ext_sales_price
                    )
                    / 2
                    for x in g
                ]
            ),
            sale_type=""w"",
        )
        for g in _items5
    ]
",tests/dataset/tpc-ds/compiler/py/q4.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q82.py,Item
survived,"def test_TPCDS_Q32_simplified():
    assert result == 20.0
",tests/dataset/tpc-ds/compiler/py/q32.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q73.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogReturn
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,DateDim
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q70.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,StoreSale
survived,"def distinct(xs):
    out = []
    for x in xs:
        if not x in out:
            out = out + [x]
    return out
",tests/dataset/tpc-ds/compiler/py/q95.py,
survived,"def abs(x):
    if x >= 0.0:
        return x
    return -x
",tests/dataset/tpc-ds/compiler/py/q57.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q20.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q67.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Store
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q93.py,StoreReturn
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q18.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q99.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q17.py,Item
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q35.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q29.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q6.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,IncomeBand
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q90.py,
survived,"def test_TPCDS_Q74_simplified():
    assert result == [
        Auto1(customer_id=1, customer_first_name=""Alice"", customer_last_name=""Smith"")
    ]
",tests/dataset/tpc-ds/compiler/py/q74.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,CustomerDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Auto1
survived,"def _q0():
    _src = store_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": item,
                ""on"": lambda ss, i: ss.item == i.i_item_sk and i.i_manager_id == 1,
            },
            {""items"": date_dim, ""on"": lambda ss, i, d: ss.sold_date == d.d_date_sk},
        ],
        {""select"": lambda ss, i, d: (ss, i, d)},
    )
    _groups = _group_by(_rows, lambda ss, i, d: Auto2(brand_id=i.i_brand_id))
    _items1 = _groups
    return [
        Auto1(brand_id=g.key[""brand_id""], ext_price=sum([x[0].price for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q55.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q64.py,StoreReturn
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogReturn
survived,"def _q0():
    _groups = {}
    _order = []
    for a in active:
        _k = Auto2(
            gender=a[""cd_gender""],
            marital=a[""cd_marital_status""],
            education=a[""cd_education_status""],
            purchase=a[""cd_purchase_estimate""],
            credit=a[""cd_credit_rating""],
            dep=a[""cd_dep_count""],
            depemp=a[""cd_dep_employed_count""],
            depcol=a[""cd_dep_college_count""],
        )
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(a)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            cd_gender=g.key[""gender""],
            cd_marital_status=g.key[""marital""],
            cd_education_status=g.key[""education""],
            cnt1=len([_ for _ in g]),
            cd_purchase_estimate=g.key[""purchase""],
            cnt2=len([_ for _ in g]),
            cd_credit_rating=g.key[""credit""],
            cnt3=len([_ for _ in g]),
            cd_dep_count=g.key[""dep""],
            cnt4=len([_ for _ in g]),
            cd_dep_employed_count=g.key[""depemp""],
            cnt5=len([_ for _ in g]),
            cd_dep_college_count=g.key[""depcol""],
            cnt6=len([_ for _ in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q10.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q36.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,CallCenter
survived,"def _q2():
    _src = inventory
    _rows = _query(
        _src,
        [{""items"": date_dim, ""on"": lambda inv, d: inv.inv_date_sk == d.d_date_sk}],
        {
            ""select"": lambda inv, d: (inv, d),
            ""where"": lambda inv, d: d.d_date >= ""2000-03-15"",
        },
    )
    _groups = _group_by(
        _rows, lambda inv, d: Auto3(w=inv.inv_warehouse_sk, i=inv.inv_item_sk)
    )
    _items3 = _groups
    return [
        Auto2(
            w=g.key[""w""], i=g.key[""i""], qty=sum([x[0].inv_quantity_on_hand for x in g])
        )
        for g in _items3
    ]
",tests/dataset/tpc-ds/compiler/py/q21.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q69.py,WebSale
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q29.py,
survived,"def _q0():
    _src = catalog_sales
    _rows = _query(
        _src,
        [
            {
                ""items"": customer,
                ""on"": lambda cs, c: cs.cs_bill_customer_sk == c.c_customer_sk,
            },
            {
                ""items"": customer_address,
                ""on"": lambda cs, c, ca: c.c_current_addr_sk == ca.ca_address_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda cs, c, ca, d: cs.cs_sold_date_sk == d.d_date_sk,
            },
        ],
        {
            ""select"": lambda cs, c, ca, d: (cs, c, ca, d),
            ""where"": lambda cs, c, ca, d: (
                (
                    (
                        ca.ca_zip[0:5]
                        in [
                            ""85669"",
                            ""86197"",
                            ""88274"",
                            ""83405"",
                            ""86475"",
                            ""85392"",
                            ""85460"",
                            ""80348"",
                            ""81792"",
                        ]
                        or ca.ca_state in [""CA"", ""WA"", ""GA""]
                    )
                    or cs.cs_sales_price > 500
                )
                and d.d_qoy == 1
            )
            and d.d_year == 2000,
        },
    )
    _groups = _group_by(_rows, lambda cs, c, ca, d: Auto2(zip=ca.ca_zip))
    _items1 = _groups
    _items1 = sorted(_items1, key=lambda g: g.key[""zip""])
    return [
        Auto1(ca_zip=g.key[""zip""], sum_sales=sum([x[0].cs_sales_price for x in g]))
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q15.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q36.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Auto1
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q70.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q94.py,WebSite
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q62.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q71.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,HouseholdDemographics
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q4.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,WebSale
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q43.py,_Group
survived,"def test_TPCDS_Q37_simplified():
    assert result == [Auto1(i_item_id=""I1"", i_item_desc=""Item1"", i_current_price=30.0)]
",tests/dataset/tpc-ds/compiler/py/q37.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q51.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q84.py,StoreReturn
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q54.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q52.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,Store
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,WebSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q99.py,Warehouse
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q34.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q92.py,DateDim
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q33.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q50.py,_Group
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q63.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q18.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q24.py,CustomerAddress
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q89.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q11.py,StoreSale
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q74.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q48.py,StoreSale
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q10.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q77.py,
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q71.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q35.py,Auto1
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q51.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,CatalogSale
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q30.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q50.py,Auto2
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q95.py,WebSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q12.py,DateDim
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q42.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Auto3
survived,"def test_TPCDS_Q50_simplified():
    assert result == [
        Auto1(s_store_name=""Main"", d30=1, d31_60=1, d61_90=1, d91_120=1, d_gt_120=1)
    ]
",tests/dataset/tpc-ds/compiler/py/q50.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q51.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q87.py,CatalogSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q3.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q16.py,Auto1
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,CustomerAddres
survived,"def _q0():
    _src = sales_detail
    _rows = _query(
        _src,
        [
            {
                ""items"": item,
                ""on"": lambda sd, i: i.i_item_sk
                == (
                    sd.get(""i_item_sk"")
                    if isinstance(sd, dict)
                    else getattr(sd, ""i_item_sk"")
                ),
            }
        ],
        {
            ""select"": lambda sd, i: (sd, i),
            ""where"": lambda sd, i: i.i_category == ""Electronics"",
        },
    )
    _groups = _group_by(
        _rows,
        lambda sd, i: Auto4(
            year=sd.get(""d_year"") if isinstance(sd, dict) else getattr(sd, ""d_year""),
            brand_id=i.i_brand_id,
            class_id=i.i_class_id,
            category_id=i.i_category_id,
            manuf_id=i.i_manufact_id,
        ),
    )
    _items1 = _groups
    return [
        Auto3(
            d_year=g.key[""year""],
            i_brand_id=g.key[""brand_id""],
            i_class_id=g.key[""class_id""],
            i_category_id=g.key[""category_id""],
            i_manufact_id=g.key[""manuf_id""],
            sales_cnt=_sum(
                [
                    (
                        x[0].get(""quantity"")
                        if isinstance(x[0], dict)
                        else getattr(x[0], ""quantity"")
                    )
                    for x in g
                ]
            ),
            sales_amt=_sum(
                [
                    (
                        x[0].get(""amount"")
                        if isinstance(x[0], dict)
                        else getattr(x[0], ""amount"")
                    )
                    for x in g
                ]
            ),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q75.py,
survived,"def test_TPCDS_Q7_result():
    assert result == [Auto1(i_item_id=""I1"", agg1=5.0, agg2=10.0, agg3=2.0, agg4=8.0)]
",tests/dataset/tpc-ds/compiler/py/q7.py,
survived,"def _avg(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""avg() expects list or group"")
    if not v:
        return 0
    s = 0.0
    for it in v:
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""avg() expects numbers"")
    return s / len(v)
",tests/dataset/tpc-ds/compiler/py/q13.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q20.py,CatalogSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q4.py,Customer
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q13.py,
survived,"def test_TPCDS_Q57_simplified():
    assert result == []
",tests/dataset/tpc-ds/compiler/py/q57.py,
survived,"def test_TPCDS_Q91_returns():
    assert result == Auto1(
        Call_Center=""CC1"", Call_Center_Name=""Main"", Manager=""Alice"", Returns_Loss=10.0
    )
",tests/dataset/tpc-ds/compiler/py/q91.py,
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q79.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q43.py,DateDim
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/tpc-ds/compiler/py/q42.py,
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q74.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q96.py,TimeDim
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q79.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q18.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q7.py,StoreSale
survived,"def test_TPCDS_Q48_simplified():
    assert result == 35
",tests/dataset/tpc-ds/compiler/py/q48.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,Customer
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,Auto1
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q91.py,Auto2
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q46.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q83.py,SrItem
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q78.py,S
survived,"def _q2():
    _src = customer
    _rows = _query(
        _src,
        [
            {
                ""items"": catalog_sales,
                ""on"": lambda c, cs: c.c_customer_sk == cs.cs_bill_customer_sk,
            },
            {
                ""items"": date_dim,
                ""on"": lambda c, cs, d: cs.cs_sold_date_sk == d.d_date_sk,
            },
        ],
        {""select"": lambda c, cs, d: (c, cs, d)},
    )
    _groups = _group_by(
        _rows,
        lambda c, cs, d: Auto3(
            id=c.c_customer_id,
            first=c.c_first_name,
            last=c.c_last_name,
            login=c.c_login,
            year=d.d_year,
        ),
    )
    _items3 = _groups
    return [
        Auto2(
            customer_id=g.key[""id""],
            customer_first_name=g.key[""first""],
            customer_last_name=g.key[""last""],
            customer_login=g.key[""login""],
            dyear=g.key[""year""],
            year_total=_sum(
                [
                    (
                        x[1].cs_ext_list_price
                        - x[1].cs_ext_wholesale_cost
                        - x[1].cs_ext_discount_amt
                        + x[1].cs_ext_sales_price
                    )
                    / 2
                    for x in g
                ]
            ),
            sale_type=""c"",
        )
        for g in _items3
    ]
",tests/dataset/tpc-ds/compiler/py/q4.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q36.py,Auto1
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q56.py,_Group
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q3.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q57.py,Auto5
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q13.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q77.py,WebReturn
survived,"    def __init__(self, key: K):
        self.key = key
        self.Items: list[T] = []
        self.items = self.Items
",tests/dataset/tpc-ds/compiler/py/q34.py,_Group
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q3.py,Auto2
survived,"def _q0():
    _groups = {}
    _order = []
    for r in records:
        _k = Auto3(
            d_year=r.d_year, i_category_id=r.i_category_id, i_category=r.i_category
        )
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(r)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto1(
            d_year=g.key[""d_year""],
            i_category_id=g.key[""i_category_id""],
            i_category=g.key[""i_category""],
            sum_ss_ext_sales_price=sum([x.price for x in g]),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q42.py,
survived,"def test_TPCDS_Q78_simplified():
    assert result == [
        Auto1(
            ss_sold_year=1998,
            ss_item_sk=1,
            ss_customer_sk=1,
            ratio=1.25,
            store_qty=10,
            store_wholesale_cost=50.0,
            store_sales_price=100.0,
            other_chan_qty=8,
            other_chan_wholesale_cost=40.0,
            other_chan_sales_price=80.0,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q78.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q15.py,Customer
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q55.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q76.py,Auto3
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q70.py,Auto2
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q72.py,
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q25.py,
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q95.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q88.py,TimeDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,Inventory
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q72.py,HouseholdDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q1.py,Store
survived,"def _group_by(src: list[T], keyfn: Callable[[T], K]) -> list[_Group[K, T]]:
    groups: dict[str, _Group[K, T]] = {}
    order: list[str] = []
    for it in src:
        if isinstance(it, (list, tuple)):
            key = keyfn(*it)
        else:
            key = keyfn(it)
        if isinstance(key, dict):
            import types

            key = types.SimpleNamespace(**key)
        ks = str(key)
        g = groups.get(ks)
        if not g:
            g = _Group(key)
            groups[ks] = g
            order.append(ks)
        g.Items.append(it)
    return [groups[k] for k in order]
",tests/dataset/tpc-ds/compiler/py/q26.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q41.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,DateDim
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q33.py,StoreSale
survived,"def _count(v):
    if isinstance(v, list):
        return len(v)
    if hasattr(v, ""Items""):
        return len(v.Items)
    raise Exception(""count() expects list or group"")
",tests/dataset/tpc-ds/compiler/py/q34.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q54.py,CustomerAddres
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q37.py,DateDim
survived,"def test_TPCDS_Q10_demographics_count():
    assert result == [
        Auto1(
            cd_gender=""F"",
            cd_marital_status=""M"",
            cd_education_status=""College"",
            cnt1=1,
            cd_purchase_estimate=5000,
            cnt2=1,
            cd_credit_rating=""Good"",
            cnt3=1,
            cd_dep_count=1,
            cnt4=1,
            cd_dep_employed_count=1,
            cnt5=1,
            cd_dep_college_count=0,
            cnt6=1,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q10.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,Item
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/dataset/tpc-ds/compiler/py/q53.py,_Group
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q21.py,DateDim
survived,"def _sum(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""sum() expects list or group"")
    s = 0.0
    for it in v:
        if it is None:
            continue
        if isinstance(it, (int, float)):
            s += float(it)
        else:
            raise Exception(""sum() expects numbers"")
    return s
",tests/dataset/tpc-ds/compiler/py/q45.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q23.py,StoreSale
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q10.py,CustomerDemographic
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q29.py,CatalogSale
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-ds/compiler/py/q8.py,_Group
survived,"def _q2():
    _groups = {}
    _order = []
    for g in grouped:
        _k = Auto4(cat=g.cat, call=g.call)
        _ks = str(_k)
        g = _groups.get(_ks)
        if not g:
            g = _Group(_k)
            _groups[_ks] = g
            _order.append(_ks)
        g.Items.append(g)
    _items1 = [_groups[k] for k in _order]
    return [
        Auto3(
            cat=gg.key[""cat""],
            call=gg.key[""call""],
            avg_sales=(
                sum([x.sum_sales for x in gg]) / len([x.sum_sales for x in gg])
                if [x.sum_sales for x in gg]
                else 0
            ),
        )
        for gg in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q57.py,
survived,"def _query(src, joins, opts):
    items = [[v] for v in src]
    for j in joins:
        joined = []
        if j.get(""right"") and j.get(""left""):
            matched = [False] * len(j[""items""])
            for left in items:
                m = False
                for ri, right in enumerate(j[""items""]):
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    matched[ri] = True
                    joined.append(left + [right])
                if not m:
                    joined.append(left + [None])
            for ri, right in enumerate(j[""items""]):
                if not matched[ri]:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        elif j.get(""right""):
            for right in j[""items""]:
                m = False
                for left in items:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if not m:
                    undef = [None] * (len(items[0]) if items else 0)
                    joined.append(undef + [right])
        else:
            for left in items:
                m = False
                for right in j[""items""]:
                    keep = True
                    if j.get(""on""):
                        keep = j[""on""](*left, right)
                    if not keep:
                        continue
                    m = True
                    joined.append(left + [right])
                if j.get(""left"") and (not m):
                    joined.append(left + [None])
        items = joined
    if opts.get(""where""):
        items = [r for r in items if opts[""where""](*r)]
    if opts.get(""sortKey""):

        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k

        items.sort(key=_key)
    if ""skip"" in opts:
        n = opts[""skip""]
        if n < 0:
            n = 0
        items = items[n:] if n < len(items) else []
    if ""take"" in opts:
        n = opts[""take""]
        if n < 0:
            n = 0
        items = items[:n] if n < len(items) else items
    res = []
    for r in items:
        res.append(opts[""select""](*r))
    return res
",tests/dataset/tpc-ds/compiler/py/q71.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q27.py,CustomerDemographic
survived,"def test_TPCDS_Q34_simplified():
    assert result == [
        Auto1(
            c_last_name=""Smith"",
            c_first_name=""John"",
            c_salutation=""Mr."",
            c_preferred_cust_flag=""Y"",
            ss_ticket_number=1,
            cnt=16,
        )
    ]
",tests/dataset/tpc-ds/compiler/py/q34.py,
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q19.py,Item
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q55.py,StoreSale
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q25.py,Auto2
survived,"def _q0():
    _src = customer
    _rows = _query(
        _src,
        [
            {
                ""items"": customer_address,
                ""on"": lambda c, ca: c.c_current_addr_sk == ca.ca_address_sk,
            },
            {
                ""items"": customer_demographics,
                ""on"": lambda c, ca, cd: c.c_current_cdemo_sk == cd.cd_demo_sk,
            },
        ],
        {
            ""select"": lambda c, ca, cd: (c, ca, cd),
            ""where"": lambda c, ca, cd: c.c_customer_sk in purchased,
        },
    )
    _groups = _group_by(
        _rows,
        lambda c, ca, cd: Auto2(
            state=ca.ca_state,
            gender=cd.cd_gender,
            marital=cd.cd_marital_status,
            dep=cd.cd_dep_count,
            emp=cd.cd_dep_employed_count,
            col=cd.cd_dep_college_count,
        ),
    )
    _items1 = _groups
    return [
        Auto1(
            ca_state=g.key[""state""],
            cd_gender=g.key[""gender""],
            cd_marital_status=g.key[""marital""],
            cd_dep_count=g.key[""dep""],
            cd_dep_employed_count=g.key[""emp""],
            cd_dep_college_count=g.key[""col""],
            cnt=len(g),
        )
        for g in _items1
    ]
",tests/dataset/tpc-ds/compiler/py/q35.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q58.py,Auto3
survived,"    def __contains__(self, key):
        return hasattr(self, key)
",tests/dataset/tpc-ds/compiler/py/q13.py,Store
survived,"def population_df(pop: list[Any]) -> pd.DataFrame:
    """"""Return a DataFrame for effectiveness vs. risk vs. complexity.""""""

    return pd.DataFrame(
        {
            ""effectiveness"": [p.fitness[0] for p in pop],
            ""risk"": [p.fitness[1] for p in pop],
            ""complexity"": [p.fitness[2] for p in pop],
            ""rank"": [p.rank for p in pop],
        }
    )
",src/interface/web_app.py,
survived,"    def __init__(self):
        self.completions = _ChatCompletions()
",openai/__init__.py,_Chat
survived,"    def _run_tests(self, errors: List[str]) -> None:
        env = os.environ.copy()
        env[""PYTHONPATH""] = str(self.bundle_dir)
        result = subprocess.run(
            [""pytest"", ""tests"", ""-c"", ""/dev/null"", ""-x""],
            cwd=self.bundle_dir,
            capture_output=True,
            text=True,
            env=env,
        )
        if result.returncode != 0:
            errors.append(""tests failed"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def _validate_agent(self, errors: List[str]) -> None:
        try:
            py_compile.compile(str(self.bundle_dir / ""agent.py""), doraise=True)
        except py_compile.PyCompileError as exc:
            errors.append(f""agent.py failed to compile: {exc.msg}"")
",src/meta_agent/bundle_validator.py,BundleValidator
survived,"    def create_response(
        self,
        response_data: GraphQLHTTPResponse,
        sub_response: Response,
        is_strict: bool,
    ) -> Response:
        sub_response.text = self.encode_json(response_data)
        sub_response.content_type = (
            ""application/graphql-response+json"" if is_strict else ""application/json""
        )
        return sub_response
",src/graphql_server/webob/views.py,GraphQLView
survived,"    def render_graphql_ide(
        self, request: Request, request_data: GraphQLRequestData
    ) -> Response:
        return Response(
            text=request_data.to_template_string(self.graphql_ide_html),
            content_type=""text/html"",
            status=200,
        )
",src/graphql_server/webob/views.py,GraphQLView
survived,"    async def start(self) -> None:
        """"""Start heartbeat and regression checks.""""""
        await self.manager.start()
",alpha_factory_v1/backend/agent_scheduler.py,AgentScheduler
survived,"    def __init__(self, ledger: Ledger, *, rng: random.Random | None = None) -> None:
        self.ledger = ledger
        self._rng = rng or random.Random()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/llm_mutator.py,LLMMutator
survived,"def test_query_speed_and_histogram(tmp_path) -> None:
    arch = SolutionArchive(tmp_path / ""sol.duckdb"")
    for i in range(10000):
        arch.add(""sec"", ""app"", float(i % 100), {""i"": i})
    start = time.perf_counter()
    res = arch.query(sector=""sec"")
    duration = time.perf_counter() - start
    assert len(res) == 10000
    assert duration < 0.2
    hist = arch.diversity_histogram()
    assert hist[(""sec"", ""app"")] == 10000",tests/test_solution_archive.py,
survived,"    def __init__(self, path: str | Path) -> None:
        self.path = Path(path)
        if duckdb is not None:
            self.conn = duckdb.connect(str(self.path))
        else:  # pragma: no cover - fallback
            self.conn = sqlite3.connect(str(self.path))
        self._ensure()
",src/archive/solution_archive.py,SolutionArchive
survived,"    def __init__(self, repo: str | Path, log_dir: str | Path, registry: StakeRegistry | None = None) -> None:
        self.repo = Path(repo)
        self.log_dir = Path(log_dir)
        self.registry = registry or StakeRegistry()
        if ""meta"" not in self.registry.stakes:
            self.registry.set_stake(""meta"", 1.0)
",src/agents/meta_refinement_agent.py,MetaRefinementAgent
survived,"    def _create_patch(self, bottleneck: str) -> str:
        goal = f""optimise around {bottleneck}""
        metric = self.repo / ""metric.txt""
        if metric.exists():
            try:
                current = int(float(metric.read_text().strip()))
            except Exception:
                current = 0
            new_val = current + 1
            diff = (
                ""--- a/metric.txt\n""
                ""+++ b/metric.txt\n""
                ""@@\n""
                f""-{current}\n""
                f""+{new_val}\n""
            )
            return diff
        return propose_diff(str(metric), goal)
",src/agents/meta_refinement_agent.py,MetaRefinementAgent
survived,"def test_mutate_rejects_traversal(server: str) -> None:
    import io
    import tarfile

    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode=""w"") as tf:
        info = tarfile.TarInfo(name=""../evil.txt"")
        data = b""bad""
        info.size = len(data)
        tf.addfile(info, io.BytesIO(data))
    buf.seek(0)

    with httpx.Client(base_url=server) as client:
        files = {""tar"": (""bad.tar"", buf.read())}
        r = client.post(""/mutate"", files=files)
        assert r.status_code == 400",tests/test_evolution_worker_safe_extract.py,
survived,"    def run(self) -> None:
        pass
",tests/resources/openai_agents.py,AgentRuntime
survived,"def add(a: int, b: int) -> int:
    return a + b
",tests/human/python/fun_call.py,
survived,"def test_capability_growth_curves() -> None:
    """"""Capability growth curves should map time into [0,1].""""""
    t = 0.5
    linear = forecast.capability_growth(t, curve=""linear"")
    logistic = forecast.capability_growth(t, curve=""logistic"")
    exponential = forecast.capability_growth(t, curve=""exponential"")
    assert linear == pytest.approx(forecast.linear_curve(t))
    assert logistic == pytest.approx(forecast.logistic_curve(10 * t))
    assert exponential == pytest.approx(forecast.exponential_curve(t))
    assert logistic > linear > exponential
    assert 0.0 <= exponential <= 1.0
    assert 0.0 <= linear <= 1.0
    assert 0.0 <= logistic <= 1.0
",tests/test_forecast.py,
survived,"    def test_create_parse_plan_invalid_segment(self):
        with self.assertRaises(ParseException) as cm:
            hl7.parser.create_parse_plan(""PID|^~\\&|GHH LAB"")
        self.assertIn(""must be one of MSH, FHS or BHS"", cm.exception.args[0])",tests/test_parse.py,ParsePlanTest
survived,"def test_index_html_has_closing_tags() -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    html = (browser_dir / ""dist"" / ""index.html"").read_text().splitlines()
    assert html[-2].strip() == ""</body>""
    assert html[-1].strip() == ""</html>""
    joined = ""\n"".join(html)
    assert ""window.PINNER_TOKEN"" in joined",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_closing_tags.py,
survived,"    def setUp(self):
        os.environ[""VECTOR_STORE_USE_SQLITE""] = ""true""
        os.environ.pop(""PGHOST"", None)
        self.fabric = memf.MemoryFabric()
        memf._MET_V_SRCH = None
",tests/test_memory_fabric_sqlite.py,TestMemoryFabricSQLiteClose
survived,"    def test_search_text_glob_with_special_chars(self):
        """"""Glob patterns containing regex special characters should match literally.""""""
        content = """"""
        def func_square():
            print(""value[42]"")

        def func_curly():
            print(""value{bar}"")
        """"""

        matches_square = search_text(r""*\[42\]*"", content=content, is_glob=True)
        assert len(matches_square) == 1
        assert ""[42]"" in matches_square[0].lines[0].line_content

        matches_curly = search_text(""*{bar}*"", content=content, is_glob=True)
        assert len(matches_curly) == 1
        assert ""{bar}"" in matches_curly[0].lines[0].line_content
",test/serena/test_text_utils.py,TestSearchText
survived,"def test_skip_backup_when_worker_has_no_space(tmp_path):
    db_path = tmp_path / ""db.sqlite""
    config[""storage""][""database""] = str(db_path)

    conn = sqlite3.connect(db_path)
    conn.execute(""CREATE TABLE t(id INTEGER)"")
    conn.commit()
    conn.close()

    output = tmp_path / ""backup.sqlite""

    with (
        patch(
            ""pioreactor.actions.leader.backup_database.long_running_managed_lifecycle"",
            dummy_lifecycle,
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.create_logger"",
            return_value=MagicMock(),
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.get_active_workers_in_inventory"",
            return_value=[""worker1""],
        ),
        patch(
            ""pioreactor.actions.leader.backup_database._remote_available_space"",
            return_value=0,
        ),
        patch(
            ""pioreactor.actions.leader.backup_database.rsync"",
        ) as mock_rsync,
    ):
        backup_database(str(output), force=True, backup_to_workers=1)
        mock_rsync.assert_not_called()",pioreactor/tests/test_backup_database.py,
survived,"def test_tracehub_broadcast():
    event = asyncio.run(_run_broadcast())
    assert event[""label""] == ""hi""
    assert event[""type""] == ""tool_call""
",tests/test_trace_hub.py,
survived,"def _require_openai_agents() -> None:
    """"""Ensure the ``openai_agents`` package is available.

    Attempts an automatic install via :mod:`check_env` when the package is
    missing so the bridge remains usable in fresh environments or Colab
    runtimes. Any installation errors are surfaced to the user.
    """"""

    try:  # soft dependency
        import openai_agents  # type: ignore
    except ModuleNotFoundError:  # pragma: no cover - optional dep
        try:
            import check_env

            print(""â„¹ï¸  openai_agents missing â€“ attempting auto-installâ€¦"")
            check_env.main([""--auto-install""])
        except Exception as exc:  # pragma: no cover - install failed
            sys.stderr.write(
                f""\nâŒ  openai_agents not installed and auto-install failed: {exc}\n""
            )
            sys.stderr.write(""   Install manually with 'pip install openai-agents'\n"")
            sys.exit(1)
        try:
            import openai_agents  # type: ignore  # noqa: F401
        except ModuleNotFoundError:
            sys.stderr.write(
                ""\nâŒ  openai_agents still missing after auto-install.\n""
            )
            sys.stderr.write(""   Install manually with 'pip install openai-agents'\n"")
            sys.exit(1)
",alpha_factory_v1/demos/alpha_agi_business_v1/openai_agents_bridge.py,
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = CrossIndustryAgent()
    runtime.register(agent)
    try:
        from alpha_factory_v1.backend.adk_bridge import auto_register, maybe_launch

        auto_register([agent])
        maybe_launch()
    except Exception as exc:  # pragma: no cover - ADK optional
        print(f""ADK bridge unavailable: {exc}"")

    print(""Registered CrossIndustryAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,
survived,"def _read_log(limit: int) -> List[Dict[str, str]]:
    path = _ledger_path(None)
    try:
        data = json.loads(Path(path).read_text())
        if isinstance(data, dict):
            data = [data]
        return data[-limit:]
    except Exception:  # pragma: no cover - missing or invalid log
        return []
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,
survived,"    def test_cross_industry_bridge_compiles(self):
        """"""Ensure the cross-industry demo bridge compiles.""""""
        path = Path('alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py')
        py_compile.compile(path, doraise=True)
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"    def setUp(self) -> None:
        self.temp_files: list[Path] = []
        self.env_vars: dict[str, str] = {}
",tests/test_alpha_opportunity_env.py,TestAlphaOpportunityEnv
survived,"def _start_server(directory: Path):
    handler = partial(http.server.SimpleHTTPRequestHandler, directory=str(directory))
    server = http.server.ThreadingHTTPServer((""localhost"", 0), handler)
    thread = threading.Thread(target=server.serve_forever, daemon=True)
    thread.start()
    return server, thread
",tests/test_pwa_offline.py,
survived,"def test_stream_options_injected_for_openai_base_url_sync() -> None:
    captured = {}

    def dummy_fn(completion, **kwargs):
        captured.update(kwargs)
        return ""ok""

    wrapped = create_wrapper_sync(OpSettings())(dummy_fn)

    wrapped(DummyCompletion(""https://api.openai.com""), stream=True)

    assert captured.get(""stream_options"") == {""include_usage"": True}
",tests/integrations/openai/test_openai_sdk.py,
survived,"def test_dslice_oob_read_and_write():
    Seq = hax.Axis(""seq"", 5)
    from haliax import ds

    arr = hax.arange((Seq,), dtype=int)
    out = arr[{""seq"": ds(3, 4)}]
    ref = jnp.take(arr.array, jnp.arange(3, 7), mode=""fill"", fill_value=0)
    assert jnp.array_equal(out.array, ref)

    upd = hax.arange((Seq.resize(4),), dtype=int)
    updated = arr.at[{""seq"": ds(3, 4)}].set(upd)
    ref_upd = arr.array.at[jnp.arange(3, 7)].set(upd.array, mode=""drop"")
    assert jnp.array_equal(updated.array, ref_upd)
",tests/core_test.py,
survived,"def test_improve_repo_cleanup(tmp_path: Path) -> None:
    repo_dir = tmp_path / ""repo""
    repo_dir.mkdir()
    _init_repo(repo_dir)

    patch = """"""--- a/metric.txt\n+++ b/metric.txt\n@@\n-1\n+2\n""""""
    patch_file = tmp_path / ""patch.diff""
    patch_file.write_text(patch)
    log_file = tmp_path / ""log.json""

    delta, clone = self_improver.improve_repo(
        str(repo_dir), str(patch_file), ""metric.txt"", str(log_file), cleanup=True
    )

    assert delta == 1
    assert not clone.exists()
",tests/test_self_improver.py,
survived,"async def file_rename_on_frame(frame_id: int, src: str, dst: str, timeout: int = 60):
    """"""Rename a file or directory on the frame via agent.""""""
    payload = {
        ""type"": ""cmd"",
        ""name"": ""file_rename"",
        ""args"": {""src"": src, ""dst"": dst},
    }
    fut, _ = queue_command(frame_id, payload)
    return await asyncio.wait_for(fut, timeout=timeout)
",backend/app/ws/agent_ws.py,
survived,"def _rich_table(headers: Iterable[str], rows: Iterable[Iterable[Any]]) -> None:
    if console and Table:
        table = Table(show_header=True, header_style=""bold cyan"")
        for h in headers:
            table.add_column(str(h))
        for row in rows:
            table.add_row(*[str(v) for v in row])
        console.print(table)
    else:
        click.echo(_plain_table(headers, rows))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def slash(self, agent_id: str) -> None:
        """"""Burn 10% of ``agent_id`` stake.""""""
        self.registry.burn(agent_id, 0.1)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,Orchestrator
survived,"    def total(self) -> float:
        """"""Return total stake across all agents.""""""
        return float(sum(self.stakes.values()))
",src/governance/stake_registry.py,StakeRegistry
survived,"def test_archive_migration(TestArchiveMigration) -> None:
    entries = [
        {""hash"": ""a"", ""parent"": None, ""score"": 0.3, ""novelty"": 0.1, ""is_live"": True, ""ts"": 1.0},
        {""hash"": ""b"", ""parent"": ""a"", ""score"": 0.4, ""novelty"": 0.2, ""is_live"": False, ""ts"": 2.0},
    ]
    db = TestArchiveMigration(entries)
    assert db.get(""a"") is not None
    assert db.get(""b"").parent == ""a""",tests/test_archive.py,
survived,"    def get(self, h: str) -> ArchiveEntry | None:
        with Session(self.engine) as session:
            row = session.get(_ArchiveRow, h)
            if row is None:
                return None
            return ArchiveEntry(
                hash=row.hash,
                parent=row.parent,
                score=row.score,
                novelty=row.novelty,
                is_live=row.is_live,
                ts=row.ts,
            )
",src/archive/db.py,ArchiveDB
survived,"def select_parent(population: Sequence[Any], temp: float) -> Any:
    """"""Return a candidate chosen via softmax of ``fitness * novelty``.

    Args:
        population: Sequence of candidates exposing ``fitness`` and ``novelty`` attributes.
        temp: Softmax temperature. Higher values yield a more uniform distribution.

    Returns:
        The selected candidate from ``population``.
    """"""
    if not population:
        raise ValueError(""population is empty"")
    if temp <= 0:
        raise ValueError(""temp must be positive"")

    scores = np.asarray([float(getattr(ind, ""fitness"")) * float(getattr(ind, ""novelty"")) for ind in population])
    logits = scores / temp
    weights = np.exp(logits - np.max(logits))
    probs = weights / weights.sum()

    index = int(np.random.choice(len(population), p=probs))
    return population[index]",src/archive/selector.py,
survived,"def secure_run(cmd: Sequence[str]) -> subprocess.CompletedProcess[str]:
    """"""Execute ``cmd`` under ``firejail`` or ``docker`` constraints.

    The sandbox runs with seccomp, ``2`` CPU cores, ``2``Â GB of RAM and a
    ``120``Â second timeout. When the command exceeds the timeout a
    :class:`SandboxTimeout` is raised.
    """"""

    timeout = 120
    firejail = shutil.which(""firejail"")
    if firejail:
        full_cmd = [
            firejail,
            ""--quiet"",
            ""--net=none"",
            ""--private"",
            ""--seccomp"",
            ""--rlimit-as=2147483648"",
            ""--rlimit-cpu=120"",
            *cmd,
        ]
    else:
        docker = shutil.which(""docker"")
        if docker:
            full_cmd = [
                docker,
                ""run"",
                ""--rm"",
                ""--network=none"",
                ""--cpus=2"",
                ""--memory=2g"",
                ""--security-opt"",
                ""seccomp=unconfined"",
                ""python:3.11-slim"",
                *cmd,
            ]
        else:
            full_cmd = list(cmd)
    try:
        return subprocess.run(
            full_cmd,
            text=True,
            capture_output=True,
            timeout=timeout,
        )
    except subprocess.TimeoutExpired as exc:  # pragma: no cover - runtime failure
        raise SandboxTimeout(str(exc)) from exc",src/utils/secure_run.py,
survived,"def test_detect_backtrack(tmp_path) -> None:
    db_path = tmp_path / ""arch.db""
    db = ArchiveDB(db_path)
    db.add(ArchiveEntry(""a"", None, 0.5, 0.0, True, 0.0))
    db.add(ArchiveEntry(""b"", ""a"", 0.6, 0.0, True, 1.0))
    db.add(ArchiveEntry(""c"", ""b"", 0.4, 0.0, True, 2.0))
    counts = ab.count_backtracks(db_path)
    assert any(c > 0 for c in counts)",tests/test_analyse_backtrack.py,
survived,"def test_suspicious_output_logs(monkeypatch, tmp_path, caplog):
    fake_client = MagicMock()
    fake_client.ping.return_value = None
    container = MagicMock()
    container.wait.return_value = {""StatusCode"": 0}
    container.logs.side_effect = [b""Traceback error"", b""""]
    fake_client.containers.run.return_value = container

    monkeypatch.setattr(sm.docker, ""from_env"", lambda: fake_client)
    manager = SandboxManager()
    code_dir = tmp_path / ""code""
    code_dir.mkdir()
    with caplog.at_level(""WARNING"", logger=""meta_agent.sandbox.sandbox_manager""):
        manager.run_code_in_sandbox(code_dir, [""python""])
    assert any(""Suspicious output"" in r.getMessage() for r in caplog.records)",tests/unit/test_sandbox_manager.py,
survived,"def pytest_pyfunc_call(pyfuncitem):
    test_func = pyfuncitem.obj
    if inspect.iscoroutinefunction(test_func):
        asyncio.run(test_func(**pyfuncitem.funcargs))
        return True
    return None",pytest_asyncio.py,
survived,"def pytest_configure(config):
    config.addinivalue_line(
        ""markers"", ""asyncio: mark a test as running with asyncio""
    )
",pytest_asyncio.py,
survived,"    def __init__(self, stream=None, level: LogLevel = LogLevel.DEBUG):
        super().__init__(level)
        self.stream = stream or os.sys.stdout
",webscout/litlogger/handlers.py,ConsoleHandler
survived,"    def trace(self, message: str):
        self.log(LogLevel.TRACE, message)
",webscout/litlogger/logger.py,Logger
survived,"    def error(self, message: str):
        self.log(LogLevel.ERROR, message)
",webscout/litlogger/logger.py,Logger
survived,"    def fake_fetch(url):
        raise RuntimeError(""fail"")
",libs/core/kiln_ai/adapters/test_remote_config.py,
survived,"def test_load_from_url():
    sample = [built_in_models[0].model_dump(mode=""json"")]

    class FakeResponse:
        def raise_for_status(self):
            pass

        def json(self):
            return {""model_list"": sample}

    with patch(
        ""kiln_ai.adapters.remote_config.requests.get"", return_value=FakeResponse()
    ):
        models = load_from_url(""http://example.com/models.json"")
    assert [m.model_dump(mode=""json"") for m in models] == sample
",libs/core/kiln_ai/adapters/test_remote_config.py,
survived,"def load_from_url(url: str) -> List[KilnModel]:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    data = response.json()
    if isinstance(data, list):
        model_data = data
    else:
        model_data = data.get(""model_list"", [])
    return [KilnModel.model_validate(item) for item in model_data]
",libs/core/kiln_ai/adapters/remote_config.py,
survived,"def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(""path"", help=""output path"")
    args = parser.parse_args()
    dump_builtin_config(args.path)
",libs/core/kiln_ai/adapters/remote_config.py,
survived,"        def fake_import(name, globals=None, locals=None, fromlist=(), level=0):
            if name == ""openai_agents"":
                raise ModuleNotFoundError(name)
            return orig_import(name, globals, locals, fromlist, level)
",tests/test_aiga_agents_import.py,TestAigaAgentsImport
survived,"def test_set_rule_aliases():
    scraper = AutoScraper()
    scraper.build(html=HTML, wanted_list=[""Banana""])
    rule_id = scraper.stack_list[0][""stack_id""]
    scraper.set_rule_aliases({rule_id: ""fruit""})
    result = scraper.get_result_similar(html=HTML, group_by_alias=True, contain_sibling_leaves=True)
    assert result == {""fruit"": [""Banana"", ""Apple"", ""Orange""]}
",tests/unit/test_additional_features.py,
survived,"    def findParent(self):
        return self.parent
",tests/conftest.py,_Node
survived,"def test_regex_name_extraction():
    scraper = AutoScraper()
    scraper.build(html=HTML_PAGE_1, wanted_list=[re.compile(r"".*PlayStation.*Console.*"")])
    result = scraper.get_result_exact(html=HTML_PAGE_1)
    assert any(""PlayStation"" in r for r in result)
",tests/integration/test_real_world.py,
survived,"def test_keep_blank_returns_empty():
    scraper = AutoScraper()
    scraper.build(html=HTML_COMPLEX, wanted_list=[""/shop""])
    html_blank = HTML_COMPLEX.replace('href=""/shop""', 'href=""""')
    result = scraper.get_result_exact(html=html_blank, keep_blank=True)
    assert result == [""""]
",tests/integration/test_complex_features.py,
survived,"def test_attr_fuzz_ratio():
    html_base = '<div><a class=""btn-primary"" href=""/item"">Buy</a></div>'
    html_variant = '<div><a class=""btn-prime"" href=""/item"">Buy</a></div>'
    scraper = AutoScraper()
    scraper.build(html=html_base, wanted_list=[""Buy""])
    res = scraper.get_result_exact(html=html_variant, attr_fuzz_ratio=0.8)
    assert res == [""Buy""]",tests/integration/test_complex_features.py,
survived,"def test_keep_blank_for_missing_rating():
    scraper = AutoScraper()
    scraper.build(html=HTML_PAGE_1, wanted_list=[""4.8""])
    html_no_rating = HTML_PAGE_2.replace(""5.0"", """")
    res = scraper.get_result_exact(html=html_no_rating, keep_blank=True)
    assert res == [""""]
",tests/integration/test_real_world.py,
survived,"def test_grouping_and_rule_removal():
    scraper = AutoScraper()
    wanted = [
        ""Sony PlayStation 4 PS4 Pro 1TB 4K Console - Black"",
        ""US $349.99"",
        ""4.8"",
        ""See details"",
    ]
    scraper.build(html=HTML_PAGE_1, wanted_list=wanted)
    grouped = scraper.get_result_exact(html=HTML_PAGE_2, grouped=True)
    unwanted = [r for r, v in grouped.items() if v == [""See details""]]
    scraper.remove_rules(unwanted)
    result = scraper.get_result_exact(html=HTML_PAGE_2)
    assert result == [
        ""Acer Predator Helios 300 15.6'' 144Hz FHD Laptop i7-9750H 16GB 512GB GTX 1660 Ti"",
        ""US $1,229.49"",
        ""5.0"",
    ]
",tests/integration/test_real_world.py,
survived,"    def log(self, env: messaging.Envelope) -> None:
        """"""Hash ``env`` and append to the ledger.""""""

        data = json.dumps(env.__dict__, sort_keys=True).encode()
        digest = blake3(data).hexdigest()
        with self.conn:
            self.conn.execute(
                ""INSERT INTO messages (ts, sender, recipient, payload, hash) VALUES (?, ?, ?, ?, ?)"",
                (env.ts, env.sender, env.recipient, json.dumps(env.payload), digest),
            )
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"def test_show_results_table(tmp_path) -> None:
    ledger = tmp_path / ""audit.db""
    ledger.touch()
    with patch.object(cli.config.CFG, ""ledger_path"", ledger):
        with patch.object(cli.logging, ""Ledger"") as led_cls:
            led = led_cls.return_value
            led.tail.return_value = [{""ts"": 1.0, ""sender"": ""a"", ""recipient"": ""b"", ""payload"": {""x"": 1}}]
            res = CliRunner().invoke(cli.main, [""show-results""])
            assert ""sender"" in res.output
            assert ""a"" in res.output",tests/test_demo_cli.py,
survived,"def _pretty_table(headers: Iterable[str], rows: Iterable[Iterable[Any]]) -> str:
    cols = [list(map(str, col)) for col in zip(*([headers] + list(rows)))]
    widths = [max(len(item) for item in col) for col in cols]
    line = ""-+-"".join(""-"" * w for w in widths)
    header = "" | "".join(h.ljust(widths[i]) for i, h in enumerate(headers))
    data_lines = ["" | "".join(str(val).ljust(widths[i]) for i, val in enumerate(row)) for row in rows]
    return ""\n"".join([header, line, *data_lines])
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"    def find_previous_sibling(self, name=None, attrs={}, **kwargs) -> Optional[Tag]:
        """"""Find the previous sibling matching given criteria.""""""
        if not self._soup.parent:
            return None

        siblings = self._soup.parent.contents
        try:
            current_index = siblings.index(self._soup)
            for sibling in reversed(siblings[:current_index]):
                if isinstance(sibling, Tag):
                    if (name is None or sibling.name == name) and all(
                        sibling.get(k) == v for k, v in attrs.items()
                    ):
                        return sibling
        except ValueError:
            pass
        return None
",webscout/scout/core/scout.py,Scout
survived,"def test_workflow_with_json_parameter(
    model_manager: ModelManager,
    dogs_image: np.ndarray,
) -> None:
    workflow_init_parameters = {
        ""workflows_core.model_manager"": model_manager,
        ""workflows_core.api_key"": None,
        ""workflows_core.step_execution_mode"": StepExecutionMode.LOCAL,
    }
    execution_engine = ExecutionEngine.init(
        workflow_definition=JSON_PARSER_WORKFLOW,
        init_parameters=workflow_init_parameters,
        max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,
    )

    result = execution_engine.run(
        runtime_parameters={
            ""image"": dogs_image,
            ""config"": ""{\""model_id\"": \""yolov8n-640\""}"",
        }
    )

    assert len(result) == 1
    assert set(result[0].keys()) == {""json_parser"", ""model_predictions""}
    assert result[0][""json_parser""] == ""yolov8n-640""
    assert isinstance(result[0][""model_predictions""], sv.Detections)",tests/workflows/integration_tests/execution/test_workflow_json_parser_config.py,
survived,"def cleanup_repl(loops, debug: bool = False) -> None:
    io_loop, io_thread, io_stop, klong_loop, klong_thread, klong_stop = loops
    cleanup_async_loop(io_loop, io_thread, io_stop, debug=debug, name='io_loop')
    cleanup_async_loop(klong_loop, klong_thread, klong_stop, debug=debug, name='klong_loop')",klongpy/repl.py,
survived,"        async def fetch():
            async with aiohttp.ClientSession() as session:
                async with session.get(f""http://localhost:{port}/"") as resp:
                    return await resp.text()
",tests/test_sys_fn_web.py,TestSysFnWeb
survived,"def _get(obj, name):
    if obj is None:
        return None
    if isinstance(obj, dict):
        if name in obj:
            return obj[name]
    if hasattr(obj, name):
        return getattr(obj, name)
    if name == ""items"" and hasattr(obj, ""Items""):
        return getattr(obj, ""Items"")
    if isinstance(obj, (list, tuple)):
        for it in obj:
            try:
                return _get(it, name)
            except Exception:
                pass
    raise Exception(""field not found: "" + name)
",tests/dataset/job/compiler/py/q1.py,
survived,"        def _key(it):
            k = opts[""sortKey""](*it)
            if isinstance(k, (list, tuple, dict)):
                return str(k)
            return k
",tests/dataset/job/compiler/py/q10.py,
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q5.py,
survived,"def test_Q8_returns_the_pseudonym_and_movie_title_for_Japanese_dubbing():
    assert result == [
        {""actress_pseudonym"": ""Y. S."", ""japanese_movie_dubbed"": ""Dubbed Film""}
    ]
",tests/dataset/job/compiler/py/q8.py,
survived,"def _min(v):
    if hasattr(v, ""Items""):
        v = v.Items
    if not isinstance(v, list):
        raise Exception(""min() expects list or group"")
    vals = [it for it in v if it is not None]
    if not vals:
        return 0
    return min(vals)
",tests/dataset/job/compiler/py/q10.py,
survived,"    def patched_run(app: Any, host: str, port: int, log_level: str = ""info"", **kw: Any) -> None:
        nonlocal server, thread
        config = uvicorn.Config(app, host=host, port=port, log_level=log_level, **kw)
        server = uvicorn.Server(config)
        thread = threading.Thread(target=server.run, daemon=True)
        thread.start()
        for _ in range(50):
            if server.started:
                break
            time.sleep(0.1)
",tests/test_adk_gateway.py,
survived,"def register_agent(meta: AgentMetadata, *, overwrite: bool = False) -> None:
    """"""Public hook for dynamically-generated agents to self-register.""""""
    _register(meta, overwrite=overwrite)
",alpha_factory_v1/backend/agents/registry.py,
survived,"    def as_dict(self) -> Dict:
        return {
            ""name"": self.name,
            ""version"": self.version,
            ""capabilities"": self.capabilities,
            ""compliance"": self.compliance_tags,
            ""requires_api_key"": self.requires_api_key,
        }
",alpha_factory_v1/backend/agents/registry.py,AgentMetadata
survived,"    def close(self) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_fuzz.py,DummyLedger
survived,"    def start_merkle_task(self, *a, **kw) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_fuzz.py,DummyLedger
survived,"    def subscribe(self, topic: str, handler) -> None:  # pragma: no cover - dummy
        pass
",tests/test_safety_guardian_fuzz.py,DummyBus
survived,"def load_sectors(path: str | os.PathLike[str], *, energy: float = 1.0, entropy: float = 1.0) -> list[Sector]:
    """"""Load sector definitions from a JSON file.

    The file may contain a list of strings representing sector names or a list
    of objects with ``name`` and optional ``energy``, ``entropy`` and ``growth``
    fields. The ``energy`` and ``entropy`` arguments provide defaults when these
    values are omitted.
    """"""
    with open(path, ""r"", encoding=""utf-8"") as f:
        data = json.load(f)

    sectors: list[Sector] = []
    for entry in data:
        if isinstance(entry, str):
            sectors.append(Sector(entry, energy, entropy))
        elif isinstance(entry, dict):
            sectors.append(
                Sector(
                    entry.get(""name"", """"),
                    float(entry.get(""energy"", energy)),
                    float(entry.get(""entropy"", entropy)),
                    float(entry.get(""growth"", 0.05)),
                    bool(entry.get(""disrupted"", False)),
                )
            )
        else:
            raise ValueError(f""Invalid sector entry: {entry!r}"")
    return sectors",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/sector.py,
survived,"    async def status(_: None = Depends(verify_token)) -> StatusResponse:
        """"""Return orchestrator agent stats.""""""

        orch = cast(Any, app_f.state.orchestrator)
        if orch is None:
            raise HTTPException(status_code=503, detail=""Orchestrator not running"")
        items = [
            AgentStatus(name=r.agent.name, last_beat=r.last_beat, restarts=r.restarts) for r in orch.runners.values()
        ]
        return StatusResponse(agents=items)
",src/interface/api_server.py,
survived,"            def update(state):
                g_tokens, g_counts = state
                pos = g_counts[""seq"", seq_id].scalar()
                g_tokens = g_tokens.at[""seq"", seq_id, ""position"", pos].set(tokens[""position"", i])
                g_counts = g_counts.at[""seq"", seq_id].add(1)
                return g_tokens, g_counts
",src/levanter/inference/jit_scheduler.py,JitScheduler
survived,"        def sendjson(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - unused
            pass
",tests/test_alpha_agi_business_3_v1.py,DummySocket
survived,"        def stop(self) -> None:
            self.stopped = True
",tests/test_alpha_agi_business_3_v1.py,DummySocket
survived,"def test_frontier_60fps() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri() + ""#seed=1&pop=5000&gen=1""

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#fps-meter"")
        page.wait_for_timeout(2000)
        fps_text = page.inner_text(""#fps-meter"")
        fps = float(fps_text.split()[0])
        assert fps >= 60.0
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_plot_perf.py,
survived,"    def get_col_spec(self, **kwargs: Any) -> str:
        return f""FLOAT[{self.dim}]"" if self.dim is not None else ""FLOAT[]""
",src/raglite/_typing.py,DuckDBVec
survived,"def _sha384(path: Path) -> str:
    digest = hashlib.sha384(path.read_bytes()).digest()
    return ""sha384-"" + base64.b64encode(digest).decode()
",tests/test_docs_bundle_hash.py,
survived,"        def avg_latency(d: dict[str, float]) -> float:
            return d[""lat""] / d[""count""] if d[""count""] else -1.0
",alpha_factory_v1/core/agents/meta_refinement_agent.py,MetaRefinementAgent
survived,"    def test_shutdown_called_on_exit(self) -> None:
        orchestrator._OAI._runtime = None
        orchestrator._OAI._hooked = False
        stub = mock.MagicMock()
        handlers = []
        with mock.patch.object(orchestrator, ""AgentRuntime"", return_value=stub, create=True):
            with mock.patch.object(orchestrator.atexit, ""register"", side_effect=lambda h: handlers.append(h)) as reg:
                self.assertIs(orchestrator._OAI.runtime(), stub)
                reg.assert_called_once()
        self.assertEqual(len(handlers), 1)
        handlers[0]()
        stub.shutdown.assert_called_once()
",tests/test_oai_runtime.py,TestOAIRuntime
survived,"def test_safety_agent_halts_on_large_loss(monkeypatch):
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    monkeypatch.setenv(""ALPHA_ASI_SILENT"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    mod = _reload_module(monkeypatch)
    mod.A2ABus._subs = {}
    safety = mod.BasicSafetyAgent()
    msgs: list[dict] = []
    mod.A2ABus.subscribe(""orch"", lambda m: msgs.append(m))
    safety.handle({""loss"": 5000.0})
    assert {""cmd"": ""stop""} in msgs
",tests/test_world_model_safety.py,
survived,"def test_auto_attach_detects_files_and_prepends_prompt():
    """"""auto_attach should detect file references and prepend the prompt.""""""
    prompt = ""Summarize sample.txt""
    root_dir = ""src/attachments/data""
    ctx = auto_attach(prompt, root_dir=root_dir)

    # Should return an Attachments-like object with the sample file
    assert isinstance(ctx, Attachments)
    assert len(ctx) == 1
    assert ctx[0].path.endswith(""sample.txt"")

    combined = ctx.text
    assert combined.startswith(prompt)
    # After the prompt, the file content should appear
    after_prompt = combined[len(prompt):].lstrip()
    assert after_prompt.startswith(""Welcome to the Attachments Library!"")",tests/test_api_methods.py,
survived,"async def run_cycle_async(
    orchestrator: Orchestrator,
    fin_agent: AgentFin,
    res_agent: AgentRes,
    ene_agent: AgentEne,
    gdl_agent: AgentGdl,
    model: Model,
) -> None:
    """"""Execute one evaluation + commitment cycle.""""""

    bundle = orchestrator.collect_signals()
    delta_h = fin_agent.latent_work(bundle)
    delta_s = res_agent.entropy(bundle)
    beta = ene_agent.market_temperature()
    if abs(beta) < 1e-9:
        log.warning(""Î² is zero; skipping cycle"")
        return
    delta_g = delta_h - (delta_s / beta)

    log.info(""Î”H=%s Î”S=%s Î²=%s â†’ Î”G=%s"", delta_h, delta_s, beta, delta_g)

    comment = await _llm_comment(delta_g)
    log.info(""LLM: %s"", comment)

    if delta_g < 0:
        orchestrator.post_alpha_job(id(bundle), delta_g)

    weight_update: Dict[str, Any] = {}
    if gdl_agent.provable(weight_update):
        model.commit(weight_update)
",alpha_factory_v1/demos/alpha_agi_business_3_v1/alpha_agi_business_3_v1.py,
survived,"    def test_missing_spec_allowed_with_flag(self) -> None:
        """"""Allow basic fallback when __spec__ is None.""""""
        fake_mod = types.SimpleNamespace(
            __version__=""0.0.17"",
            __spec__=None,
            OpenAIAgent=object,
        )

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return importlib.import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            if name == ""agents"":
                return None
            return importlib.util.find_spec(name, *args, **kwargs)

        def _raise() -> bool:
            raise AssertionError(""check_openai_agents_version should not run"")

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [""openai_agents""]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
            mock.patch.object(check_env, ""check_openai_agents_version"", _raise),
        ):
            self.assertEqual(check_env.main([""--allow-basic-fallback""]), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def __init__(self, agent: object) -> None:
        self.cls: Callable[..., object] = type(agent)
        self.agent = agent
        self.period = getattr(agent, ""CYCLE_SECONDS"", 1.0)
        self.capabilities = getattr(agent, ""CAPABILITIES"", [])
        self.last_beat = time.time()
        self.restarts = 0
        self.task: asyncio.Task[None] | None = None
        self.error_count = 0
        self.restart_streak = 0
",alpha_factory_v1/backend/agent_supervisor.py,AgentRunner
survived,"def main(demo: str) -> None:
    url = _demo_url(demo)
    if _remote_available(url):
        print(f""Opening {url}"")
        webbrowser.open(url)
        return

    repo_root = Path(__file__).resolve().parents[1]
    site_dir = repo_root / ""site"" / demo
    local_page = site_dir / ""index.html""
    if not local_page.is_file():
        print(""Remote page unavailable. Building local copy..."", file=sys.stderr)
        if not _build_local_site(repo_root) or not local_page.is_file():
            print(
                f""Demo {demo} not found. Build the gallery with ./scripts/build_gallery_site.sh"",
                file=sys.stderr,
            )
            sys.exit(1)

    handler = partial(SimpleHTTPRequestHandler, directory=str(site_dir))
    with ThreadingHTTPServer((""127.0.0.1"", 0), handler) as httpd:
        port = httpd.server_address[1]
        local_url = f""http://127.0.0.1:{port}/index.html""
        print(f""Serving local copy at {local_url}"", file=sys.stderr)
        thread = threading.Thread(target=httpd.serve_forever, daemon=True)
        thread.start()
        try:
            webbrowser.open(local_url)
            thread.join()
        except KeyboardInterrupt:
            pass
",scripts/open_demo.py,
survived,"def main() -> None:
    run([""python"", ""alpha_factory_v1/scripts/preflight.py""])
    run([""node"", str(BROWSER_DIR / ""build/version_check.js"")])
    run([""python"", ""scripts/check_python_deps.py""])
    run([""python"", ""check_env.py"", ""--auto-install""])
    run([""python"", ""scripts/verify_disclaimer_snippet.py""])
    run([""python"", ""-m"", ""alpha_factory_v1.demos.validate_demos""])
    run([""python"", ""scripts/publish_demo_gallery.py""])
    run([""python"", ""scripts/verify_workbox_hash.py"", ""site/alpha_agi_insight_v1""])

    try:
        import importlib.util

        if importlib.util.find_spec(""playwright"") is not None:
            with subprocess.Popen(
                [sys.executable, ""-m"", ""http.server"", ""--directory"", ""site"", ""8000""],
                cwd=REPO_ROOT,
            ) as proc:
                try:
                    run([""python"", ""scripts/verify_insight_offline.py""])
                finally:
                    proc.terminate()
        else:
            print(""Playwright not found; skipping offline re-check"", file=sys.stderr)
    except Exception:
        print(""Playwright not found; skipping offline re-check"", file=sys.stderr)
",scripts/edge_human_knowledge_pages_sprint.py,
survived,"    async def kill_switch(request: Request, _: None = Depends(verify_token)) -> dict[str, str]:
        token = request.headers.get(""X-Kill-Token"")
        if token is None:
            data = await request.json()
            token = data.get(""token"")
        if token not in _kill_tokens:
            raise HTTPException(status_code=403, detail=""Invalid kill token"")
        _pending_votes[token] = time.time()
        if len(_pending_votes) >= 2:
            task = getattr(app_f.state, ""orch_task"", None)
            if task:
                task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await task
                app_f.state.orch_task = None
                app_f.state.orchestrator = None
            alerts.send_alert(""Kill-switch activated â€“ orchestrator disabled"")
            _pending_votes.clear()
            return {""status"": ""disabled""}
        return {""status"": f""{len(_pending_votes)}/2 confirmations""}
",src/interface/api_server.py,
survived,"def test_pareto_rank_deterministic() -> None:
    pop = [
        Candidate(0.8, 40, 10),
        Candidate(0.9, 45, 20),
        Candidate(0.6, 60, 15),
    ]
    rng = random.Random(0)
    selections = [select_parent(pop, epsilon=0.0, rng=rng) for _ in range(100)]
    assert all(s is not pop[1] for s in selections)
",tests/test_sim_selector.py,
survived,"    def op(_g: str) -> str:
        return ""asdf qwer zxcv""  # nonsense thesis
",tests/test_reviewer_agent.py,
survived,"def lead_signal_improvement(
    history: Sequence[float],
    forecast: Sequence[float],
    *,
    months: int = 6,
    threshold: float | None = None,
) -> float:
    """"""Return relative lead-time improvement over the baseline.""""""
    base = _arima_baseline(history, months)
    thr = threshold if threshold is not None else (history[-1] if history else 0.0)

    def first_cross(seq: Sequence[float]) -> int:
        for i, v in enumerate(seq, 1):
            if v >= thr:
                return i
        return months + 1

    base_idx = first_cross(base)
    cand_idx = first_cross(forecast[:months])
    if base_idx <= cand_idx:
        return 0.0
    return (base_idx - cand_idx) / base_idx
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/evaluators/lead_time.py,
survived,"def generate_score_proof(scores: Sequence[float], threshold: float) -> str:
    """"""Return proof that the weighted score exceeds ``threshold``.""""""
    # hidden evaluator weights
    weighted = 0.7 * scores[0] + 0.3 * scores[1]
    if weighted < threshold:
        raise ValueError(""score below threshold"")
    h = _hash_scores(scores)
    blob = json.dumps({""hash"": h, ""threshold"": threshold}, separators=("","", "":"")).encode()
    return sha256(blob).hexdigest()
",src/snark/proof.py,
survived,"def test_get_output_path_without_subdirs(monkeypatch, tmp_path):
    monkeypatch.setenv('NOTES_EXPORT_USE_SUBDIRS', 'false')
    tracker = utils.NotesExportTracker(root_directory=str(tmp_path))
    output = tracker.get_output_path('pdf', 'folder', 'note', '.pdf')
    expected = Path(tmp_path) / 'pdf' / 'note.pdf'
    assert output == expected
    assert output.parent.is_dir()",tests/test_tracker.py,
survived,"def test_uses_subdirs_true(monkeypatch, tmp_path):
    monkeypatch.setenv('NOTES_EXPORT_USE_SUBDIRS', 'true')
    tracker = utils.NotesExportTracker(root_directory=str(tmp_path))
    assert tracker._uses_subdirs() is True
",tests/test_tracker.py,
survived,"    def fake_run(*_a, **_k):
        return subprocess.CompletedProcess([], 1, """", """")
",tests/test_preflight_sandbox.py,
survived,"def download_model(dest: Path, model: str = ""124M"") -> None:
    """"""Download GPT-2 weights using the official helper script.""""""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        subprocess.run([""git"", ""clone"", ""--depth"", ""1"", OPENAI_REPO, str(tmp_path)], check=True)
        script = tmp_path / ""download_model.py""
        subprocess.run([sys.executable, str(script), model], cwd=tmp_path, check=True)
        target = dest / model
        target.mkdir(parents=True, exist_ok=True)
        shutil.copytree(tmp_path / ""models"" / model, target, dirs_exist_ok=True)
",scripts/download_gpt2_small.py,
survived,"async def get_order(order_id: int):
    order = next((o for o in ORDERS if o[""id""] == order_id), None)
    if not order:
        raise HTTPException(status_code=404, detail=""Order not found"")
    return order",examples/shop_api_gateway/server.py,
survived,"def degrees2compasspoint(h):
    return compassPoint[cpx(h)]
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def cpx(h):
    x = int(((h / 11.25) + 0.5))
    x = x % 32
    if x < 0:
        x = x + 32
    return x
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def padRight(s, w):
    out = s
    i = len(s)
    while i < w:
        out = out + "" ""
        i = i + 1
    return out
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/box-the-compass.py,
survived,"def padLeft(n, width):
    s = str(n)
    while len(s) < width:
        s = "" "" + s
    return s
",tests/rosetta/transpiler/Python/blum-integer.py,
survived,"def zeroval(ival):
    x = ival
    x = 0
    return x
",tests/rosetta/transpiler/Python/call-a-function-11.py,
survived,"def isPrime(n):
    if n < 2:
        return False
    if n % 2 == 0:
        return n == 2
    if n % 3 == 0:
        return n == 3
    d = 5
    while d * d <= n:
        if n % d == 0:
            return False
        d = d + 2
        if n % d == 0:
            return False
        d = d + 4
    return True
",tests/rosetta/transpiler/Python/brazilian-numbers.py,
survived,"def makePatterns():
    digits = [""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]
    pats = []
    i = 0
    while i < len(digits):
        j = 0
        while j < len(digits):
            if j != i:
                k = 0
                while k < len(digits):
                    if k != i and k != j:
                        l = 0
                        while l < len(digits):
                            if l != i and l != j and l != k:
                                pats = pats + [digits[i] + digits[j] + digits[k] + digits[l]]
                            l = l + 1
                    k = k + 1
            j = j + 1
        i = i + 1
    return pats
",tests/rosetta/transpiler/Python/bulls-and-cows-player.py,
survived,"def decipher(s, k):
    return encipher(s, (26 - k % 26) % 26)
",tests/rosetta/transpiler/Python/caesar-cipher-2.py,
survived,"def commatize(n):
    s = str(n)
    i = len(s) - 3
    while i >= 1:
        s = """".join(s[0:i]) + "","" + """".join(s[i:len(s)])
        i = i - 3
    return s
",tests/rosetta/transpiler/Python/brilliant-numbers.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            return i
        i = i + 1
    return -1
",tests/rosetta/transpiler/Python/caesar-cipher-2.py,
survived,"def main():
    add2 = mkAdd(2)
    add3 = mkAdd(3)
    print(str(add2(5)) + "" "" + str(add3(6)))
    partial = partialSum(13)
    print(str(partial(5)))
",tests/rosetta/transpiler/Python/call-a-function-12.py,
survived,"def toContinued(r):
    a = r.numerator
    b = r.denominator
    res = []
    while True:
        res = res + [int((a // b))]
        t = a % b
        a = b
        b = t
        if a == 1:
            break
    if len(res) % 2 == 0:
        res[len(res) - 1] = res[len(res) - 1] - 1
        res = res + [1]
    return res
",tests/rosetta/transpiler/Python/calkin-wilf-sequence.py,
survived,"def osm(args, tolerant, dryrun):
    """"""Compile one or more OSM files into an OSM database.""""""
    if len(args) < 2:
        raise click.UsageError(""OSM file(s) and destination database required"")
    *osm_files, osmdb_filename = args
    osm_to_osmdb(osm_files, osmdb_filename, tolerant, dryrun)
",pygs/graphserver/cli.py,
survived,"def test_retry_on_rate_limit(thread_and_agent, monkeypatch):
    thread, agent = thread_and_agent
    thread._run.last_error = Mock()
    thread._run.last_error.message = ""Rate limit is exceeded. Try again in 2 seconds.""

    called = []

    def fake_sleep(sec):
        called.append(sec)

    monkeypatch.setattr(time, ""sleep"", fake_sleep)

    result = thread._try_run_failed_recovery(
        error_attempts=0,
        recipient_agent=agent,
        additional_instructions=None,
        event_handler=None,
        tool_choice=None,
        response_format=None,
        parent_run_id=None,
    )

    assert result is True
    thread._create_run.assert_called_once()
    assert called and called[0] == 2",tests/test_thread_retry.py,
survived,"            def run(self) -> None:
                import asyncio

                if self._agent is None:
                    raise RuntimeError(""No agent registered"")
                asyncio.run(self._runner.run(self._agent, """"))
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,_FallbackAgentRuntime
survived,"def primeSieve(n):
    sieve = []
    i = 0
    while i <= n:
        sieve = sieve + [False]
        i = i + 1
    sieve[0] = True
    sieve[1] = True
    p = 2
    while p * p <= n:
        if not sieve[p]:
            m = p * p
            while m <= n:
                sieve[m] = True
                m = m + p
        p = p + 1
    sys.exit(sieve)
",tests/rosetta/transpiler/Python/equal-prime-and-composite-sums.py,
survived,"def binom(n, k):
    if k < 0 or k > n:
        sys.exit(0)
    kk = k
    if kk > n - kk:
        kk = n - kk
    res = 1
    i = 0
    while i < kk:
        res = res * ((n - i))
        i = i + 1
        res = res // (i)
    sys.exit(res)
",tests/rosetta/transpiler/Python/evaluate-binomial-coefficients.py,
survived,"def pad2(x):
    s = str(x)
    if len(s) < 2:
        s = "" "" + s
    sys.exit(s)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/extensible-prime-generator.py,
survived,"def add(a, b):
    sys.exit(Complex(re=a.re + b.re, im=a.im + b.im))
",tests/rosetta/transpiler/Python/eulers-identity.py,
survived,"def copyInts(xs):
    out = []
    for v in xs:
        out = out + [v]
    return out
",tests/rosetta/transpiler/Python/faces-from-a-mesh.py,
survived,"def expF(b, p):
    neg = False
    if p < 0:
        neg = True
        p = -p
    r = 1.0
    pow = b
    while p > 0:
        if p % 2 == 1:
            r = r * pow
        pow = pow * pow
        p = p // 2
    if neg:
        r = 1.0 / r
    return r
",tests/rosetta/transpiler/Python/exponentiation-operator.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    print(""The listed extensions are:"")
    print(extensions)
    tests = [""MyData.a##"", ""MyData.tar.Gz"", ""MyData.gzip"", ""MyData.7z.backup"", ""MyData..."", ""MyData"", ""MyData_v1.0.tar.bz2"", ""MyData_v1.0.bz2""]
    for t in tests:
        res = fileExtInList(t)
        ok = bool(res[0])
        ext = str(res[1])
        print(pad(t, 20) + "" => "" + str(ok) + ""  (extension = "" + ext + "")"")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/file-extension-is-in-extensions-list.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    k = 0.07
    tempRoom = 20.0
    tempObject = 100.0
    fcr = newCoolingRateDy(k, tempRoom)
    analytic = newTempFunc(k, tempRoom, tempObject)
    for step in [2.0, 5.0, 10.0]:
        print(""Step size = "" + fmtF(step, 0, 1))
        print("" Time Euler's Analytic"")
        temp = tempObject
        time = 0.0
        while time <= 100.0:
            line = fmtF(time, 5, 1) + "" "" + fmtF(temp, 7, 3) + "" "" + fmtF(analytic(time), 7, 3)
            print(line)
            temp = eulerStep(fcr, time, temp, step)
            time = time + step
        print("""")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def showDistribution(sizes):
    bins = []
    i = 0
    while i < 12:
        bins = bins + [0]
        i = i + 1
    total = 0
    for sz in sizes:
        total = total + sz
        idx = 0
        if sz > 0:
            idx = log10floor(sz) + 1
        bins[idx] = bins[idx] + 1
    print(""File size distribution:\n"")
    i = 0
    while i < len(bins):
        prefix = ""  ""
        if i > 0:
            prefix = ""+ ""
        print(prefix + ""Files less than 10 ^ "" + str(i) + "" bytes : "" + str(bins[i]))
        i = i + 1
    print(""                                  -----"")
    print(""= Total number of files         : "" + str(len(sizes)))
    print(""  Total size of files           : "" + commatize(total) + "" bytes"")
",tests/rosetta/transpiler/Python/file-size-distribution.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/fibonacci-n-step-number-sequences.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            sys.exit(i)
        i = i + 1
    sys.exit(-1)
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    err = foo()
    if len(err) > 0:
        print(""Recovered from "" + err)
    print(""glad that's over."")
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/exceptions.py,
survived,"def newCoolingRate(k):
    sys.exit(lambda dt: -k * dt)
",tests/rosetta/transpiler/Python/euler-method.py,
survived,"def sinApprox(x):
    term = x
    sum = x
    n = 1
    while n <= 10:
        denom = float(((2 * n) * (2 * n + 1)))
        term = -term * x * x / denom
        sum = sum + term
        n = n + 1
    sys.exit(sum)
",tests/rosetta/transpiler/Python/eulers-identity.py,
survived,"def indexOf(s, ch):
    i = 0
    while i < len(s):
        if s[i:i + 1] == ch:
            sys.exit(i)
        i = i + 1
    sys.exit(0 - 1)
",tests/rosetta/transpiler/Python/feigenbaum-constant-calculation.py,
survived,"    def log(self, env: messaging.Envelope) -> None:  # pragma: no cover - stub
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,DummyLedger
survived,"def test_skip_unsafe_execution(monkeypatch) -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    ledger = DummyLedger()
    agent = codegen_agent.CodeGenAgent(bus, ledger)

    called = False

    def fake_exec(code: str) -> tuple[str, str]:
        nonlocal called
        called = True
        return """", """"

    monkeypatch.setattr(codegen_agent, ""is_code_safe"", lambda c: False)
    monkeypatch.setattr(agent, ""execute_in_sandbox"", fake_exec)

    env = messaging.Envelope(sender=""market"", recipient=""codegen"", ts=0.0)
    env.payload.update({""analysis"": ""x""})
    asyncio.run(agent.handle(env))
    assert not called",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_codegen_safety.py,
survived,"    async def run(self) -> None:
        iteration = 0
        while not await self._should_stop():
            if self._max_iters is not None and iteration >= self._max_iters:
                break

            # ------------------------------------------------------------------
            # Sample a problem (reuse the dataset when exhausted).
            # ------------------------------------------------------------------
            if self._example_idx >= len(self._examples):
                self._rng.shuffle(self._examples)
                self._example_idx = 0
            example = self._examples[self._example_idx]
            self._example_idx += 1

            user_prompt: str = example[""prompt""]
            gt_answer: str = example[""answer""]

            # ------------------------------------------------------------------
            # Call inference server.
            # ------------------------------------------------------------------
            completion = self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {""role"": ""user"", ""content"": user_prompt},
                ],
            )

            assistant_msg: str = completion.choices[0].message.content

            # ------------------------------------------------------------------
            # Reward calculation.
            # ------------------------------------------------------------------
            parsed = validate_format(assistant_msg + "">"")  # util expects trailing >
            is_valid = parsed[""is_valid""]
            extracted_answer = parsed[""answer""]
            is_correct = grade_answer(extracted_answer, gt_answer) if is_valid else False
            reward = float(is_correct)

            # ------------------------------------------------------------------
            # Build rollout & emit.
            # ------------------------------------------------------------------
            turns = [
                Turn(
                    message=user_prompt,
                    role=""user"",
                    logprobs=None,
                    reward=None,
                    inference_metadata={},
                ),
                Turn(
                    message=assistant_msg,
                    role=""assistant"",
                    logprobs=None,  # logprobs not available via OpenAI client
                    reward=reward,
                    inference_metadata={
                        ""model"": self._model,
                        ""finish_reason"": completion.choices[0].finish_reason,
                    },
                ),
            ]
            rollout = Rollout(turns=turns, metadata={""problem"": user_prompt})
            group = RolloutGroup(
                id=f""math-{iteration}"",
                source=""math_env"",
                created=time.time(),
                rollouts=[rollout],
                metadata={""valid_format"": is_valid, ""correct"": is_correct},
            )

            self._rollout_sink([group])

            iteration += 1
            await asyncio.sleep(0)  # yield control to Ray scheduler
",marin/rl/envs/math_env.py,MathEnv
survived,"    def resources(self) -> RayResources:
        """"""Return Ray resource specs (CPU/GPU/TPU etc.) needed per replica.""""""
",marin/rl/config.py,AbstractEnvConfig
survived,"    async def act(self) -> Dict[str, Any]:  # type: ignore[override]
        return {""action"": ""noop""}
",alpha_factory_v1/demos/era_of_experience/stub_agents.py,ExperienceAgent
survived,"def main(argv: list[str] | None = None) -> None:
    args = _parse_args(argv)
    payload: dict[str, object] = {""action"": args.action}
    if args.job:
        payload[""job""] = json.loads(Path(args.job).read_text(encoding=""utf-8""))
    headers = {}
    api_key = os.getenv(""OPENAI_API_KEY"")
    if api_key:
        headers[""Authorization""] = f""Bearer {api_key}""
    url = f""{args.host}/v1/agents/business_helper/invoke""
    resp = requests.post(url, json=payload, headers=headers, timeout=10)
    try:
        print(json.dumps(resp.json(), indent=2))
    except Exception:
        print(resp.text)
",alpha_factory_v1/demos/alpha_agi_business_v1/examples/openai_agent_client.py,
survived,"def pulse_source(
    token: str,
    start_date: str,
    end_date: Optional[str] = None,
    metrics: Optional[Iterable[str]] = None,
    topsites: Optional[bool] = None,
    ip_version: Optional[str] = None,
) -> Iterable[dlt.sources.DltResource]:
    """"""Create resources for Internet Society Pulse metrics.

    Args:
        token: Bearer token for the API.
        start_date: First date of the data range (YYYY-MM-DD).
        end_date: Last date of the data range.
        metrics: Subset of metrics to fetch. Defaults to all available metrics.
        topsites: Optional flag used by some endpoints.
        ip_version: IP version parameter used by some endpoints.
    """"""
    if metrics is None:
        metrics = GLOBAL_METRICS.keys()

    headers = {""Authorization"": f""Bearer {token}""}

    resources: List[EndpointResource] = []
    for name in metrics:
        path = GLOBAL_METRICS.get(name)
        if not path:
            continue

        endpoint: Dict[str, Any] = {
            ""path"": path,
            ""params"": {
                ""start_date"": ""{incremental.start_value}"",
            },
            ""incremental"": {
                ""cursor_path"": ""date"",
                ""start_param"": ""start_date"",
                ""end_param"": ""end_date"",
                ""initial_value"": start_date,
                ""end_value"": end_date,
                ""range_start"": ""closed"",
                ""range_end"": ""closed"",
            },
            ""paginator"": ""single_page"",
        }

        if end_date is not None:
            endpoint[""params""][""end_date""] = end_date
        if topsites is not None and name in {""http"", ""https""}:
            endpoint[""params""][""topsites""] = topsites
        if ip_version is not None and name in {""roa"", ""rov"", ""tls"", ""tls13""}:
            endpoint[""params""][""ip_version""] = ip_version

        resources.append({""name"": name, ""endpoint"": endpoint})

    config: RESTAPIConfig = {
        ""client"": {
            ""base_url"": ""https://pulse.internetsociety.org/api/"",
            ""headers"": headers,
        },
        ""resource_defaults"": {
            ""write_disposition"": ""merge"",
            ""primary_key"": ""date"",
        },
        ""resources"": resources,
    }

    yield from rest_api_resources(config)",ingestr/src/pulse/__init__.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitwise-io-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/binary-search.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/bitcoin-address-validation.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/constrained-genericity-1.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/copy-stdin-to-stdout-2.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-in-factors.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/conditional-structures-4.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/continued-fraction.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/constrained-genericity-3.py,
survived,"def test_refinement_proposes_cycle_adjustment(tmp_path: Path) -> None:
    repo = _make_repo(tmp_path)
    logs = tmp_path / ""logs""
    logs.mkdir()
    (logs / ""log.json"").write_text(
        ""\n"".join(['{""agent"":""demo"",""latency_ms"":6000,""ts"":0}', '{""agent"":""demo"",""latency_ms"":7000,""ts"":1}']),
        encoding=""utf-8"",
    )

    reg = StakeRegistry()
    reg.set_stake(""meta"", 1.0)

    with patch.object(harness, ""vote_and_merge"") as vote:
        agent = MetaRefinementAgent(repo, logs, reg)
        agent.refine()

    called_diff = vote.call_args.args[1]
    assert ""increase cycle"" in called_diff",tests/test_meta_refinement_agent.py,
survived,"def _read(name: str) -> str:
    return (FIXTURES / name).read_text()
",tests/test_patch_validation.py,
survived,"def _wait_results(
    url: str,
    sim_id: str,
    headers: dict[str, str],
    proc: subprocess.Popen[str],
    max_attempts: int = 60,
) -> dict[str, object]:
    delay = 0.05
    for _ in range(max_attempts):
        if proc.poll() is not None:
            out, err = proc.communicate()
            raise AssertionError(
                f""server exited with {proc.returncode}:\n{out}{err}""
                if err
                else f""server exited with {proc.returncode}:\n{out}""
            )
        r = httpx.get(f""{url}/results/{sim_id}"", headers=headers)
        if r.status_code == 200:
            data = r.json()
            if data.get(""population"") is not None:
                return data
        time.sleep(delay)
        delay = min(delay * 1.5, 1.0)
    raise AssertionError(""Timed out waiting for results"")
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_api_server_subprocess.py,
survived,"def test_get_version_writes_file():
    repo_root = Path(__file__).resolve().parents[1]
    version_file = repo_root / ""_scm_version.py""
    try:
        version = get_version(root=repo_root, write_to=version_file)
        assert version_file.exists()
        content = version_file.read_text()
        assert version in content
        assert re.match(r""\d+\.\d+\.\d+"", version)
    finally:
        if version_file.exists():
            version_file.unlink()",tests/test_version_scm.py,
survived,"def test_self_improve_template_parses(tmp_path, monkeypatch):
    data = {""system"": ""sys"", ""user"": ""usr""}
    path = tmp_path / ""tpl.yaml""
    path.write_text(yaml.safe_dump(data), encoding=""utf-8"")
    monkeypatch.setenv(""SELF_IMPROVE_TEMPLATE"", str(path))
    config.init_config()
    cfg = config.Settings()
    assert cfg.self_improve.system == ""sys""
    assert cfg.self_improve.user == ""usr""",tests/test_prompts.py,
survived,"def test_comparative_advantage():
    client = get_client()
    resp = client.post(
        ""/comparative-advantage/execute"",
        json={""skills"": {""a"": 1}, ""tasks"": {""t1"": [""a""]}},
    )
    assert resp.status_code == 200
    data = resp.json()
    assert set(data.keys()) == {""advantage_map""}
",servers/server_clear_thought/tests/test_new_tools.py,
survived,"def test_existing_tool_example():
    app = create_app()
    client = TestClient(app)
    resp = client.post(""/existing-tool-example/execute"", json={""text"": ""hi""})
    assert resp.status_code == 200
    assert resp.json() == {""echoed"": ""hi""}",servers/server_clear_thought/tests/test_existing_tools.py,
survived,"    def execute(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        voi_score = sum(payload[""payoffs""]) / (len(payload[""uncertainties""]) or 1)
        questions = [f""Resolve {u}?"" for u in payload[""uncertainties""]]
        return {
            ""voi_score"": round(voi_score, 2),
            ""high_impact_questions"": questions,
        }",servers/server_clear_thought/tools/value_of_information.py,ValueOfInformation
survived,"    async def json(self):
        return {}
",src/aiohttp/__init__.py,Response
survived,"    def start_timer(self) -> None:
        """"""Start the latency timer.""""""
        self._start = time.perf_counter()
",src/meta_agent/telemetry.py,TelemetryCollector
survived,"def test_summary_line():
    t = TelemetryCollector()
    t.start_timer()
    t.stop_timer()
    line = t.summary_line()
    assert ""Telemetry:"" in line
    assert ""cost=$"" in line
    assert ""tokens=0"" in line",tests/unit/test_telemetry_collector.py,
survived,"def test_with_retry_sync(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setattr(retry, ""backoff"", None)
    monkeypatch.setattr(retry.time, ""sleep"", lambda *_: None)
    calls = {""n"": 0}

    def func() -> str:
        calls[""n""] += 1
        if calls[""n""] < 2:
            raise ValueError(""fail"")
        return ""ok""

    wrapped = retry.with_retry(func, max_tries=2)
    assert wrapped() == ""ok""
    assert calls[""n""] == 2
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_retry.py,
survived,"def test_storage_access_toast() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(storage_state=None)
        context.add_init_script(""document.hasStorageAccess = () => Promise.resolve(false)"")
        page = context.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        page.wait_for_function(
            ""document.getElementById('toast').textContent.includes('no storage access')""
        )
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_storage_access_toast.py,
survived,"  def _format_param(self, value: Any) -> Any:
    """"""Format parameters for logging.""""""
    if isinstance(value, Resource):
      return value.name
    try:
      if isinstance(value, Sequence) and len(value) > 0 and isinstance(value[0], Resource):
        return [v.name for v in value]
    except Exception:
      pass
    return value
",pylabrobot/liquid_handling/liquid_handler.py,LiquidHandler
survived,"def test_evolve_tool() -> None:
    """"""Invoke ``evolve`` once and verify ``best_alpha`` output.""""""
    mod = importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.openai_agents_bridge"")
    runtime = mod.AgentRuntime(api_key=None)
    agent = mod.EvolverAgent()
    runtime.register(agent)

    asyncio.run(mod.evolve(1))
    result = asyncio.run(mod.best_alpha())
    assert ""architecture"" in result",tests/test_aiga_agents_bridge.py,
survived,"def _construct_payload(action: str, job_path: str | None) -> dict[str, object]:
    """"""Return request payload for ``action``.

    Parameters
    ----------
    action:
        Name of the helper action to invoke via ``business_helper``.
    job_path:
        Optional path to a JSON file with additional job parameters.

    The resulting dictionary is posted as JSON to the ``/invoke`` endpoint of
    the OpenAI Agents runtime. When ``job_path`` is provided the file contents
    are loaded and included under the ``job`` key.
    """"""

    payload: dict[str, object] = {""action"": action}
    if job_path:
        try:
            job_json = json.loads(Path(job_path).read_text(encoding=""utf-8""))
            payload[""job""] = job_json
        except Exception as exc:  # pragma: no cover - malformed file
            raise SystemExit(f""Failed to load job JSON: {exc}"")
    return payload
",alpha_factory_v1/demos/alpha_agi_business_v1/examples/openai_agent_client.py,
survived,"    def tail(self, count: int = 10) -> List[dict[str, object]]:
        """"""Return the last ``count`` ledger entries.""""""

        assert self.conn is not None
        if self.db_type == ""postgres"":
            with self.conn.cursor() as cur:
                cur.execute(
                    ""SELECT ts, sender, recipient, payload FROM messages ORDER BY id DESC LIMIT %s"",
                    (count,),
                )
                rows = cur.fetchall()
        else:
            cur = self.conn.execute(
                ""SELECT ts, sender, recipient, payload FROM messages ORDER BY id DESC LIMIT ?"",
                (count,),
            )
            rows = cur.fetchall()
        result: List[dict[str, object]] = []
        for ts, sender, recipient, payload in reversed(rows):
            try:
                data = json.loads(payload)
            except Exception:
                data = payload
            result.append({""ts"": ts, ""sender"": sender, ""recipient"": recipient, ""payload"": data})
        return result
",alpha_factory_v1/common/utils/logging.py,Ledger
survived,"            def call_llama(prompt: str, s: Settings) -> str:
                out = cast(Any, _MODEL)(prompt, temperature=s.temperature)
                return cast(str, out[""choices""][0][""text""]).strip()
",alpha_factory_v1/common/utils/local_llm.py,
survived,"def chat(prompt: str, cfg: Settings | None = None) -> str:
    """"""Return a completion using the local model or a simple echo.""""""
    cfg = cfg or config.CFG
    if _CALL is None:
        _load_model(cfg)
    assert _CALL is not None
    try:
        with span(""local_llm.chat""):
            return _CALL(prompt, cfg)
    except Exception as exc:  # pragma: no cover - runtime error
        _log.exception(""Local chat failed: %s"", exc)
        return f""[offline] {prompt}""",alpha_factory_v1/common/utils/local_llm.py,
survived,"    def subscribe(self, topic: str, handler: Callable[[EnvelopeLike], Awaitable[None] | None]) -> None:
        self._subs.setdefault(topic, []).append(handler)
",alpha_factory_v1/common/utils/messaging.py,A2ABus
survived,"    def rest_task(self) -> Optional[asyncio.Task]:
        return self._rest_task
",alpha_factory_v1/backend/services/api_server_service.py,APIServer
survived,"        def publish(self, topic, msg):
            events.append((topic, msg))
",tests/test_kafka_service.py,DummyBus
survived,"    async def stop(self) -> None:  # pragma: no cover - close handled by EventBus
        return None
",alpha_factory_v1/backend/services/kafka_service.py,KafkaService
survived,"def test_memory_agent_file_cap(tmp_path: Path) -> None:
    mem_file = tmp_path / ""mem.log""
    cfg = config.Settings(bus_port=0, memory_path=str(mem_file))
    bus = messaging.A2ABus(cfg)
    ledger = logging.Ledger(str(tmp_path / ""ledger.db""))
    agent = memory_agent.MemoryAgent(bus, ledger, str(mem_file), memory_limit=2)

    envs = [messaging.Envelope(""a"", ""memory"", {""v"": i}, 0.0) for i in range(3)]

    async def _run() -> None:
        async with bus, ledger:
            for env in envs:
                await agent.handle(env)

    asyncio.run(_run())

    entries = [json.loads(line) for line in mem_file.read_text(encoding=""utf-8"").splitlines()]
    assert [e[""v""] for e in entries] == [1, 2]

    agent2 = memory_agent.MemoryAgent(bus, ledger, str(mem_file), memory_limit=2)
    assert [r[""v""] for r in agent2.records] == [1, 2]
",tests/test_memory_agent_file_persistence.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/dataset/tpc-h/compiler/py/q21.py,Auto1
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q9.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/dataset/tpc-h/compiler/py/q20.py,_Group
survived,"def test_safari_offline_reload() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    try:
        with sync_playwright() as p:
            browser = p.webkit.launch()
            context = browser.new_context(
                user_agent=(
                    ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ""
                    ""AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15""
                )
            )
            page = context.new_page()
            errors: list[str] = []
            page.on(""console"", lambda msg: errors.append(msg.text) if msg.type == ""error"" else None)
            page.on(""pageerror"", lambda err: errors.append(str(err)))

            page.goto(url)
            page.wait_for_selector(""#controls"")
            page.wait_for_function(""navigator.serviceWorker.ready"")

            context.set_offline(True)
            page.reload()
            page.wait_for_selector(""#controls"")
            context.set_offline(False)

            assert not errors, f""Console errors: {errors}""
            assert page.evaluate(""navigator.serviceWorker.controller !== null"")
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_safari_offline.py,
survived,"        def __init__(self) -> None:
            self.random = _RandomNormal()
",src/meta_agent/embedding_models.py,_NPStub
survived,"            def _decorator(func):
                return func
",alpha_factory_v1/demos/cross_industry_alpha_factory/openai_agents_bridge.py,
survived,"            async def step(self):
                return None
",tests/test_agents_registry.py,TestAgentRegistryFunctions.DummyAgent
survived,"    def __init__(self):
        super().__init__()
        self.calls = 0
",tests/test_agent_base.py,DummyAgent
survived,"    def test_run_cycle_publishes(self):
        agent = PingAgent()
        agent.orchestrator = DummyOrch()
        asyncio.run(agent.setup())
        asyncio.run(agent.run_cycle())
        asyncio.run(agent.teardown())
        self.assertEqual(len(agent.orchestrator.published), 1)
        topic, payload = agent.orchestrator.published[0]
        self.assertEqual(topic, ""agent.ping"")
        self.assertIn(""agent"", payload)
        self.assertEqual(payload[""agent""], agent.NAME)
",tests/test_ping_agent.py,TestPingAgent
survived,"    async def run_cycle(self) -> None:  # pragma: no cover - default wrapper
        """"""Single orchestrator cycle â€“ runs :meth:`step` once.""""""
        await self.step()
",alpha_factory_v1/backend/agents/base.py,AgentBase
survived,"    def test_sha_deterministic(self):
        payload = {""a"": 1, ""b"": 2}
        expected = energy_agent.hashlib.sha256(
            energy_agent.json.dumps(payload, separators=("","", "":"")).encode()
        ).hexdigest()
        self.assertEqual(energy_agent._sha(payload), expected)
",tests/test_energy_utils.py,TestEnergyUtils
survived,"    def __init__(self) -> None:
        self.calls = 0
",tests/test_agent_runner.py,DummyAgent
survived,"def test_run_evolution_different_seeds() -> None:
    def fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    pop1 = mats.run_evolution(fn, 2, population_size=3, generations=1, seed=1)
    pop2 = mats.run_evolution(fn, 2, population_size=3, generations=1, seed=2)

    assert [ind.genome for ind in pop1] != [ind.genome for ind in pop2]",tests/test_mats.py,
survived,"def test_env_value_escaped(tmp_path: Path) -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    target = tmp_path / ""browser""
    shutil.copytree(browser_dir, target)
    token = ""foo</script>bar""
    (target / "".env"").write_text(f""PINNER_TOKEN={token}\n"")
    subprocess.check_call([""npm"", ""run"", ""build""], cwd=target)

    html_text = (target / ""dist"" / ""index.html"").read_text()
    assert token not in html_text
    assert ""window.PINNER_TOKEN=atob("" in html_text

    url = (target / ""dist"" / ""index.html"").as_uri()
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        assert page.evaluate(""window.PINNER_TOKEN"") == token
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_env_escaping.py,
survived,"async def test_parallel():
    outputs = []

    async def r1(prompt, **kwargs):
        outputs.append(""r1"")
        return prompt + ""-r1""

    async def r2(prompt, **kwargs):
        outputs.append(""r2"")
        return prompt + ""-r2""

    wf = Workflow(
        name=""wf"",
        steps=[WorkflowStep(runner=[r1, r2], mode=StepMode.PARALLEL)],
    )

    result = await wf.run(""x"")
    assert sorted(outputs) == [""r1"", ""r2""]
    assert sorted(result) == [""x-r1"", ""x-r2""]
",tests/test_workflow.py,
survived,"async def test_sequential_defaults():
    called = {}

    async def runner(prompt, user_id=None, session_id=None, llm=None, sdk_context=None):
        called['params'] = (user_id, session_id, llm, sdk_context)
        return prompt + ""-done""

    wf = Workflow(
        name=""wf"",
        instruction=""instr"",
        description=""desc"",
        default_llm=""llm"",
        sdk_context=SDKContext(""./swarmzero_config_test.toml""),
        default_user_id=""u"",
        default_session_id=""s"",
        steps=[WorkflowStep(runner=runner)],
    )

    result = await wf.run(""hi"")
    assert result == ""hi-done""
    assert called[""params""][:3] == (""u"", ""s"", ""llm"")
    assert isinstance(called[""params""][3], SDKContext)
",tests/test_workflow.py,
survived,"    async def _run() -> None:
        await agents[0].run_cycle()
        assert plan_msgs, ""planning agent did not emit research plan""

        await agents[1].handle(plan_msgs[0])
        assert strat_msgs, ""research agent did not emit strategy payload""

        await agents[2].handle(strat_msgs[0])
        assert market_msgs, ""strategy agent did not emit market analysis""

        await agents[3].handle(market_msgs[0])
        assert code_msgs, ""market agent did not emit code generation task""

        await agents[4].handle(code_msgs[0])
        assert safety_in, ""codegen agent did not emit safety event""

        await agents[5].handle(safety_in[0])
        assert memory_msgs, ""safety agent did not emit memory payload""

        await mem_agent.handle(memory_msgs[0])
        assert mem_agent.records, ""memory agent did not store payload""
        ledger.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_agents.py,
survived,"async def test_example_runs(example):
    example_path = Path(__file__).resolve().parents[1] / ""examples"" / example
    db_path = example_path.parent / ""shop.db""
    if db_path.exists():
        db_path.unlink()

    params = StdioServerParameters(command=sys.executable, args=[str(example_path)])
    async with ClientSessionGroup() as group:
        session = await group.connect_to_server(params)
        await session.list_tools()

    # Clean up database file if created by shop_api_sqlite example
    if db_path.exists():
        db_path.unlink()",tests/test_examples.py,
survived,"def test_scatter_add():
    B, S, V = Axis(""batch"", 2), Axis(""seq"", 3), Axis(""vocab"", 5)
    x = hax.zeros((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    ones = hax.ones((B, S))
    y = x.at[{V: idx}].add(ones)
    ref = jnp.zeros((2, 3, 5)).at[jnp.arange(2)[:, None], jnp.arange(3)[None, :], idx.array].add(1.0)
    assert jnp.array_equal(y.array, ref)
",tests/test_scatter_gather.py,
survived,"def test_noncontig_selectors():
    B, X, Z, Y = Axis(""batch"", 2), Axis(""x"", 4), Axis(""z"", 6), Axis(""y"", 5)
    a = hax.arange((B, X, Z, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a[""x"", ix, ""y"", iy]
    assert out.axes == (B, Z)
    ref = a.array[jnp.arange(2), ix.array, :, iy.array]
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,
survived,"def test_selector_adds_new_axis():
    B, S, V, T = Axis(""batch"", 2), Axis(""seq"", 3), Axis(""vocab"", 5), Axis(""step"", 4)
    logits = hax.arange((B, S, V))
    idx = hax.arange((B, T), dtype=jnp.int32) % V.size
    out = logits[""vocab"", idx]
    assert set(out.axes) == {B, S, T}
    ref = jnp.transpose(_ref_gather(logits, V, idx), (0, 2, 1))
    assert jnp.array_equal(out.array, ref)
",tests/test_scatter_gather.py,
survived,"    def num_completion_tokens(self) -> int:
        return len(self.token_ids) - self.num_prompt_tokens
",src/levanter/inference/sequence.py,Sequence
survived,"        def _finish(st):
            st.active = st.active.at[idx].set(False)
            st.tail = (st.tail + 1) % self.max_seqs
            return st
",src/levanter/inference/scheduler.py,JittedScheduler
survived,"    def __len__(self) -> int:
        return len(self.token_ids)
",src/levanter/inference/sequence.py,Sequence
survived,"    def generate(
        self, prompts: List[str] | List[List[int]], sampling_params: SamplingParams | List[SamplingParams]
    ) -> List[dict]:
        if not isinstance(sampling_params, list):
            sampling_params = [sampling_params] * len(prompts)

        prompt_ids_list: List[List[int]] = [
            self.tokenizer.encode(p, add_special_tokens=False) if isinstance(p, str) else list(p)
            for p in prompts
        ]

        if len(prompt_ids_list) > MAX_SEQS:
            raise ValueError(f""Too many prompts: got {len(prompt_ids_list)}, max {MAX_SEQS}"")

        max_prompt = max(len(p) for p in prompt_ids_list)
        max_tokens = max(sp.max_tokens for sp in sampling_params)
        page_size = _round_preferred(max_prompt + max_tokens)
        page_table = PageTable.init(
            max_pages=MAX_SEQS,
            max_seqs=MAX_SEQS,
            page_size=page_size,
            max_pages_per_seq=1,
        )
        seq_ids = []
        for _ in prompt_ids_list:
            page_table, seq_id = page_table.assign_seq_id_to_seq()
            seq_ids.append(seq_id)
        cache = self.model.initial_cache(page_table, dtype=self.trainer_cfg.mp.compute_dtype)

        scheduler = Scheduler(self.eos)
        seq_objs = []
        for p, sp, seq_id in zip(prompt_ids_list, sampling_params, seq_ids):
            seq = Sequence(p, sp, seq_id=seq_id)
            seq_objs.append(seq)
            scheduler.add(seq)

        outputs = {}
        while not scheduler.is_finished():
            seqs, is_prefill = scheduler.schedule()
            token_ids = []
            for seq in seqs:
                if is_prefill and seq.status is SequenceStatus.WAITING:
                    tok, cache, page_table = self._prefill(seq, cache, page_table)
                    seq.status = SequenceStatus.RUNNING
                else:
                    tok, cache, page_table = self._decode(seq, cache, page_table, len(seq))
                token_ids.append(tok)
            scheduler.postprocess(seqs, token_ids)
            for seq in seqs:
                if seq.is_finished:
                    outputs[seq.seq_id] = seq.token_ids
        return [
            {""text"": self.tokenizer.decode(out, skip_special_tokens=True), ""token_ids"": out}
            for _, out in sorted(outputs.items())
        ]
",src/levanter/inference/llm_engine.py,LLMEngine
survived,"    def postprocess(self, seqs: List[Sequence], token_ids: List[int]) -> None:
        for seq, token_id in zip(seqs, token_ids):
            seq.append_token(int(token_id))
            if (
                (not seq.sampling_params.ignore_eos and token_id == self.eos)
                or seq.num_completion_tokens >= seq.sampling_params.max_tokens
            ):
                seq.status = SequenceStatus.FINISHED
",src/levanter/inference/scheduler.py,Scheduler
survived,"def gen_wd_table(n):
    goal = [[0] * i + [n] + [0] * (n - 1 - i) for i in range(n)]
    goal[-1][-1] = n - 1
    goal = tuple(sum(goal, []))

    table = {}
    to_visit = [(goal, 0, n-1)]
    while to_visit:
        cfg, cost, e = to_visit.pop(0)
        enccfg = encode_cfg(cfg, n)
        if enccfg in table: continue
        table[enccfg] = cost

        for d in [-1, 1]:
            if 0 <= e + d < n:
                for c in range(n):
                    if cfg[n*(e+d) + c] > 0:
                        ncfg = list(cfg)
                        ncfg[n*(e+d) + c] -= 1
                        ncfg[n*e + c] += 1
                        to_visit.append((tuple(ncfg), cost + 1, e+d))

    return table
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,
survived,"    def __repr__(self):
        # printable version of self

        return str(self.tiles[0])+'\n'+str(self.tiles[1])+'\n'+str(self.tiles[2])+'\n'+str(self.tiles[3])+'\n'
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,Position
survived,"def slide_randomize(p, neighbours):
    for _ in range(len(p) ** 2):
        _, p, _ = random.choice(list(neighbours(p)))
    return p
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,
survived,"def find_zero(tiles):
    """""" file the 0 tile """"""
    for row in range(4):
        for col in range(4):
            if tiles[row][col] == 0:
                return (row, col)
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-2.py,
survived,"    def _search(self, g, bound):
        self.nodes_evaluated += 1

        node = self.path[-1]
        f = g + self.h(node)
        if f > bound: return f
        if self.is_goal(node): return self.FOUND

        m = None # Lower bound on cost.
        for cost, n, descr in self.neighbours(node):
            if n in self.is_in_path: continue

            self.path.append(n)
            self.is_in_path.add(n)
            self.path_descrs.append(descr)
            t = self._search(g + cost, bound)

            if t == self.FOUND: return self.FOUND
            if m is None or (t is not None and t < m): m = t

            self.path.pop()
            self.path_descrs.pop()
            self.is_in_path.remove(n)

        return m
",tests/rosetta/x/Python/15-puzzle-solver/15-puzzle-solver-1.py,IDAStar
survived,"    def Init(logger:logging.Logger, moonrakerConfigFilePath:Optional[str], isCompanionMode:bool):
        MoonrakerCredentialManager._Instance = MoonrakerCredentialManager(logger, moonrakerConfigFilePath, isCompanionMode)
",moonraker_octoeverywhere/moonrakercredentialmanager.py,MoonrakerCredentialManager
survived,"    def test_sanity_check_patch_nonexistent(self):
        with tempfile.TemporaryDirectory() as repo:
            open(os.path.join(repo, ""file.py""), ""w"").close()
            bad_patch = """"""--- a/missing.py
+++ b/missing.py
@@
-print('x')
+print('y')
""""""
            with self.assertRaises(ValueError):
                patcher_core._sanity_check_patch(bad_patch, pathlib.Path(repo))
",tests/test_self_healing_patcher.py,TestPatcherCore
survived,"    def test_meta_bridge_compiles(self):
        """"""Ensure the meta-agentic demo bridge compiles.""""""
        path = Path('alpha_factory_v1/demos/meta_agentic_agi/openai_agents_bridge.py')
        py_compile.compile(path, doraise=True)
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"def summarise_with_agent(mean_coop: float, *, agents: int, rounds: int, delta: float, stake: float) -> str:
    """"""Return a natural-language summary of a simulation result.

    If the ``openai`` package and an API key are available, the summary is
    generated with an LLM via the OpenAI Agents SDK.  Otherwise a simple
    fallback string is returned.
    """"""

    base_msg = (
        ""Simulation with {agents} agents, {rounds} rounds, delta={delta}, stake={stake} ""
        ""yielded mean cooperation â‰ˆ {coop:.3f}.""
    ).format(agents=agents, rounds=rounds, delta=delta, stake=stake, coop=mean_coop)

    try:  # optional dependency
        import openai

        client = openai.OpenAI(api_key=os.getenv(""OPENAI_API_KEY""))
        completion = client.chat.completions.create(
            model=""gpt-4o"",
            messages=[
                {""role"": ""system"", ""content"": ""Summarise AGIALPHA governance simulation results.""},
                {""role"": ""user"", ""content"": base_msg},
            ],
            max_tokens=60,
        )
        return completion.choices[0].message.content.strip()
    except Exception:
        return base_msg
",alpha_factory_v1/demos/solving_agi_governance/governance_sim.py,
survived,"    def test_basic_convergence(self) -> None:
        coop = run_sim(agents=50, rounds=500, delta=0.85, stake=2.5, seed=1)
        self.assertGreater(coop, 0.7)
",tests/test_governance_sim.py,TestGovernanceSim
survived,"    def reset(self) -> None:
        """"""Return to generation zero with a fresh population.""""""
        self._init_population()
        self.gen = 0
        self.history.clear()
        self._archive.clear()
        self._best_fitness = -math.inf
        self.best_genome = self.population[0]
        self._last_scores.clear()
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"def add(a, b):
    return a + b
",tests/machine/x/python/fun_call.py,
survived,"def inc(x: int) -> int:
    return x + k
",tests/machine/x/python/pure_global_fold.py,
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",tests/test_agents.py,
survived,"    def test_cpu_alignment(self):
        sw = SmithWatermanGPU()
        score, _ = sw.align(""ACACACTA"", ""AGCACACA"")
        self.assertEqual(score, 17)
",src/test/python/test_gpu_smith_waterman.py,TestSmithWatermanGPU
survived,"    def _plot_nicely(self, mat, title, xlabel, ylabel, outfile=None):
        fig = plt.figure()
        ax = fig.add_subplot(111)
        im = ax.matshow(mat)
        ax.set_title(title)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.set_aspect(2)
        ax.set_aspect(""auto"")
        plt.colorbar(im)
        if outfile is not None:
            plt.savefig(outfile)
        plt.show()
",examples/synthetic_data.py,HldaDataGenerator
survived,"    def labels(self, *a, **kw):
        return self
",tests/test_eventbus.py,_M
survived,"    def inc(self, *a, **kw):
        pass
",tests/test_eventbus.py,_M
survived,"    def read_and_clear(self, topic: str | None = None) -> Dict[str, list[Dict[str, Any]]]:
        """"""Return queued events and clear the buffers (dev mode helper).""""""
        if self._queues is None:
            return {}
        topics = [topic] if topic else list(self._queues)
        result: Dict[str, list[Dict[str, Any]]] = {}
        for t in topics:
            q = self._queues.get(t)
            if not q:
                continue
            items: list[Dict[str, Any]] = []
            while not q.empty():
                try:
                    items.append(q.get_nowait())
                except asyncio.QueueEmpty:
                    break
            if items:
                result[t] = items
        return result
",alpha_factory_v1/backend/agent_runner.py,EventBus
survived,"async def test_guardrails_called_in_order():
    adapter = MockAdapter()
    router = GuardrailModelRouter({""a"": adapter}, default_model=""a"")
    order: list[str] = []

    async def input_guardrail(prompt: str):
        order.append(f""in:{prompt}"")

    async def output_guardrail(output: str):
        order.append(f""out:{output}"")

    router.add_input_guardrail(input_guardrail)
    router.add_output_guardrail(output_guardrail)

    res = await router.invoke(""test"")

    assert res == ""test:ok""
    assert order == [""in:test"", ""out:test:ok""]",tests/test_guardrail_router.py,
survived,"    def _check_mixed_args_grad(self, name: str):
        """"""Verify gradient of the dot product xÂ·y with respect to each argument.""""""
        try:
            backend.set_backend(name)
        except ImportError:
            raise unittest.SkipTest(f""{name} backend not available"")
        b = backend.current()

        def f(x, y):
            return b.sum(b.mul(x, y))

        gx = b.grad(f, wrt=0)
        gy = b.grad(f, wrt=1)
        x = b.array([1.0, 2.0, 3.0], requires_grad=True)
        y = b.array([4.0, 5.0, 6.0], requires_grad=True)
        gradx = to_numpy(gx(x, y))
        grady = to_numpy(gy(x, y))
        np.testing.assert_allclose(np.array(gradx), np.array([4.0, 5.0, 6.0]))
        np.testing.assert_allclose(np.array(grady), np.array([1.0, 2.0, 3.0]))
",tests/test_autograd.py,TestAutograd
survived,"def update_pyodide(version: str) -> None:
    base_url = f""https://cdn.jsdelivr.net/pyodide/v{version}/full""
    root = Path(__file__).resolve().parent
    fetch_assets = root / ""fetch_assets.py""
    text = fetch_assets.read_text()

    text = re.sub(r""DEFAULT_PYODIDE_BASE_URL = \""[^\""]+\"""", f'DEFAULT_PYODIDE_BASE_URL = ""{base_url}""', text)
    text = re.sub(r""# Updated to Pyodide [^\n]+"", f""# Updated to Pyodide {version}"", text)

    files = [""pyodide.js"", ""pyodide.asm.wasm""]
    checksums: Dict[str, str] = {}
    for name in files:
        data = fetch(f""{base_url}/{name}"")
        checksums[name] = f""sha384-{sha384_b64(data)}""

    for name, checksum in checksums.items():
        pattern = rf'""{name}"":\s*""[^""]+""'
        text = re.sub(pattern, f'""{name}"": ""{checksum}""', text)

    fetch_assets.write_text(text)

    subprocess.run([sys.executable, str(root / ""generate_build_manifest.py"")], check=True)
",scripts/update_pyodide.py,
survived,"def download(cid: str, path: Path) -> None:
    url = f""{GATEWAY}/{cid}""
    path.parent.mkdir(parents=True, exist_ok=True)
    with _session().get(url, timeout=60) as resp:
        resp.raise_for_status()
        path.write_bytes(resp.content)
",scripts/fetch_assets.py,
survived,"def _session() -> requests.Session:
    retry = Retry(total=5, backoff_factor=1)
    adapter = HTTPAdapter(max_retries=retry)
    s = requests.Session()
    s.mount(""https://"", adapter)
    s.mount(""http://"", adapter)
    return s
",scripts/fetch_assets.py,
survived,"    def _tool(*_a: object, **_k: object) -> Callable[[object], object]:
        def dec(f: object) -> object:
            return f

        return dec
",tests/test_alpha_opportunity_stub.py,
survived,"        def __init__(self, *a: object, port: int = 5001, **_k: object) -> None:
            captured[""port""] = port
",tests/test_alpha_opportunity_stub.py,DummyRuntime
survived,"def test_offline_queue_flushes_on_reconnect() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""Object.defineProperty(navigator,'onLine',{get:()=>false,configurable:true});""
            ""navigator.sendBeacon=()=>false;""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.click(""text=Share"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert page.evaluate(""JSON.parse(localStorage.getItem('telemetryQueue')).length > 0"")
        page.evaluate(
            ""navigator.sendBeacon=(...a)=>{(window.sent=window.sent||[]).push(a);return true;}""
            ""Object.defineProperty(navigator,'onLine',{get:()=>true});""
            ""window.dispatchEvent(new Event('online'));""
        )
        page.wait_for_function(""window.sent && window.sent.length > 0"")
        assert page.evaluate(""localStorage.getItem('telemetryQueue')"") == ""[]""
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,
survived,"def test_has_network_all_fail(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Return False when none of the hosts are reachable.""""""

    attempts = []

    def _connect(_addr: tuple[str, int], timeout: float = 1.0) -> None:
        attempts.append(_addr)
        raise OSError

    monkeypatch.setattr(check_env.socket, ""create_connection"", _connect)  # type: ignore[attr-defined]
    assert check_env.has_network() is False
    assert len(attempts) >= 3",tests/test_check_env_network.py,
survived,"def pg_container():
    cid = subprocess.check_output([
        ""docker"",
        ""run"",
        ""-d"",
        ""-e"",
        ""POSTGRES_USER=insight"",
        ""-e"",
        ""POSTGRES_PASSWORD=insight"",
        ""-e"",
        ""POSTGRES_DB=insight"",
        ""-p"",
        ""55432:5432"",
        ""postgres:16-alpine"",
    ]).decode().strip()
    try:
        for _ in range(30):
            res = subprocess.run(
                [""docker"", ""exec"", cid, ""pg_isready"", ""-U"", ""insight""],
                capture_output=True,
            )
            if res.returncode == 0:
                break
            time.sleep(1)
        else:
            subprocess.run([""docker"", ""logs"", cid], check=False)
            raise RuntimeError(""postgres not ready"")
        yield cid
    finally:
        subprocess.run([""docker"", ""rm"", ""-f"", cid], check=False)
",tests/test_postgres_ledger.py,
survived,"def test_ledger_postgres_persistence(pg_container):
    os.environ.update(
        {
            ""PGHOST"": ""localhost"",
            ""PGPORT"": ""55432"",
            ""PGUSER"": ""insight"",
            ""PGPASSWORD"": ""insight"",
            ""PGDATABASE"": ""insight"",
        }
    )
    ledger = Ledger(""/tmp/ignore.db"", db=""postgres"", broadcast=False)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    ledger.close()

    conn = psycopg2.connect(host=""localhost"", port=55432, user=""insight"", password=""insight"", dbname=""insight"")
    with conn, conn.cursor() as cur:
        cur.execute(""SELECT count(*) FROM messages"")
        count = cur.fetchone()[0]
    conn.close()
    assert count == 1",tests/test_postgres_ledger.py,
survived,"def test_restart_unsubscribes_handler() -> None:
    bus = messaging.A2ABus(orchestrator.config.Settings(bus_port=0))
    ledger = _Ledger()
    agent = DummyBaseAgent(bus, ledger)
    runner = orchestrator.AgentRunner(agent)

    async def _run() -> tuple[int, int]:
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        before = agent.count
        await runner.restart(bus, ledger)
        new_agent = runner.agent  # type: ignore[assignment]
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        return before, getattr(new_agent, ""count"")

    before, after = asyncio.run(_run())

    assert before == 1
    assert agent.count == 1
    assert after == 1
    assert len(bus._subs.get(""dummy"", [])) == 1",tests/test_agent_runner.py,
survived,"    async def handle(self, _env: messaging.Envelope) -> None:
        self.count += 1
",tests/test_agent_runner.py,DummyBaseAgent
survived,"    async def _run() -> tuple[int, int]:
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        before = agent.count
        await runner.restart(bus, ledger)
        new_agent = runner.agent  # type: ignore[assignment]
        bus.publish(""dummy"", messaging.Envelope(""a"", ""dummy"", {}, 0.0))
        await asyncio.sleep(0)
        return before, getattr(new_agent, ""count"")
",tests/test_agent_runner.py,
survived,"    def start_merkle_task(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - test helper
        pass
",tests/test_agent_runner.py,_Ledger
survived,"def test_apply_patch_rollback_on_failure(tmp_path: Path, monkeypatch: mock.MagicMock) -> None:
    target = tmp_path / ""hello.txt""
    target.write_text(""hello\n"", encoding=""utf-8"")

    def fake_run(cmd, cwd):
        return 1, ""patch failed""

    monkeypatch.setattr(patcher_core, ""_run"", fake_run)
    with pytest.raises(RuntimeError):
        patcher_core.apply_patch(_DEF_DIFF, repo_path=str(tmp_path))

    assert target.read_text(encoding=""utf-8"") == ""hello\n""
    assert not (tmp_path / ""hello.txt.bak"").exists()
",tests/test_patcher_core_additional.py,
survived,"    def fake_run(cmd, cwd):
        return 1, ""patch failed""
",tests/test_patcher_core_additional.py,
survived,"def test_import_with_agents_only(monkeypatch: pytest.MonkeyPatch) -> None:
    stub = types.ModuleType(""agents"")
    stub.Agent = object
    stub.AgentRuntime = object

    class DummyOpenAI:
        def __init__(self, *args: object, **kwargs: object) -> None:
            pass

    stub.OpenAIAgent = DummyOpenAI

    def _tool(*_a: object, **_k: object) -> object:
        def _decorator(func: object) -> object:
            return func

        return _decorator

    stub.Tool = _tool

    monkeypatch.setitem(sys.modules, ""agents"", stub)
    sys.modules.pop(""openai_agents"", None)

    orig_import = builtins.__import__

    def fake_import(name: str, globals=None, locals=None, fromlist=(), level=0):
        if name == ""openai_agents"":
            raise ModuleNotFoundError(name)
        if name == ""alpha_opportunity_stub"":
            return importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_opportunity_stub"")
        if name == ""alpha_conversion_stub"":
            return importlib.import_module(""alpha_factory_v1.demos.aiga_meta_evolution.alpha_conversion_stub"")
        return orig_import(name, globals, locals, fromlist, level)

    monkeypatch.setattr(builtins, ""__import__"", fake_import)

    for mod_name in MODULES:
        mod = importlib.reload(importlib.import_module(mod_name))
        assert mod.OpenAIAgent is stub.OpenAIAgent",tests/test_aiga_agents_import.py,
survived,"    def _model_dump(self: BaseModel, *args: Any, **kwargs: Any) -> Any:
        return self.dict(*args, **kwargs)
",src/meta_agent/__init__.py,
survived,"def _diversity(values):
    if len(values) < 2:
        return 0.0
    d = 0.0
    c = 0
    for i in range(len(values)):
        for j in range(i + 1, len(values)):
            d += abs(values[i] - values[j])
            c += 1
    return d / c
",tests/test_backtrack_boost.py,
survived,"async def _evaluate(g):
    await asyncio.sleep(0)
    return g, 0.01
",tests/test_backtrack_boost.py,
survived,"    def set_state(self, key: str, value: str) -> None:
        """"""Store ``key`` as ``value`` in the ``state`` table.""""""
        with Session(self.engine) as session:
            session.merge(_StateRow(key=key, value=value))
            session.commit()",src/archive/db.py,ArchiveDB
survived,"async def _phase_loop(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], Awaitable[tuple[float, float]]],
    archive: InMemoryArchive,
    *,
    phase: Phase,
    max_cost: float | None = None,
    wallclock: float | None = None,
    backtrack_rate: float = 0.0,
    phase_hook: Optional[Callable[[Phase], None]] = None,
) -> None:
    if not archive.all():
        await archive.accept(Candidate(genome=0.0, fitness=0.0, novelty=1.0, cost=0.0))

    spent = 0.0
    start = time.time()

    while True:
        if max_cost is not None and spent >= max_cost:
            break
        if wallclock is not None and time.time() - start >= wallclock:
            break

        population = archive.all()
        parent = backtrack_boost(population, population, backtrack_rate)
        genome = operator(parent.genome)
        if phase_hook:
            phase_hook(phase)
        fitness, cost = await evaluate(genome)
        child = Candidate(genome=genome, fitness=fitness, novelty=random.random(), cost=cost)
        await archive.accept(child)
        metrics.dgm_children_total.inc()
        spent += cost
",src/evolve.py,
survived,"async def self_mod_phase(
    operator: Callable[[Any], Any],
    evaluate: Callable[[Any], Awaitable[tuple[float, float]]],
    archive: InMemoryArchive,
    *,
    max_cost: float | None = None,
    wallclock: float | None = None,
    backtrack_rate: float = 0.0,
    phase_hook: Optional[Callable[[Phase], None]] = None,
) -> None:
    await _phase_loop(
        operator,
        evaluate,
        archive,
        phase=Phase.SELF_MOD,
        max_cost=max_cost,
        wallclock=wallclock,
        backtrack_rate=backtrack_rate,
        phase_hook=phase_hook,
    )
",src/evolve.py,
survived,"def _write_heatmap(data: Dict[str, Dict[str, float]]) -> None:
    if plt is None or np is None:
        return

    patches = list(data.keys())
    cols = INNOVATIONS
    arr = np.zeros((len(patches), len(cols)))
    for i, p in enumerate(patches):
        baseline = data[p][""baseline""]
        for j, c in enumerate(cols):
            arr[i, j] = baseline - data[p][c]
    fig, ax = plt.subplots(figsize=(2 + len(cols), 1 + len(patches)))
    im = ax.imshow(arr, cmap=""viridis"")
    ax.set_xticks(np.arange(len(cols)))
    ax.set_xticklabels(cols)
    ax.set_yticks(np.arange(len(patches)))
    ax.set_yticklabels(patches)
    for i in range(arr.shape[0]):
        for j in range(arr.shape[1]):
            ax.text(j, i, f""{arr[i, j]:.2f}"", ha=""center"", va=""center"", color=""white"")
    fig.colorbar(im, ax=ax, label=""âˆ† pass rate"")
    plt.tight_layout()
    HEATMAP_OUT.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(HEATMAP_OUT)
",src/tools/ablation_runner.py,
survived,"def test_pareto_front_after_five_generations() -> None:
    def fn(genome: list[float]) -> tuple[float, float]:
        x, y = genome
        return x**2, y**2

    pop = mats.run_evolution(
        fn,
        2,
        population_size=20,
        generations=5,
        seed=42,
        scenario_hash=""test"",
    )
    front = mats.pareto_front(pop)
    assert len(front) >= 10",tests/test_mats.py,
survived,"def _build_tree(records: List[Dict[str, Any]]) -> Dict[str, Any]:
    tree: Dict[str, Any] = {""name"": ""Start"", ""children"": []}
    best_score = float(""-inf"")
    best_path: List[str] = []
    for rec in records:
        path = rec.get(""path"")
        if not isinstance(path, list):
            continue
        score = float(rec.get(""score"", 0))
        _add_path(tree, path)
        if score > best_score:
            best_score = score
            best_path = [""Start""] + path
    tree[""bestPath""] = best_path
    return tree
",alpha_factory_v1/demos/alpha_agi_insight_v1/tools/export_tree.py,
survived,"    def test_index_3d_wildcard(self):
        """"""Indexing into 3D array with wildcard""""""
        expr = '[[[1 2] [3 4]] [[5 6] [7 8]]]:@[1 0 []]'
        self.assert_eval_cmp(expr, '[5 6]')",tests/test_numpy_slice.py,TestNumpySliceBehavior
survived,"    def test_index_column_wildcard(self):
        """"""Select entire third column using wildcard""""""
        self.assert_eval_cmp('[[1 2 3] [4 5 6]]:@[[] 2]', '[3 6]')
",tests/test_numpy_slice.py,TestNumpySliceBehavior
survived,"def test_auto_rebuild_on_drift(tmp_path) -> None:
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo""), ""hello foo"")

    index = TemplateIndex(reg)
    index.rebuild()

    # modify template to trigger checksum mismatch
    tpl_path = reg.templates_dir / ""foo"" / ""v0_1_0"" / ""template.yaml""
    tpl_path.write_text(""hi foo"", encoding=""utf-8"")

    index.ensure_up_to_date()
    results = index.search(""hi foo"")
    assert results and results[0][""slug""] == ""foo""",tests/test_template_index.py,
survived,"    def test_high_delta_promotes_cooperation(self):
        coop = run_sim(agents=20, rounds=100, delta=0.8, stake=2.5, seed=0)
        self.assertGreaterEqual(coop, 0.8)
",alpha_factory_v1/tests/test_governance_sim.py,GovernanceSimTest
survived,"    def _thread_eval(self):
        with ThreadPoolExecutor() as pool:
            results = list(pool.map(self._simulate, self.population))
        return self._post_eval(results)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def __init__(
        self,
        env_cls: Callable,
        pop_size: int = 32,
        elitism: int = 2,
        parallel: bool = True,
        checkpoint_dir: pathlib.Path = CHKPT_DIR,
        llm: Callable[[str], str] | None = None,
    ):
        self.env_cls, self.pop_size, self.elitism = env_cls, pop_size, elitism
        self.parallel = parallel
        self.ckpt_dir = checkpoint_dir
        self.llm = llm
        self.rng = random.Random(int(""A1GA"", 16))
        self.gen = 0
        self.history: List[Tuple[int, float]] = []
        self._archive: List[np.ndarray] = []
        self._best_fitness = -math.inf
        self.best_genome: Genome | None = None
        self._last_scores: List[float] = []
        self._init_population()
        if self.parallel and _HAS_RAY and not ray.is_initialized():
            ray.init(ignore_reinit_error=True, _temp_dir=str(self.ckpt_dir / ""ray""))
        LOG.info(""Evolver ready â–¶ pop=%d device=%s"", self.pop_size, Device)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def _save(self):
        data = {
            ""gen"": self.gen,
            ""pop"": [g.to_json() for g in self.population],
            ""hist"": self.history,
            ""arc"": [a.tolist() for a in self._archive[-256:]],
            ""seed"": self.rng.random(),
            ""sha"": self.population_sha(),
            ""best_fitness"": self._best_fitness,
            ""best_genome"": self.best_genome.to_json() if self.best_genome else None,
            ""ts"": datetime.now(timezone.utc).isoformat()
        }
        p = CHKPT_DIR / f""gen_{self.gen:04d}.json.tmp""
        p.write_text(json.dumps(data)); p.replace(p.with_suffix(""""))
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,MetaEvolver
survived,"    def forward(self, x: torch.Tensor):
        act_fn = _ACT[self.genome.activation]
        h = x
        for layer in self.model[:-1]:
            if isinstance(layer, nn.Linear):
                h = act_fn(layer(h))
                if self.genome.hebbian:
                    with torch.no_grad():
                        dw = 0.03 * torch.bmm(h.unsqueeze(2), x.unsqueeze(1))
                        self.hFast = (self.hFast + dw.mean(0)).clamp(-0.02, 0.02)
                        layer.weight.data += self.hFast
            else:
                h = layer(h)
        return self.model[-1](h)
",alpha_factory_v1/demos/aiga_meta_evolution/meta_evolver.py,EvoNet
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/gamma-function.py,
survived,"def ln(x):
    k = 0.0
    v = x
    while v >= 2.0:
        v = v / 2.0
        k = k + 1.0
    while v < 1.0:
        v = v * 2.0
        k = k - 1.0
    z = (v - 1.0) / (v + 1.0)
    zpow = z
    sum = z
    i = 3
    while i <= 9:
        zpow = zpow * z * z
        sum = sum + zpow / (float(i))
        i = i + 2
    ln2 = 0.6931471805599453
    return (k * ln2) + 2.0 * sum
",tests/rosetta/transpiler/Python/gamma-function.py,
survived,"def test_png(h, f):
    """"""Verify if the image is a PNG.""""""
    if h.startswith(b'\211PNG\r\n\032\n'):
        return 'png'
",metaflow/_vendor/imghdr/__init__.py,
survived,"    def render(self, task):
        from .convert_to_native_type import TaskToDict
        import base64

        png_bytes = base64.b64decode(
            ""iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR4nGNgYGBgAAAABQABRDE8UwAAAABJRU5ErkJggg==""
        )
        img_src = TaskToDict().parse_image(png_bytes)
        return f""<html><img src='{img_src}' /></html>""",metaflow/plugins/cards/card_modules/test_cards.py,TestImageCard
survived,"def _run_sim(page):
    page.evaluate(""document.querySelector('#simulator-panel #sim-seeds').value='1'"")
    page.evaluate(f""document.querySelector('#simulator-panel #sim-gen').value={DEF_GEN}"")
    page.evaluate(""document.querySelector('#simulator-panel #sim-pop').value=3"")
    page.click(""#simulator-panel #sim-start"")
    page.wait_for_function(""window.pop && window.pop[0] && window.pop[0].umap"")
    coords = page.evaluate(""window.pop.map(p=>p.umap)"")
    page.click('#simulator-panel #sim-cancel')
    return coords
",tests/test_umap_fallback.py,
survived,"    def lookup_artist(name: str) -> str | None:
        url = f""https://api.lidarr.audio/api/v0.4/search?type=artist&query=\""{urllib.parse.quote_plus(name)}\""""
        resp = session.get(url, headers=headers)
        if resp.status_code == 200 and resp.text not in ("""", ""[]""):
            data = resp.json()
            if isinstance(data, list):
                return data[0].get(""id"")
            return data.get(""id"")
        return None
",arr_gui.py,
survived,"def sonarr_import(csv_path: str, cfg: configparser.ConfigParser) -> None:
    baseurl = cfg[""sonarr""][""baseurl""]
    urlbase = cfg[""sonarr""].get(""urlbase"", """")
    api_key = cfg[""sonarr""][""api_key""]
    root = cfg[""sonarr""][""rootfolderpath""]
    profile = cfg[""sonarr""][""qualityProfileId""]
    search = cfg.getboolean(""sonarr"", ""searchForShow"", fallback=False)

    headers = {""Content-type"": ""application/json"", ""X-Api-Key"": api_key}
    session = requests.Session()

    with open(csv_path, encoding=""utf-8"") as f:
        reader = csv.DictReader(f)
        for row in reader:
            title = row.get(""title"")
            year = row.get(""year"")
            imdbid = row.get(""imdbid"")
            if imdbid:
                url = f""{baseurl}{urlbase}/api/v3/series/lookup?term=imdb:{imdbid}""
            else:
                term = urllib.parse.quote_plus(f""{title} {year}"" if year else title)
                url = f""{baseurl}{urlbase}/api/v3/series/lookup?term={term}""
            rsp = session.get(url, headers=headers)
            if rsp.status_code != 200 or rsp.text in ("""", ""[]""):
                messagebox.showwarning(""Sonarr"", f""{title} not found"")
                continue
            data = rsp.json()
            if isinstance(data, list):
                data = data[0]
            payload = {
                ""title"": data.get(""title""),
                ""tvdbId"": data.get(""tvdbId""),
                ""year"": data.get(""year""),
                ""titleSlug"": data.get(""titleSlug""),
                ""qualityProfileId"": int(profile),
                ""rootFolderPath"": root,
                ""monitored"": True,
                ""seasonFolder"": True,
                ""images"": data.get(""images"", []),
                ""seasons"": data.get(""seasons"", []),
                ""addOptions"": {""searchForMissingEpisodes"": search},
            }
            add_url = f""{baseurl}{urlbase}/api/v3/series""
            session.post(add_url, headers=headers, json=payload)
",arr_gui.py,
survived,"    def __init__(self, std: float = 0.1, bounds: tuple[float, float] = (-1.0, 1.0), rng: random.Random | None = None) -> None:
        self.std = std
        self.bounds = bounds
        self.rng = rng or random.Random()
",src/simulation/mats_ops.py,GaussianParam
survived,"        def json(self) -> dict:
            return self._data
",tests/test_selfheal_entrypoint_offline.py,DummyResp
survived,"def test_demo_index_loads(demo_dir: Path) -> None:
    url = (demo_dir / ""index.html"").resolve().as_uri()
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            page.goto(url)
            page.wait_for_selector(""body"")
            assert page.query_selector(""h1""), ""h1 element missing""
            browser.close()
    except PlaywrightError as exc:
        pytest.skip(f""Playwright browser not installed: {exc}"")",tests/test_docs_demos.py,
survived,"    def test_entries_feed_includes_subscribe_note(self):
        EntryFactory()
        response = self.client.get(""/atom/entries/"")
        self.assertIn(
            ""You are only seeing the entries from my blog. Subscribe to"",
            response.content.decode(),
        )
",blog/tests.py,BlogTests
survived,"    def tearDown(self) -> None:
        agents_mod._WHEEL_PUBKEY = self.orig_pub
        self.tmpdir.cleanup()
",tests/test_verify_wheel_sig.py,VerifyWheelSigTests
survived,"    def __init__(self, bus: orchestrator.messaging.A2ABus, ledger: orchestrator.Ledger) -> None:
        super().__init__(""dummy"", bus, ledger)
",tests/test_orchestrator.py,DummyAgent
survived,"def test_publish_kafka_disabled() -> None:
    events: list[messaging.Envelope] = []

    async def handler(env: messaging.Envelope) -> None:
        events.append(env)

    cfg = config.Settings(bus_port=0)
    with mock.patch.object(messaging, ""AIOKafkaProducer"", None):
        bus = messaging.A2ABus(cfg)
        bus.subscribe(""x"", handler)

        async def run() -> None:
            async with bus:
                env = messaging.Envelope(""a"", ""x"", {""ok"": True}, 0.0)
                bus.publish(""x"", env)
                await asyncio.sleep(0)

        asyncio.run(run())

    assert len(events) == 1
    assert events[0].payload[""ok""] is True",tests/test_message_bus.py,
survived,"    def test_init_with_dsn_no_pg(self):
        mem = mv.VectorMemory(dsn=""postgres://user:pass@localhost/db"")
        self.assertEqual(mem.backend, ""numpy"")
",tests/test_memory_vector.py,TestVectorMemoryOffline
survived,"        def _tool(*_a, **_k):
            def _decorator(func):
                return func

            return _decorator
",tests/test_openai_bridge.py,TestOpenAIBridge
survived,"  async def take_in_plate(self, plate: Plate, site: PlateHolder):
    """"""Place a plate from the transfer station into storage at the given site.""""""
    m, n = self._site_to_m_n(site)
    await self._send_command(f""WR DM0 {m}"")  # carousel pos
    await self._send_command(f""WR DM5 {n}"")  # handler level
    await self._send_command(""ST 1904"")  # plate to storage
    await self._wait_ready()
    await self._send_command(""ST 1903"")  # terminate access
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  def serialize(self):
    return {
      **Machine.serialize(self),
      **Resource.serialize(self),
      ""backend"": self.backend.serialize(),
      ""racks"": [rack.serialize() for rack in self._racks],
      ""loading_tray_location"": serialize(self.loading_tray.location),
    }
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def open_door(self):
    pass
",pylabrobot/storage/backend.py,IncubatorBackend
survived,"  async def set_shaking_frequency(
    self, frequency: int, shakers: Optional[List[int]] = None
  ) -> List[str]:
    shakers = shakers or [1, 2]
    assert all(shaker in [1, 2] for shaker in shakers), ""Shaker index must be 1 or 2""
    return [await self.send_command(""se"", f""pb 2{idx-1}"", f""{frequency:04}"") for idx in shakers]
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def open_door(self):
    await self._send_command(""ST 1901"")
    await self._wait_ready()
",pylabrobot/storage/cytomat/heraeus_cytomat_backend.py,HeraeusCytomatBackend
survived,"  async def action_exposed_to_storage(self, site: PlateHolder) -> OverviewRegisterState:
    """"""Return with MTP from exposed to storage, move to wait, close door""""""
    return await self.send_action(""mv"", ""hs"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def get_temperature(self) -> float:
    return await self.backend.get_temperature()
",pylabrobot/storage/incubator.py,Incubator
survived,"  async def action_storage_to_exposed(self, site: PlateHolder) -> OverviewRegisterState:
    """"""Move from wait to storage, load MTP, transport to exposed""""""
    return await self.send_action(""mv"", ""sh"", self._site_to_firmware_string(site))
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  async def action_wait_to_transfer(self) -> OverviewRegisterState:
    """"""Open door, place on transfer, return to wait, close door""""""
    return await self.send_action(""mv"", ""wt"", """")
",pylabrobot/storage/cytomat/cytomat.py,CytomatBackend
survived,"  def test_serialization(self):
    i = Incubator(
      name=""test_tc"",
      size_x=10,
      size_y=10,
      size_z=10,
      backend=IncubatorChatterboxBackend(),
      loading_tray_location=Coordinate(0, 0, 0),
      racks=[],
    )

    serialized = i.serialize()
    deserialized = Incubator.deserialize(serialized)
    self.assertEqual(i, deserialized)",pylabrobot/storage/incubator_tests.py,IncubatorTests
survived,"def cytomat_rack_38mm_13(name: str):
  return _cytomat_rack(name=name, site_height=38, num_sites=13, model=""cytomat_rack_38mm_13"")
",pylabrobot/storage/cytomat/racks.py,
survived,"  async def stop_shaking(self):
    await self.backend.stop_shaking()
",pylabrobot/storage/incubator.py,Incubator
survived,"    def render(
        self,
        slug: str,
        *,
        version: str = ""latest"",
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """"""Render a template and all of its dependencies.""""""
        name = slug if version == ""latest"" else f""{slug}@{version}""
        template = self.env.get_template(name)
        return template.render(context or {})
",src/meta_agent/template_mixer.py,TemplateMixer
survived,"def test_template_validator_performance_fail() -> None:
    validator = TemplateValidator()
    case = TemplateTestCase(context={}, expected_output=""Hello"")
    result = validator.validate(""Hello"", [case], max_render_seconds=0.0)
    assert not result.success
    assert any(""too slow"" in e for e in result.errors)",tests/test_template_validator.py,
survived,"    def __init__(self, env: Optional[Environment] = None) -> None:
        self.env = env or Environment()
",src/meta_agent/template_validator.py,TemplateValidator
survived,"def test_export_import_and_rating(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    manager = TemplateSharingManager(reg)
    reg.register(_meta(""greet""), ""hello"", version=""0.1.0"")

    exported = manager.export_template(""greet"")
    assert exported[""content""] == ""hello""

    reg2 = TemplateRegistry(base_dir=tmp_path / ""other"")
    manager2 = TemplateSharingManager(reg2)
    manager2.import_template(exported)
    assert reg2.load_template(""greet"") == ""hello""

    manager.add_rating(""greet"", 5)
    manager.add_rating(""greet"", 3)
    count, avg = manager.get_rating(""greet"")
    assert count == 2 and avg == 4.0

    top = manager.showcase()
    assert top and top[0][0] == ""greet""
",tests/test_template_sharing.py,
survived,"    def add_rating(self, slug: str, rating: int) -> None:
        """"""Store a user rating (1-5) for a template.""""""
        if rating < 1 or rating > 5:
            raise ValueError(""Rating must be between 1 and 5"")
        ratings = self._load_ratings()
        key = slug.replace("" "", ""_"").lower()
        ratings.setdefault(key, []).append(rating)
        self._save_ratings(ratings)
",src/meta_agent/template_sharing.py,TemplateSharingManager
survived,"    def call_stub(prompt: str) -> str:
        return f""[offline] {prompt}""
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/local_llm.py,
survived,"    def __init__(self, *a, **k):
        pass
",tests/test_selfheal_import_stubs.py,DummyButton
survived,"def test_agents_status_outputs_names() -> None:
    runner = CliRunner()
    from unittest.mock import patch

    with patch.object(cli.orchestrator, ""Orchestrator"") as orch_cls:  # type: ignore[attr-defined]
        orch = orch_cls.return_value
        runner_obj = type(
            ""Runner"",
            (),
            {""agent"": type(""Agent"", (), {""name"": ""AgentZ""})()},
        )()
        orch.runners = {""AgentZ"": runner_obj}
        result = runner.invoke(cli.main, [""agents-status""])
    assert result.exit_code == 0
    assert ""AgentZ"" in result.output",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_demo_cli.py,
survived,"def cleanup_ledger_dir() -> None:
    """"""Remove the default ledger directory created during tests.""""""
    yield
    ledger = Path(""ledger"")
    if ledger.exists():
        shutil.rmtree(ledger, ignore_errors=True)",tests/conftest.py,
survived,"def generate_basic_tests(spec: ToolSpecification) -> str:
    """"""Generate minimal pytest code exercising the generated tool.""""""
    param_assignments = []
    for param in spec.input_parameters:
        value = _example_value(param.type_)
        param_assignments.append(f""{param.name}={value}"")
    args = "", "".join(param_assignments)
    test_code = f""""""
import importlib

def test_call():
    mod = importlib.import_module('tool')
    func = getattr(mod, '{spec.name}')
    func({args})
""""""
    return test_code",src/meta_agent/generators/test_generator.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/count-the-coins-2.py,
survived,"def add(a, b):
    return newFps(lambda n: extract(a, n) + extract(b, n))
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/formatted-numeric-output.py,
survived,"def say(n):
    t = """"
    if n < 0:
        t = ""negative ""
        n = -n
    if n < 20:
        return t + small[n]
    else:
        if n < 100:
            t = tens[n // 10]
            s = n % 10
            if s > 0:
                t = t + ""-"" + small[s]
            return t
        else:
            if n < 1000:
                t = small[n // 100] + "" hundred""
                s = n % 100
                if s > 0:
                    t = t + "" "" + say(s)
                return t
    sx = """"
    i = 0
    nn = n
    while nn > 0:
        p = nn % 1000
        nn = nn // 1000
        if p > 0:
            ix = say(p) + illions[i]
            if sx != """":
                ix = ix + "" "" + sx
            sx = ix
        i = i + 1
    return t + sx
",tests/rosetta/transpiler/Python/four-is-magic.py,
survived,"def newBoard():
    b = []
    r = 0
    while r < rows:
        row = []
        c = 0
        while c < cols:
            if _now() % 2 == 0:
                row = row + [""T""]
            else:
                row = row + ["" ""]
            c = c + 1
        b = b + [row]
        r = r + 1
    return b
",tests/rosetta/transpiler/Python/forest-fire.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    cleaning = newNode(""cleaning"", 1, 0.0)
    addChildren(h1_bathrooms, [h1_bathroom1, h1_bathroom2, h1_outside])
    addChildren(h1_living_rooms, [h1_lounge, h1_dining, h1_conservatory, h1_playroom])
    addChildren(house1, [h1_bedrooms, h1_bathrooms, h1_attic, h1_kitchen, h1_living_rooms, h1_basement, h1_garage, h1_garden])
    addChildren(h2_bedrooms, [h2_suite1, h2_suite2, h2_bedroom3, h2_bedroom4])
    addChildren(h2_upstairs, [h2_bedrooms, h2_bathroom, h2_toilet, h2_attics])
    addChildren(h2_living_rooms, [h2_lounge, h2_dining, h2_conservatory, h2_playroom])
    addChildren(h2_groundfloor, [h2_kitchen, h2_living_rooms, h2_wet_room, h2_garage, h2_garden, h2_hot_tub])
    addChildren(h2_basement, [h2_cellars, h2_wine_cellar, h2_cinema])
    addChildren(house2, [h2_upstairs, h2_groundfloor, h2_basement])
    addChildren(cleaning, [house1, house2])
    topCoverage = computeCoverage(cleaning)
    print(""TOP COVERAGE = "" + formatFloat(topCoverage, 6))
    print("""")
    print(""NAME HIERARCHY                 | WEIGHT | COVERAGE |"")
    show(cleaning, 0)
    setCoverage(h2_cinema, 1.0)
    diff = computeCoverage(cleaning) - topCoverage
    print("""")
    print(""If the coverage of the Cinema node were increased from 0.75 to 1"")
    print(""the top level coverage would increase by "" + formatFloat(diff, 6) + "" to "" + formatFloat(topCoverage + diff, 6))
    setCoverage(h2_cinema, 0.75)
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/functional-coverage-tree.py,
survived,"def totalLength():
    tot = 0
    i = 0
    while i < len(words):
        tot = tot + len(words[i])
        if i < len(words) - 1:
            tot = tot + 1
        i = i + 1
    return tot
",tests/rosetta/transpiler/Python/four-is-the-number-of-letters-in-the-....py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    grid = clearGrid()
    ftree(grid, float((width // 2)), float((height - 1)), length, 0.0, depth)
    print(render(grid))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/fractal-tree.py,
survived,"def floorf(x):
    y = int(x)
    return float(y)
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def extract(f, n):
    while len(f.coeffs) <= n:
        idx = len(f.coeffs)
        v = f.compute(idx)
        f = dataclasses.replace(f, coeffs=f.coeffs + [v])
    return f.coeffs[n]
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def fourIsMagic(n):
    s = say(n)
    s = capitalize(s)
    t = s
    while n != 4:
        n = len(s)
        s = say(n)
        t = t + "" is "" + s + "", "" + s
    t = t + "" is magic.""
    return t
",tests/rosetta/transpiler/Python/four-is-magic.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/floyds-triangle.py,
survived,"def main():
    _bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    _bench_start = _now()
    p = sinCos()
    print(""sin:"" + partialSeries(p.sin))
    print(""cos:"" + partialSeries(p.cos))
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({""duration_us"": (_bench_end - _bench_start)//1000, ""memory_bytes"": _bench_mem_end*1024, ""name"": ""main""}, indent=2))
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"def floydWarshall(graph):
    n = len(graph)
    dist = []
    next = []
    i = 0
    while i < n:
        drow = []
        nrow = []
        j = 0
        while j < n:
            drow = drow + [graph[i][j]]
            if graph[i][j] < INF and i != j:
                nrow = nrow + [j]
            else:
                nrow = nrow + [-1]
            j = j + 1
        dist = dist + [drow]
        next = next + [nrow]
        i = i + 1
    k = 0
    while k < n:
        i = 0
        while i < n:
            j = 0
            while j < n:
                if dist[i][k] < INF and dist[k][j] < INF:
                    alt = dist[i][k] + dist[k][j]
                    if alt < dist[i][j]:
                        dist[i][j] = alt
                        next[i][j] = next[i][k]
                j = j + 1
            i = i + 1
        k = k + 1
    return FWResult(dist=dist, next=next)
",tests/rosetta/transpiler/Python/floyd-warshall-algorithm2.py,
survived,"def _lambda3(n):
    if n == 0:
        return 0.0
    return extract(a, n - 1) / (float(n))
",tests/rosetta/transpiler/Python/formal-power-series.py,
survived,"    def fake_eval(self, env, policy, episodes):
        return calls.pop(0)
",tests/test_world_model_open_endedness.py,
survived,"    async def step(self) -> None:
        await self.publish(""alpha.risk"", {""risk"": ""risk level nominal""})
",alpha_factory_v1/demos/alpha_agi_business_v1/alpha_agi_business_v1.py,AlphaRiskAgent
survived,"        async def __aexit__(self, *_a: object, **_k: object) -> None:
            self.closed = True
",tests/test_alpha_agi_business_3_v1.py,DummyADK
survived,"        async def run(self, _msg: str) -> None:
            pass
",tests/test_alpha_agi_business_3_v1.py,DummyADK
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/print_hello.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/left_join_multi.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/substring_builtin.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/load_yaml.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/string_in_operator.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/cross_join_filter.py,
survived,"def _fmt(v):
    if isinstance(v, list):
        return "" "".join((_fmt(x) for x in v))
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)
",tests/machine/x/python/string_contains.py,
survived,"def classify(n: int) -> str:
    if n == 0:
        return ""zero""
    elif n == 1:
        return ""one""
    else:
        return ""many""
",tests/human/x/python/match_full.py,
survived,"    def boom(*args, **kwargs):
        raise RuntimeError(""boom"")
",tests/test_fetch_assets.py,
survived,"def test_tree_visualization(tmp_path: Path) -> None:
    browser_dir = Path(__file__).resolve().parents[1]
    target = tmp_path / ""browser""
    shutil.copytree(browser_dir, target)
    subprocess.check_call([""npm"", ""run"", ""build""], cwd=target)

    url = (target / ""dist"" / ""index.html"").as_uri()
    tree_path = Path(__file__).resolve().parents[4] / ""docs"" / ""alpha_agi_insight_v1"" / ""tree.json""
    tree = json.loads(tree_path.read_text())

    def count_nodes(node: Mapping[str, Any]) -> int:
        return 1 + sum(count_nodes(c) for c in node.get(""children"", []))

    node_count = count_nodes(tree)
    best_path = tree.get(""bestPath"", [])

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context()
        page = context.new_page()
        page.goto(url)
        page.wait_for_selector(""#tree-container"")
        page.wait_for_selector(""#tree-container .node"")

        count_initial = page.eval_on_selector_all(""#tree-container .node"", ""els => els.length"")
        page.wait_for_timeout(1000)
        count_later = page.eval_on_selector_all(""#tree-container .node"", ""els => els.length"")
        assert count_later > count_initial

        context.route(""**"", lambda route: route.abort())
        page.wait_for_function(f""document.querySelectorAll('#tree-container .node').length >= {node_count}"")
        page.wait_for_timeout(len(best_path) * 800 + 500)
        highlighted = page.evaluate(
            ""Array.from(document.querySelectorAll('#tree-container circle[fill='#d62728']'))""
            "".map(n => n.parentNode.querySelector('text').textContent)""
        )
        assert highlighted == best_path
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_tree_visualization.py,
survived,"def test_localstorage_failure_disables_telemetry() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        errors: list[str] = []
        page.on(""pageerror"", lambda err: errors.append(str(err)))
        page.goto(url)
        page.evaluate(
            ""window.OTEL_ENDPOINT='https://example.com';""
            ""window.confirm=() => true;""
            ""Object.defineProperty(localStorage,'setItem',{value:()=>{throw new Error('fail');},configurable:true});""
        )
        page.reload()
        page.wait_for_selector(""#controls"")
        page.evaluate(""window.dispatchEvent(new Event('beforeunload'))"")
        assert not errors
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_telemetry.py,
survived,"    def test_exception_logged(self):
        saved = wm_mod._kafka
        wm_mod._kafka = DummyKafka()
        with mock.patch.object(wm_mod._LOG, ""exception"") as log_exc:
            wm_mod._kafka_send(""test.topic"", {""x"": 1})
            log_exc.assert_called_once()
            msg, topic_arg = log_exc.call_args.args
            self.assertIn(""Kafka emit failed"", msg)
            self.assertEqual(topic_arg, ""test.topic"")
        wm_mod._kafka = saved
",tests/test_world_model_kafka.py,TestKafkaSend
survived,"    def produce(self, topic, payload):
        raise RuntimeError(""boom"")
",tests/test_world_model_kafka.py,DummyKafka
survived,"    def test_invalid_env_fallback(self) -> None:
        env = {""PORT"": ""foo"", ""CYCLE"": ""bar"", ""METRICS_PORT"": ""baz"", ""A2A_PORT"": ""qux""}
        with patch.dict(os.environ, env, clear=True):
            args = edge_runner.parse_args([])
        self.assertEqual(args.port, 8000)
        self.assertIsNone(args.cycle)
        self.assertIsNone(args.metrics_port)
        self.assertIsNone(args.a2a_port)
",tests/test_edge_runner_parse.py,TestParseArgs
survived,"        def generate_text(self, prompt: str) -> str:
            calls.append(prompt)
            return ""sum""
",tests/test_adk_agent.py,StubADK
survived,"    async def run_cycle(self) -> None:
        """"""Generate and emit an ADK summary when data is available.""""""
        with span(""summariser.run_cycle""):
            if not self._records or not self.adk:
                return
            try:
                summary = self.adk.generate_text(""\n"".join(self._records))
            except Exception:
                return
            await self.emit(""strategy"", {""summary"": summary})
            self._records.clear()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/adk_summariser_agent.py,ADKSummariserAgent
survived,"    async def fake_sleep(sec: float):
        delays.append(sec)
        await orig_sleep(0)
",tests/test_orchestrator_backoff.py,
survived,"    async def step(self) -> None:
        return None
",tests/test_skill_test_route.py,SimpleAgent
survived,"def tesla_setup_signal(sig: Signal, dbc_name: str, line_num: int) -> None:
    if sig.name.endswith(""Counter""):
        sig.type = SignalType.COUNTER
    elif sig.name.endswith(""Checksum""):
        sig.type = SignalType.TESLA_CHECKSUM
        sig.calc_checksum = tesla_checksum
",opendbc/can/packer.py,
survived,"def set_signal_type(sig: Signal, chk: Optional[ChecksumState], dbc_name: str, line_num: int) -> None:
    sig.calc_checksum = None
    if chk:
        if chk.setup_signal:
            chk.setup_signal(sig, dbc_name, line_num)
        if sig.name == 'CHECKSUM':
            sig.type = chk.checksum_type
            sig.calc_checksum = chk.calc_checksum
        elif sig.name == 'COUNTER':
            sig.type = SignalType.COUNTER
",opendbc/can/packer.py,
survived,"    def test_cache_header_for_old_content(self):
        old_date = timezone.now() - datetime.timedelta(days=181)
        entry = EntryFactory(created=old_date)
        response = self.client.get(entry.get_absolute_url())
        assert response.headers[""cache-control""] == ""s-maxage=%d"" % (
            24 * 60 * 60
        )
",blog/tests.py,BlogTests
survived,"def test_propose_diff_smoke(tmp_path: Path) -> None:
    target = tmp_path / ""demo.py""
    target.write_text(""def demo():\n    return 1\n"", encoding=""utf-8"")
    diff = propose_diff(str(target), ""extra feature"")
    patcher_core.apply_patch(diff, repo_path=tmp_path)
    assert ""extra feature"" in target.read_text(encoding=""utf-8"")
    subprocess.check_call([sys.executable, ""-m"", ""py_compile"", str(target)])
",tests/test_diff_mutation.py,
survived,"    async def _skill_test(request: Request, name: str) -> Any:
        payload = await request.json()
        if name not in runners:
            raise HTTPException(404, ""Agent not found"")
        inst = runners[name].inst
        if not hasattr(inst, ""skill_test""):
            raise HTTPException(501, ""Agent does not support skill_test"")
        return await inst.skill_test(payload)  # type: ignore[func-returns-value]
",alpha_factory_v1/backend/api_server.py,
survived,"    async def maybe_step(self) -> None:
        if time.time() < self.next_ts:
            return
        self._calc_next()

        async def _cycle() -> None:
            t0 = time.time()
            span_cm = tracer.start_as_current_span(self.name) if tracer else contextlib.nullcontext()
            with span_cm:
                try:
                    await asyncio.wait_for(maybe_await(self.inst.run_cycle), timeout=self._max_cycle_sec)
                except asyncio.TimeoutError:
                    MET_ERR.labels(self.name).inc()
                    log.error(""%s run_cycle exceeded %ss budget â€“ skipped"", self.name, self._max_cycle_sec)
                except Exception as exc:  # noqa: BLE001
                    MET_ERR.labels(self.name).inc()
                    log.exception(""%s.run_cycle crashed: %s"", self.name, exc)
                finally:
                    dur_ms = (time.time() - t0) * 1_000
                    MET_LAT.labels(self.name).observe(dur_ms)
                    self.last_beat = time.time()
                    self._publish(""agent.cycle"", {""agent"": self.name, ""latency_ms"": dur_ms, ""ts"": utc_now()})

        self.task = asyncio.create_task(_cycle())
",alpha_factory_v1/backend/agent_manager.py,AgentRunner
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/save_jsonl_stdout.py,Person
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/right_join.py,Order
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/dataset_sort_take_limit.py,Product
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/order_by_map.py,Data
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_multi_join_sort.py,Nation
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/cross_join.py,Customer
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/group_by_join.py,Order
survived,"    def list_agents(_detail: bool = False) -> list[str]:  # noqa: D401
        return [""dummy"", ""fail""]
",tests/test_backend_orchestrator_dev.py,
survived,"    def get_agent(name: str) -> object:  # noqa: D401
        agent = DummyAgent() if name == ""dummy"" else FailingAgent()

        if hasattr(agent, ""step"") and inspect.iscoroutinefunction(agent.step):
            orig = agent.step

            async def _wrapped(*a: object, **kw: object) -> object:
                t0 = time.perf_counter()
                ok = True
                try:
                    return await orig(*a, **kw)
                except Exception:
                    ok = False
                    raise
                finally:
                    _HEALTH_Q.put((name, (time.perf_counter() - t0) * 1000, ok))

            agent.step = _wrapped
        return agent
",tests/test_backend_orchestrator_dev.py,
survived,"def _gen_certs(tmp: Path) -> tuple[str, str, bytes, str]:
    root = Path(__file__).resolve().parents[1]
    script = root / ""alpha_factory_v1"" / ""demos"" / ""alpha_agi_insight_v1"" / ""infrastructure"" / ""gen_bus_certs.sh""
    subprocess.run([""bash"", str(script)], cwd=tmp, check=True, capture_output=True)
    cert = tmp / ""certs"" / ""bus.crt""
    key = tmp / ""certs"" / ""bus.key""
    token = ""change_this_token""
    ca = cert.read_bytes()
    return str(cert), str(key), ca, token
",tests/test_bus_ssl_gen.py,
survived,"def test_memory_agent_file_persistence(tmp_path: Path) -> None:
    mem_file = tmp_path / ""mem.log""
    cfg = config.Settings(bus_port=0, memory_path=str(mem_file))
    bus = messaging.A2ABus(cfg)
    ledger = logging.Ledger(str(tmp_path / ""ledger.db""))
    agent = memory_agent.MemoryAgent(bus, ledger, str(mem_file))

    envs = [messaging.Envelope(""a"", ""memory"", {""idx"": i}, 0.0) for i in range(3)]

    async def _run() -> None:
        for env in envs:
            await agent.handle(env)

    asyncio.run(_run())

    entries = [json.loads(line) for line in mem_file.read_text(encoding=""utf-8"").splitlines()]
    assert [e[""idx""] for e in entries] == list(range(3))

    agent2 = memory_agent.MemoryAgent(bus, ledger, str(mem_file))
    assert [r[""idx""] for r in agent2.records] == list(range(3))

    asyncio.run(bus.stop())
    ledger.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_memory_agent_file_persistence.py,
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_bus_secure.py,
survived,"def test_simulate_invalid_option() -> None:
    """"""Invoke simulate with an invalid export option.""""""
    res = CliRunner().invoke(
        cli.main,
        [""simulate"", ""--horizon"", ""1"", ""--offline"", ""--export"", ""xml""],
    )
    assert res.exit_code != 0
    assert ""Invalid value for '--export'"" in res.output",tests/test_demo_cli.py,
survived,"def test_compute_merkle_root(tmp_path: Path) -> None:
    ledger = Ledger(str(tmp_path / ""l.db""), broadcast=False)
    envs = [
        messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0),
        messaging.Envelope(""b"", ""c"", {""v"": 2}, 1.0),
        messaging.Envelope(""c"", ""d"", {""v"": 3}, 2.0),
    ]
    for env in envs:
        ledger.log(env)
    computed = ledger.compute_merkle_root()
    hashes = []
    for env in envs:
        data = json.dumps(asdict(env), sort_keys=True).encode()
        hashes.append(insight_logging.blake3(data).hexdigest())  # type: ignore[attr-defined]
    manual = insight_logging._merkle_root(hashes)
    assert computed == manual
",tests/test_logging.py,
survived,"def test_broadcast_merkle_root_logs_root_when_disabled(
    tmp_path: Path, caplog: pytest.LogCaptureFixture
) -> None:
    ledger = Ledger(str(tmp_path / ""l.db""), broadcast=False)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    root = ledger.compute_merkle_root()

    caplog.set_level(logging.INFO)

    dummy = mock.Mock(side_effect=AssertionError(""AsyncClient should not be used""))
    with mock.patch.object(insight_logging, ""AsyncClient"", dummy):
        asyncio.run(ledger.broadcast_merkle_root())

    assert not dummy.called
    assert any(f""Merkle root {root}"" in r.getMessage() for r in caplog.records)",tests/test_logging.py,
survived,"    def add_remote(self, name: str, url: str) -> None:
        self._run(""remote"", ""add"", name, url)
",src/meta_agent/git_utils.py,GitManager
survived,"    def __init__(self) -> None:
        self.completions = _ChatCompletions()
",src/meta_agent/services/openai_stub.py,_Chat
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/100-doors-2.py,
survived,"def plus(m, n):
    return lambda f: compose(m(f), n(f))
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def add(c, d):
    return lambda f: lambda x: c(f)(d(f)(x))
",tests/rosetta/transpiler/Python/church-numerals-1.py,
survived,"def zero():
    return lambda f: id
",tests/rosetta/transpiler/Python/church-numerals-2.py,
survived,"def primeFactors(n):
    factors = []
    x = n
    while x % 2 == 0:
        factors = factors + [2]
        x = int((x // 2))
    p = 3
    while p * p <= x:
        while x % p == 0:
            factors = factors + [p]
            x = int((x // p))
        p = p + 2
    if x > 1:
        factors = factors + [x]
    return factors
",tests/rosetta/transpiler/Python/composite-numbers-k-with-no-single-digit-factors-whose-factors-are-all-substrings-of-k.py,
survived,"def modInv(a, m):
    r = egcd(a, m)
    if r[0] != 1:
        return 0
    x = r[1]
    if x < 0:
        return x + m
    return x
",tests/rosetta/transpiler/Python/chinese-remainder-theorem.py,
survived,"def printMat(m):
    i = 0
    while i < len(m):
        line = """"
        j = 0
        while j < len(m[i]):
            line = line + str(m[i][j])
            if j < len(m[i]) - 1:
                line = line + "" ""
            j = j + 1
        print(line)
        i = i + 1
",tests/rosetta/transpiler/Python/cholesky-decomposition.py,
survived,"def parseIntStr(str):
    i = 0
    neg = False
    if len(str) > 0 and str[0:1] == ""-"":
        neg = True
        i = 1
    n = 0
    digits = {""0"": 0, ""1"": 1, ""2"": 2, ""3"": 3, ""4"": 4, ""5"": 5, ""6"": 6, ""7"": 7, ""8"": 8, ""9"": 9}
    while i < len(str):
        n = n * 10 + digits[str[i:i + 1]]
        i = i + 1
    if neg:
        n = -n
    return n
",tests/rosetta/transpiler/Python/compiler-virtual-machine-interpreter.py,
survived,"def convexHull(ps):
    ps = sortPoints(ps)
    h = []
    for pt in ps:
        while len(h) >= 2 and ccw(h[len(h) - 2], h[len(h) - 1], pt) == False:
            h = h[:len(h) - 1]
        h = h + [pt]
    i = len(ps) - 2
    t = len(h) + 1
    while i >= 0:
        pt = ps[i]
        while len(h) >= t and ccw(h[len(h) - 2], h[len(h) - 1], pt) == False:
            h = h[:len(h) - 1]
        h = h + [pt]
        i = i - 1
    return h[:len(h) - 1]
",tests/rosetta/transpiler/Python/convex-hull.py,
survived,"        def __init__(self, app: FastAPI, window: int = 60) -> None:
            super().__init__(app)
            self.window = window
            self.window_start = time.time()
            self.req_count = 0
            self.resp_429 = 0
",src/interface/api_server.py,MetricsMiddleware
survived,"    def __init__(
        self,
        cli_output: CLIOutput | None = None,
        log: logging.Logger | None = None,
    ) -> None:
        from .cli_output import CLIOutput  # local import to avoid circular

        self.cli_output = cli_output or CLIOutput()
        self.logger = log or logger
",src/meta_agent/ux/error_handler.py,ErrorHandler
survived,"    def list_files(self) -> List[str]:
        files: List[str] = []
        for path in self.bundle_dir.rglob(""*""):
            if (
                path.is_file()
                and path.name != ""bundle.json""
                and "".git"" not in path.parts
            ):
                files.append(str(path.relative_to(self.bundle_dir)))
        return files
",src/meta_agent/bundle.py,Bundle
survived,"    def __enter__(self) -> ""Ledger"":
        """"""Return ``self`` for context manager support.""""""
        return self
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/utils/logging.py,Ledger
survived,"        async def invoke_tool(self, name: str, args: dict[str, object] | None = None) -> object:
            args = args or {}
            self.called.append((name, args))
            return {""ok"": True}
",tests/test_adapters.py,StubMCP
survived,"    def _raise(_name: str):
        raise ModuleNotFoundError
",tests/test_adapters.py,
survived,"        def __init__(self) -> None:
            self.instructions: list[object] = []
",tests/test_safety_guardian_property.py,DummyTx
survived,"def _dummy_classes():
    captured: dict[str, str] = {}

    class DummyClient:
        def __init__(self, url: str) -> None:
            captured[""url""] = url

        async def send_transaction(self, tx: object, *args: object) -> None:
            captured[""data""] = tx.instructions[0].data.decode()

        async def close(self) -> None:  # pragma: no cover - dummy
            pass

    class DummyTx:
        def __init__(self) -> None:
            self.instructions: list[object] = []

        def add(self, instr: object) -> ""DummyTx"":
            self.instructions.append(instr)
            return self

    class DummyInstr:
        def __init__(self, program_id: object, data: bytes, keys: list[object]):
            self.data = data

    class DummyPk:
        def __init__(self, val: str) -> None:  # pragma: no cover - dummy
            pass

    return captured, DummyClient, DummyTx, DummyInstr, DummyPk
",tests/test_safety_guardian_property.py,
survived,"def test_run_macro_demo_health_check(tmp_path: Path) -> None:
    """"""Health gate should hit the expected endpoint.""""""
    _, curl_log = _run_script(tmp_path, env={""OPENAI_API_KEY"": ""dummy-key""})
    assert ""http://localhost:7864/healthz"" in curl_log",tests/test_macro_launcher.py,
survived,"    def task(*_a, **_kw):
        def decorator(func):
            return func

        return decorator
",stubs/google_adk/__init__.py,
survived,"def _sa_to_enrich(instance: Any, model_cls: type) -> Any:
    data: dict[str, Any] = {}
    for name in model_cls.model_fields:
        if name in model_cls.relationship_fields():
            continue
        if hasattr(instance, name):
            data[name] = getattr(instance, name)
    return model_cls(**data)
",src/enrichmcp/sqlalchemy/auto.py,
survived,"    def fn(genome: list[float]) -> tuple[float]:
        time.sleep(0.2)
        return (sum(genome),)
",tests/test_experiments.py,
survived,"    def health(self) -> str:
        """"""Return ``'ok'`` if the orchestrator is healthy.""""""
        url = f""{self.base_url}/healthz""
        resp = requests.get(url)
        resp.raise_for_status()
        return resp.text
",alpha_factory_v1/demos/alpha_agi_marketplace_v1/marketplace.py,MarketplaceClient
survived,"    def __init__(self, target: int = 5, market_data: List[int] | None = None) -> None:
        super().__init__(target=target)
        self.market_data = list(market_data) if market_data else []
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/mats/env.py,LiveBrokerEnv
survived,"def test_chinese_labels() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context(locale=""zh-CN"")
        page = context.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        label_text = page.locator(""#controls label"").first.inner_text()
        assert ""ç§å­"" in label_text
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_chinese_locale.py,
survived,"def health() -> dict[str, str]:
    return {""status"": ""ok""}",backend/main.py,
survived,"def main(argv: List[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description=""Process files with Attachments DSL"")
    parser.add_argument(""paths"", nargs=""*"", help=""Files, URLs or directories to process"")
    parser.add_argument(""-c"", ""--cwd"", help=""Change working directory before processing"")
    parser.add_argument(""-q"", ""--quiet"", action=""store_true"", help=""Silence verbose logs"")
    parser.add_argument(
        ""-y"",
        ""--copy"",
        action=""store_true"",
        help=""Copy result text to clipboard using to_clipboard_text"",
    )
    parser.add_argument(""--prompt"", default="""", help=""Prompt when copying to clipboard"")

    args, extra = parser.parse_known_args(argv)

    if args.cwd:
        os.chdir(args.cwd)

    set_verbose(not args.quiet)

    dsl_fragment = _build_dsl(extra)
    paths = args.paths or ["".""]
    paths_with_dsl = [p + dsl_fragment for p in paths]

    try:
        ctx = Attachments(*paths_with_dsl)
        if args.copy:
            adapt.to_clipboard_text(ctx, prompt=args.prompt)
        else:
            output = str(ctx)
            if output:
                print(output)
    except Exception as exc:
        print(f""Error running attachments CLI: {exc}"", file=sys.stderr)
        sys.exit(1)
",src/attachments/cli.py,
survived,"  def supports_active_cooling(self) -> bool:
    return False
",pylabrobot/heating_shaking/inheco_backend.py,InhecoThermoShakeBackend
survived,"  def supports_active_cooling(self) -> bool:
    return False
",pylabrobot/temperature_controlling/opentrons_backend.py,OpentronsTemperatureModuleBackend
survived,"  def __init__(self, temperature: float = 25.0):
    super().__init__()
    self.temperature = temperature
    self.set_called = False
",pylabrobot/temperature_controlling/temperature_controller_tests.py,_FakeBackend
survived,"        def eval_fn(genome: list[float]) -> tuple[float, float, float]:
            x, y = genome
            return x**2, y**2, (x + y) ** 2
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/cli.py,
survived,"                        def select(self, *_):
                            return self
",no-ocr-api/tests/test_ingest_search.py,FakeTable.Limiter.Selector
survived,"def test_apply_diff_failure_returns_output():
    with tempfile.TemporaryDirectory() as repo:
        open(os.path.join(repo, ""file.txt""), ""w"").close()
        success, output = diff_utils.apply_diff(""bad diff"", repo_dir=repo)
        assert not success
        assert ""patch"" in output.lower()
",tests/test_diff_utils_apply.py,
survived,"def test_transform_image_missing_cv2(monkeypatch) -> None:
    img = Image.new(""RGB"", (10, 10), ""red"")

    monkeypatch.setattr(""mistral_common.tokens.tokenizers.multimodal.is_cv2_installed"", lambda: False)

    with pytest.raises(ImportError) as exc_info:
        transform_image(img, (16, 16))

    assert ""pip install mistral-common[opencv]"" in str(exc_info.value)",tests/test_multimodal.py,
survived,"        async def start(self) -> None:
            return None
",tests/test_bus_large_payloads_property.py,Prod
survived,"def test_adk_generate_text_success(httpx_mock, stub_adk):
    httpx_mock.add_response(url=""https://adk.example/generate"", json={""text"": ""ok""})
    adapter = ADKAdapter()
    result = adapter.generate_text(""hi"")
    assert result == ""ok""
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_adapters.py,
survived,"    def api_name(self) -> str:
        return {
            FluxKontextModelName.PRO: ""black-forest-labs/flux-kontext-pro"",
            FluxKontextModelName.MAX: ""black-forest-labs/flux-kontext-max"",
        }[self]
",autogpt_platform/backend/backend/blocks/flux_kontext.py,FluxKontextModelName
survived,"def main(config: SampleLmConfig):
    levanter.initialize(config)
    tokenizer = load_tokenizer(config.tokenizer)

    vocab_size = len(tokenizer)
    Vocab = round_axis_for_partitioning(Axis(""vocab"", vocab_size), config.trainer.compute_axis_mapping)

    key = jrandom.PRNGKey(0)

    with config.trainer.device_mesh, hax.axis_mapping(config.trainer.parameter_axis_mapping):
        model = _load_model(config, Vocab, key=key)
        assert isinstance(model, LlamaLMHeadModel), ""Only LlamaLMHeadModel supported""

        sampler = Sampler(Vocab)

        prompt_ids = tokenizer.encode(config.prompt, add_special_tokens=False)
        prompt_axis = Axis(""position"", len(prompt_ids))
        prompt_tokens = hax.NamedArray(jnp.array(prompt_ids, dtype=jnp.int32), axes=(prompt_axis,))

        page_table = PageTable.init(
            max_pages=1,
            max_seqs=1,
            page_size=len(prompt_ids) + config.max_new_tokens,
            max_pages_per_seq=1,
        )
        page_table, seq_id = page_table.assign_seq_id_to_seq()
        cache = model.initial_cache(page_table, dtype=jnp.float32)

        page_table, binfo = page_table.allocate_for_seqs(
            updated_seqs=hax.named([seq_id], ""seq""),
            new_counts=hax.named([len(prompt_ids)], ""seq""),
            tokens=hax.named([seq_id] * len(prompt_ids), prompt_axis),
        )
        state = KvPageState.from_batch(binfo, cache)
        pos_ids = hax.arange(prompt_axis, dtype=jnp.int32)
        _, state = model.decode(prompt_tokens, state, pos_ids)

        generated = list(prompt_ids)
        temps = hax.full(Axis(""batch"", 1), config.temperature, dtype=jnp.float32)

        for i in range(config.max_new_tokens):
            page_table, binfo = page_table.allocate_for_seqs(
                updated_seqs=hax.named([seq_id], ""seq""),
                new_counts=hax.named([1], ""seq""),
                tokens=hax.named([seq_id], Axis(""position"", 1)),
            )
            state = KvPageState.from_batch(binfo, state.cache)
            pos_id = hax.arange(Axis(""position"", 1), start=len(generated))
            logits, state = model.decode(
                hax.NamedArray(jnp.array([generated[-1]], dtype=jnp.int32), axes=(Axis(""position"", 1),)),
                state,
                pos_id,
            )
            logits = logits[""position"", 0]
            tok, _ = sampler(logits, temps, key=jrandom.PRNGKey(i + 1))
            next_token = int(tok.array)
            generated.append(next_token)

        text = tokenizer.decode(generated, skip_special_tokens=True)
        print(text)
",src/levanter/main/sample_lm.py,
survived,"    def text(self) -> str:
        try:
            return self.content.decode()
        except UnicodeDecodeError:
            return self.content.decode(""latin1"", errors=""replace"")
",alpha_factory_v1/af_requests.py,Response
survived,"    def ok(self) -> bool:
        return self.status_code < 400
",alpha_factory_v1/af_requests.py,Response
survived,"    def json(self):
        return _json.loads(self.text)
",alpha_factory_v1/af_requests.py,Response
survived,"    def search(
        self,
        query: str,
        *,
        category: str | None = None,
        tags: Optional[List[str]] = None,
        limit: int = 5,
    ) -> List[TemplateMatch]:
        """"""Return templates matching the query and optional filters.""""""
        if not self._index:
            self.build_index()
        tokens = [t.lower() for t in query.split() if t]
        results: List[TemplateMatch] = []
        for item in self._index:
            meta = item.get(""metadata"", {})
            if category and meta.get(""category"") != category:
                continue
            if tags and not all(t in meta.get(""tags"", []) for t in tags):
                continue
            haystack = "" "".join(
                [
                    item.get(""content"", """"),
                    meta.get(""title"", """"),
                    meta.get(""description"", """"),
                    meta.get(""slug"", """"),
                    "" "".join(meta.get(""tags"", [])),
                ]
            ).lower()
            score = sum(1 for tok in tokens if tok in haystack)
            if score:
                preview = item.get(""content"", """")[:100].strip()
                results.append(
                    TemplateMatch(
                        slug=item[""slug""],
                        version=item[""version""],
                        score=float(score),
                        preview=preview,
                        metadata=meta,
                    )
                )
        results.sort(key=lambda r: r.score, reverse=True)
        return results[:limit]",src/meta_agent/template_search.py,TemplateSearchEngine
survived,"def test_search_filters(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""foo"", TemplateCategory.CONVERSATION), ""foo content"")
    reg.register(_meta(""bar"", TemplateCategory.REASONING), ""bar content"")

    engine = TemplateSearchEngine(reg)
    res_cat = engine.search(""content"", category=TemplateCategory.CONVERSATION.value)
    assert len(res_cat) == 1 and res_cat[0].slug == ""foo""

    res_tag = engine.search(""content"", tags=[""bar""])
    assert len(res_tag) == 1 and res_tag[0].slug == ""bar""

    res_none = engine.search(""content"", tags=[""missing""])
    assert res_none == []",tests/test_template_search.py,
survived,"def _meta(slug: str, category: TemplateCategory) -> TemplateMetadata:
    return TemplateMetadata(
        slug=slug,
        title=slug.title(),
        description=f""Template {slug}"",
        category=category,
        complexity=TemplateComplexity.BASIC,
        tags=[slug],
    )
",tests/test_template_search.py,
survived,"def test_search_basic(tmp_path):
    reg = TemplateRegistry(base_dir=tmp_path)
    reg.register(_meta(""greet"", TemplateCategory.CONVERSATION), ""hello world"")
    reg.register(_meta(""calc"", TemplateCategory.REASONING), ""1 + 1"")

    engine = TemplateSearchEngine(reg)
    results = engine.search(""hello"")
    assert results
    assert results[0].slug == ""greet""
    assert ""hello"" in results[0].preview
",tests/test_template_search.py,
survived,"    def __init__(self, registry: Optional[TemplateRegistry] = None) -> None:
        self.registry = registry or TemplateRegistry()
        self._index: List[Dict[str, Any]] = []
",src/meta_agent/template_search.py,TemplateSearchEngine
survived,"def test_insight_helm_template_renders_env_vars() -> None:
    result = subprocess.run(
        [""helm"", ""template"", ""insight"", str(CHART_DIR), ""-f"", str(VALUES_FILE)],
        check=True,
        cwd=CHART_DIR,
        capture_output=True,
        text=True,
    )
    rendered = result.stdout
    assert ""OPENAI_API_KEY"" in rendered
    assert ""AGI_INSIGHT_OFFLINE"" in rendered
    assert ""AGI_INSIGHT_BUS_PORT"" in rendered
    assert ""AGI_INSIGHT_LEDGER_PATH"" in rendered",tests/test_insight_helm_template.py,
survived,"    def _target(q: _mp.Queue) -> None:
        _apply_limits()
        try:
            proc = subprocess.Popen(
                [sys.executable, script],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            try:
                out, err = proc.communicate(inp_json, timeout=SOFT_T)
            except subprocess.TimeoutExpired:
                proc.kill()
                out, err = proc.communicate()
            q.put((out, err))
        except Exception as exc:  # pragma: no cover
            q.put(("""", str(exc)))
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,
survived,"    def solve(self, tasks: Sequence[Triplet]) -> List[TaskResult]:
        results: List[TaskResult] = []
        for t in tasks:
            start = time.time()
            stdout, stderr = _exec_trusted(t.program, t.inp)
            lat = time.time() - start
            solved = stderr == """" and stdout.strip() == t.out.strip()
            complexity = _complexity(t.program)
            results.append(TaskResult(t, solved, lat, stdout, stderr, complexity))
        return results
",alpha_factory_v1/demos/meta_agentic_agi_v3/curriculum/azr_engine.py,AZREngine
survived,"    def start(self, bus: messaging.A2ABus, ledger: Ledger) -> None:
        self.task = asyncio.create_task(self.loop(bus, ledger))
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,AgentRunner
survived,"            def abort(self, *_a, **_kw):
                raise RuntimeError(""denied"")
",tests/test_insight_orchestrator_features.py,TestMessaging.Ctx
survived,"    def setUp(self) -> None:
        self.tmp = tempfile.TemporaryDirectory()
        self.settings = config.Settings(bus_port=0, ledger_path=os.path.join(self.tmp.name, ""ledger.db""))
        self.orch = orchestrator.Orchestrator(self.settings)
",tests/test_insight_orchestrator_features.py,TestInsightOrchestrator
survived,"    def baz(x: Float[""b""]):  # type: ignore  # noqa: F722
        pass
",tests/test_dtype_typing.py,
survived,"def run_demo(args):
    corpus = load_documents(args.data_dir)
    vocab, index = build_vocab(corpus)
    int_corpus = convert_corpus(corpus, index)

    hlda = HierarchicalLDA(
        int_corpus,
        vocab,
        alpha=args.alpha,
        gamma=args.gamma,
        eta=args.eta,
        num_levels=args.num_levels,
        seed=args.seed,
    )

    hlda.estimate(
        args.iterations,
        display_topics=args.display_topics,
        n_words=args.n_words,
        with_weights=False,
    )

    print(""\nFinal topic hierarchy:"")
    hlda.print_nodes(args.n_words, with_weights=False)

    return hlda
",scripts/bbc_demo.py,
survived,"def _to_mochi(v: Any) -> str:
    if v is None:
        return ""null""
    if isinstance(v, bool):
        return ""true"" if v else ""false""
    if isinstance(v, (int, float)):
        return str(v)
    if isinstance(v, str):
        return ""\"""" + v.replace(""\\"", ""\\\\"").replace(""\"""", ""\\\"""") + ""\""""
    if isinstance(v, Sequence) and not isinstance(v, (str, bytes, bytearray)):
        return ""["" + "", "".join(_to_mochi(x) for x in v) + ""]""
    if isinstance(v, dict):
        items = "", "".join(f'{_to_mochi(k)}: {_to_mochi(val)}' for k, val in v.items())
        return ""{"" + items + ""}""
    raise TypeError(f""unsupported value type: {type(v).__name__}"")",tools/libmochi/python/libmochi.py,
survived,"def run(code: str, mochi_bin: str = ""mochi"") -> str:
    """"""Execute Mochi source code and return its standard output.""""""
    return _run(code, mochi_bin)
",tools/libmochi/python/libmochi.py,
survived,"    def fake_download(url: str, dest: Path) -> None:
        dest.parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(""ok"")
        calls.append((url, dest))
",tests/test_download_hf_gpt2.py,
survived,"def parse_requirements(path: Path) -> list[str]:
    """"""Parse a requirements file, resolving ``-r`` inclusions.""""""
    requirements: list[str] = []
    for line in path.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""-r ""):
            nested = (path.parent / line.split(maxsplit=1)[1]).resolve()
            requirements.extend(parse_requirements(nested))
        else:
            requirements.append(line)
    return requirements
",pioreactor/tests/test_requirements_sync.py,
survived,"def test_llama_paged_decode_ragged_fill_in_chunks():
    B = Axis(""batch"", 2)
    Pos = Axis(""position"", 8)
    Embed = Axis(""embed"", 8)
    Vocab = Axis(""vocab"", 64)

    cfg = LlamaConfig(
        seq_len=Pos.size,
        hidden_dim=Embed.size,
        intermediate_dim=16,
        num_layers=2,
        num_heads=2,
        num_kv_heads=2,
        rope=None,
        gradient_checkpointing=False,
        scan_layers=True,
        attn_backend=AttentionBackend.VANILLA,
    )

    model_key, input_key = jrandom.split(jrandom.PRNGKey(0))
    model = LlamaLMHeadModel.init(Vocab=Vocab, config=cfg, key=model_key)

    input_ids = hax.random.randint(input_key, (B, Pos), 0, Vocab.size)
    full_out = model.activations(input_ids, attn_mask=AttentionMask.causal(), key=jrandom.PRNGKey(1))

    pt = PageTable.init(max_pages=8, max_seqs=2, page_size=4, max_pages_per_seq=4)
    pt, seq1 = pt.assign_seq_id_to_seq()
    pt, seq2 = pt.assign_seq_id_to_seq()
    layer_caches = model.transformer.initial_cache(pt, dtype=jnp.float32)

    x = model.embeddings.embed(input_ids)
    x0 = x[B, 0]
    x1 = x[B, 1]

    chunk_sizes = [[4, 2], [0, 1], [0, 1], [2, 1], [1, 2], [1, 1]]
    off0 = off1 = 0
    outputs0 = []
    outputs1 = []

    seq_axis = Axis(""seq"", 2)
    for step0, step1 in chunk_sizes:
        tok_axis = Axis(""position"", step0 + step1)
        updated = hax.named([seq1, seq2], seq_axis)
        new_counts = hax.named([step0, step1], seq_axis)
        tokens = hax.named([seq1] * step0 + [seq2] * step1, tok_axis)
        pt, binfo = pt.allocate_for_seqs(updated, new_counts, tokens)
        state = KvPageState.from_batch(binfo, layer_caches)

        x_chunk = hax.concatenate(
            ""position"",
            [x0[Pos, hax.dslice(off0, step0)], x1[Pos, hax.dslice(off1, step1)]],
        )
        pos_ids = hax.named(list(range(off0, off0 + step0)) + list(range(off1, off1 + step1)), tok_axis)
        with jax.disable_jit():
            output, state = _jit_paged_decode(model.transformer, x_chunk, pos_ids, state)
        layer_caches = state.cache
        outputs0.append(output[""position"", hax.dslice(0, step0)])
        outputs1.append(output[""position"", hax.dslice(step0, step1)])

        assert_trees_all_close(
            full_out[B, 0, ""position"", hax.dslice(off0, step0)].array,
            outputs0[-1].array,
            atol=1e-4,
            rtol=1e-4,
        )
        assert_trees_all_close(
            full_out[B, 1, ""position"", hax.dslice(off1, step1)].array,
            outputs1[-1].array,
            atol=1e-4,
            rtol=1e-4,
        )

        off0 += step0
        off1 += step1

    outputs0_cat = hax.concatenate(""position"", outputs0)
    outputs1_cat = hax.concatenate(""position"", outputs1)
    decoded_arr = hax.stack(""batch"", [outputs0_cat, outputs1_cat])
    assert_trees_all_close(full_out.array, decoded_arr.array, atol=1e-4, rtol=1e-4)",tests/test_llama_decode.py,
survived,"        def __init__(self) -> None:
            self.spans: list[str] = []
",tests/test_metrics.py,DummyTracer
survived,"def test_metrics_endpoint_subprocess() -> None:
    port = _free_port()
    proc = _start_server(port)
    url = f""http://127.0.0.1:{port}""
    try:
        _wait_ready(url)
        resp = httpx.get(f""{url}/metrics"")
        assert resp.status_code == 200
        text = resp.text
        assert ""api_requests_total"" in text
        assert ""api_request_duration_seconds"" in text
        assert text.startswith(""# HELP"")
    finally:
        proc.terminate()
        proc.wait(timeout=5)
",tests/test_metrics.py,
survived,"        def set_meter_provider(self, _provider: Any) -> None:  # noqa: D401 - simple stub
            pass
",tests/test_metrics.py,DummyMetrics
survived,"async def test_input_guardrail_exception_propagates():
    adapter = MockAdapter()
    router = GuardrailModelRouter({""a"": adapter}, default_model=""a"")

    async def bad_guard(_prompt: str):
        raise RuntimeError(""bad"")

    router.add_input_guardrail(bad_guard)

    with pytest.raises(RuntimeError):
        await router.invoke(""x"")
    assert not adapter.prompts
",tests/test_guardrail_router.py,
survived,"    async def run_cycle(self) -> None:
        if self.first:
            self.first = False
            raise RuntimeError(""boom"")
        await asyncio.sleep(0)
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,BoomAgent
survived,"        def close(self) -> None:
            pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/tests/test_orchestrator.py,DummyLedger
survived,"def test_publish_to_async_subscriber() -> None:
    """"""Envelopes published to a subscribed coroutine should be delivered.""""""
    bus = messaging.A2ABus(config.Settings(bus_port=0))
    received: list[messaging.Envelope] = []

    async def handler(env: messaging.Envelope) -> None:
        received.append(env)

    bus.subscribe(""x"", handler)
    env = messaging.Envelope(""a"", ""x"", {""v"": 42}, 0.0)

    async def run() -> None:
        bus.publish(""x"", env)
        await asyncio.sleep(0)  # allow handler task to run

    asyncio.run(run())
    assert len(received) == 1
    assert received[0].payload[""v""] == 42",tests/test_messaging.py,
survived,"    async def run() -> None:
        bus.publish(""x"", env)
        await asyncio.sleep(0)  # allow handler task to run
",tests/test_messaging.py,
survived,"    async def run_cycle(self) -> None:
        await asyncio.sleep(999)
",tests/test_agents.py,FreezeAgent
survived,"        def __init__(self) -> None:  # noqa: D401 - simple stub
            for name, default in self.__class__.__dict__.items():
                if name.startswith(""_"") or name == ""Config"" or callable(default):
                    continue
                value = os.getenv(name, default)
                if isinstance(default, bool):
                    value = str(value).lower() in {""1"", ""true"", ""yes"", ""on""}
                elif isinstance(default, int) and default is not None:
                    try:
                        value = int(value)
                    except Exception:
                        value = default
                self.__dict__[name] = value
",alpha_factory_v1/backend/memory_fabric.py,BaseSettings
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q1.py,Lineitem
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Nation
survived,"    def __iter__(self):
        return iter(self.Items)
",tests/machine/x/python/q3.py,_Group
survived,"    def __repr__(self):
        return str(self.__dict__)
",tests/machine/x/python/q2.py,Part
survived,"def test_multi_env_reporting(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""Aggregated metrics should reflect all environments.""""""
    monkeypatch.setenv(""NO_LLM"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_MAX_STEPS"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_UI_TICK"", ""1"")
    monkeypatch.setenv(""ALPHA_ASI_ENV_BATCH"", ""2"")

    mod = importlib.import_module(
        ""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo""
    )

    class DummyEnv:
        def __init__(self, reward: float) -> None:
            self.reward = reward

        def reset(self):
            return None

        def step(self, _a: int):
            return None, self.reward, True, {}

    class DummyLearner:
        def __init__(self, loss: float) -> None:
            self.loss = loss

        def act(self, _obs):
            return 0

        def remember(self, _obs, _reward) -> None:
            pass

        def train_once(self) -> float:
            return self.loss

    mod.A2ABus._subs = {}
    orch = mod.Orchestrator()
    orch.envs = [DummyEnv(1.0), DummyEnv(0.0)]
    orch.learners = [DummyLearner(0.2), DummyLearner(0.4)]

    msgs: list[dict] = []
    mod.A2ABus.subscribe(""ui"", lambda m: msgs.append(m))

    orch.loop()

    assert msgs
    msg = msgs[-1]
    assert msg[""t""] == 0
    assert msg[""r""] == pytest.approx(0.5)
    assert msg[""loss""] == pytest.approx(0.3)",tests/test_world_model_demo.py,
survived,"    def test_to_csv_sort_by_duration(self):
        """"""Ensure sorting by dataclass field like 'duration' works.""""""
        input = dedent_strip(""""""
            Activity;Predecessor;Duration
            A;-;2
            B;-;5
            C;-;3
        """""")

        project_schedule = ProjectSchedule.create(parse_schedule_input_data(input))
        csv_output = project_schedule.to_csv(sort_by=""duration"")
        durations = [line.split("";"")[1] for line in csv_output.splitlines()[1:]]
        self.assertEqual(durations, [""2"", ""3"", ""5""])
",src/schedule/tests/test_schedule.py,TestSchedule
survived,"def test_pack_empty_tree():
    tree = {}
    offsets, packed = pack_pytree(tree, dtype=jnp.float32)
    assert packed.size == 0
    rebuilt = unpack_pytree(offsets, packed)
    assert rebuilt == tree",tests/test_pack_tree.py,
survived,"def test_cli_transfer_test_invokes(monkeypatch) -> None:
    called = {}

    def fake_run(models, top_n):
        called[""models""] = models
        called[""top_n""] = top_n

    monkeypatch.setattr(tt, ""run_transfer_test"", fake_run)

    res = CliRunner().invoke(cli.main, [""transfer-test"", ""--models"", ""x,y"", ""--top-n"", ""2""])
    assert res.exit_code == 0
    assert called == {""models"": [""x"", ""y""], ""top_n"": 2}",tests/test_transfer_test.py,
survived,"def run() -> None:
    parts = [""poly"", ""task"", ""13""]
    joined = ""-"".join(parts)
    assert joined.split(""-"")[2] == str(13)",benchmarks/poly_mini/task_013.py,
survived,"def is_code_safe(code: str) -> bool:
    """"""Return ``True`` if ``code`` appears safe.""""""
    lowered = code.lower()
    for pat in _DENY_PATTERNS:
        if re.search(pat, lowered):
            return False

    try:
        tree = ast.parse(code)
    except SyntaxError:
        return False

    for node in ast.walk(tree):
        if isinstance(node, ast.Call):
            name = _full_name(node.func)
            if name in _BANNED_CALLS:
                return False
            if name == ""open"" and node.args:
                arg = node.args[0]
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if arg.value.startswith(""/etc""):
                        return False
    return True
",src/self_edit/safety.py,
survived,"def _read(name: str) -> str:
    return (FIXTURES / name).read_text()
",tests/test_safety_filter.py,
survived,"    def resume(self) -> None:
        """"""Resume execution after a pause.""""""
        self.paused_at = None
        self.next_ts = 0
",alpha_factory_v1/backend/agent_runner.py,AgentRunner
survived,"def get_default_tools() -> List[Any]:
    """"""Return the hardened default tool-chain.

    The selection is recalculated each time to honour environment variables
    that may change at runtime.  The returned list is safe to mutate.
    """"""
    base: List[Any] = [
        FileSearchTool(max_num_results=5),
        WebSearchTool(),
        run_pytest,
    ]

    # Remote tools (ComputerTool runs in OpenAI's sandbox) need an API key.
    if SDK_AVAILABLE and os.getenv(""OPENAI_API_KEY""):
        base.append(ComputerTool())

    # PythonTool executes *locally* â€“ only enable if user opts in explicitly.
    if SDK_AVAILABLE and os.getenv(""ALPHAFAC_ALLOW_LOCAL_CODE"") == ""1"":
        base.append(PythonTool())

    return base
",alpha_factory_v1/backend/agent_factory.py,
survived,"    def test_bollinger_bands(self):
        prices = [1, 2, 3, 4, 5]
        lower, upper = am.bollinger_bands(prices, window=4, num_std=1)
        self.assertLess(lower, upper)
",alpha_factory_v1/tests/test_alpha_model.py,AlphaModelTest
survived,"def serve() -> None:
    """"""Run the RPC server with `uvicorn`.""""""

    uvicorn.run(""backend.rpc_server:app"", host=RPC_HOST, port=RPC_PORT)
",alpha_factory_v1/backend/rpc_server.py,
survived,"    def reset(self) -> float:
        """"""Reset the environment and return the starting price.""""""
        self.price = self.start_price
        return self.price
",alpha_factory_v1/backend/environments/market_sim.py,MarketEnv
survived,"    def __init__(self, resp: str):
        self.resp = resp
",alpha_factory_v1/tests/test_planner_agent.py,DummyModel
survived,"            def handle(self, _msg):  # noqa
                LOG.debug(""[Stub:%s] â† %s"", cls_name, _msg)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Stub
survived,"    def __init__(self, name: str):
        self.name = name
        A2ABus.subscribe(name, self._on)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,Agent
survived,"    def _password_for_db(self, password: str) -> str:
        """"""Return the password value to store in the local DB.""""""
        return password
",app/services/media/jellyfin.py,JellyfinClient
survived,"def test_guard_added_to_route_dependencies():
    router = GuardController.get_router()
    route = router.routes[0]
    deps = route.dependencies
    assert len(deps) == 1
    assert isinstance(deps[0].dependency, SimpleGuard)",tests/test_core/test_decorators/test_guard.py,
survived,"def test_patcher_core_cli(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    repo = tmp_path / ""repo""
    tests_dir = repo / ""tests""
    tests_dir.mkdir(parents=True)

    # buggy source file
    (repo / ""calc.py"").write_text(""def add(a, b):\n    return a - b\n"", encoding=""utf-8"")

    # failing test
    (tests_dir / ""test_calc.py"").write_text(
        ""from calc import add\n\ndef test_add():\n    assert add(1, 2) == 3\n"",
        encoding=""utf-8"",
    )

    # patch to fix the bug
    patch_file = tmp_path / ""fix.diff""
    patch_file.write_text(
        """"""--- a/calc.py
+++ b/calc.py
@@ -1,2 +1,2 @@
 def add(a, b):
-    return a - b
+    return a + b
\ No newline at end of file
"""""",
        encoding=""utf-8"",
    )

    import openai_agents

    class StubAgent:
        def __init__(self, *a, **k):
            self.patch_file = os.environ.get(""PATCH_FILE"")

        def __call__(self, _prompt: str) -> str:
            return Path(self.patch_file).read_text() if self.patch_file else """"

    monkeypatch.setattr(openai_agents, ""OpenAIAgent"", StubAgent)

    env = os.environ.copy()
    env[""PATCH_FILE""] = str(patch_file)

    result = subprocess.run(
        [
            sys.executable,
            ""-m"",
            ""alpha_factory_v1.demos.self_healing_repo.patcher_core"",
            ""--repo"",
            str(repo),
        ],
        capture_output=True,
        text=True,
        env=env,
    )

    assert result.returncode == 0, result.stdout + result.stderr
    combined = result.stdout + result.stderr
    assert ""Patch fixed the tests"" in combined",tests/test_patcher_core_cli.py,
survived,"def test_offline_pwa_and_share() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        context = browser.new_context()
        page = context.new_page()

        page.goto(url)
        page.wait_for_selector(""#controls"")

        # Service worker should be ready
        page.wait_for_function(""navigator.serviceWorker && navigator.serviceWorker.controller || navigator.serviceWorker.ready"")

        # Go offline and reload
        context.route(""**"", lambda route: route.abort())
        page.reload()
        page.wait_for_selector(""#controls"")

        # Stub Web3Storage to avoid network
        page.evaluate(
            f""window.PINNER_TOKEN='tok'; window.Web3Storage = class {{ async put() {{ return '{CID}'; }} }}""
        )

        page.click(""text=Share"")
        page.wait_for_selector(""#toast.show"")
        assert CID in page.inner_text(""#toast"")
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py,
survived,"    async def policy(self, obs, ctx):  # type: ignore[override]
        if isinstance(obs, dict):
            return await self.tools.run_sim(
                int(obs.get(""agents"", 100)),
                int(obs.get(""rounds"", 1000)),
                float(obs.get(""delta"", 0.8)),
                float(obs.get(""stake"", 2.5)),
            )
        return await self.tools.run_sim()
",alpha_factory_v1/demos/solving_agi_governance/openai_agents_bridge.py,GovernanceSimAgent
survived,"def boom(a, b):
    print(""boom"")
    return True
",tests/transpiler/x/py/short_circuit.py,
survived,"    def _decorator(func):
        return func
",tests/test_openai_bridge_integration.py,
survived,"def makeAdder(n):
    def adder(x):
        return x + n
    return adder
",tests/human/py/closure.py,
survived,"    def _save_result(result: ResultsResponse) -> None:
        path = _results_dir / f""{result.id}.json""
        path.write_text(result.json())
        _simulations[result.id] = result
        while len(_simulations) > _max_results:
            old_id, _ = _simulations.popitem(last=False)
            with contextlib.suppress(FileNotFoundError):
                (_results_dir / f""{old_id}.json"").unlink()
        global _latest_id
        _latest_id = result.id
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py,
survived,"    def __getitem__(self, key):
        return getattr(self, key)
",tests/machine/x/python/q1.py,Auto2
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_having.py,_Group
survived,"    def __len__(self):
        return len(self.Items)
",tests/machine/x/python/group_by_multi_join.py,_Group
survived,"def _expected_moe_linear_output(moe: MoELinear, x: hax.NamedArray, group_sizes: hax.NamedArray):
    dim_numbers = jax.lax.RaggedDotDimensionNumbers(
        (
            ((x.axis_indices(moe.In),), (moe.weight.axis_indices(moe.In),)),
            ((), ()),
        ),
        x.axis_indices(hax.axis.without_axes(x.axes, moe.In)),
        (moe.weight.axis_indices(moe.Experts),),
    )
    out_raw = jax.lax.ragged_dot_general(
        lhs=x.array,
        rhs=moe.weight.array,
        group_sizes=group_sizes.array,
        ragged_dot_dimension_numbers=dim_numbers,
    )
    out_axes = hax.replace_axis(x.axes, moe.In, moe.Out)
    out = hax.named(out_raw, out_axes)
    if moe.bias is not None:
        out = out + moe.bias
    return out
",tests/test_moe_linear.py,
survived,"def test_moe_linear_out_first_property():
    E, In, Out = hax.make_axes(E=2, In=4, Out=3)
    moe = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(0), out_first=True)
    assert moe.out_first
    assert moe.weight.axes[:3] == (E, Out, In)

    moe2 = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(1), out_first=False)
    assert not moe2.out_first
    assert moe2.weight.axes[:3] == (E, In, Out)
",tests/test_moe_linear.py,
survived,"        def pct(p):
            return data[int(n*p/100)] if n else 0
",alpha_factory_v1/demos/macro_sentinel/simulation_core.py,MonteCarloSimulator
survived,"        def reset(self, *, seed=None):
            return [0.0]*4, {}
",alpha_factory_v1/demos/muzero_planning/minimuzero.py,_StubEnv
survived,"    def _run_search_helper(episodes: int, target: int) -> str:
        """"""Execute the search loop and return a summary string.""""""
        run(episodes=episodes, target=target)
        return f""completed {episodes} episodes toward target {target}""
",alpha_factory_v1/demos/meta_agentic_tree_search_v0/openai_agents_bridge.py,
survived,"def test_llm_offline_pipeline() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")

        out = page.evaluate(""window.llmChat('hello')"")
        assert not out.startswith('[offline]')
        browser.close()
",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_browser_ui.py,
survived,"    def start_background_tasks() -> None:
        pass
",tests/test_agent_manager_consumer.py,
survived,"        async def stop_consumer(self) -> None:
            nonlocal stopped
            stopped = True
",tests/test_agent_manager_consumer.py,DummyBus
survived,"def test_manager_starts_and_stops_bus_consumer(monkeypatch: pytest.MonkeyPatch) -> None:
    started = False
    stopped = False

    class DummyBus:
        def __init__(self, *_a: object, **_k: object) -> None:
            pass

        async def start_consumer(self) -> None:
            nonlocal started
            started = True

        async def stop_consumer(self) -> None:
            nonlocal stopped
            stopped = True

        def publish(self, *_a: object, **_kw: object) -> None:
            pass

    async def dummy_run_cycle() -> None:
        return None

    class DummyAgent:
        NAME = ""dummy""
        CYCLE_SECONDS = 0.0
        run_cycle = dummy_run_cycle

    def list_agents(_detail: bool = False) -> list[str]:
        return [""dummy""]

    def get_agent(name: str) -> DummyAgent:
        assert name == ""dummy""
        return DummyAgent()

    def start_background_tasks() -> None:
        pass

    monkeypatch.setattr(""alpha_factory_v1.backend.agent_manager.EventBus"", DummyBus)
    monkeypatch.setattr(""backend.agents.list_agents"", list_agents)
    monkeypatch.setattr(""backend.agents.get_agent"", get_agent)
    monkeypatch.setattr(""backend.agents.start_background_tasks"", start_background_tasks)
    monkeypatch.setattr(""alpha_factory_v1.backend.agent_runner.get_agent"", get_agent)

    mgr = AgentManager({""dummy""}, True, None, 60, 30)

    async def _run() -> None:
        await mgr.start()
        await mgr.stop()

    asyncio.run(_run())

    assert started
    assert stopped",tests/test_agent_manager_consumer.py,
survived,"    def test_disabled_when_deps_missing(self) -> None:
        with mock.patch.object(mod, ""MetaEvolver"", None), \
             mock.patch.object(mod, ""CurriculumEnv"", None):
            agent = mod.AIGAEvolverAgent()
            self.assertIsNone(agent.evolver)
            asyncio.run(agent.step())
",tests/test_aiga_evolver_agent_logic.py,TestEvolverAgentLogic
survived,"    def test_step_publishes_best(self) -> None:
        class Dummy:
            def __init__(self) -> None:
                self.gen = 2
                self.best_fitness = 0.5

            def run_generations(self, _n: int) -> None:
                pass

        with mock.patch.object(mod, ""MetaEvolver"", lambda *a, **k: Dummy()), \
             mock.patch.object(mod, ""CurriculumEnv"", object), \
             mock.patch.object(mod, ""_publish"") as pub:
            agent = mod.AIGAEvolverAgent()
            asyncio.run(agent.step())
            pub.assert_called_with(""aiga.best"", {""gen"": 2, ""fitness"": 0.5})",tests/test_aiga_evolver_agent_logic.py,TestEvolverAgentLogic
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/100-prisoners.py,
survived,"def shuffle(xs):
    arr = xs
    i = 99
    while i > 0:
        j = _now() % (i + 1)
        tmp = arr[i]
        arr[i] = arr[j]
        arr[j] = tmp
        i = i - 1
    return arr
",tests/rosetta/transpiler/Python/100-prisoners.py,
survived,"    def test_selects_supply_chain_bottleneck(self) -> None:
        signals = {
            ""yield_curve"": ""yield curve normal"",
            ""supply_chain"": ""flows 12m usd â€“ POTENTIAL BOTTLENECK"",
        }
        self.assertEqual(alpha_report.best_alpha(signals), signals[""supply_chain""])
",tests/test_alpha_report.py,TestBestAlpha
survived,"def test_sync_parallel_tools_or(client):
    client = instructor.from_anthropic(
        client, mode=instructor.Mode.ANTHROPIC_PARALLEL_TOOLS
    )
    resp = client.chat.completions.create(
        model=""claude-3-5-haiku-latest"",
        messages=[
            {""role"": ""system"", ""content"": ""You must always use tools""},
            {
                ""role"": ""user"",
                ""content"": ""What is the weather in toronto and dallas and who won the super bowl?"",
            },
        ],
        response_model=Iterable[Union[Weather, GoogleSearch]],
    )
    assert len(list(resp)) == 3
",tests/llm/test_anthropic/test_parallel.py,
survived,"def test_time_dependent_discontinuity(tmp_path):
    """"""Models with time dependent discontinuities are handled.""""""

    from amici.antimony_import import antimony2sbml
    from amici.sbml_import import SbmlImporter
    from amici.jax.petab import DEFAULT_CONTROLLER_SETTINGS

    ant_model = """"""
    model time_disc
        x' = piecewise(1, time - sin(time) - 1 < 0, 2)
        x = 0
    end
    """"""

    sbml = antimony2sbml(ant_model)
    importer = SbmlImporter(sbml, from_file=False)
    importer.sbml2jax(""time_disc"", output_dir=tmp_path)

    module = amici._module_from_path(""time_disc"", tmp_path / ""__init__.py"")
    model = module.Model()

    p = jnp.array([1.0])
    x0_full = model._x0(0.0, p)
    tcl = model._tcl(x0_full, p)
    x0 = model._x_solver(x0_full)
    ts = jnp.array([0.0, 1.0, 2.0])

    assert len(model._root_cond_fns(p)) > 0
    assert model._known_discs(p).size == 0

    ys, _ = model._solve(
        p,
        ts,
        tcl,
        x0,
        diffrax.Tsit5(),
        diffrax.PIDController(**DEFAULT_CONTROLLER_SETTINGS),
        1000,
        diffrax.DirectAdjoint(),
    )

    assert ys.shape[0] == ts.shape[0]
",python/tests/test_jax.py,
survived,"    async def simulate(req: SimRequest) -> dict[str, str]:
        sim_id = secrets.token_hex(8)
        asyncio.create_task(_background_run(sim_id, req))
        return {""id"": sim_id}
",src/interface/api_server.py,
survived,"def test_nsga2_step_evolves_population() -> None:
    random.seed(0)
    pop = [mats.Individual([0.0, 0.0]) for _ in range(4)]

    def fn(genome):
        x, y = genome
        return x ** 2, y ** 2

    new = mats.nsga2_step(pop, fn, mu=4)
    assert len(new) == 4
    assert all(ind.fitness is not None for ind in new)
    genomes = {tuple(ind.genome) for ind in new}
    assert len(genomes) >= 1",tests/test_mats.py,
survived,"def test_replay_missing(tmp_path) -> None:
    with patch.object(cli.config, ""Settings"") as settings:
        settings.return_value.ledger_path = tmp_path / ""led.txt""
        out = CliRunner().invoke(cli.main, [""replay""])
        assert ""No ledger"" in out.output
",tests/test_cli.py,
survived,"    def __init__(self, inputs: str):
        self.inputs = inputs.encode(""utf-8"")  # Convert to bytes
",scripts/utils/lcb_runner.py,MockBuffer
survived,"    def add(self, meta: dict[str, Any], score: float) -> None:
        with sqlite3.connect(self.path) as cx:
            cx.execute(""INSERT INTO agents(meta, score) VALUES (?, ?)"", (json.dumps(meta), score))
",src/archive.py,Archive
survived,"        async def _spawn():  # pragma: no cover - Rocketry callback
            await self._spawn_jobs()
",src/scheduler.py,SelfImprovementScheduler
survived,"    def replace_task(self, *, path: str, pattern: str, repl: str) -> dict[str, int]:
        return {""count"": replace(path, pattern, repl)}",src/self_edit/tools.py,FileToolsADK
survived,"        def task(**_kw):
            return _StubDecor()
",src/self_edit/tools.py,adk
survived,"def test_edit_and_view(temp_file: Path) -> None:
    temp_file.write_text(""a\nb\nc\n"")
    edit(temp_file, 1, 2, ""X"")
    assert temp_file.read_text() == ""a\nX\nc""
    assert view(temp_file, 0, 2) == ""a\nX""
",tests/test_self_edit_tools.py,
survived,"def replace(path: str | Path, pattern: str, repl: str) -> int:
    """"""Regex replace ``pattern`` with ``repl`` inside ``path``.""""""
    p = _safe_path(path)
    text = p.read_text(encoding=""utf-8"", errors=""replace"")
    new_text, n = re.subn(pattern, repl, text, flags=re.MULTILINE)
    if n:
        p.write_text(new_text, encoding=""utf-8"")
    return n
",src/self_edit/tools.py,
survived,"async def _op(genome):
    return genome + 1
",tests/test_evolve.py,
survived,"    def all(self) -> Sequence[Candidate]:
        return list(self._items)
",src/evolve.py,InMemoryArchive
survived,"def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    req_txt = repo_root / ""requirements.txt""
    lock_file = repo_root / ""requirements.lock""

    with tempfile.TemporaryDirectory() as tmpdir:
        out_path = Path(tmpdir) / ""requirements.lock""
        pip_compile = shutil.which(""pip-compile"")
        if pip_compile:
            cmd = [pip_compile]
        else:
            cmd = [sys.executable, ""-m"", ""piptools"", ""compile""]
        cmd += [""--quiet"", ""--generate-hashes"", str(req_txt), ""-o"", str(out_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        sys.stdout.write(result.stdout)
        sys.stderr.write(result.stderr)
        if result.returncode != 0:
            return result.returncode
        if out_path.read_bytes() != lock_file.read_bytes():
            sys.stderr.write(
                ""requirements.lock is outdated. Run 'pip-compile --quiet --generate-hashes requirements.txt'\n""
            )
            return 1
    return 0
",scripts/verify_requirements_lock.py,
survived,"    def parse_kwargs():
        param = dict()
        for k, v in args.kwargs:
            if v.isdigit():
                param[k] = int(v)
            elif v == 'True' or v == 'true':
                param[k] = True
            elif v == 'False' or v == 'false':
                param[k] = False
            elif isfloat(v):
                param[k] = float(v)
            else:
                param[k] = v
        return param
",label_studio_ml/examples/timeseries_segmenter/_wsgi.py,
survived,"        def __init__(self, base_url=None, api_key=None):
            pass
",no-ocr-api/tests/test_utils.py,FakeOpenAI
survived,"def extract_title(readme: Path) -> str:
    """"""Return a reasonable title for the given README.""""""
    lines = readme.read_text(encoding=""utf-8"").splitlines()
    # Search the first 50 lines for a level-one heading
    for line in lines[:50]:
        m = TITLE_RE.match(line.strip())
        if m:
            return m.group(1).strip()
    # Fallback to folder name if no heading found early in the file
    return readme.parent.name.replace(""_"", "" "").title()
",scripts/generate_demo_docs.py,
survived,"    def test_missing_file(self):
        with mock.patch.dict(os.environ, {'GRAFANA_TOKEN': 'x'}):
            with self.assertRaises(SystemExit):
                with mock.patch.object(import_dashboard.sys, 'argv', ['imp.py', '/nope']):
                    import_dashboard.main()
",alpha_factory_v1/tests/test_scripts_import_dashboard.py,ImportDashboardScriptTest
survived,"    def __init__(
        self,
        name: str | None = None,
        instructions: str | None = None,
        tools: list | None = None,
    ) -> None:
        self.name = name or """"
        self.instructions = instructions or """"
        self.tools = tools or []
",src/agents/__init__.py,Agent
survived,"    def mocker():
        return MockerFixture()
",src/pytest_mock/__init__.py,
survived,"def expo(*_args: Any, **_kwargs: Any) -> float:
    return 0.0
",src/backoff/__init__.py,
survived,"def select_autoescape(*_args: Any, **_kwargs: Any) -> bool:
    return False
",src/jinja2/__init__.py,
survived,"def pytest_configure(config):  # pragma: no cover - register fixture
    @pytest.fixture
    def mocker():
        return MockerFixture()

    config.pluginmanager.register(sys.modules[__name__])",src/pytest_mock/__init__.py,
survived,"    def test_seed_reproducibility(self) -> None:
        try:
            import numpy as np  # noqa: F401
            import pandas as pd  # noqa: F401
        except ModuleNotFoundError:
            self.skipTest(""numpy/pandas not available"")

        sim = simulation_core.MonteCarloSimulator(n_paths=3, horizon=2, seed=42)
        obs = {
            ""yield_10y"": 4.0,
            ""yield_3m"": 4.5,
            ""stable_flow"": 10.0,
            ""es_settle"": 5000.0,
        }
        factors = sim.simulate(obs)
        expected = [0.989483, 1.02894, 0.998621]
        for f, e in zip(factors, expected):
            self.assertAlmostEqual(f, e, places=6)
        self.assertAlmostEqual(sim.var(factors), -0.009602935809998603)
        self.assertAlmostEqual(sim.cvar(factors), -0.010516713095912844)
",tests/test_simulation_core.py,TestSimulationCore
survived,"def make_client() -> TestClient:
    return TestClient(cast(Any, api.app))
",tests/test_metrics_exposure.py,
survived,"def _rescan_loop() -> None:  # pragma: no cover
    while True:
        try:
            discover_hot_dir()
        except Exception:  # noqa: BLE001
            logger.exception(""Hot-dir rescan failed"")
        time.sleep(_RESCAN_SEC)
",alpha_factory_v1/backend/agents/health.py,
survived,"def test_no_placeholder() -> None:
    files = asset_files()
    assert files, ""no wasm assets found""
    for path in files:
        data = path.read_bytes()
        assert b""placeholder"" not in data.lower(), f""placeholder found in {path}""",tests/test_integrity.py,
survived,"def _set_seed(val: int) -> None:
    global _SEED
    _SEED = val
    random.seed(val)
    np.random.seed(val)
    torch.manual_seed(val)
",alpha_factory_v1/demos/alpha_asi_world_model/alpha_asi_world_model_demo.py,
survived,"def test_governance_bridge_adk_runtime(tmp_path: Path) -> None:
    """"""Launch governance-bridge with ADK enabled and verify logs.""""""
    stub = tmp_path / ""google_adk.py""
    stub.write_text(
        """"""
class Router:
    def __init__(self):
        self.app = type('app', (), {'middleware': lambda *a, **k: lambda f: f})()
    def register_agent(self, agent):
        pass
class Agent: ...
class AgentException(Exception):
    pass
""""""
    )

    env = os.environ.copy()
    env[""PYTHONPATH""] = f""{tmp_path}:{env.get('PYTHONPATH', '')}""
    env[""ALPHA_FACTORY_ENABLE_ADK""] = ""true""

    proc = subprocess.Popen(
        [""governance-bridge"", ""--enable-adk"", ""--port"", ""0""],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        env=env,
    )
    try:
        time.sleep(2)
        proc.terminate()
        out, _ = proc.communicate(timeout=5)
    finally:
        if proc.poll() is None:
            proc.kill()
            proc.wait(timeout=5)

    assert ""Registered GovernanceSimAgent"" in out
    assert ""ADK"" in out",tests/test_governance_bridge_adk_runtime.py,
survived,"        def create_task(self, coro: Any) -> None:
            self.coro = coro
",tests/test_alpha_agi_business_3_v1.py,DummyLoop
survived,"def test_run_cycle_uses_asyncio_run(monkeypatch: pytest.MonkeyPatch) -> None:
    """"""`run_cycle` should call ``asyncio.run`` when no loop is running.""""""
    mod = importlib.import_module(MODULE)

    monkeypatch.setattr(asyncio, ""get_running_loop"", lambda: (_ for _ in ()).throw(RuntimeError()))

    called: dict[str, Any] = {}

    def fake_run(coro: Any) -> None:
        called[""coro""] = coro

    monkeypatch.setattr(asyncio, ""run"", fake_run)

    async def dummy_cycle(*_a: object, **_k: object) -> None:
        pass

    monkeypatch.setattr(mod, ""run_cycle_async"", dummy_cycle)

    mod.run_cycle(mod.Orchestrator(), mod.AgentFin(), mod.AgentRes(), mod.AgentEne(), mod.AgentGdl(), mod.Model())

    assert called.get(""coro"") is not None
    assert getattr(called[""coro""], ""cr_code"", None) is dummy_cycle.__code__
",tests/test_alpha_agi_business_3_v1.py,
survived,"def load_sector_equity_map(path: str | Path = _MAP_PATH) -> Dict[str, list[str]]:
    """"""Return the sector-to-equity mapping from ``path``.""""""

    mapping: Dict[str, list[str]] = {}
    with Path(path).open(newline="""", encoding=""utf-8"") as fh:
        reader = csv.DictReader(fh)
        for row in reader:
            sector = (row.get(""sector"") or """").strip()
            ticker = (row.get(""ticker"") or """").strip()
            if not sector or not ticker:
                continue
            mapping.setdefault(sector, []).append(ticker)
    return mapping
",src/finance/adapter.py,
survived,"def improve_repo(repo_url: str, patch_file: str, metric_file: str, log_file: str) -> Tuple[float, Path]:
    """"""Clone ``repo_url``, apply ``patch_file`` and log score delta.

    Returns the score delta and path to the cloned repository.
    """"""
    if git is None:
        raise RuntimeError(""GitPython is required"")
    repo_dir = Path(tempfile.mkdtemp(prefix=""selfimprover-""))
    repo = git.Repo.clone_from(repo_url, repo_dir)
    baseline = _evaluate(repo_dir, metric_file)
    repo.git.apply(patch_file)
    repo.index.add([metric_file])
    repo.index.commit(""apply patch"")
    new_score = _evaluate(repo_dir, metric_file)
    delta = new_score - baseline
    _log_delta(delta, Path(log_file))
    return delta, repo_dir",alpha_factory_v1/demos/alpha_agi_insight_v1/src/self_improver.py,
survived,"def _data_dir() -> pathlib.Path:
    """"""Return offline CSV directory from env or default.""""""
    return pathlib.Path(os.getenv(""OFFLINE_DATA_DIR"", str(_DEFAULT_DATA_DIR)))
",alpha_factory_v1/demos/macro_sentinel/data_feeds.py,
survived,"def test_load_translations_unknown(monkeypatch):
    monkeypatch.delenv('DEVICONS_LANG', raising=False)
    assert devicons.load_translations('unknown') == {}",tests/test_translations.py,
survived,"def main() -> int:
    if os.getenv(""ALLOW_PRIVATE_TEXT"") == ""1"":
        return 0

    flagged: List[str] = []
    for path in staged_files():
        if scan_file(path):
            flagged.append(str(path))

    if flagged:
        sys.stderr.write(
            ""Private or pay-walled text detected in:\n"" + ""\n"".join(flagged) + ""\n""
        )
        sys.stderr.write(""Set ALLOW_PRIVATE_TEXT=1 to override.\n"")
        return 1
    return 0
",scripts/dp_scrubber.py,
survived,"    def allocate(
        self,
        top_children: Sequence[str],
        other_children: Sequence[str],
        *,
        dry_run: bool = False,
    ) -> Mapping[str, int]:
        """"""Return GPU allocation for ``top_children`` and ``other_children``.""""""
        price = self.price_fetcher(self.region)
        hourly_budget = self.budget_per_day / 24
        result: dict[str, int] = {}
        spent = 0.0
        for child in top_children:
            cost = 8 * price
            if spent + cost <= hourly_budget:
                result[child] = 8
                spent += cost
                if dry_run:
                    _log.info(
                        ""Allocate 8Ã—A10 to %s: cost %.2f/h (remaining %.2f/h)"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
            else:
                if dry_run:
                    _log.info(
                        ""Skip %s: need %.2f/h, remaining %.2f/h"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
        for child in other_children:
            cost = price
            if spent + cost <= hourly_budget:
                result[child] = 1
                spent += cost
                if dry_run:
                    _log.info(
                        ""Allocate 1Ã—A10 to %s: cost %.2f/h (remaining %.2f/h)"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
            else:
                if dry_run:
                    _log.info(
                        ""Skip %s: need %.2f/h, remaining %.2f/h"",
                        child,
                        cost,
                        hourly_budget - spent,
                    )
        if dry_run:
            _log.info(""Total hourly cost: %.2f of %.2f"", spent, hourly_budget)
        return result
",src/scheduler/spot_gpu.py,SpotGPUAllocator
survived,"def test_dry_run_respects_budget(caplog: pytest.LogCaptureFixture) -> None:
    alloc = SpotGPUAllocator(price_fetcher=lambda r: 0.5)
    caplog.set_level(logging.INFO)
    result = alloc.allocate([""a"", ""b""], [""c""], dry_run=True)
    assert result == {""a"": 8, ""b"": 8}
    msgs = [r.getMessage() for r in caplog.records]
    assert any(""Total hourly cost"" in m for m in msgs)
    end_msg = [m for m in msgs if ""Total hourly cost"" in m][0]
    assert ""8.00"" in end_msg and ""8.33"" in end_msg
    assert any(""Skip c"" in m for m in msgs)",tests/test_spot_gpu.py,
survived,"    async def stop_merkle_task(self) -> None:
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:  # pragma: no cover - expected
                pass
            self._task = None
",src/archive/service.py,ArchiveService
survived,"def _parse_spec(spec: str) -> tuple[str, str]:
    if "":"" in spec:
        path, goal = spec.split("":"", 1)
    else:
        parts = spec.split(maxsplit=1)
        if len(parts) != 2:
            raise ValueError(""spec must contain 'path goal'"")
        path, goal = parts
    return path.strip(), goal.strip()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/mutators/code_diff.py,
survived,"def test_latency_benchmark(benchmark: Any) -> None:
    service = DualCriticService([""alpha""])

    def run() -> None:
        service.score(""alpha"", ""alpha"")

    result = benchmark(run)
    p95 = 0.0
    if getattr(result, ""stats"", None) and result.stats[""data""]:
        p95 = quantiles(result.stats[""data""], n=20)[18]
    assert p95 >= 0.0",tests/test_critics.py,
survived,"def _free_port() -> int:
    s = socket.socket()
    s.bind((""localhost"", 0))
    port = s.getsockname()[1]
    s.close()
    return port
",tests/test_critics.py,
survived,"    def run() -> None:
        service.score(""alpha"", ""alpha"")
",tests/test_critics.py,
survived,"        async def _critique(req: CritiqueRequest = Body(...)) -> Any:  # noqa: D401
            result = self.score(req.context, req.response)
            return JSONResponse(result)
",src/critics/dual_critic_service.py,DualCriticService
survived,"    def logic_score(self, context: str, response: str) -> float:
        """"""Return a naive logic score based on substring matching.""""""
        if not context or not response:
            return 0.0
        return 1.0 if response.lower() in context.lower() else 0.0
",src/critics/dual_critic_service.py,DualCriticService
survived,"def __dir__() -> list[str]:  # pragma: no cover - environment driven
    """"""Return module attributes for ``dir()`` calls.""""""

    return sorted(list(globals().keys()) + __all__)",alpha_factory_v1/__init__.py,
survived,"        def sendjson(self, *_a: object, **_kw: object) -> None:  # pragma: no cover - unused
            pass
",tests/test_alpha_agi_business_3_v1.py,DummySock
survived,"            def add(self, instr: object) -> ""DummyTx"":
                self.instructions.append(instr)
                return self
",tests/test_insight_orchestrator_features.py,TestLedger.DummyTx
survived,"            async def send_transaction(self, tx: object, *args: object) -> None:
                captured[""root""] = tx.instructions[0].data.decode()
",tests/test_insight_orchestrator_features.py,TestLedger.DummyClient
survived,"def get_kill_after_minutes() -> int:
    """"""Return minutes after which to kill a running task.""""""
    return _load_config_timeout_minutes()
",anomstack/sensors/timeout.py,
survived,"def test_authorize_button_state_mismatch(monkeypatch):
    st.session_state.clear()
    client = OAuth2(""id"", ""secret"", ""auth"", ""token"")
    oauth = OAuth2Component(client=client)

    monkeypatch.setattr(oauth.client, ""get_authorization_url"", AsyncMock(return_value=""http://auth""))
    monkeypatch.setattr(oauth.client, ""get_access_token"", AsyncMock(return_value={""access_token"": ""tok""}))
    monkeypatch.setattr(""streamlit_oauth._generate_state"", lambda key=None: ""GOOD"")
    monkeypatch.setattr(""streamlit_oauth._authorize_button"", lambda **kwargs: {""code"": ""CODE"", ""state"": ""BAD""})

    with pytest.raises(StreamlitOauthError):
        oauth.authorize_button(""Login"", ""http://cb"", ""scope"", key=""k"")
",tests/test_oauth_component.py,
survived,"    def test_fitness_reward_parses_sleep(self) -> None:
        evt = {""payload"": {""activity"": ""Sleep 7 h 45 m""}}
        val = demo._fitness_reward(evt)
        self.assertIsInstance(val, float)
",tests/test_era_experience.py,TestEraOfExperience
survived,"    def forward(
        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, *, return_attn: bool = True, **kwargs
    ):
        if ""need_weights"" in kwargs:
            return_attn = kwargs.pop(""need_weights"")
        bsz, seq_len, _ = query.size()

        q = self.wq(query)
        k = self.wk(key)
        v = self.wv(value)

        q = self.split_heads(q, bsz)
        k = self.split_heads(k, bsz)
        v = self.split_heads(v, bsz)

        cos = self.cos_cached[:, :, :seq_len, :].to(q.dtype)
        sin = self.sin_cached[:, :, :seq_len, :].to(q.dtype)
        q = apply_rotary_pos_emb(q, cos, sin)
        k = apply_rotary_pos_emb(k, cos, sin)

        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        context = torch.matmul(attn, v)
        context = context.permute(0, 2, 1, 3).contiguous()
        context = context.view(bsz, seq_len, self.d_model)
        output = self.dense(context)

        if return_attn:
            return output, attn
        return output",src/model/u2tokenizer/rope.py,RotaryMultiheadAttention
survived,"    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.wq.weight)
        nn.init.xavier_uniform_(self.wk.weight)
        nn.init.xavier_uniform_(self.wv.weight)
        nn.init.xavier_uniform_(self.dense.weight)
        if self.wq.bias is not None:
            nn.init.zeros_(self.wq.bias)
        if self.wk.bias is not None:
            nn.init.zeros_(self.wk.bias)
        if self.wv.bias is not None:
            nn.init.zeros_(self.wv.bias)
        if self.dense.bias is not None:
            nn.init.zeros_(self.dense.bias)
",src/model/u2tokenizer/rope.py,RotaryMultiheadAttention
survived,"def linear_curve(t: float) -> float:
    return max(0.0, min(1.0, t))
",alpha_factory_v1/core/simulation/forecast.py,
survived,"def span(name: str) -> ContextManager[Any]:
    """"""Return a context manager for ``name``.""""""
    if tracer:
        return cast(ContextManager[Any], tracer.start_as_current_span(name))
    return nullcontext()
",alpha_factory_v1/core/utils/tracing.py,
survived,"        def labels(self, *_a: Any, **_kw: Any) -> ""_N"":
            return self
",alpha_factory_v1/core/utils/tracing.py,_N
survived,"def test_repeat_within_day_high_score() -> None:
    _reset()
    res1 = {""context"": ""run 5k"", ""time"": ""2025-04-22T07:00:00Z""}
    res2 = {""context"": ""run 5k"", ""time"": ""2025-04-23T06:00:00Z""}
    hc.reward(None, None, res1)
    value = hc.reward(None, None, res2)
    assert 0.0 <= value <= 1.0
    assert value > 0.5
",tests/test_habit_consistency_reward.py,
survived,"def test_learning_event_in_range() -> None:
    state = DummyState()
    result = {""context"": ""duolingo spanish lesson"", ""duration"": 1800}
    value = ed.reward(state, None, result)
    assert isinstance(value, float)
    assert 0.0 <= value <= 1.0
",tests/test_education_reward.py,
survived,"async def test_broadcast_merkle_root_devnet() -> None:
    if os.getenv(""PYTEST_NET_OFF"") == ""1"" or not await _devnet_available():
        pytest.skip(""network disabled or devnet unreachable"")
    tmp = tempfile.TemporaryDirectory()
    ledger = Ledger(os.path.join(tmp.name, ""l.db""), rpc_url=""https://api.devnet.solana.com"", broadcast=True)
    env = messaging.Envelope(""a"", ""b"", {""v"": 1}, 0.0)
    ledger.log(env)
    try:
        await ledger.broadcast_merkle_root()
    finally:
        await ledger.stop_merkle_task()
        ledger.close()
        tmp.cleanup()",tests/test_devnet_broadcast.py,
survived,"    def log(self, env: messaging.Envelope) -> None:  # type: ignore[override]
        self.logged.append(env)
",tests/test_safety_guardian_property.py,DummyLedger
survived,"        async def grab_two() -> float:
            gen = demo.experience_stream()
            t1 = time.perf_counter()
            await anext(gen)
            t2 = time.perf_counter()
            await anext(gen)
            t3 = time.perf_counter()
            return (t2 - t1 + t3 - t2) / 2
",tests/test_era_experience.py,TestEraOfExperience
survived,"def test_dist_self_contained() -> None:
    dist = Path(__file__).resolve().parents[1] / ""dist"" / ""index.html""
    url = dist.as_uri()
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector(""#controls"")
        attrs = page.eval_on_selector_all(
            ""script[src], link[href]"",
            ""els => els.map(e => e.getAttribute('src') || e.getAttribute('href'))"",
        )
        assert all("".."" not in a for a in attrs)
        browser.close()",alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_dist_self_contained.py,
survived,"    async def step(self) -> None:  # noqa: D401
        """"""Delegate step execution to :meth:`run_cycle`.""""""
        await self.run_cycle()
",alpha_factory_v1/backend/agents/smart_contract_agent.py,SmartContractAgent
survived,"    async def emit(self, recipient: str, payload: Any) -> None:
        env = messaging.Envelope(self.name, recipient, payload, time.time())
        self.ledger.log(env)
        self.bus.publish(recipient, env)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/base_agent.py,BaseAgent
survived,"def evaluate(pop: Population, fn: Callable[[List[float]], Tuple[float, float]]) -> None:
    for ind in pop:
        ind.fitness = fn(ind.genome)
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"    async def run_cycle(self) -> None:
        pass
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/agents/safety_agent.py,SafetyGuardianAgent
survived,"async def _main() -> None:  # pragma: no cover - CLI helper
    orch = Orchestrator()
    await orch.run_forever()
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/orchestrator.py,
survived,"def _non_dominated_sort(pop: Population) -> List[Population]:
    fronts: List[Population] = []
    S = {id(ind): [] for ind in pop}
    n = {id(ind): 0 for ind in pop}
    for p in pop:
        for q in pop:
            if p is q:
                continue
            if all(pf <= qf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                if any(pf < qf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                    S[id(p)].append(q)
            elif all(qf <= pf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                if any(qf < pf for pf, qf in zip(p.fitness, q.fitness)):  # type: ignore[arg-type]
                    n[id(p)] += 1
        if n[id(p)] == 0:
            p.rank = 0
            if not fronts:
                fronts.append([])
            fronts[0].append(p)
    i = 0
    while i < len(fronts):
        nxt: Population = []
        for p in fronts[i]:
            for q in S[id(p)]:
                n[id(q)] -= 1
                if n[id(q)] == 0:
                    q.rank = i + 1
                    nxt.append(q)
        if nxt:
            fronts.append(nxt)
        i += 1
    return fronts
",alpha_factory_v1/demos/alpha_agi_insight_v1/src/simulation/mats.py,
survived,"    def clients(self) -> List[GeminiClientWrapper]:
        """"""Return managed clients.""""""
        return self._clients
",app/services/pool.py,GeminiClientPool
survived,"    def test_save_equity_cache_logs_error(self) -> None:
        with patch(""pathlib.Path.write_text"", side_effect=IOError(""boom"")) as mock_write:
            with patch.object(rm._LOG, ""debug"") as mock_log:
                rm._save_equity_cache([1.0, 2.0])
                mock_log.assert_called()
            mock_write.assert_called()
",tests/test_risk_management.py,TestRiskManagementCache
survived,"    def update_window_region(self):
        '''
        Update window region
        '''
        self.region = get_window_region(self.window_title)
        if self.region is None:
            text = f""Cannot find window: {self.window_title}""
            logger.error(text)
            raise RuntimeError(text)
",src/input/GameWindowCapturorForMac.py,GameWindowCapturor
survived,"    def is_in_rune_game(self):
        '''
        is_in_rune_game
        '''
        # Get lastest game screen frame buffer
        self.frame = self.capture.get_frame()
        # Resize game screen to 1296x759
        self.img_frame = cv2.resize(self.frame, (1296, 759), interpolation=cv2.INTER_NEAREST)

        # Crop arrow detection box
        x, y = self.cfg.arrow_box_start_point
        size = self.cfg.arrow_box_size
        img_roi = self.img_frame[y:y+size, x:x+size]

        # Check if arrow exist on screen
        best_score = float('inf')
        for direc, arrow_list in self.img_arrows.items():
            for img_arrow in arrow_list:
                _, score, _ = find_pattern_sqdiff(
                                img_roi, img_arrow,
                                mask=get_mask(img_arrow, (0, 255, 0)))
                if score < best_score:
                    best_score = score

        draw_rectangle(
            self.img_frame_debug, (x, y), (size, size),
            (0, 0, 255), str(round(best_score, 2))
        )

        if best_score < self.cfg.arrow_box_diff_thres:
            logger.info(f""Arrow screen detected with score({score})"")
            return True
        return False
",src/legacy/mapleStoryAutoLevelUp_legacy.py,MapleStoryBot
survived,"        def latest_log(self) -> str:
            return self.llm(""hi"") if self.llm else ""done""
",tests/test_aiga_openai_bridge_offline.py,DummyEvolver
survived,"    def log(self, env) -> None:  # type: ignore[override]
        self.events.append(env.payload.get(""event""))
",tests/test_alert_webhook.py,DummyLedger
survived,"def _build_local_site(repo_root: Path) -> bool:
    script = repo_root / ""scripts"" / ""build_gallery_site.sh""
    if not script.is_file():
        return False
    try:
        subprocess.run([str(script)], check=True)
    except Exception:
        return False
    return True
",scripts/launch_gallery.py,
survived,"    def test_mdot(self):
        a = np.eye(2)
        b = np.array([[2, 0], [0, 2]])
        result = common.mdot(a, b)
        np.testing.assert_array_equal(result, b)
",tests/test_common.py,TestCommonFunctions
survived,"    def test_anorm(self):
        vec = np.array([3.0, 4.0])
        self.assertAlmostEqual(common.anorm(vec), 5.0)
        self.assertAlmostEqual(common.anorm2(vec), 25.0)
",tests/test_common.py,TestCommonFunctions
survived,"def test_agents_list_offline(non_network: None) -> None:
    """"""Verify /agents lists all required demo agents.""""""
    os.environ[""NO_LLM""] = ""1""
    os.environ.setdefault(""ALPHA_ASI_SILENT"", ""1"")
    os.environ.setdefault(""ALPHA_ASI_MAX_STEPS"", ""1"")

    mod = importlib.import_module(""alpha_factory_v1.demos.alpha_asi_world_model.alpha_asi_world_model_demo"")
    client = TestClient(cast(Any, mod.app))

    resp = client.get(""/agents"")
    assert resp.status_code == 200
    agents = resp.json()
    expected = {
        ""PlanningAgent"",
        ""ResearchAgent"",
        ""StrategyAgent"",
        ""MarketAnalysisAgent"",
        ""CodeGenAgent"",
        ""SafetyAgent"",
    }
    assert expected.issubset(set(agents))",tests/test_world_model_demo.py,
survived,"    def test_new_version_ok(self) -> None:
        fake_mod = types.SimpleNamespace(__version__=""0.0.15"")
        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == ""openai_agents"":
                return object()
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
        ):
            self.assertTrue(preflight.check_openai_agents_version())
",tests/test_preflight_openai_agents_version.py,TestPreflightOpenAIAgentsVersion
survived,"def temp_path():
    path = REPO_ROOT / ""tmp_tools_undo.txt""
    try:
        yield path
    finally:
        if path.exists():
            path.unlink()
",tests/test_tools_undo.py,
survived,"def _record_history(p: Path) -> None:
    """"""Save the current contents of ``p`` for undo.""""""
    _EDIT_HISTORY.append((p, p.read_text(encoding=""utf-8"", errors=""replace"")))
",src/self_edit/tools.py,
survived,"def test_undo_multiple_edits(temp_path: Path) -> None:
    temp_path.write_text(""alpha\nbeta\n"")
    replace_str(temp_path, ""alpha"", ""A"")
    first = temp_path.read_text()
    edit(temp_path, 1, 2, ""B"")
    assert undo_last_edit() is True
    assert temp_path.read_text() == first
    assert undo_last_edit() is True
    assert temp_path.read_text() == ""alpha\nbeta\n""
    replace(temp_path, ""A"", ""alpha"")
    insert_after(temp_path, ""alpha"", ""gamma"")
    assert ""gamma"" in temp_path.read_text()
    assert undo_last_edit() is True
    assert ""gamma"" not in temp_path.read_text()
    assert undo_last_edit() is False
    assert temp_path.read_text() == ""alpha\nbeta\n""
    assert view_lines(temp_path, 1, 2) == ""alpha\nbeta""",tests/test_tools_undo.py,
survived,"    def list_entries(self) -> List[Tuple[int, str, str, int]]:
        with sqlite3.connect(self.db_path) as cx:
            rows = list(cx.execute(""SELECT id, path, cid, pinned FROM tarballs ORDER BY id""))
        return [(int(r[0]), str(r[1]), str(r[2]), int(r[3])) for r in rows]
",src/archive/hash_archive.py,HashArchive
survived,"def publish_proof(
    transcript_path: str | Path,
    agent_hash: str,
    score: Sequence[float],
    db: ""ArchiveDB"",
) -> str:
    """"""Generate proof, publish to IPFS and store CID in ``db``.""""""
    proof = generate_proof(transcript_path, agent_hash, score)
    proof_path = Path(transcript_path).with_suffix("".proof"")
    proof_path.write_text(proof, encoding=""utf-8"")
    cid = _ipfs_add(proof_path)
    db.set_state(f""snark:{agent_hash}"", cid)
    return cid
",src/utils/snark.py,
survived,"    def set_proof_cid(self, agent_hash: str, cid: str) -> None:
        """"""Store the IPFS CID of the SNARK proof for ``agent_hash``.""""""
        self.set_state(f""snark:{agent_hash}"", cid)
",src/archive/db.py,ArchiveDB
survived,"    def get_proof_cid(self, agent_hash: str) -> str | None:
        """"""Return the stored proof CID for ``agent_hash`` if present.""""""
        return self.get_state(f""snark:{agent_hash}"")",src/archive/db.py,ArchiveDB
survived,"def _gen_crc8_table(poly: int) -> list[int]:
  table = []
  for i in range(256):
    crc = i
    for _ in range(8):
      if crc & 0x80:
        crc = ((crc << 1) ^ poly) & 0xFF
      else:
        crc = (crc << 1) & 0xFF
    table.append(crc)
  return table
",opendbc/car/crc.py,
survived,"    def test_invalid_env_ports_default(self) -> None:
        env = {""PORT"": ""0"", ""METRICS_PORT"": ""-1"", ""A2A_PORT"": ""0""}
        with patch.dict(os.environ, env, clear=True):
            args = edge_runner.parse_args([])
        self.assertEqual(args.port, 8000)
        self.assertIsNone(args.metrics_port)
        self.assertIsNone(args.a2a_port)
",tests/test_edge_runner_cli.py,TestParseArgs
survived,"    def test_invalid_numeric_fallback(self) -> None:
        env = {
            ""DEV_MODE"": ""true"",
            ""PORT"": ""foo"",
            ""METRICS_PORT"": ""bar"",
            ""A2A_PORT"": ""baz"",
            ""ALPHA_CYCLE_SECONDS"": ""qux"",
            ""MAX_CYCLE_SEC"": ""zap"",
            ""ALPHA_MODEL_MAX_BYTES"": ""oops"",
        }
        with mock.patch.dict(os.environ, env, clear=True):
            orch = importlib.reload(
                importlib.import_module(""alpha_factory_v1.backend.orchestrator"")
            )
        self.assertEqual(orch.PORT, 8000)
        self.assertEqual(orch.METRICS_PORT, 0)
        self.assertEqual(orch.A2A_PORT, 0)
        self.assertEqual(orch.CYCLE_DEFAULT, 60)
        self.assertEqual(orch.MAX_CYCLE_SEC, 30)
        self.assertEqual(orch.MODEL_MAX_BYTES, 64 * 1024 * 1024)
",tests/test_orchestrator_env.py,TestOrchestratorEnv
survived,"    def generate_report(self, result: CollectionResult, output_format: str = ""text"") -> str:
        """"""Generate a formatted report for a result.""""""
        summary = self.summarize(result)
        if output_format == ""json"":
            return self.to_json(summary)
        if output_format == ""html"":
            return self.to_html(summary)
        return self.to_text(summary)",src/meta_agent/evaluation/reporting.py,ReportingModule
survived,"def test_execute_and_collect_logs(monkeypatch, tmp_path, caplog):
    fake_exec = MagicMock()
    fake_exec.run_tests.return_value = ExecutionResult(0, ""out"", ""err"")
    module = ResultCollectionModule(fake_exec)
    with caplog.at_level(""INFO"", logger=""meta_agent.evaluation.result_collection""):
        module.execute_and_collect(tmp_path)
    assert any(
        ""Executing and collecting results"" in r.getMessage() for r in caplog.records
    )",tests/unit/test_result_collection_module.py,
survived,"    def __init__(self) -> None:
        self.logger = logging.getLogger(__name__)
",src/meta_agent/evaluation/reporting.py,ReportingModule
survived,"def evolve(state, ruleNum):
    out = []
    p = 0
    while p < 10:
        b = 0
        q = 7
        while q >= 0:
            st = state
            b = b + st[0] * pow2(q)
            next = []
            i = 0
            while i < n:
                lidx = i - 1
                if lidx < 0:
                    lidx = n - 1
                left = st[lidx]
                center = st[i]
                ridx = i + 1
                if ridx >= n:
                    ridx = 0
                right = st[ridx]
                index = left * 4 + center * 2 + right
                next = next + [ruleBit(ruleNum, index)]
                i = i + 1
            state = next
            q = q - 1
        out = out + [b]
        p = p + 1
    return out
",tests/rosetta/transpiler/Python/elementary-cellular-automaton-random-number-generator.py,
survived,"def entropy(data):
    if data == """":
        return 0.0
    counts = {}
    i = 0
    while i < len(data):
        ch = data[i:i + 1]
        if ch in counts:
            counts[ch] = counts[ch] + 1
        else:
            counts[ch] = 1
        i = i + 1
    e = 0.0
    l = float(len(data))
    for ch in counts:
        px = (float(counts[ch])) / l
        if px > 0.0:
            e = e - px * log2(px)
    return e
",tests/rosetta/transpiler/Python/entropy-narcissist.py,
survived,"def outputState(state):
    line = """"
    i = 0
    while i < len(state):
        if state[i:i + 1] == ""1"":
            line = line + ""#""
        else:
            line = line + "" ""
        i = i + 1
    print(line)
",tests/rosetta/transpiler/Python/elementary-cellular-automaton.py,
survived,"def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())
",tests/rosetta/transpiler/Python/egyptian-division.py,
survived,"def printMatrix(heading, m):
    print(heading)
    i = 0
    while i < len(m):
        print(rowString(m[i]))
        i = i + 1
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def mul(a, b):
    return a * b
",tests/rosetta/transpiler/Python/element-wise-operations.py,
survived,"def mul(p, n):
    r = zero()
    q = p
    k = n
    while k > 0:
        if k % 2 == 1:
            r = add(r, q)
        q = dbl(q)
        k = k // 2
    return r
",tests/rosetta/transpiler/Python/elliptic-curve-arithmetic.py,
survived,"def test_fstringify_files_charcount(tmp_path, monkeypatch):
    source = ""'{}'.format(1)\n""
    f = tmp_path / ""a.py""
    f.write_text(source)

    captured = {}

    def fake_print_report(state, found_files, changed_files, total_cc_new, total_cc_original, total_expr, total_time):
        captured[""new""] = total_cc_new
        captured[""orig""] = total_cc_original

    monkeypatch.setattr(api, ""_print_report"", fake_print_report)

    state = State()
    api.fstringify_files([str(f)], state)

    assert captured[""orig""] == len(source)
    assert captured[""new""] == len(""f'{1}'\n"")",test/integration/test_api.py,
survived,"    def test_best_only_sorting(self):
        data = [
            {""alpha"": ""low"", ""score"": 1},
            {""alpha"": ""high"", ""score"": 5}
        ]
        tmp = Path(""/tmp/opps2.json"")
        tmp.write_text(json.dumps(data), encoding=""utf-8"")
        os.environ[""ALPHA_OPPS_FILE""] = str(tmp)
        os.environ[""ALPHA_BEST_ONLY""] = ""1""
        try:
            agent = biz.AlphaOpportunityAgent()
            self.assertEqual(agent._opportunities[0][""alpha""], ""high"")
        finally:
            del os.environ[""ALPHA_OPPS_FILE""]
            del os.environ[""ALPHA_BEST_ONLY""]
            tmp.unlink()
",tests/test_alpha_opportunity_env.py,TestAlphaOpportunityEnv
survived,"def main() -> None:
    runtime = AgentRuntime(api_key=None)
    agent = AlphaDiscoveryAgent()
    runtime.register(agent)
    print(""Registered AlphaDiscoveryAgent with runtime"")
    runtime.run()
",alpha_factory_v1/demos/aiga_meta_evolution/alpha_opportunity_stub.py,
survived,"def main() -> None:
    ap = argparse.ArgumentParser(description=""Launch the AI-GA meta-evolution demo"")
    ap.add_argument(""--pull"", action=""store_true"", help=""pull signed image instead of building"")
    ap.add_argument(""--gpu"", action=""store_true"", help=""enable NVIDIA runtime"")
    ap.add_argument(""--logs"", action=""store_true"", help=""tail container logs after start-up"")
    ap.add_argument(""--reset"", action=""store_true"", help=""remove volumes and images"")
    ap.add_argument(""--stop"", action=""store_true"", help=""stop running containers"")
    args = ap.parse_args()

    dc = docker_compose_cmd()
    compose = dc + [""--project-name"", PROJECT, ""--env-file"", str(CONFIG_ENV), ""-f"", str(COMPOSE_YAML)]

    os.chdir(ROOT_DIR)
    ensure_env_file()

    if args.reset:
        run(compose + [""down"", ""-v"", ""--rmi"", ""all""])
        return
    if args.stop:
        run(compose + [""down""])
        return

    if args.pull:
        run([""docker"", ""pull"", GHCR_IMAGE])

    gpu_args = [""--compatibility"", ""--profile"", ""gpu""] if args.gpu else []
    extra = [""--no-build""] if args.pull else []
    run(compose + gpu_args + [""up"", ""-d"", *extra])

    print(""Dashboard â†’ http://localhost:7862"")
    print(""OpenAPI  â†’ http://localhost:8000/docs"")
    print(""Stop     â†’ start_aiga_demo.py --stop"")

    if args.logs:
        subprocess.run(compose + [""logs"", ""-f""])
",alpha_factory_v1/demos/aiga_meta_evolution/start_aiga_demo.py,
survived,"            def __init__(self, val: str) -> None:
                pass
",tests/test_ledger_broadcast.py,DummyPk
survived,"            def __init__(self, program_id: Any, data: bytes, keys: list[Any]):
                self.data = data
",tests/test_ledger_client_close.py,DummyInstr
survived,"def test_broadcast_merkle_root_closes_client() -> None:
    with tempfile.TemporaryDirectory() as tmp:
        ledger = Ledger(os.path.join(tmp, ""l.db""), rpc_url=""http://rpc.test"", broadcast=True)
        env = messaging.Envelope(sender=""a"", recipient=""b"", payload={""v"": 1}, ts=0.0)
        ledger.log(env)
        root = ledger.compute_merkle_root()

        calls: list[tuple[str, Any]] = []

        class DummyClient:
            def __init__(self, url: str) -> None:
                calls.append((""url"", url))

            async def send_transaction(self, tx: Any, *args: Any) -> None:
                calls.append((""sent"", tx.instructions[0].data.decode()))

            async def close(self) -> None:
                calls.append((""closed"", True))

        class DummyTx:
            def __init__(self) -> None:
                self.instructions: list[Any] = []

            def add(self, instr: Any) -> ""DummyTx"":
                self.instructions.append(instr)
                return self

        class DummyInstr:
            def __init__(self, program_id: Any, data: bytes, keys: list[Any]):
                self.data = data

        class DummyPk:
            def __init__(self, val: str) -> None:
                pass

        with (
            mock.patch.dict(
                sys.modules,
                {
                    ""solana"": ModuleType(""solana""),
                    ""solana.rpc"": ModuleType(""solana.rpc""),
                    ""solana.rpc.async_api"": ModuleType(""solana.rpc.async_api""),
                },
            ),
            mock.patch(""solana.rpc.async_api.AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""AsyncClient"", DummyClient, create=True),
            mock.patch.object(insight_logging, ""Transaction"", DummyTx, create=True),
            mock.patch.object(insight_logging, ""TransactionInstruction"", DummyInstr, create=True),
            mock.patch.object(insight_logging, ""PublicKey"", DummyPk, create=True),
        ):
            asyncio.run(ledger.broadcast_merkle_root())

        assert (""sent"", root) in calls
        assert (""closed"", True) in calls",tests/test_ledger_client_close.py,
survived,"    def _verify_checksum(self, mnemonic_ids):
        from shamir_mnemonic.share import Share
        try:
            Share.from_mnemonic("" "".join(self.id_to_word(i) for i in mnemonic_ids))
            return True
        except Exception:
            return False
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    def return_verified_password_or_false(self, mnemonic_ids_list):
        for count, mnemonic_ids in enumerate(mnemonic_ids_list, 1):
            if None not in mnemonic_ids and self._verify_checksum(mnemonic_ids):
                return mnemonic_ids, count
        return False, count
",btcrecover/btcrseed.py,WalletSLIP39Seed
survived,"    def mochi(self, line, cell):
        """"""Run Mochi code contained in the cell.""""""
        with tempfile.NamedTemporaryFile(""w"", suffix="".mochi"", delete=False) as f:
            f.write(cell)
            fname = f.name
        try:
            cmd = [""mochi"", ""run"", fname]
            proc = subprocess.run(cmd, capture_output=True, text=True)
            if proc.stdout:
                sys.stdout.write(proc.stdout)
            if proc.stderr:
                sys.stderr.write(proc.stderr)
            if proc.returncode != 0:
                raise RuntimeError(f""mochi exited with status {proc.returncode}"")
        finally:
            os.unlink(fname)
",tools/notebook/mochi_magic.py,MochiMagics
survived,"    def __init__(self):
        self.lines = []
        self.indent = 0
",tools/any2mochi/py_simple.py,Conv
survived,"    def delete(
        self,
        func: Callable[..., Any] | None = None,
        *,
        name: str | None = None,
        description: str | None = None,
    ) -> Callable[..., Any] | DecoratorCallable:
        """"""Register a delete operation.""""""

        def decorator(fn: Callable[..., Any]) -> Callable[..., Any]:
            return self.resource(fn, name=name, description=description)

        if func is not None:
            return decorator(func)
        return cast(""DecoratorCallable"", decorator)
",src/enrichmcp/app.py,EnrichMCP
survived,"async def update_customer(customer_id: int, patch: Customer.PatchModel) -> Customer:
    """"""Update an existing customer.""""""
    customer = CUSTOMERS[customer_id]
    data = patch.dict(exclude_unset=True)
    updated = customer.copy(update=data)
    CUSTOMERS[customer_id] = updated
    return updated
",examples/mutable_crud/app.py,
survived,"    async def create_item(name: str) -> Item:
        """"""Create item.""""""
        return Item(id=1, name=name)
",tests/test_mutability.py,
survived,"    def _project_dir(self, project: str) -> Path:
        path = self.root / project
        path.mkdir(parents=True, exist_ok=True)
        return path
",examples/basic_memory/memory.py,FileMemoryStore
survived,"    def list(self, project: str, page: int, page_size: int) -> list[MemoryNoteSummary]:
        """"""Return a paginated list of notes for ``project``.""""""
",examples/basic_memory/memory.py,MemoryStore
survived,"def has_network() -> bool:
    """"""Return ``True`` if DNS resolution for ``pypi.org`` succeeds.""""""
    try:
        socket.gethostbyname(""pypi.org"")
        return True
    except OSError:
        return False
",check_env.py,
survived,"        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return object()
            if name in {""openai_agents"", ""agents""}:
                return None
            return orig_find_spec(name, *args, **kwargs)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def test_missing_version_fails(self) -> None:
        for name in (""openai_agents"", ""agents""):
            with self.subTest(module=name):
                self.assertNotEqual(self._run_check(name, None), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def test_new_version_ok(self) -> None:
        for name in (""openai_agents"", ""agents""):
            with self.subTest(module=name):
                self.assertEqual(self._run_check(name, ""0.0.15""), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def test_old_version_fails(self) -> None:
        for name in (""openai_agents"", ""agents""):
            with self.subTest(module=name):
                self.assertNotEqual(self._run_check(name, ""0.0.13""), 0)
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"    def _run_check(self, module_name: str, version: str | None) -> int:
        fake_mod = types.SimpleNamespace()
        if version is not None:
            fake_mod.__version__ = version
        orig_import_module = importlib.import_module
        orig_find_spec = importlib.util.find_spec

        def _fake_import(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return fake_mod
            return orig_import_module(name, *args, **kwargs)

        def _fake_find_spec(name: str, *args: Any, **kwargs: Any) -> object:
            if name == module_name:
                return object()
            if name in {""openai_agents"", ""agents""}:
                return None
            return orig_find_spec(name, *args, **kwargs)

        with (
            mock.patch(""importlib.import_module"", side_effect=_fake_import),
            mock.patch(""importlib.util.find_spec"", side_effect=_fake_find_spec),
            mock.patch.object(check_env, ""REQUIRED"", []),
            mock.patch.object(check_env, ""OPTIONAL"", [module_name]),
            mock.patch.object(check_env, ""warn_missing_core"", lambda: []),
        ):
            return check_env.main([])
",tests/test_check_env_openai_agents_version.py,TestCheckEnvOpenAIAgentsVersion
survived,"def warn_missing_core() -> None:
    missing = [pkg for pkg in CORE if importlib.util.find_spec(pkg) is None]
    if missing:
        print(""WARNING: Missing core packages:"", "", "".join(missing))
",check_env.py,
survived,"def lead_time(truth: list[bool], pred: list[bool]) -> float:
    """"""Return ``pred`` onset minus ``truth`` onset.""""""
    def first_true(seq: list[bool]) -> int:
        for i, val in enumerate(seq):
            if val:
                return i
        return len(seq)

    return first_true(pred) - first_true(truth)
",src/simulation/replay.py,
survived,"    def run_compute_pressure_force_acceleration(self):
        """"""Compute pressure forces and update acceleration.""""""
        pos = self.sorted_position[:, :3]
        i_idx = torch.repeat_interleave(
            torch.arange(pos.shape[0], device=self.device),
            self.neighbor_map.shape[1],
        )
        j_idx = self.neighbor_map.reshape(-1)
        mask = j_idx >= 0
        i_idx = i_idx[mask]
        j_idx = j_idx[mask]
        diff = pos[i_idx] - pos[j_idx]
        dist = diff.norm(dim=1)
        dir = diff / (dist.unsqueeze(1) + 1e-12)
        grad = (
            self.config[""mass_mult_gradWspikyCoefficient""]
            * (self.config[""h""] - dist).clamp(min=0) ** 2
        ).unsqueeze(1) * dir
        pres = (
            self.pressure[i_idx] / (self.rho[i_idx] ** 2)
            + self.pressure[j_idx] / (self.rho[j_idx] ** 2)
        ).unsqueeze(1)
        force = -pres * grad
        acc = torch.zeros_like(self.sorted_position)
        acc.scatter_add_(0, i_idx.unsqueeze(1).expand(-1, 3), force)
        gravity = torch.tensor(
            [
                self.config.get(""gravity_x"", 0.0),
                self.config.get(""gravity_y"", -9.8),
                self.config.get(""gravity_z"", 0.0),
            ],
            device=self.device,
        )
        acc[:, :3] += gravity
        self.acceleration = acc
",pytorch_solver.py,PytorchSolver
survived,"def test_simple_flow():
    pos = torch.tensor(
        [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.4, 1.0], [2.0, 2.0, 2.0, 1.0]]
    )
    vel = torch.zeros_like(pos)
    cfg = {
        ""xmin"": 0.0,
        ""ymin"": 0.0,
        ""zmin"": 0.0,
        ""hash_grid_cell_size_inv"": 1.0,
        ""grid_cells_x"": 4,
        ""grid_cells_y"": 4,
        ""grid_cells_z"": 4,
        ""grid_cell_count"": 64,
        ""h"": 0.5,
        ""mass_mult_Wpoly6Coefficient"": 1.0,
        ""mass_mult_gradWspikyCoefficient"": 1.0,
        ""rho0"": 1.0,
        ""delta"": 1.0,
        ""time_step"": 0.01,
    }
    solver = PytorchSolver(pos, vel, cfg)
    solver.run_hash_particles()
    solver.run_sort()
    solver.run_index()
    solver.run_index_post_pass()
    solver.run_find_neighbors()
    solver.run_compute_density()
    solver.run_compute_pressure()
    solver.run_compute_pressure_force_acceleration()
    solver.run_integrate()

    # the first two particles should be neighbours
    neigh0 = solver.neighbor_map[0]
    assert 1 in neigh0[:2]
    assert solver.position.shape == pos.shape
    # velocities should change due to gravity
    assert torch.allclose(solver.velocity[0, 1], torch.tensor(-0.098), atol=1e-3)",tests/test_pytorch_solver.py,
survived,"    def run_index(self):
        """"""Compute starting index in the sorted array for each grid cell.""""""
        num_cells = self.config[""grid_cell_count""]
        counts = torch.bincount(self.particle_index[:, 0], minlength=num_cells)
        start = torch.cumsum(
            torch.cat(
                [torch.zeros(1, device=self.device, dtype=torch.long), counts[:-1]]
            ),
            dim=0,
        )
        index = torch.where(counts > 0, start, torch.full_like(start, -1))
        index = torch.cat(
            [index, torch.tensor([self.particle_index.shape[0]], device=self.device)]
        )
        self.grid_cell_index = index
",pytorch_solver.py,PytorchSolver
survived,"def convert_backgrounds(v1_path, out_dir, doc_slug):
    bgs = load_json(v1_path)
    out_bg = []
    out_bgb = []
    for bg in bgs:
        f = bg[""fields""]
        slug = bg[""pk""]
        pk = f""{doc_slug}_{slug}""
        out_bg.append({
            ""model"": ""api_v2.background"",
            ""pk"": pk,
            ""fields"": {""name"": f[""name""], ""desc"": f[""desc""], ""document"": doc_slug},
        })
        mapping = [
            (""skill_proficiencies"", ""Skill Proficiencies"", ""skill_proficiency""),
            (""tool_proficiencies"", ""Tool Proficiencies"", ""tool_proficiency""),
            (""languages"", ""Languages"", ""language""),
            (""equipment"", ""Equipment"", ""equipment""),
        ]
        for key, name, typ in mapping:
            val = f.get(key)
            if val:
                out_bgb.append({
                    ""model"": ""api_v2.backgroundbenefit"",
                    ""pk"": f""{pk}_{slugify(name)}"",
                    ""fields"": {""name"": name, ""desc"": val, ""type"": typ, ""parent"": pk},
                })
        if f.get(""feature"") or f.get(""feature_desc""):
            out_bgb.append({
                ""model"": ""api_v2.backgroundbenefit"",
                ""pk"": f""{pk}_{slugify(f.get('feature','feature'))}"",
                ""fields"": {""name"": f.get(""feature"", ""Feature""), ""desc"": f.get(""feature_desc"", """"), ""type"": ""feature"", ""parent"": pk},
            })
        if f.get(""suggested_characteristics""):
            out_bgb.append({
                ""model"": ""api_v2.backgroundbenefit"",
                ""pk"": f""{pk}_suggested-characteristics"",
                ""fields"": {""name"": ""Suggested Characteristics"", ""desc"": f[""suggested_characteristics""], ""type"": ""suggested_characteristics"", ""parent"": pk},
            })
    if out_bg:
        save_json(out_bg, os.path.join(out_dir, ""Background.json""))
    if out_bgb:
        save_json(out_bgb, os.path.join(out_dir, ""BackgroundBenefit.json""))
",convert_missing.py,
survived,"            def inc(self, *_a: Any) -> None: ...
",src/interface/api_server.py,_N
survived,"    async def _metrics() -> Response:
        if ""generate_latest"" not in globals():
            raise HTTPException(status_code=503, detail=""prometheus_client not installed"")
        return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)
",src/interface/api_server.py,
survived,"def test_settings_offline_enabled_when_missing_key(monkeypatch):
    monkeypatch.delenv(""OPENAI_API_KEY"", raising=False)
    importlib.reload(config)
    s = config.Settings()
    assert s.offline",tests/test_config_settings.py,
survived,"def test_codegen_agent_emits_to_safety(monkeypatch) -> None:
    cfg = config.Settings(bus_port=0)
    bus = DummyBus(cfg)
    led = DummyLedger()
    agent = codegen_agent.CodeGenAgent(bus, led)
    monkeypatch.setattr(agent, ""execute_in_sandbox"", lambda code: ("""", """"))
    env = messaging.Envelope(""market"", ""codegen"", {""analysis"": ""x""}, 0.0)
    asyncio.run(agent.handle(env))
    assert bus.published[-1][0] == ""safety""",tests/test_agent_handle_methods.py,
survived,"def test_bundle_metadata_custom_fields_preserved():
    data = {
        ""schema_version"": BUNDLE_SCHEMA_VERSION,
        ""meta_agent_version"": ""0.1.0"",
        ""foo"": ""bar"",
        ""custom"": {""x"": 1},
    }
    meta = BundleMetadata(**data)
    assert meta.meta_agent_version == ""0.1.0""
    assert meta.custom == {""x"": 1}
    assert getattr(meta, ""foo"") == ""bar""",tests/test_bundle_metadata.py,
survived,"    def generate(
        self,
        agent_code: str,
        tests: Optional[Mapping[str, str]] = None,
        requirements: Optional[Sequence[str]] = None,
        readme: str = """",
        guardrails_manifest: str = """",
        templates: Optional[Mapping[str, str]] = None,
    ) -> BundleMetadata:
        """"""Generate bundle files and return metadata.""""""

        checksums: dict[str, str] = {}

        checksums[""agent.py""] = self._write_file(""agent.py"", agent_code)

        tests = tests or {}
        if not tests:
            (self.bundle_dir / ""tests"").mkdir(parents=True, exist_ok=True)
        for name, content in tests.items():
            checksums[f""tests/{name}""] = self._write_file(Path(""tests"") / name, content)

        req_content = ""\n"".join(requirements or [])
        checksums[""requirements.txt""] = self._write_file(""requirements.txt"", req_content)

        checksums[""README.md""] = self._write_file(""README.md"", readme)

        (self.bundle_dir / ""traces"").mkdir(parents=True, exist_ok=True)

        (self.bundle_dir / ""guardrails"").mkdir(parents=True, exist_ok=True)
        if guardrails_manifest:
            checksums[""guardrails/manifest.json""] = self._write_file(
                ""guardrails/manifest.json"", guardrails_manifest
            )

        for rel, content in (templates or {}).items():
            checksums[str(rel)] = self._write_file(rel, content)

        metadata = BundleMetadata(meta_agent_version=__version__)
        metadata.custom[""checksums""] = checksums
        with open(self.bundle_dir / ""bundle.json"", ""w"", encoding=""utf-8"") as f:
            json.dump(json.loads(metadata.model_dump_json()), f, indent=2)
        return metadata",src/meta_agent/bundle_generator.py,BundleGenerator
survived,"    def __init__(self, bundle_dir: str | Path) -> None:
        self.bundle_dir = Path(bundle_dir)
",src/meta_agent/bundle_generator.py,BundleGenerator
survived,"    def _write_detailed_memory_maps(self, f):
        """"""
        å†™å…¥è¯¦ç»†çš„å†…å­˜æ˜ å°„åˆ†æž
        """"""
        f.write(""3. å†…å­˜æ˜ å°„è¯¦ç»†åˆ†æž\n"")
        f.write(""-"" * 50 + ""\n"")
        
        process = psutil.Process()
        memory_maps = process.memory_maps()
        
        # æŒ‰æƒé™åˆ†ç±»
        perm_stats = {}
        file_stats = {}
        
        for mmap in memory_maps:
            size_mb = mmap.size / 1024 / 1024
            perms = mmap.perms
            
            # æŒ‰æƒé™ç»Ÿè®¡
            if perms not in perm_stats:
                perm_stats[perms] = {'count': 0, 'size': 0}
            perm_stats[perms]['count'] += 1
            perm_stats[perms]['size'] += size_mb
            
            # æŒ‰æ–‡ä»¶ç»Ÿè®¡
            if mmap.path:
                if mmap.path not in file_stats:
                    file_stats[mmap.path] = {'count': 0, 'size': 0}
                file_stats[mmap.path]['count'] += 1
                file_stats[mmap.path]['size'] += size_mb
        
        f.write(""æŒ‰æƒé™åˆ†ç±»çš„å†…å­˜æ˜ å°„:\n"")
        f.write(f""{'æƒé™':<10} {'æ•°é‡':<8} {'å¤§å°(MB)':<12}\n"")
        f.write(""-"" * 35 + ""\n"")
        for perms, stats in sorted(perm_stats.items(), key=lambda x: x[1]['size'], reverse=True):
            f.write(f""{perms:<10} {stats['count']:<8} {stats['size']:<12.2f}\n"")
        
        f.write(f""\næŒ‰æ–‡ä»¶åˆ†ç±»çš„å†…å­˜æ˜ å°„ (å‰10ä¸ª):\n"")
        f.write(f""{'æ–‡ä»¶è·¯å¾„':<50} {'å¤§å°(MB)':<12}\n"")
        f.write(""-"" * 70 + ""\n"")
        for path, stats in sorted(file_stats.items(), key=lambda x: x[1]['size'], reverse=True)[:10]:
            if len(path) > 47:
                path = path[:44] + ""...""
            f.write(f""{path:<50} {stats['size']:<12.2f}\n"")
        
        f.write(""\n"" + ""="" * 100 + ""\n\n"")
",app/helper/memory.py,MemoryHelper
deleted,"        def filter_difficulty(example):
            answer_length = len(example[""info""][""best_answer""].split())
            if difficulty == ""easy"":
                return answer_length < 10
            elif difficulty == ""medium"":
                return 10 <= answer_length < 30
            else:  # hard
                return answer_length >= 30
",environments/truthful_qa/truthful_qa.py,
survived,"    def parse_judge_scores(prompt, completion, answer, state, **kwargs) -> float:
        # Call the judge to get evaluation
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)
        try:
            import json
            # Extract JSON from the response
            response_text = judge_response.strip()
            # Try to find JSON object in the response
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            if start_idx >= 0 and end_idx > start_idx:
                json_text = response_text[start_idx:end_idx]
                scores = json.loads(json_text)
                return float(scores.get(""overall_score"", 0.0))
        except Exception as e:
            # If parsing fails, return 0
            pass
        return 0.0
",environments/toxicity_explanation/toxicity_explanation.py,
survived,"def run_claude_code_task(task_id):
    """"""Run Claude Code automation in a container""""""
    try:
        task = tasks[task_id]
        task['status'] = TaskStatus.RUNNING
        
        logger.info(f""Starting Claude Code task {task_id}"")
        
        # Escape special characters in prompt for shell safety
        escaped_prompt = task['prompt'].replace('""', '\\""').replace('$', '\\$').replace('`', '\\`')
        
        # Create container environment variables
        env_vars = {
            'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
        }
        
        # Create the command to run in container
        container_command = f'''
set -e
echo ""Setting up repository...""

# Clone repository
git clone -b {task['branch']} {task['repo_url']} /workspace/repo
cd /workspace/repo

# Configure git
git config user.email ""claude-code@automation.com""
git config user.name ""Claude Code Automation""

echo ""Starting Claude Code with prompt...""

# Run Claude Code with the prompt
echo ""{escaped_prompt}"" | claude

# Check if there are changes
if git diff --quiet; then
    echo ""No changes made""
    exit 1
fi

# Commit changes
git add .
git commit -m ""Claude Code: {escaped_prompt[:100]}""

# Get commit hash and diff
echo ""COMMIT_HASH=$(git rev-parse HEAD)""
echo ""=== GIT DIFF START ===""
git diff HEAD~1 HEAD
echo ""=== GIT DIFF END ===""
'''
        
        # Run container with Claude Code
        container = docker_client.containers.run(
            'claude-code-automation:latest',
            command=['bash', '-c', container_command],
            environment=env_vars,
            detach=True,
            remove=True,
            working_dir='/workspace',
            network_mode='bridge'  # Ensure proper networking
        )
        
        task['container_id'] = container.id
        
        # Wait for container to finish with timeout
        try:
            result = container.wait(timeout=300)  # 5 minute timeout
            logs = container.logs().decode('utf-8')
        except Exception as e:
            logger.error(f""Container timeout or error: {str(e)}"")
            task['status'] = TaskStatus.FAILED
            task['error'] = f""Container execution timeout or error: {str(e)}""
            return
        
        if result['StatusCode'] == 0:
            # Parse output to extract commit hash and diff
            lines = logs.split('\n')
            commit_hash = None
            git_diff = []
            capturing_diff = False
            
            for line in lines:
                if line.startswith('COMMIT_HASH='):
                    commit_hash = line.split('=', 1)[1]
                elif line == '=== GIT DIFF START ===':
                    capturing_diff = True
                elif line == '=== GIT DIFF END ===':
                    capturing_diff = False
                elif capturing_diff:
                    git_diff.append(line)
            
            task['status'] = TaskStatus.COMPLETED
            task['commit_hash'] = commit_hash
            task['git_diff'] = '\n'.join(git_diff)
            
            logger.info(f""Task {task_id} completed successfully"")
            
        else:
            task['status'] = TaskStatus.FAILED
            task['error'] = f""Container exited with code {result['StatusCode']}: {logs}""
            logger.error(f""Task {task_id} failed: {task['error']}"")
            
    except Exception as e:
        task['status'] = TaskStatus.FAILED
        task['error'] = str(e)
        logger.error(f""Task {task_id} failed with exception: {str(e)}"")
",server/main.py,
survived,"    async def test_latest_run_includes_metadata(
        self,
        test_api_client: AsyncClient,
        returned_run: AgentRun,
    ):
        """"""Test that metadata is included in the RunV1 response""""""
        returned_run.metadata = {""environment"": ""test"", ""user_id"": ""123"", ""custom_field"": ""value""}

        response = await test_api_client.get(""/v1/_/agents/bla/runs/latest"")
        assert response.status_code == 200

        response_data = response.json()
        assert response_data[""id""] == returned_run.id
        assert response_data[""metadata""] == {
            ""environment"": ""test"",
            ""user_id"": ""123"",
            ""custom_field"": ""value"",
        }
",api/api/routers/runs_v1_test.py,TestLatestRun
survived,"def create_pkgx_package(
    distributables: list[str] | None = None,
    dependencies: list[str] | None = None,
    build_deps: list[str] | None = None,
    test_deps: list[str] | None = None,
) -> PkgxPackage:
    """"""Helper to create PkgxPackage instances for testing""""""

    # Create distributable blocks
    distributable_blocks = []
    if distributables:
        for url in distributables:
            distributable_blocks.append(Distributable(url=url))

    # Create dependency objects
    dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (dependencies or [])
            ],
        )
    ]
    build_dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (build_deps or [])
            ],
        )
    ]
    test_dep_objects = [
        DependencyBlock(
            platform=""all"",
            dependencies=[
                Dependency(name=dep, semver=""*"") for dep in (test_deps or [])
            ],
        )
    ]

    # Create version object
    version = Version()

    return PkgxPackage(
        distributable=distributable_blocks,
        versions=version,
        dependencies=dep_objects,
        build=DependencyBlock(platform=""linux"", dependencies=build_dep_objects),
        test=DependencyBlock(platform=""linux"", dependencies=test_dep_objects),
    )
",tests/package_managers/pkgx/test_pkgx_diff.py,
survived,"def canonicalize(url: str) -> str:
    return normalize_url(url)
",package_managers/pkgx/url.py,
survived,"    def test_sort_by_quality_index_asc(self):
        """"""Test sorting by quality index (lowest first).""""""
        models: list[ConciseModelResponse | ConciseLatestModelResponse] = [
            create_test_model(""model1"", quality_index=50),
            create_test_model(""model2"", quality_index=100),
            create_test_model(""model3"", quality_index=75),
        ]

        sorted_models = sort_models(models, ""quality_index"", ""asc"")

        assert [m.id for m in sorted_models] == [""model1"", ""model3"", ""model2""]
",api/api/routers/mcp/_utils/model_sorting_test.py,TestSortModels
survived,"    async def rollback(self):
        # Drop the new index
        await self._drop_index_if_exists(self._organization_collection, ""unique_api_key_id"")

        # Recreate the old index with the problematic partial filter expression
        await self._organization_collection.create_index(
            [(""api_keys.id"", 1)],
            name=""org_settings_api_key_id_index"",
            unique=True,
            background=True,
            partialFilterExpression={""api_keys"": {""$exists"": True}},
        )",api/core/storage/mongo/migrations/migrations/m2025_05_06_fix_api_key_id_index.py,FixAPIKeyIdIndexMigration
survived,"    def test_speed_index(self, speed_data: SpeedData, expected_index: int):
        mapping = {
            Model.O3_2025_04_16_MEDIUM_REASONING_EFFORT: _md(speed_data=SpeedData(index=600)),
        }
        assert speed_data.speed_index(mapping) == expected_index
",api/core/domain/models/model_data_test.py,TestModelDataSpeedIndex
survived,"    def custom_llm_provider(self) -> Optional[str]:
        return ""perplexity""
",litellm/llms/perplexity/chat/transformation.py,PerplexityChatConfig
survived,"    def test_perplexity_reasoning_effort_parameter_mapping(self, model, reasoning_effort):
        """"""
        Test that reasoning_effort parameter is correctly mapped for Perplexity Sonar reasoning models
        """"""
        # Set up local model cost map
        os.environ[""LITELLM_LOCAL_MODEL_COST_MAP""] = ""True""
        litellm.model_cost = litellm.get_model_cost_map(url="""")

        # Get provider and optional params
        _, provider, _, _ = litellm.get_llm_provider(model=model)
        
        optional_params = get_optional_params(
            model=model,
            custom_llm_provider=provider,
            reasoning_effort=reasoning_effort,
        )
        
        # Verify that reasoning_effort is preserved in optional_params for Perplexity
        assert ""reasoning_effort"" in optional_params
        assert optional_params[""reasoning_effort""] == reasoning_effort
",tests/llm_translation/test_perplexity_reasoning.py,TestPerplexityReasoning
survived,"    def test_perplexity_non_reasoning_models_dont_support_reasoning(self):
        """"""
        Test that non-reasoning Perplexity models don't support reasoning
        """"""
        from litellm.utils import supports_reasoning
        
        # Set up local model cost map
        os.environ[""LITELLM_LOCAL_MODEL_COST_MAP""] = ""True""
        litellm.model_cost = litellm.get_model_cost_map(url="""")
        
        non_reasoning_models = [
            ""perplexity/sonar"",
            ""perplexity/sonar-pro"",
            ""perplexity/llama-3.1-sonar-large-128k-chat"",
            ""perplexity/mistral-7b-instruct"",
        ]
        
        for model in non_reasoning_models:
            # These models should not support reasoning (should return False or raise exception)
            try:
                result = supports_reasoning(model, None)
                # If it doesn't raise an exception, it should return False
                assert result is False, f""{model} should not support reasoning""
            except Exception:
                # If it raises an exception, that's also acceptable behavior
                pass
",tests/llm_translation/test_perplexity_reasoning.py,TestPerplexityReasoning
survived,"def test_semantic_unnest_list():
    """"""Test semantic unnest operation with list values.""""""
    df = pd.DataFrame({
        ""id"": [1, 2],
        ""tags"": [[""python"", ""pandas"", ""data""], [""ml"", ""ai""]]
    })
    
    result = df.semantic.unnest(unnest_key=""tags"")
    
    assert isinstance(result, pd.DataFrame)
    assert len(result) == 5  # 3 + 2 tags
    assert all(result.columns == [""id"", ""tags""])
    
    # Check that each tag becomes a separate row
    expected_tags = [""python"", ""pandas"", ""data"", ""ml"", ""ai""]
    actual_tags = result[""tags""].tolist()
    assert set(actual_tags) == set(expected_tags)
    
    # Check that original data is preserved
    python_rows = result[result[""tags""] == ""python""]
    assert len(python_rows) == 1
    assert python_rows.iloc[0][""id""] == 1
",tests/test_pandas_accessors.py,
survived,"    def test_cost_report_with_corrupted_resources_data(self):
        """"""Test cost report handles corrupted/unpicklable resources data.""""""
        mock_cluster_record = {
            'name': 'corrupted-cluster',
            'status': None,
            'num_nodes': 1,
            'resources': mock.Mock(),
            'total_cost': 0.0,
            'launched_at': 1640995200,
            'duration': 1800,
            'cluster_hash': 'def456',
            'usage_intervals': [(1640995200, 1640997000)],
            'user_hash': 'user456',
            'user_name': 'testuser2',
            'workspace': 'default',
        }
        
        # Mock resources with missing/invalid attributes
        mock_cluster_record['resources'].instance_type = None
        mock_cluster_record['resources'].cloud = None
        
        with mock.patch('sky.global_user_state.get_clusters_from_history', 
                      return_value=[mock_cluster_record]):
            
            # Should handle gracefully and not crash
            result = core.cost_report(days=30)
            self.assertIsInstance(result, list)
",tests/unit_tests/test_sky_cost_report.py,TestHistoricalClusterRobustness
survived,"def test_severity_greater_than_info_bug_fix(
    db_session, workflow_manager, create_workflow, create_alert
):
    """"""
    Test the specific bug case from GitHub issue #5086:
    severity > 'info' should match 'warning', 'high', and 'critical' severities
    
    Before fix: This would fail because 'high' < 'info' lexicographically (h < i)
    After fix: This works because high (4) > info (2) numerically
    """"""
    # Create a workflow with the exact CEL expression from the bug report
    workflow = create_workflow(
        ""test-severity-gt-info-bug"", 
        ""severity > 'info' && source.contains('prometheus')""
    )

    # These alerts should match (severity > info)
    high_alert = create_alert(
        severity=AlertSeverity.HIGH, 
        fingerprint=""fp-high""
    )
    critical_alert = create_alert(
        severity=AlertSeverity.CRITICAL, 
        fingerprint=""fp-critical""
    )
    warning_alert = create_alert(
        severity=AlertSeverity.WARNING, 
        fingerprint=""fp-warning""
    )

    # These alerts should NOT match
    info_alert = create_alert(
        severity=AlertSeverity.INFO, 
        fingerprint=""fp-info""
    )
    low_alert = create_alert(
        severity=AlertSeverity.LOW, 
        fingerprint=""fp-low""
    )

    # Test high severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [high_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test critical severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [critical_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test warning severity alert (should match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [warning_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before + 1
    assert workflow_manager.scheduler.workflows_to_run[-1][""workflow_id""] == workflow.id

    # Test info severity alert (should NOT match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [info_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before

    # Test low severity alert (should NOT match)
    workflows_to_run_before = len(workflow_manager.scheduler.workflows_to_run)
    workflow_manager.insert_events(SINGLE_TENANT_UUID, [low_alert])
    assert len(workflow_manager.scheduler.workflows_to_run) == workflows_to_run_before
",tests/test_workflow_severity_comparisons.py,
survived,"def multiline_binary():
    """"""Fixture for binary fields, specifically multi-lines ones""""""
    return """"""
Package: binutils
Binary: binutils-for-host, binutils-for-build,
 binutils-ia64-linux-gnu-dbg, binutils-m68k-linux-gnu,
 binutils-mips64el-linux-gnuabin32-dbg, binutils-mipsisa64r6-linux-gnuabin32,
 binutils-mipsisa64r6el-linux-gnuabi64-dbg

""""""
",tests/package_managers/debian/test_debian_parser.py,
survived,"    def test_build_package_to_source_mapping_no_binary_list(
        self, tmp_path, mock_logger
    ):
        """"""Test building mapping when source has no explicit binary list""""""

        # Create a test sources file with no Binary field
        sources_content = """"""Package: single-source
Vcs-Git: https://github.com/test/single-source.git
Homepage: https://example.com/single-source
""""""

        sources_file = tmp_path / ""sources""
        sources_file.write_text(sources_content)

        # Build mapping
        mapping = build_package_to_source_mapping(str(sources_file), mock_logger)

        # Verify mapping - should use source package name as binary name
        assert len(mapping) == 1
        assert ""single-source"" in mapping
        assert mapping[""single-source""].package == ""single-source""
        # URLs are normalized by the parser - expect normalized format
        assert mapping[""single-source""].vcs_git == ""github.com/test/single-source""
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"    def _generate_chai_urls(self, debian_data: DebianData) -> list[URLKey]:
        """"""Generate URLs for a debian package""""""
        urls = []

        # Homepage URL
        if debian_data.homepage:
            urls.append(URLKey(debian_data.homepage, self.config.url_types.homepage))

        # Source URL
        source_url = (
            debian_data.vcs_git if debian_data.vcs_git else debian_data.vcs_browser
        )
        if source_url:
            urls.append(URLKey(source_url, self.config.url_types.source))

        # Repository URL
        if is_github_url(source_url):
            urls.append(URLKey(source_url, self.config.url_types.repository))

        return urls",package_managers/debian/diff.py,DebianDiff
survived,"    def ingest_wrapper(self, diff_result: DiffResult) -> None:
        """"""Wrapper for the main ingest function to handle DiffResult""""""
        final_new_urls = list(diff_result.new_urls.values())
        self.ingest(
            diff_result.new_packages,
            final_new_urls,
            diff_result.new_package_urls,
            diff_result.new_deps,
            diff_result.removed_deps,
            diff_result.updated_packages,
            diff_result.updated_package_urls,
        )",package_managers/debian/db.py,DebianDB
survived,"    def test_enrich_package_with_explicit_source(self, mock_logger):
        """"""Test enriching package that has explicit source reference""""""

        # Create package data with explicit source reference
        package_data = create_debian_package(
            package=""binary-pkg"",
            description=""A binary package"",
        )
        package_data.source = ""source-pkg""

        # Create source mapping
        source_data = create_debian_package(
            package=""source-pkg"",
            vcs_git=""github.com/test/source-pkg"",  # Already normalized format
            homepage=""example.com/source-pkg"",  # Already normalized format
            build_depends=[""build-dep1"", ""build-dep2""],
        )
        source_mapping = {""binary-pkg"": source_data}

        # Enrich package
        enriched = enrich_package_with_source(package_data, source_mapping, mock_logger)

        # Verify enrichment
        assert enriched.package == ""binary-pkg""
        assert enriched.description == ""A binary package""
        assert enriched.vcs_git == ""github.com/test/source-pkg""
        assert enriched.homepage == ""example.com/source-pkg""
        assert len(enriched.build_depends) == 2

        build_depend_names = [item.package for item in enriched.build_depends]
        assert build_depend_names == [""build-dep1"", ""build-dep2""]
",tests/package_managers/debian/test_debian_sources.py,TestPackageSourceMapping
survived,"    def test_dependency_type_priority_new_package(
        self, mock_config, mock_logger, mock_db
    ):
        """"""
        Scenario:
          - p1 has no dependencies to p2 in cache
          - p1 has both runtime and build dependencies to p2 in parsed data

        Expect one new runtime dependency (priority over build).
        """"""

        p1_id = uuid4()
        p2_id = uuid4()

        p1_pkg = Package(id=p1_id, derived_id=""debian/p1"", name=""p1"", import_id=""p1"")
        p2_pkg = Package(id=p2_id, derived_id=""debian/p2"", name=""p2"", import_id=""p2"")

        cache = Cache(
            package_map={""debian/p1"": p1_pkg, ""debian/p2"": p2_pkg},
            url_map={},
            package_urls={},
            dependencies={},  # No existing dependencies
        )

        # Parsed data has both runtime and build dependencies to p2
        new_pkg_data = create_debian_package(
            package=""p1"",
            depends=[""p2""],  # runtime
            build_depends=[""p2""],  # build
        )

        diff = DebianDiff(mock_config, cache, mock_db, mock_logger)
        new_deps, removed_deps = diff.diff_deps(""debian/p1"", new_pkg_data)

        # Should only create one new dependency - runtime (higher priority)
        assert len(removed_deps) == 0
        assert len(new_deps) == 1
        assert new_deps[0].dependency_id == p2_id
        assert new_deps[0].dependency_type_id == mock_config.dependency_types.runtime
",tests/package_managers/debian/test_debian_diff.py,TestDebianDifferentialLoading
survived,"def save_tasks():
    """"""Save tasks to file for persistence""""""
    try:
        with open(TASKS_FILE, 'w') as f:
            json.dump(tasks, f, indent=2, default=str)
        logger.info(f""ðŸ’¾ Saved {len(tasks)} tasks to {TASKS_FILE}"")
    except Exception as e:
        logger.warning(f""âš ï¸ Failed to save tasks: {e}"")
",server/utils.py,
survived,"    def create_project(user_id: str, name: str, description: str, repo_url: str, 
                      repo_name: str, repo_owner: str, settings: Dict = None) -> Dict:
        """"""Create a new project""""""
        try:
            project_data = {
                'user_id': user_id,
                'name': name,
                'description': description,
                'repo_url': repo_url,
                'repo_name': repo_name,
                'repo_owner': repo_owner,
                'settings': settings or {},
                'is_active': True
            }
            
            result = supabase.table('projects').insert(project_data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error creating project: {e}"")
            raise
",server/database.py,DatabaseOperations
survived,"    def get_task_by_id(task_id: int, user_id: str) -> Optional[Dict]:
        """"""Get a specific task by ID for a user""""""
        try:
            result = supabase.table('tasks').select('*').eq('id', task_id).eq('user_id', user_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f""Error fetching task {task_id}: {e}"")
            raise
",server/database.py,DatabaseOperations
survived,"    def sort_by_duration(self) -> None:
        """"""Sort tasks by estimated duration (longest first) for optimal concurrent execution.""""""
        task_durations = []
        for task_path in self._tasks:
            try:
                task_paths_obj = TaskPaths(task_path)
                task = Task.from_yaml(task_paths_obj.task_config_path)
                duration = task.effective_estimated_duration_sec
            except Exception as e:
                self._logger.warning(
                    f""Failed to load task {task_path.name}: {e}. Using fallback duration.""
                )
                duration = 210.0
            task_durations.append((task_path, duration))

        task_durations.sort(key=lambda x: x[1], reverse=True)
        self._tasks = [task_path for task_path, _ in task_durations]

        # Log the task execution order
        table_data = []
        for i, (task_path, duration) in enumerate(task_durations, 1):
            task_name = task_path.name
            minutes = int(duration // 60)
            seconds = int(duration % 60)
            duration_str = f""{minutes}m {seconds}s""

            try:
                task_paths_obj = TaskPaths(task_path)
                task = Task.from_yaml(task_paths_obj.task_config_path)
                if task.estimated_duration_sec is not None:
                    source = ""historical""
                else:
                    source = ""calculated""
            except Exception:
                source = ""fallback""

            table_data.append([i, task_name, duration_str, source])

        headers = [""#"", ""Task Name"", ""Duration"", ""Source""]
        table = tabulate(table_data, headers=headers, tablefmt=""grid"")

        self._logger.info(""Processing tasks in duration order (longest first):"")
        self._logger.info(f""\n{table}"")
        self._logger.info(f""Total tasks: {len(task_durations)}"")",terminal_bench/dataset/dataset.py,Dataset
survived,"    def _set_dummy_api_key(self, monkeypatch, request):
        # MCP mode doesn't require API key, so we don't set it
        # This allows us to test MCP functionality without API key requirements
        pass
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"        def mock_resource_decorator(uri):
            def decorator(func):
                registered_resources.append((uri, func))
                return func
            return decorator
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
deleted,"    def test_mcp_server_tool_execution_success(self, mock_fastmcp, integration_graphs_and_metas):
        """"""Test successful tool execution through MCP server.""""""
        graphs, metas = integration_graphs_and_metas
        mock_mcp_instance = MagicMock()
        
        # Track registered tools
        registered_tools = []
        
        def mock_tool_decorator(func):
            registered_tools.append(func)
            return func
        
        mock_mcp_instance.tool.side_effect = lambda: mock_tool_decorator
        mock_fastmcp.return_value = mock_mcp_instance

        # Create the server
        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Integration Test Server""
        )

        # Verify tools were registered
        assert len(registered_tools) == len(graphs)

        # Test tool execution (simulate calling one of the registered tools)
        if registered_tools:
            tool_func = registered_tools[0]
            flow_input = FlowInput(input_value=""test input"")
            
            # Mock the graph execution context
            with patch(""time.time"", side_effect=[0, 1.5]):  # Mock execution time
                result = tool_func(flow_input)
            
            assert isinstance(result, FlowOutput)
            assert ""Processed: test input"" in str(result.result)
            assert result.execution_time == 1.5
            assert result.error is None
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerIntegration
survived,"    def test_mcp_valid_transports(self, runner, temp_python_script):
        """"""Test that valid MCP transports are accepted.""""""
        valid_transports = [""stdio"", ""sse"", ""websocket""]
        
        for transport in valid_transports:
            # We just test that the validation passes and the command would start
            # We'll use a timeout or patch to avoid actually starting the server
            with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
                mock_run_mcp.side_effect = KeyboardInterrupt(""Test interrupt"")
                
                result = runner.invoke(app, [
                    ""serve"", str(temp_python_script),
                    ""--mcp"", ""--mcp-transport"", transport,
                    ""--verbose""
                ])
                
                # Should either exit cleanly (0) or with KeyboardInterrupt handling
                assert result.exit_code in [0, 1]
                # Should show MCP mode is enabled
                assert f""MCP mode enabled with {transport} transport"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
survived,"    def test_mcp_server_keyboard_interrupt(self, runner, temp_python_script):
        """"""Test graceful handling of keyboard interrupt in MCP server.""""""
        with patch(""langflow.cli.commands.run_mcp_server"") as mock_run_mcp:
            mock_run_mcp.side_effect = KeyboardInterrupt(""User interrupt"")
            
            result = runner.invoke(app, [
                ""serve"", str(temp_python_script),
                ""--mcp"", ""--verbose""
            ])
            
            assert result.exit_code == 0
            assert ""MCP server stopped"" in result.output
",src/backend/tests/unit/test_cli.py,TestMCPServeCommand
deleted,"            def flow_tool(input_data: FlowInput) -> FlowOutput:
                f""""""Execute the {flow_name} flow.
                
                {flow_desc}
                """"""
                try:
                    # Import here to avoid circular imports
                    import time
                    
                    start_time = time.time()
                    
                    # Execute the flow
                    # Note: This follows the same pattern as the REST API execution
                    result = graph_obj.run(
                        inputs={""input_value"": input_data.input_value},
                        tweaks=input_data.tweaks or {}
                    )
                    
                    execution_time = time.time() - start_time
                    
                    return FlowOutput(
                        result=result,
                        execution_time=execution_time
                    )
                    
                except Exception as e:
                    return FlowOutput(
                        result=None,
                        error=str(e)
                    )
",src/backend/base/langflow/cli/mcp_server.py,
survived,"    def test_create_mcp_server_basic(self, mock_fastmcp, sample_graphs_and_metas):
        """"""Test basic MCP server creation.""""""
        graphs, metas = sample_graphs_and_metas
        mock_mcp_instance = MagicMock()
        mock_fastmcp.return_value = mock_mcp_instance

        server = create_mcp_server(
            graphs=graphs,
            metas=metas,
            server_name=""Test MCP Server""
        )

        # Verify FastMCP was called with correct name
        mock_fastmcp.assert_called_once_with(""Test MCP Server"")
        assert server == mock_mcp_instance

        # Verify tools were registered (one for each flow)
        assert mock_mcp_instance.tool.call_count == len(graphs)

        # Verify resources were registered
        assert mock_mcp_instance.resource.call_count >= 3  # At least 3 resources

        # Verify prompts were registered
        assert mock_mcp_instance.prompt.call_count >= 2  # At least 2 prompts
",src/backend/tests/unit/test_mcp_server.py,TestMCPServerCreation
survived,"    def _transform_messages(
        self, messages: List[AllMessageValues], model: str, is_async: bool = False
    ) -> Union[List[AllMessageValues], Coroutine[Any, Any, List[AllMessageValues]]]:
        """"""
        Moonshot AI does not support content in list format.
        """"""
        messages = handle_messages_with_content_list_to_str_conversion(messages)
        if is_async:
            return super()._transform_messages(
                messages=messages, model=model, is_async=True
            )
        else:
            return super()._transform_messages(
                messages=messages, model=model, is_async=False
            )
",litellm/llms/moonshot/chat/transformation.py,MoonshotChatConfig
survived,"def parse_args() -> tuple[RunConfig, TauBenchTrainingConfig, argparse.Namespace]:
    """"""Parse command line arguments for RL training""""""
    parser = argparse.ArgumentParser(description=""Train an agent on tau-bench using ART RL"")
    
    # tau-bench arguments (reuse from original run.py)
    parser.add_argument(""--num-trials"", type=int, default=1)
    parser.add_argument(
        ""--env"", type=str, choices=[""retail"", ""airline""], default=""retail""
    )
    parser.add_argument(
        ""--model"",
        type=str,
        help=""The model to use for the agent"",
        required=True,
    )
    parser.add_argument(
        ""--model-provider"",
        type=str,
        choices=provider_list,
        help=""The model provider for the agent"",
        required=True,
    )
    parser.add_argument(
        ""--user-model"",
        type=str,
        default=""gpt-4o"",
        help=""The model to use for the user simulator"",
    )
    parser.add_argument(
        ""--user-model-provider"",
        type=str,
        choices=provider_list,
        help=""The model provider for the user simulator"",
    )
    parser.add_argument(
        ""--agent-strategy"",
        type=str,
        default=""tool-calling"",
        choices=[""tool-calling"", ""act"", ""react"", ""few-shot""],
    )
    parser.add_argument(
        ""--temperature"",
        type=float,
        default=0.0,
        help=""The sampling temperature for the action model"",
    )
    parser.add_argument(
        ""--task-split"",
        type=str,
        default=""train"",  # Default to train for RL
        choices=[""train"", ""test"", ""dev""],
        help=""The split of tasks to run"",
    )
    parser.add_argument(""--start-index"", type=int, default=0)
    parser.add_argument(""--end-index"", type=int, default=100, help=""End index for training tasks"")
    parser.add_argument(""--task-ids"", type=int, nargs=""+"", help=""(Optional) run only the tasks with the given IDs"")
    parser.add_argument(""--log-dir"", type=str, default=""rl_results"")
    parser.add_argument(""--seed"", type=int, default=10)
    parser.add_argument(""--shuffle"", type=int, default=0)
    parser.add_argument(""--user-strategy"", type=str, default=""llm"", choices=[item.value for item in UserStrategy])
    parser.add_argument(""--few-shot-displays-path"", type=str, help=""Path to a jsonlines file containing few shot displays"")
    
    # RL-specific arguments
    parser.add_argument(""--model-name"", type=str, required=True, help=""Name for the trainable model"")
    parser.add_argument(""--base-model"", type=str, default=""Qwen/Qwen2.5-14B-Instruct"", help=""Base model for training"")
    parser.add_argument(""--trajectories-per-group"", type=int, default=6, help=""Number of trajectories per group"")
    parser.add_argument(""--groups-per-step"", type=int, default=8, help=""Number of groups per training step"")
    parser.add_argument(""--learning-rate"", type=float, default=1.2e-5, help=""Learning rate for training"")
    parser.add_argument(""--eval-steps"", type=int, default=30, help=""Evaluate every N steps"")
    parser.add_argument(""--val-set-size"", type=int, default=100, help=""Validation set size"")
    parser.add_argument(""--training-dataset-size"", type=int, default=1000, help=""Training dataset size"")
    parser.add_argument(""--num-epochs"", type=int, default=1, help=""Number of training epochs"")
    
    args = parser.parse_args()
    print(args)
    
    # Create RunConfig for tau-bench
    run_config = RunConfig(
        model_provider=args.model_provider,
        user_model_provider=args.user_model_provider,
        model=args.model,
        user_model=args.user_model,
        num_trials=args.num_trials,
        env=args.env,
        agent_strategy=args.agent_strategy,
        temperature=args.temperature,
        task_split=args.task_split,
        start_index=args.start_index,
        end_index=args.end_index,
        task_ids=args.task_ids,
        log_dir=args.log_dir,
        max_concurrency=1,  # RL training is sequential
        seed=args.seed,
        shuffle=args.shuffle,
        user_strategy=args.user_strategy,
        few_shot_displays_path=args.few_shot_displays_path,
    )
    
    # Create training config
    training_config = TauBenchTrainingConfig(
        trajectories_per_group=args.trajectories_per_group,
        groups_per_step=args.groups_per_step,
        learning_rate=args.learning_rate,
        eval_steps=args.eval_steps,
        val_set_size=args.val_set_size,
        training_dataset_size=args.training_dataset_size,
        num_epochs=args.num_epochs,
    )
    
    return run_config, training_config, args
",dev/tau-bench/run_rl.py,
survived,"    async def test_a_generate_with_score_rollouts(self, mock_openai_client, sample_dataset):
        """"""Test async generate with scoring enabled.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        # Mock the rubric scoring
        env.rubric.score_rollouts = AsyncMock(return_value={
            ""reward"": [1.0]
        })
        
        inputs = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""answer"": [""Hi""]
        }
        
        results = await env.a_generate(inputs, score_rollouts=True)
        
        assert ""completion"" in results
        assert ""state"" in results
        assert ""reward"" in results
        assert results[""reward""] == [1.0]
",tests/test_environment.py,TestEnvironmentBase
survived,"    def test_make_dataset(self, mock_openai_client, sample_dataset):
        """"""Test creating a dataset from evaluation results.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        results = {
            ""prompt"": [[{""role"": ""user"", ""content"": ""Hello""}]],
            ""completion"": [[{""role"": ""assistant"", ""content"": ""Hi""}]],
            ""answer"": [""Hi""],
            ""reward"": [1.0],
            ""task"": [""default""],
            ""state"": [{""custom_field"": ""value""}]
        }
        
        dataset = env.make_dataset(results, state_columns=[""custom_field""])
        
        assert len(dataset) == 1
        assert ""prompt"" in dataset.column_names
        assert ""completion"" in dataset.column_names
        assert ""answer"" in dataset.column_names
        assert ""reward"" in dataset.column_names
        assert ""task"" in dataset.column_names
        assert ""custom_field"" in dataset.column_names",tests/test_environment.py,TestEnvironmentBase
survived,"    def test_process_chat_format(self, mock_openai_client, sample_dataset):
        """"""Test processing chat format conversations.""""""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model=""test-model"",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric()
        )
        
        # Create a mock tokenizer
        mock_tokenizer = Mock()
        mock_tokenizer.apply_chat_template = Mock(side_effect=lambda messages, tokenize=False, add_generation_prompt=True: 
            ""User: What is 2+2?Assistant:"" if add_generation_prompt else ""User: What is 2+2?Assistant: 4"")
        mock_tokenizer.encode = Mock(side_effect=lambda text: list(range(len(text.split()))))
        
        prompt = [{""role"": ""user"", ""content"": ""What is 2+2?""}]
        completion = [{""role"": ""assistant"", ""content"": ""4""}]
        
        prompt_ids, prompt_mask, completion_ids, completion_mask = env.process_chat_format(
            prompt, completion, mock_tokenizer, mask_env_responses=False
        )
        
        assert isinstance(prompt_ids, list)
        assert isinstance(prompt_mask, list)
        assert isinstance(completion_ids, list) 
        assert isinstance(completion_mask, list)
        assert len(prompt_ids) == len(prompt_mask)
        assert len(completion_ids) == len(completion_mask)
        assert all(m == 0 for m in prompt_mask)  # Prompt mask should be all 0s
        assert all(m == 1 for m in completion_mask)  # Completion mask should be all 1s
",tests/test_environment.py,TestEnvironmentBase
survived,"    def test_get_env_for_task(self, mock_openai_client):
        """"""Test getting environment for a specific task.""""""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q1""], ""answer"": [""a1""]}),
            rubric=Rubric()
        )
        
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=Dataset.from_dict({""question"": [""q2""], ""answer"": [""a2""]}),
            rubric=Rubric()
        )
        
        env_group = EnvGroup(envs=[env1, env2], env_names=[""math"", ""code""])
        
        assert env_group.get_env_for_task(""math"") == env1
        assert env_group.get_env_for_task(""code"") == env2
        # Unknown task returns first environment as fallback
        assert env_group.get_env_for_task(""unknown"") == env1
",tests/test_env_group.py,TestEnvGroup
deleted,"    def _parse_custom_tools(self, tool_calls: List[ChatCompletionMessageToolCall], tools: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """"""Parse responses from custom tools.""""""
        results = []
        for tool_call in tool_calls:
            for tool in tools:
                if tool_call.function.name == tool[""function""][""name""]:
                    try:
                        function_args = (
                            json.loads(tool_call.function.arguments)
                            if isinstance(tool_call.function.arguments, str)
                            else tool_call.function.arguments
                        )
                    except json.JSONDecodeError:
                        return [{}]
                    
                    # Execute the function defined in the tool's code
                    local_scope = {}
                    exec(tool[""code""].strip(), globals(), local_scope)
                    function_result = local_scope[tool[""function""][""name""]](**function_args)
                    function_args.update(function_result)
                    results.append(function_args)
        return results
",docetl/operations/utils/api.py,ResponseParser
deleted,"    def build_tool_schema(output_schema: Dict[str, Any], scratchpad: Optional[str] = None, model: str = """") -> Dict[str, Any]:
        """"""Build a tool schema from an output schema.""""""
        props = {key: convert_val(value) for key, value in output_schema.items()}
        
        if scratchpad is not None:
            props[""updated_scratchpad""] = {""type"": ""string""}

        parameters = {""type"": ""object"", ""properties"": props}
        parameters[""required""] = list(props.keys())

        # Some models don't support additionalProperties
        if ""gemini"" not in model and ""claude"" not in model:
            parameters[""additionalProperties""] = False

        return parameters
",docetl/operations/utils/api.py,OutputSchemaBuilder
deleted,"    def handle_validation(
        self,
        response: Any,
        output_schema: Dict[str, Any],
        output_mode: OutputMode,
        validation_config: Optional[Dict[str, Any]],
        gleaning_config: Optional[Dict[str, Any]],
        model: str,
        op_type: str,
        messages: List[Dict[str, str]],
        tools: Optional[str] = None,
        scratchpad: Optional[str] = None,
        litellm_completion_kwargs: Dict[str, Any] = {},
        op_config: Dict[str, Any] = {},
        verbose: bool = False,
    ) -> tuple[Any, float, bool]:
        """"""Handle validation and gleaning processes.""""""
        total_cost = completion_cost(response)
        
        if gleaning_config:
            response, additional_cost, validated = self._handle_gleaning(
                response, output_schema, output_mode, gleaning_config, model, op_type, 
                messages, tools, scratchpad, litellm_completion_kwargs, op_config, verbose
            )
            total_cost += additional_cost
        elif validation_config:
            response, additional_cost, validated = self._handle_validation_retries(
                response, output_schema, output_mode, validation_config, model, op_type,
                messages, tools, scratchpad, litellm_completion_kwargs, op_config
            )
            total_cost += additional_cost
        else:
            validated = True
            
        return response, total_cost, validated
",docetl/operations/utils/api.py,ValidationHandler
deleted,"    def _make_tool_call(
        self, 
        model: str, 
        messages: List[Dict[str, str]], 
        output_schema: Dict[str, Any], 
        tools: Optional[str],
        scratchpad: Optional[str],
        extra_kwargs: Dict[str, Any]
    ) -> Any:
        """"""Make a tool-based call.""""""
        # Determine if we should use tools
        props = {key: convert_val(value) for key, value in output_schema.items()}
        use_tools = not (
            len(props) == 1
            and list(props.values())[0].get(""type"") == ""string""
            and scratchpad is None
            and (""sagemaker"" in model or is_deepseek_r1(model))
        )

        if tools is None and use_tools:
            tools_config, tool_choice = self._build_send_output_tool(output_schema, scratchpad, model)
        elif tools is not None:
            tools_config, tool_choice = self._build_custom_tools(tools)
        else:
            tools_config, tool_choice = None, None

        try:
            if tools_config is not None:
                return completion(
                    model=model,
                    messages=messages,
                    tools=tools_config,
                    tool_choice=tool_choice,
                    **extra_kwargs,
                )
            else:
                return completion(
                    model=model,
                    messages=messages,
                    **extra_kwargs,
                )
        except Exception as e:
            self._handle_model_error(model, e)
",docetl/operations/utils/api.py,LLMCallHandler
survived,"    def __init__(self):
        self.chat_completions = {}  # Maps conversation history to responses
        self.text_completions = {}  # Maps prompts to responses
        self.default_chat_response = ""This is a test response""
        self.default_text_response = ""This is a test completion""
        self.base_url = ""http://localhost/v1/""  # For testing URL parsing
        
        # Create mock structure
        self.chat = MagicMock()
        self.completions = MagicMock()
        self.chat.completions = MagicMock()
        
        # Set up async methods
        self.chat.completions.create = AsyncMock(side_effect=self._handle_chat_completion)
        self.completions.create = MagicMock(side_effect=self._handle_text_completion)
",tests/conftest.py,MockAsyncOpenAI
survived,"    async def test_score_rollouts_with_default_infos(self):
        """"""Test scoring rollouts with default empty infos.""""""
        def simple_func(completion, **kwargs):
            return 1.0
        
        rubric = Rubric(funcs=[simple_func], weights=[1.0])
        
        results = await rubric.score_rollouts(
            prompts=[""test""],
            completions=[""test""],
            answers=[""test""],
            states=[{}],
            tasks=[""test""],
            infos=[{}]  # Explicitly provide infos to match other lists
        )
        
        assert ""simple_func"" in results
        assert results[""simple_func""] == [1.0]
",tests/test_rubric.py,TestRubric
survived,"        def list_func(completion, **kwargs):
            return len(completion) if isinstance(completion, list) else 0.0
",tests/test_rubric.py,TestRubric
survived,"        def kwargs_func(completion, **kwargs):
            return len(kwargs)
",tests/test_rubric.py,TestRubric
survived,"        def func2(completion, **kwargs):
            return 0.5
",tests/test_rubric_group.py,TestRubricGroup
survived,"    def test_get_assistant_messages(self, basic_parser):
        """"""Test extraction of assistant messages from completion.""""""
        completion = [
            {""role"": ""user"", ""content"": ""Hello""},
            {""role"": ""assistant"", ""content"": ""Hi there""},
            {""role"": ""user"", ""content"": ""How are you?""},
            {""role"": ""assistant"", ""content"": ""I'm doing well""}
        ]
        assistant_messages = basic_parser.get_assistant_messages(completion)
        assert len(assistant_messages) == 2
        assert assistant_messages[0][""content""] == ""Hi there""
        assert assistant_messages[1][""content""] == ""I'm doing well""
",tests/test_parser.py,TestParser
survived,"    def test_format_reward_function_good_format(self, think_parser):
        """"""Test format reward function with well-formatted content.""""""
        reward_func = think_parser.get_format_reward_func()
        
        completion = [
            {""role"": ""assistant"", ""content"": ""<think>Let me think</think>Final answer""}
        ]
        reward = reward_func(completion)
        assert reward == 1.0
",tests/test_think_parser.py,TestThinkParser
survived,"    def test_singleturn_env_initialization_completion(self, mock_openai_client):
        """"""Test SingleTurnEnv initialization with completion format.""""""
        completion_dataset = Dataset.from_dict({
            ""prompt"": [""Calculate 2+2:"", ""What is the capital?""],
            ""answer"": [""4"", ""It depends on the country""]
        })
        
        env = SingleTurnEnv(
            client=mock_openai_client,
            model=""test-model"",
            dataset=completion_dataset,
            message_type=""completion"",
            parser=Parser(),
            rubric=Rubric()
        )
        assert env.message_type == ""completion""
",tests/test_singleturn_env.py,TestSingleTurnEnv
survived,"    def test_rubric_group_score_rollouts_single_rubric(self):
        """"""Test scoring rollouts with a single rubric (edge case).""""""
        # Note: This test is skipped because RubricGroup.score_rollouts() has a bug
        pass
",tests/test_rubric_group.py,TestRubricGroup
survived,"    async def test_call_reward_func_with_all_args(self):
        """"""Test calling reward function with all possible arguments.""""""
        def comprehensive_func(prompt, completion, answer, state, task, info, **kwargs):
            return len(completion) + len(answer) + len(task)
        
        rubric = Rubric(funcs=[], weights=[])
        
        result = await rubric.call_reward_func(
            func=comprehensive_func,
            prompt=""test prompt"",
            completion=""test completion"",
            answer=""test answer"",
            state={""key"": ""value""},
            task=""test task"",
            info={""info_key"": ""info_value""}
        )
        
        # len(""test completion"") + len(""test answer"") + len(""test task"")
        expected = len(""test completion"") + len(""test answer"") + len(""test task"")
        assert result == expected
",tests/test_rubric.py,TestRubric
survived,"    def test_think_parser_with_custom_extractor(self, think_parser_with_extractor):
        """"""Test ThinkParser with custom extraction function.""""""
        assert isinstance(think_parser_with_extractor, ThinkParser)
",tests/test_think_parser.py,TestThinkParser
survived,"    def test_multiturn_env_initialization(self, mock_multiturn_env):
        """"""Test MultiTurnEnv initialization.""""""
        assert mock_multiturn_env.max_turns == 3
        assert mock_multiturn_env.message_type == 'chat'  # Default from parent
",tests/test_multiturn_env.py,TestMultiTurnEnv
survived,"    def capture_ingest(new_canons, new_canon_packages, updated_canon_packages):
        ingest_calls.append((new_canons, new_canon_packages, updated_canon_packages))
",tests/ranker/test_dedupe.py,
survived,"    def chaos(self) -> None:
        """"""Run chaos-client to discover subdomains from Chaos database.""""""
        if self.htb or self.is_ip_address(self.site):
            return
        
        if not self.pdcp_api_key:
            self.print(""Chaos"", ""PDCP_API_KEY not found in environment, skipping"", Colors.WARNING)
            return
        
        self.print(""Chaos"", ""Querying ProjectDiscovery Chaos database"")
        
        chaos_cmd = (
            f""chaos -d {self.site} ""
            f""-key {self.pdcp_api_key} ""
            f""{'-silent' if not self.verbose else ''} ""
            f""-o {self.dir_path}/chaos_subdomains.txt""
        )
        self.cmd(chaos_cmd)
        
        self.ask_to_add(self.read(""chaos_subdomains.txt""))
",main.py,HaxUnit
survived,"    def test_go_simple(self):
        patch = """"""
@@ -152,10 +152,6 @@ func Hello(name string) string

@@ -152,10 +152,6 @@ func (r *Receiver) MethodName() error

@@ -152,10 +152,6 @@ func (r Receiver) ValueMethod() string

@@ -152,10 +152,6 @@ var myFunc = func() {

@@ -152,10 +152,6 @@ myFunc := func() {

@@ -152,10 +152,6 @@ func Calculate(x, y int) int

@@ -152,10 +152,6 @@ func ProcessData() (string, error)

@@ -152,10 +152,6 @@ func noParams()

""""""

        assert GoParser.extract_functions_from_patch(patch) == {
            ""Hello"",
            ""MethodName"",
            ""ValueMethod"",
            ""myFunc"",
            ""Calculate"",
            ""ProcessData"",
            ""noParams"",
        }
",tests/sentry/integrations/source_code_management/test_language_parsers.py,GoParserTestCase
survived,"    def test_simple_with_go(self):
        data = [
            {""filename"": ""foo.py"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""bar.js"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""baz.py"", ""changes"": 100, ""status"": ""added""},
            {""filename"": ""main.go"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""service.go"", ""changes"": 50, ""status"": ""modified""},
            {""filename"": ""handler.rb"", ""changes"": 100, ""status"": ""modified""},
        ]
        responses.add(
            responses.GET,
            self.gh_path.format(pull_number=self.pr.key),
            status=200,
            json=data,
        )

        pr_files = self.open_pr_comment_workflow.safe_for_comment(repo=self.gh_repo, pr=self.pr)
        assert pr_files == [
            {""filename"": ""foo.py"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""bar.js"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""main.go"", ""changes"": 100, ""status"": ""modified""},
            {""filename"": ""service.go"", ""changes"": 50, ""status"": ""modified""},
            {""filename"": ""handler.rb"", ""changes"": 100, ""status"": ""modified""},
        ]
",tests/sentry/integrations/github/tasks/test_open_pr_comment.py,TestSafeForComment
survived,"    def test_empty_from_name(self, mock_smtp_class, smtp_provider):
        """"""Test sending email with empty from_name.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send email with empty from_name
        smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name="""",
            to_email=""recipient@example.com"",
            subject=""Test Subject"",
            html=""<p>Test</p>"",
        )

        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        
        # Verify the From header contains only email
        email_content = call_args[0][2]
        assert ""From: sender@example.com"" in email_content
        assert ""Test Sender"" not in email_content
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def context_manager(self):
        """"""Create a mock context manager.""""""
        return ContextManager(tenant_id=""test_tenant"", workflow_id=""test_workflow"")
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def test_send_html_email(self, mock_smtp_class, smtp_provider):
        """"""Test sending an HTML email.""""""
        # Setup mock SMTP instance
        mock_smtp = MagicMock()
        mock_smtp_class.return_value = mock_smtp

        # Send HTML email
        result = smtp_provider._notify(
            from_email=""sender@example.com"",
            from_name=""Test Sender"",
            to_email=""recipient@example.com"",
            subject=""Test HTML Subject"",
            html=""<p>This is an <strong>HTML</strong> email</p>"",
        )

        # Verify SMTP was called correctly
        mock_smtp_class.assert_called_once_with(""smtp.example.com"", 587)
        mock_smtp.starttls.assert_called_once()
        mock_smtp.login.assert_called_once_with(""test@example.com"", ""testpassword"")
        
        # Verify email was sent
        mock_smtp.sendmail.assert_called_once()
        call_args = mock_smtp.sendmail.call_args
        assert call_args[0][0] == ""sender@example.com""
        assert call_args[0][1] == ""recipient@example.com""
        
        # Verify the email content contains HTML
        email_content = call_args[0][2]
        assert ""Content-Type: text/html"" in email_content
        assert ""<p>This is an <strong>HTML</strong> email</p>"" in email_content
        
        # Verify return value
        assert result == {
            ""from"": ""sender@example.com"",
            ""to"": ""recipient@example.com"",
            ""subject"": ""Test HTML Subject"",
            ""html"": ""<p>This is an <strong>HTML</strong> email</p>"",
        }
",tests/test_smtp_provider.py,TestSmtpProvider
survived,"    def reset_snapshot(self, storage: str) -> bool:
        """"""
        é‡ç½®å¿«ç…§ï¼Œå¼ºåˆ¶ä¸‹æ¬¡æ‰«ææ—¶é‡æ–°å»ºç«‹åŸºå‡†
        :param storage: å­˜å‚¨åç§°
        :return: æ˜¯å¦æˆåŠŸ
        """"""
        try:
            cache_file = self._snapshot_cache_dir / f""{storage}_snapshot.json""
            if cache_file.exists():
                cache_file.unlink()
                logger.info(f""å¿«ç…§å·²é‡ç½®: {storage}"")
                return True
            logger.debug(f""å¿«ç…§æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€é‡ç½®: {storage}"")
            return True
        except Exception as e:
            logger.error(f""é‡ç½®å¿«ç…§å¤±è´¥: {storage} - {e}"")
            return False
",app/monitor.py,Monitor
survived,"    async def test_delete_api_key_user_with_empty_id_fails(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test that users with empty user_id are still unauthorized.""""""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock user with empty user_id (invalid user)
        mock_user_dep.return_value = Mock(user_id="""")

        # Should not even attempt to delete
        response = await test_api_client.delete(""/_/api/keys/test_key_id"")

        assert response.status_code == 401
        assert response.json() == {""detail"": ""You are not authorized to delete this API key""}
        # Delete should not be called since authorization failed first
        mock_api_keys_service.delete_key.assert_not_called()
",api/api/routers/api_keys_test.py,TestDeleteAPIKey
survived,"    async def test_delete_api_key_with_user_auth_success(
        self,
        test_api_client: AsyncClient,
        mock_user_org_dep: Mock,
        mock_api_keys_service: Mock,
        mock_user_dep: Mock,
    ):
        """"""Test successful deletion with user authentication.""""""
        # Setup non-anonymous organization
        mock_user_org_dep.return_value.org_id = ""org_123""
        mock_user_org_dep.return_value.is_anonymous = False

        # Mock user authentication
        mock_user_dep.return_value = Mock(user_id=""user123"")

        # Mock successful deletion
        mock_api_keys_service.delete_key.return_value = True

        response = await test_api_client.delete(""/_/api/keys/test_key_id"")

        assert response.status_code == 204
        mock_api_keys_service.delete_key.assert_called_once_with(""test_key_id"")",api/api/routers/api_keys_test.py,TestDeleteAPIKey
survived,"def print_results(python_results: List[BenchmarkResult], rust_results: List[BenchmarkResult]):
    """"""Print benchmark results in a table.""""""
    table = Table(title=""Benchmark Results"")
    
    table.add_column(""Endpoint"", style=""cyan"")
    table.add_column(""Implementation"", style=""magenta"")
    table.add_column(""Concurrency"", style=""blue"")
    table.add_column(""Requests/s"", style=""green"")
    table.add_column(""Avg Latency (ms)"", style=""yellow"")
    table.add_column(""Median Latency (ms)"", style=""yellow"")
    table.add_column(""Max Latency (ms)"", style=""red"")
    table.add_column(""Errors"", style=""red"")
    
    endpoints = set(r.name for r in python_results + rust_results)
    concurrencies = set(r.concurrency for r in python_results + rust_results)
    
    for endpoint in endpoints:
        for concurrency in sorted(concurrencies):
            py_result = next((r for r in python_results if r.name == endpoint and r.concurrency == concurrency), None)
            rust_result = next((r for r in rust_results if r.name == endpoint and r.concurrency == concurrency), None)
            
            if py_result:
                table.add_row(
                    endpoint,
                    ""Python"",
                    str(py_result.concurrency),
                    f""{py_result.rps:.2f}"",
                    f""{py_result.avg_latency:.2f}"",
                    f""{py_result.median_latency:.2f}"",
                    f""{py_result.max_latency:.2f}"",
                    str(py_result.errors)
                )
            
            if rust_result:
                table.add_row(
                    endpoint,
                    ""Rust"",
                    str(rust_result.concurrency),
                    f""{rust_result.rps:.2f}"",
                    f""{rust_result.avg_latency:.2f}"",
                    f""{rust_result.median_latency:.2f}"",
                    f""{rust_result.max_latency:.2f}"",
                    str(rust_result.errors)
                )
            
            if py_result and rust_result:
                speedup = rust_result.rps / py_result.rps if py_result.rps > 0 else float('inf')
                latency_improvement = py_result.avg_latency / rust_result.avg_latency if rust_result.avg_latency > 0 else float('inf')
                
                table.add_row(
                    """",
                    f""[bold]Rust vs Python[/bold]"",
                    """",
                    f""[bold]{speedup:.2f}x[/bold]"",
                    f""[bold]{latency_improvement:.2f}x[/bold]"",
                    """",
                    """",
                    """"
                )
                
                table.add_row("""", """", """", """", """", """", """", """")
    
    console.print(table)
",benchmarks/benchmark.py,
survived,"    def to_dict(self) -> Dict:
        """"""Convert result to dictionary.""""""
        return {
            ""name"": self.name,
            ""concurrency"": self.concurrency,
            ""requests"": self.requests,
            ""duration"": self.duration,
            ""rps"": self.rps,
            ""avg_latency"": self.avg_latency,
            ""median_latency"": self.median_latency,
            ""min_latency"": self.min_latency,
            ""max_latency"": self.max_latency,
            ""stdev_latency"": self.stdev_latency,
            ""errors"": self.errors
        }
",benchmarks/benchmark.py,BenchmarkResult
survived,"    async def get_token_trades(self, parameters: dict):
        """"""Get trades for a specific token from Nansen""""""
        url = f""{self.base_url}/token/dex_trades""
        params = {
            ""address"": parameters[""address""],
            ""start_date"": parameters[""start_date""],
            ""end_date"": parameters[""end_date""]
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"    async def get_smart_money_status(self, parameters: dict):
        """"""Get the flows of tokens associated with smart money addresses""""""
        url = f""{self.base_url}/token_flows""
        params = {
            ""start_date"": parameters[""start_date""],
            ""end_date"": parameters[""end_date""]
        }
        if parameters.get(""token_address""):
            params[""token_address""] = parameters[""token_address""]
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"    async def get_token_details(self, parameters: dict):
        """"""Get details for a specific token from Nansen""""""
        url = f""{self.base_url}/token?address={parameters['address']}""
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers={""api-key"": self.api_key}) as response:
                if not response.ok:
                    raise Exception(f""HTTP error! status: {response.status} {await response.text()}"")
                return await response.json()
",python/src/plugins/nansen/goat_plugins/nansen/service.py,NansenService
survived,"    async def get_recently_verified_tokens(self, parameters: dict):
        """"""Get recently verified tokens from RugCheck""""""
        return await self._make_request(""/stats/verified"")
",python/src/plugins/rugcheck/goat_plugins/rugcheck/service.py,RugCheckService
deleted,"    def test_run_version_incremented(self, mock_current_version, mock_master_version, version_increment_check, connector):
        mock_master_version.return_value = semver.Version.parse(""0.9.0"")
        mock_current_version.return_value = semver.Version.parse(""1.0.0"")
        
        result = version_increment_check._run(connector)
        
        assert result.status == CheckStatus.PASSED
        assert ""Version was properly incremented"" in result.message
",airbyte-ci/connectors/connectors_qa/tests/checks/test_version.py,TestVersionIncrementCheck
survived,"def run_test_polars_code(reasoning: str, polars_python_code: str, csv_path: str) -> str:
    """"""Executes test Polars Python code and returns results.

    The agent uses this to validate code before finalizing it.
    Results are only shown to the agent, not the user.
    The code should use Polars' lazy evaluation (LazyFrame) for better performance.

    Args:
        reasoning: Explanation of why we're running this test code
        polars_python_code: The Polars Python code to test. Should use pl.scan_csv() for lazy evaluation.
        csv_path: Path to the CSV file

    Returns:
        Code execution results as a string

    Example:
        result = run_test_polars_code(
            ""Testing average age calculation"",
            '''
            # Calculate average age using lazy evaluation
            result = df.select(pl.col(""age"").mean().alias(""avg_age"")).collect()
            print(""Average age:"", float(result[0, ""avg_age""]))
            ''',
            ""data.csv""
        )
    """"""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            # Ensure code is properly indented
            indented_code = ""\n"".join(""    "" + line if line.strip() else line 
                                    for line in polars_python_code.splitlines())
            
            script = '''import polars as pl
import sys

# Read the CSV file using lazy evaluation
df = pl.scan_csv(""{csv_path}"")

try:
{code}
    
    # If no result was explicitly printed, try to collect and display
    if 'result' not in locals():
        if any(var for var in locals().values() if isinstance(var, (pl.LazyFrame, pl.DataFrame))):
            result = next(var for var in reversed(list(locals().values())) 
                      if isinstance(var, (pl.LazyFrame, pl.DataFrame)))
            if isinstance(result, pl.LazyFrame):
                result = result.collect()
        else:
            result = df.collect()
    
    # Convert result to string for display
    if isinstance(result, pl.DataFrame):
        print(result.select(pl.all()).write_csv(None))
    else:
        print(str(result))
except Exception as e:
    print(""Error: "" + str(e), file=sys.stderr)
    sys.exit(1)
'''
            script_content = script.format(csv_path=csv_path, code=indented_code)
            f.write(script_content)
            temp_file = f.name

        result = subprocess.run(['uv', 'run', '--with', 'polars', temp_file], 
                              capture_output=True, text=True)
        os.unlink(temp_file)

        if result.returncode != 0:
            return f""Error: {result.stderr}""

        console.log(f""[blue]Test Code Tool[/blue] - Reasoning: {reasoning}"")
        console.log(f""[dim]Code:\n{polars_python_code}[/dim]"")
        return result.stdout
    except Exception as e:
        console.log(f""[red]Error running test code: {str(e)}[/red]"")
        return str(e)
",sfa_polars_csv_agent_openai_v2.py,
deleted,"def test_scrape_url_with_parse_pdf_true():
    if TEST_API_KEY:
        app = FirecrawlApp(api_url=API_URL, api_key=TEST_API_KEY)
        response = app.scrape_url('https://arxiv.org/pdf/astro-ph/9301001.pdf', parse_pdf=True)
        assert response is not None
        assert 'markdown' in response
        assert len(response['markdown']) > 100
",apps/python-sdk/firecrawl/__tests__/v1/e2e_withAuth/test.py,
survived,"def plot_pose_cube(img, yaw, pitch, roll, tdx=None, tdy=None, size=150.):
    p = pitch * np.pi / 180
    y = -(yaw * np.pi / 180)
    r = roll * np.pi / 180
    if tdx != None and tdy != None:
        face_x = tdx - 0.50 * size
        face_y = tdy - 0.50 * size

    else:
        height, width = img.shape[:2]
        face_x = width / 2 - 0.5 * size
        face_y = height / 2 - 0.5 * size

    x1 = size * (cos(y) * cos(r)) + face_x
    y1 = size * (cos(p) * sin(r) + cos(r) * sin(p) * sin(y)) + face_y
    x2 = size * (-cos(y) * sin(r)) + face_x
    y2 = size * (cos(p) * cos(r) - sin(p) * sin(y) * sin(r)) + face_y
    x3 = size * (sin(y)) + face_x
    y3 = size * (-cos(y) * sin(p)) + face_y

    cv2.line(img, (int(face_x), int(face_y)), (int(x1), int(y1)), (0, 0, 255), 3)
    cv2.line(img, (int(face_x), int(face_y)), (int(x2), int(y2)), (0, 0, 255), 3)
    cv2.line(img, (int(x2), int(y2)), (int(x2 + x1 - face_x), int(y2 + y1 - face_y)), (0, 0, 255), 3)
    cv2.line(img, (int(x1), int(y1)), (int(x1 + x2 - face_x), int(y1 + y2 - face_y)), (0, 0, 255), 3)
    cv2.line(img, (int(face_x), int(face_y)), (int(x3), int(y3)), (255, 0, 0), 2)
    cv2.line(img, (int(x1), int(y1)), (int(x1 + x3 - face_x), int(y1 + y3 - face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x2), int(y2)), (int(x2 + x3 - face_x), int(y2 + y3 - face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x2 + x1 - face_x), int(y2 + y1 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (255, 0, 0), 2)
    cv2.line(img, (int(x3 + x1 - face_x), int(y3 + y1 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x2 + x3 - face_x), int(y2 + y3 - face_y)),
             (int(x3 + x1 + x2 - 2 * face_x), int(y3 + y2 + y1 - 2 * face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x3), int(y3)), (int(x3 + x1 - face_x), int(y3 + y1 - face_y)), (0, 255, 0), 2)
    cv2.line(img, (int(x3), int(y3)), (int(x3 + x2 - face_x), int(y3 + y2 - face_y)), (0, 255, 0), 2)

    return img
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,
survived,"def cross_product(u, v):
    batch = u.shape[0]
    i = u[:, 1] * v[:, 2] - u[:, 2] * v[:, 1]
    j = u[:, 2] * v[:, 0] - u[:, 0] * v[:, 2]
    k = u[:, 0] * v[:, 1] - u[:, 1] * v[:, 0]

    out = torch.cat((i.view(batch, 1), j.view(batch, 1), k.view(batch, 1)), 1)

    return out
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,
survived,"def get_pt2d_from_mat(mat_path):
    mat = sio.loadmat(mat_path)
    pt2d = mat['pt2d']
    return pt2d
",face_recognition/6d_repnet_360/utils_6d_repnet_360/utils.py,
survived,"def py_cpu_nms(dets, thresh):
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep
",face_recognition/6d_repnet_360/utils_6d_repnet_360/functions.py,
survived,"    def _convert_pdf_to_markdown(self, pdf_path: str) -> List[Tuple[str, Dict[str, Any]]]:
        """"""Convert PDF with OCR detection logic.""""""
        # Quick heuristic: if the PDF already contains a text layer, skip OCR for speed
        def _pdf_has_text(path: str) -> bool:
            try:
                doc = fitz.open(path)
                for page in doc:
                    if page.get_text(""text"").strip():
                        return True
            except Exception:
                pass
            return False

        use_ocr = not _pdf_has_text(pdf_path)
        converter = self.converter_ocr if use_ocr else self.converter_no_ocr
        ocr_msg = ""(OCR enabled)"" if use_ocr else ""(no OCR)""

        print(f""Converting {pdf_path} to Markdown using docling {ocr_msg}..."")
        return self._perform_conversion(pdf_path, converter, ocr_msg)
",rag_system/ingestion/document_converter.py,DocumentConverter
survived,"                def __init__(self, provider, session, init_timestamp, kwargs):
                    self.provider = provider
                    self.session = session
                    self.init_timestamp = init_timestamp
                    self.kwargs = kwargs
                    self.stream = None
                    # Create a new LLM event for this stream
                    self.llm_event = LLMEvent(init_timestamp=init_timestamp, params=kwargs)
                    if session is not None:
                        self.llm_event.session_id = session.session_id
",agentops/llms/providers/cohere.py,CohereProvider.StreamWrapper
survived,"def test_parallel_dataset_creation():
  """"""Test dataset creation with parallel processing.""""""
  threads = 4
  parsed_data = test_dataset_creation(threads=threads)

  print(f""Successfully processed dataset with {threads} threads"")
  return parsed_data
",tests/dataset_creation_test.py,
survived,"    def __init__(self, options: JupiterPluginOptions):
        super().__init__(""jupiter"", [JupiterService(options.api_key)])
",python/src/plugins/jupiter/goat_plugins/jupiter/__init__.py,JupiterPlugin
survived,"    def supports_chain(self, chain) -> bool:
        return chain['type'] == 'solana'
",python/src/plugins/spl_token/goat_plugins/spl_token/__init__.py,SplTokenPlugin
survived,"    def test_change_tracking_format(self, mock_post):
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            'success': True,
            'data': {
                'markdown': 'Test markdown content',
                'changeTracking': {
                    'previousScrapeAt': '2023-01-01T00:00:00Z',
                    'changeStatus': 'changed',
                    'visibility': 'visible'
                }
            }
        }
        mock_post.return_value = mock_response

        app = FirecrawlApp(api_key=os.environ.get('FIRECRAWL_API_KEY', 'dummy-api-key-for-testing'))
        result = app.scrape_url('https://example.com', {
            'formats': ['markdown', 'changeTracking']
        })

        args, kwargs = mock_post.call_args
        self.assertEqual(kwargs['json']['formats'], ['markdown', 'changeTracking'])
        
        self.assertEqual(result['changeTracking']['previousScrapeAt'], '2023-01-01T00:00:00Z')
        self.assertEqual(result['changeTracking']['changeStatus'], 'changed')
        self.assertEqual(result['changeTracking']['visibility'], 'visible')
",apps/python-sdk/tests/test_change_tracking.py,TestChangeTracking
survived,"    def test_change_tracking_options(self, mock_post):
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            'success': True,
            'data': {
                'markdown': 'Test markdown content',
                'changeTracking': {
                    'previousScrapeAt': '2023-01-01T00:00:00Z',
                    'changeStatus': 'changed',
                    'visibility': 'visible',
                    'diff': {
                        'text': '@@ -1,1 +1,1 @@\n-old content\n+new content',
                        'json': {
                            'files': [{
                                'from': None,
                                'to': None,
                                'chunks': [{
                                    'content': '@@ -1,1 +1,1 @@',
                                    'changes': [{
                                        'type': 'del',
                                        'content': '-old content',
                                        'del': True,
                                        'ln': 1
                                    }, {
                                        'type': 'add',
                                        'content': '+new content',
                                        'add': True,
                                        'ln': 1
                                    }]
                                }]
                            }]
                        }
                    },
                    'json': {
                        'title': {
                            'previous': 'Old Title',
                            'current': 'New Title'
                        }
                    }
                }
            }
        }
        mock_post.return_value = mock_response

        app = FirecrawlApp(api_key=os.environ.get('FIRECRAWL_API_KEY', 'dummy-api-key-for-testing'))
        result = app.scrape_url('https://example.com', {
            'formats': ['markdown', 'changeTracking'],
            'changeTrackingOptions': {
                'modes': ['git-diff', 'json'],
                'schema': {'type': 'object', 'properties': {'title': {'type': 'string'}}}
            }
        })

        args, kwargs = mock_post.call_args
        self.assertEqual(kwargs['json']['formats'], ['markdown', 'changeTracking'])
        self.assertEqual(kwargs['json']['changeTrackingOptions']['modes'], ['git-diff', 'json'])
        
        self.assertEqual(result['changeTracking']['diff']['text'], '@@ -1,1 +1,1 @@\n-old content\n+new content')
        self.assertEqual(result['changeTracking']['json']['title']['previous'], 'Old Title')
        self.assertEqual(result['changeTracking']['json']['title']['current'], 'New Title')",apps/python-sdk/tests/test_change_tracking.py,TestChangeTracking
survived,"def test_deprecated_async_warning(mock_isinstance):
    """"""Test that using _async parameter raises a deprecation warning.""""""
    mock_model = MagicMock()
    mock_model.generate_content = MagicMock()
    mock_model.generate_content_async = MagicMock()
    
    with pytest.warns(DeprecationWarning, match=""'_async' is deprecated. Use 'use_async' instead.""):
        client = from_vertexai(
            mock_model, 
            _async=True
        )
",tests/llm/test_vertexai/test_deprecated_async.py,
survived,"def test_default_bucket():
    # Verify DEFAULT_BUCKET covers appropriate ranges for both fast APIs and LLM workloads
    from bentoml._internal.utils.metrics import DEFAULT_BUCKET

    # Test smallest bucket
    assert DEFAULT_BUCKET[0] == 0.005  # 5ms for fast APIs

    # Test largest finite bucket
    assert DEFAULT_BUCKET[-2] == 180.0  # 180s for LLM workloads

    # Test infinity is present
    assert DEFAULT_BUCKET[-1] == float(""inf"")

    # Test monotonic increase
    for i in range(len(DEFAULT_BUCKET) - 1):
        assert DEFAULT_BUCKET[i] < DEFAULT_BUCKET[i + 1]
",tests/unit/_internal/utils/test_metrics.py,
survived,"    def test_pause_resume_exception_handling(self):
        """"""Test that resume is called even if exception occurs during human input.""""""
        from crewai.agents.agent_builder.base_agent_executor_mixin import CrewAgentExecutorMixin
        
        executor = CrewAgentExecutorMixin()
        executor.crew = MagicMock()
        executor.crew._train = False
        executor._printer = MagicMock()
        
        formatter = event_listener.formatter
        
        original_paused_state = formatter._live_paused
        
        try:
            with patch.object(formatter, 'pause_live_updates') as mock_pause, \
                 patch.object(formatter, 'resume_live_updates') as mock_resume, \
                 patch('builtins.input', side_effect=KeyboardInterrupt(""Test exception"")):
                
                with pytest.raises(KeyboardInterrupt):
                    executor._ask_human_input(""Test result"")
                
                mock_pause.assert_called_once()
                mock_resume.assert_called_once()
        finally:
            formatter._live_paused = original_paused_state
",tests/test_flow_human_input_integration.py,TestFlowHumanInputIntegration
survived,"    def test_pause_live_updates_when_already_paused(self):
        """"""Test pausing when already paused does nothing.""""""
        formatter = ConsoleFormatter()
        
        mock_live = MagicMock(spec=Live)
        formatter._live = mock_live
        formatter._live_paused = True
        
        formatter.pause_live_updates()
        
        mock_live.stop.assert_not_called()
        assert formatter._live_paused
",tests/utilities/test_console_formatter_pause_resume.py,TestConsoleFormatterPauseResume
survived,"def safe_path_join(*parts: str, root: Union[str, Path, None] = None) -> str:
    """"""
    Safely join path components and ensure the result is within allowed boundaries.

    Parameters
    ----------
    *parts : str
        Variable number of path components to join.
    root : Union[str, Path, None], optional
        Root directory to use as base. If None, uses current working directory.

    Returns
    -------
    str
        String representation of the resolved path.

    Raises
    ------
    ValueError
        If the resulting path would be outside the root directory
        or if any path component is invalid.
    """"""
    if not parts:
        raise ValueError(""No path components provided"")

    try:
        # Convert all parts to strings and clean them
        clean_parts = [str(part).strip() for part in parts if part]
        if not clean_parts:
            raise ValueError(""No valid path components provided"")

        # Establish root directory
        root_path = Path(root).resolve() if root else Path.cwd()
        
        # Join and resolve the full path
        full_path = Path(root_path, *clean_parts).resolve()
        
        # Check if the resolved path is within root
        if not str(full_path).startswith(str(root_path)):
            raise ValueError(
                f""Invalid path: Potential directory traversal. Path must be within {root_path}""
            )
            
        return str(full_path)
        
    except Exception as e:
        if isinstance(e, ValueError):
            raise
        raise ValueError(f""Invalid path components: {str(e)}"")
",src/crewai/flow/path_utils.py,
survived,"    def from_dict(cls, data: Dict[str, Any]) -> 'Fingerprint':
        """"""
        Create a Fingerprint from a dictionary representation.

        Args:
            data (Dict[str, Any]): Dictionary representation of a fingerprint

        Returns:
            Fingerprint: A new Fingerprint instance
        """"""
        if not data:
            return cls()

        fingerprint = cls(metadata=data.get(""metadata"", {}))

        # For consistency with existing stored fingerprints, we need to manually set these
        if ""uuid_str"" in data:
            object.__setattr__(fingerprint, 'uuid_str', data[""uuid_str""])
        if ""created_at"" in data and isinstance(data[""created_at""], str):
            object.__setattr__(fingerprint, 'created_at', datetime.fromisoformat(data[""created_at""]))

        return fingerprint",src/crewai/security/fingerprint.py,Fingerprint
deleted,"    async def init_runtime_embeddings_model(
        self,
        model_info: persistence_model.EmbeddingsModel | sqlalchemy.Row[persistence_model.EmbeddingsModel] | dict,
    ):
        """"""åˆå§‹åŒ–è¿è¡Œæ—¶ Embeddings æ¨¡åž‹""""""
        if isinstance(model_info, sqlalchemy.Row):
            model_info = persistence_model.EmbeddingsModel(**model_info._mapping)
        elif isinstance(model_info, dict):
            model_info = persistence_model.EmbeddingsModel(**model_info)

        requester_inst = self.requester_dict[model_info.requester](ap=self.ap, config=model_info.requester_config)

        await requester_inst.initialize()

        runtime_embeddings_model = requester.RuntimeEmbeddingsModel(
            model_entity=model_info,
            token_mgr=token.TokenManager(
                name=model_info.uuid,
                tokens=model_info.api_keys,
            ),
            requester=requester_inst,
        )

        return runtime_embeddings_model
",pkg/provider/modelmgr/modelmgr.py,ModelManager
deleted,"    async def create_embeddings_model(self, model_data: dict) -> str:
        model_data['uuid'] = str(uuid.uuid4())

        await self.ap.persistence_mgr.execute_async(sqlalchemy.insert(persistence_model.EmbeddingsModel).values(**model_data))

        embeddings_model = await self.get_embeddings_model(model_data['uuid'])

        await self.ap.model_mgr.load_embeddings_model(embeddings_model)

        return model_data['uuid']
",pkg/api/http/service/model.py,EmbeddingsModelsService
deleted,"    async def get_embeddings_model_by_uuid(self, uuid: str) -> requester.RuntimeEmbeddingsModel:
        """"""é€šè¿‡uuidèŽ·å– Embeddings æ¨¡åž‹""""""
        for model in self.embeddings_models:
            if model.model_entity.uuid == uuid:
                return model
        raise ValueError(f'Embeddings model {uuid} not found')
",pkg/provider/modelmgr/modelmgr.py,ModelManager
deleted,"    def validate_one_of_millisecs_selector(self) -> ""WaitAction"":
        if (self.milliseconds is None) == (self.selector is None):
            raise ValueError(""Exactly one of milliseconds or selector must be provided"")
        return self
",apps/python-sdk/firecrawl/firecrawl.py,WaitAction
survived,"def test_get_configured_catalog_without_overrides(mock_catalog, mock_stream):
    """"""Test that get_configured_catalog uses source-defined keys when no overrides exist.""""""
    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(executor=Mock(), name=""test-source"")

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.cursor_field == [""original_cursor""]
        assert configured_stream.primary_key == [[""original_pk""]]
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"def test_get_configured_catalog_composite_primary_key(mock_catalog, mock_stream):
    """"""Test that get_configured_catalog correctly handles composite primary keys.""""""
    mock_stream.source_defined_primary_key = [[""pk1"", ""pk2""]]
    with patch.object(Source, ""_discover"", return_value=mock_catalog):
        source = Source(
            executor=Mock(),
            name=""test-source"",
            primary_key_overrides={""test_stream"": [""custom_pk1"", ""custom_pk2""]},
        )

        catalog = source.get_configured_catalog()

        assert len(catalog.streams) == 1
        configured_stream = catalog.streams[0]
        assert configured_stream.primary_key == [[""custom_pk1"", ""custom_pk2""]]
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"def test_set_cursor_keys():
    """"""Test that set_cursor_keys properly updates the cursor key overrides.""""""
    with patch.object(Source, ""_discover"", return_value=Mock()):
        source = Source(executor=Mock(), name=""test-source"")

        source.set_cursor_keys(kwargs={""stream1"": ""cursor1""})
        assert source._cursor_key_overrides == {""stream1"": ""cursor1""}

        source.set_cursor_keys(kwargs={""stream2"": ""cursor2""})
        assert source._cursor_key_overrides == {
            ""stream1"": ""cursor1"",
            ""stream2"": ""cursor2"",
        }

        source.set_cursor_keys(kwargs={""stream1"": ""new_cursor1""})
        assert source._cursor_key_overrides == {
            ""stream1"": ""new_cursor1"",
            ""stream2"": ""cursor2"",
        }
",tests/unit_tests/sources/test_source_key_overrides.py,
survived,"    def test_tool(input_text: str) -> str:
        """"""A test tool.""""""
        return f""Result: {input_text}""
",tests/tools/test_tool_usage_limit.py,
survived,"    def test_cache_hit_miss(self) -> None:
        """"""Test cache hit and miss.""""""
        loader = PickleLoader(""test"", self.save_path)
        
        # No file exists yet
        assert not loader.cache_hit(""hash1"", ""Pure"")
        
        # Create a cache file
        cache_path = loader.build_path(""hash1"", ""Pure"")
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        
        with open(cache_path, ""wb"") as f:
            pickle.dump(cache, f)
        
        # Now it should hit
        assert loader.cache_hit(""hash1"", ""Pure"")
        
        # Different hash should miss
        assert not loader.cache_hit(""hash2"", ""Pure"")
        
        # Different cache type should miss
        assert not loader.cache_hit(""hash1"", ""Deferred"")
        
        # Empty file should miss
        empty_path = loader.build_path(""empty"", ""Pure"")
        with open(empty_path, ""wb"") as f:
            pass
        assert not loader.cache_hit(""empty"", ""Pure"")
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def save_cache(self, cache: Cache) -> None:
        key = f""{cache.cache_type}_{cache.hash}""
        self.saved_caches[key] = cache
",tests/_save/loaders/test_loader.py,MockPersistenceLoader
survived,"    def teardown_method(self) -> None:
        """"""Clean up the temporary directory.""""""
        self.temp_dir.cleanup()
",tests/_save/loaders/test_pickle_loader.py,TestPickleLoader
survived,"    def test_load_cache(self) -> None:
        """"""Test loading a cache.""""""
        loader = MemoryLoader(""test"")
        
        # Create and save a cache
        # Use string directly instead of Name constructor
        stateful_refs = set()
        cache = Cache(
            {""var1"": ""value1""}, 
            ""hash1"", 
            stateful_refs,
            ""Pure"",
            True,
            {}
        )
        loader.save_cache(cache)
        
        # Load the cache
        loaded_cache = loader.load_cache(""hash1"", ""Pure"")
        assert loaded_cache.hash == ""hash1""
        assert loaded_cache.cache_type == ""Pure""
        assert loaded_cache.hit is True
        
        # Should raise for non-existent cache
        with pytest.raises(LoaderError, match=""Unexpected cache miss""):
            loader.load_cache(""nonexistent"", ""Pure"")
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader
survived,"    def test_lru_eviction(self) -> None:
        """"""Test LRU cache eviction.""""""
        loader = MemoryLoader(""test"", max_size=2)
        
        # Create and save 3 caches
        for i in range(3):
            # Use string directly instead of Name constructor
            cache = Cache(
                {f""var{i}"": f""value{i}""}, 
                f""hash{i}"", 
                set(),
                ""Pure"",
                True,
                {}
            )
            loader.save_cache(cache)
        
        # The first one should be evicted
        assert not loader.cache_hit(""hash0"", ""Pure"")
        assert loader.cache_hit(""hash1"", ""Pure"")
        assert loader.cache_hit(""hash2"", ""Pure"")
        
        # Access hash1 to move it to the end of the LRU
        loader.load_cache(""hash1"", ""Pure"")
        
        # Add another cache
        cache = Cache(
            {""var3"": ""value3""}, 
            ""hash3"", 
            set(),
            ""Pure"",
            True,
            {}
        )
        loader.save_cache(cache)
        
        # hash2 should now be evicted
        assert not loader.cache_hit(""hash0"", ""Pure"")
        assert loader.cache_hit(""hash1"", ""Pure"")
        assert not loader.cache_hit(""hash2"", ""Pure"")
        assert loader.cache_hit(""hash3"", ""Pure"")
",tests/_save/loaders/test_memory_loader.py,TestMemoryLoader
deleted,"def mem0_storage_with_local_config(mock_mem0_memory):
    """"""Fixture to create a Mem0Storage instance with local mem0 config""""""

    # Patch the Memory class to return our mock
    with patch(""mem0.memory.main.Memory.from_config"", return_value=mock_mem0_memory) as mock_from_config:
        local_config = {
            ""vector_store"": {""provider"": ""mock_vector_store""},
            ""llm"": {""provider"": ""mock_llm""},
            ""embedder"": {""provider"": ""mock_embedder""},
        }
        
        crew = MockCrew(
            memory_config={
                ""provider"": ""mem0"",
                ""config"": {""user_id"": ""test_user"", ""local_mem0_config"": local_config},
            }
        )

        mem0_storage = Mem0Storage(type=""short_term"", crew=crew)
        return mem0_storage, mock_from_config, local_config
",tests/storage/test_mem0_storage.py,
survived,"    def _get_current_schema(self) -> list:
        """"""Get the current schema of the runs table from the database.

        Returns:
            list: List of tuples containing column information.
                  Each tuple contains (cid, name, type, notnull, dflt_value, pk)
        """"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(""PRAGMA table_info(runs)"")
            schema_info = cursor.fetchall()
        return schema_info
",src/bespokelabs/curator/db.py,MetadataDB
survived,"    def get_chain(self) -> Chain:
        """"""Get the chain type for Solana.""""""
        return {""type"": ""solana""}
",python/src/wallets/solana/goat_wallets/solana/wallet.py,SolanaWalletClient
survived,"def solana_keypair(client: SolanaClient, keypair: Keypair) -> SolanaKeypairWalletClient:
    """"""Create a new SolanaKeypairWalletClient instance.

    Args:
        client: A Solana RPC client instance
        keypair: A Solana keypair for signing transactions

    Returns:
        A new SolanaKeypairWalletClient instance
    """"""
    return SolanaKeypairWalletClient(client, keypair)",python/src/wallets/solana/goat_wallets/solana/wallet.py,
survived,"    def run(self):
        """"""Run the dead link checker.""""""
        print(f""Starting dead link check for {self.base_url}"")
        print(f""Max pages: {self.max_pages}, Timeout: {self.timeout}s"")
        
        while self.pages_to_visit and len(self.visited_pages) < self.max_pages:
            url = self.pages_to_visit.popleft()
            self.crawl_page(url)
        
        print(f""\nCrawl complete!"")
        print(f""Pages visited: {len(self.visited_pages)}"")
        print(f""Links checked: {len(self.checked_links)}"")
        print(f""Dead links found: {len(self.dead_links)}"")
        
        if self.dead_links:
            print(""\nâŒ DEAD LINKS FOUND:"")
            for link_info in self.dead_links:
                print(f""  URL: {link_info['url']}"")
                print(f""  Error: {link_info['error']}"")
                print(f""  Found on: {link_info['source_page']}"")
                print()
            return False
        else:
            print(""\nâœ… No dead links found!"")
            return True
",scripts/check_dead_links.py,DeadLinkChecker
survived,"async def _search_for_implementation(tool: ToolDefinition) -> str:
    """"""
    Use web_search to find implementation details for a tool.
    
    Args:
        tool: The tool definition
        
    Returns:
        Implementation details as a string
    """"""
    try:
        try:
            from agents import Agent, Runner, function_tool
            from agents.tools import web_search
        except ImportError:
            return f""# Note: OpenAI Agents SDK not installed. Install with: pip install openai-agents\n""
            
        query = f""python implementation for {tool.name} function""
        if tool.description:
            query += f"" that {tool.description.lower()}""
            
        if tool.parameters:
            param_names = [p.name for p in tool.parameters]
            query += f"" with parameters {', '.join(param_names)}""
            
        search_results = await web_search(query, num_results=3)
        
        if not search_results or isinstance(search_results, str) and ""error"" in search_results.lower():
            return f""    # Could not find implementation details via web search\n    # Implement the {tool.name} functionality here\n    return f\""Implementation for {tool.name} with parameters: {{{', '.join([p.name + '=' + p.name for p in tool.parameters])}}}\""""
            
        return search_results
    except Exception as e:
        return f""    # Error during web search: {str(e)}\n    # Implement the {tool.name} functionality here\n    return f\""Implementation for {tool.name} with parameters: {{{', '.join([p.name + '=' + p.name for p in tool.parameters])}}}\""""
",meta_agent/generators/tool_generator.py,
survived,"def generate_tool_code_sync(tool: ToolDefinition) -> str:
    """"""
    Synchronous wrapper for generate_tool_code.
    
    Args:
        tool: The tool definition
        
    Returns:
        Python code implementing the tool
    """"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    if isinstance(tool, list):
        if len(tool) > 0:
            tool = tool[0]
        else:
            raise ValueError(""Empty tool list provided to generate_tool_code_sync"")
            
    return loop.run_until_complete(generate_tool_code(tool))
",meta_agent/generators/tool_generator.py,
survived,"    def test_init(self) -> None:
        """"""Test initialization of the google class.""""""
        model = google(""gemini-pro"")
        assert model.model == ""gemini-pro""
        assert model.system_message == DEFAULT_SYSTEM_MESSAGE
        assert model.api_key is None

        model = google(
            ""gemini-pro"",
            system_message=""Custom system message"",
            api_key=""test-key"",
        )
        assert model.model == ""gemini-pro""
        assert model.system_message == ""Custom system message""
        assert model.api_key == ""test-key""
",tests/_ai/llm/_impl.py,TestGoogle
survived,"    def test_print_override_with_custom_sep_and_end(self) -> None:
        # Test print_override with custom separator and end
        thread_id = threading.get_ident()
        THREADS.add(thread_id)

        try:
            stream = MockStream()

            # Create a mock context
            context = MagicMock(spec=RuntimeContext)
            context.stream = stream
            context.execution_context = MagicMock(spec=ExecutionContext)
            context.execution_context.cell_id = ""cell1""

            with patch(""marimo._messaging.print_override._original_print"") as mock_print:
                with patch(
                    ""marimo._messaging.print_override.get_context"",
                    return_value=context,
                ):
                    print_override(""Hello"", ""world"", sep=""-"", end=""!"")

                    # Original print should not be called
                    mock_print.assert_not_called()

                    # Message should be sent to the stream with custom sep and end
                    assert len(stream.messages) == 1
                    assert stream.messages[0][1][""console""][""data""] == ""Hello-world!""
        finally:
            # Clean up
            if thread_id in THREADS:
                THREADS.remove(thread_id)
",tests/_messaging/test_print_override.py,TestPrintOverride
deleted,"    def test_multiple_definition_error(self) -> None:
        error = MultipleDefinitionError(
            name=""test_var"", cells=(""cell1"", ""cell2"")
        )

        # Test properties
        assert error.type == ""multiple-defs""
        assert ""test_var"" in error.describe()
        assert ""defined by another cell"" in error.describe()
",tests/_messaging/test_errors.py,TestErrorClasses
survived,"    def test_post_init_strips_trailing_quotes(self) -> None:
        # Test that __post_init__ strips trailing quotes
        option1 = CompletionOption(
            name='test_string""',
            type=""string"",
            completion_info=None,
        )
        assert option1.name == ""test_string""

        option2 = CompletionOption(
            name=""test_string'"",
            type=""string"",
            completion_info=None,
        )
        assert option2.name == ""test_string""

        # Test with multiple quotes
        option3 = CompletionOption(
            name='test_string""""""',
            type=""string"",
            completion_info=None,
        )
        assert option3.name == 'test_string'

        # Test with no quotes
        option4 = CompletionOption(
            name=""test_string"",
            type=""string"",
            completion_info=None,
        )
        assert option4.name == ""test_string""",tests/_messaging/test_completion_option.py,TestCompletionOption
survived,"    def test_stderr_write(self) -> None:
        stderr = self.MockStderr()

        # Test write method
        result = stderr.write(""Error message"")

        # Should return the length of the string
        assert result == 13

        # Should call _write_with_mimetype with text/plain mimetype
        assert len(stderr.written_data) == 1
        assert stderr.written_data[0] == (""Error message"", ""text/plain"")
",tests/_messaging/test_types.py,TestStdoutStderr
survived,"    def test_add_output_to_buffer_no_merge(self) -> None:
        # Test adding output for an existing cell with different stream or mimetype
        outputs_buffered_per_cell = {
            ""cell1"": [
                ConsoleMsg(
                    stream=CellChannel.STDOUT,
                    cell_id=""cell1"",
                    data=""Hello"",
                    mimetype=""text/plain"",
                )
            ]
        }
        msg = ConsoleMsg(
            stream=CellChannel.STDERR,
            cell_id=""cell1"",
            data=""Error"",
            mimetype=""text/plain"",
        )

        _add_output_to_buffer(msg, outputs_buffered_per_cell)

        assert len(outputs_buffered_per_cell[""cell1""]) == 2
        assert outputs_buffered_per_cell[""cell1""][0].data == ""Hello""
        assert outputs_buffered_per_cell[""cell1""][1].data == ""Error""
",tests/_messaging/test_console_output_worker.py,TestConsoleOutputWorker
survived,"    async def initialize(self):
        genai.configure(
            api_key='',
            transport='rest',
            client_options={
                'api_endpoint': self.requester_cfg['base_url'],
                'timeout': self.requester_cfg['timeout'],
            },
        )
",pkg/provider/modelmgr/requesters/geminichatcmpl.py,GeminiChatCompletions
survived,"def uniswap(options: UniswapPluginOptions) -> UniswapPlugin:
    """"""Create a new instance of the Uniswap plugin.
    
    Args:
        options: Configuration options for the plugin
        
    Returns:
        A configured UniswapPlugin instance
    """"""
    return UniswapPlugin(options)",python/src/plugins/uniswap/goat_plugins/uniswap/__init__.py,
survived,"def test_evm_transaction():
    """"""Fixture providing a test EVM transaction.""""""
    return {
        ""to"": ""0x742d35Cc6634C0532925a3b844Bc454e4438f44e"",
        ""value"": 1000000000000000
    }
",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_user_id():
    """"""Fixture providing test user ID for wallet creation.""""""
    return 12345",python/src/wallets/crossmint/tests/conftest.py,
survived,"def test_keypair():
    """"""Fixture providing test keypair.""""""
    account = Account.create()
    return {
        ""secretKey"": account.key.hex(),
        ""address"": account.address
    }
",python/src/wallets/crossmint/tests/test_smart_wallet.py,
survived,"def test_custodial_wallet_creation_with_email(custodial_api, test_email, solana_connection):
    """"""Test custodial wallet creation with email.""""""
    # Create wallet
    wallet = custodial_api.create_custodial_wallet(test_email)
    assert wallet[""type""] == ""solana-custodial-wallet""
    
    # Verify retrieval
    retrieved = custodial_api.get_wallet(f""email:{test_email}:solana-custodial-wallet"")
    compare_wallet_responses(wallet, retrieved)
    
    # Test client creation
    client = CustodialSolanaWalletClient(
        wallet[""address""],
        custodial_api,
        solana_connection,
        {""email"": test_email}
    )
    assert client.get_address() == wallet[""address""]
",python/src/wallets/crossmint/tests/test_custodial_wallet.py,
survived,"def test_error_handling_invalid_key(test_email):
    """"""Test error handling with invalid API key.""""""
    invalid_api = CrossmintWalletsAPI(
        api_key=""invalid_key"",
        base_url=""https://staging.crossmint.com""
    )
    with pytest.raises(Exception) as exc:
        invalid_api.get_wallet(f""email:{test_email}:solana-custodial-wallet"")
    assert ""Error"" in str(exc.value)
    assert ""401"" in str(exc.value) or ""403"" in str(exc.value)
",python/src/wallets/crossmint/tests/test_api_client.py,
survived,"    def get_json_schema(self):
        return get_generic_json_schema()
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/source.py,StreamAIExtractStructuredFolder
survived,"def box_file_ai_extract_structured(client: BoxClient, file_id: str, fields_json_str: str) -> str:
    ai_item = AiItemBase(id=file_id, type=AiItemBaseTypeField.FILE)
    fields_list = json.loads(fields_json_str)
    ai_fields = []
    options = []
    for field in fields_list:
        field_options = field.get(""options"")
        if field_options is not None:
            for option in field.get(""options""):
                options.append(CreateAiExtractStructuredFieldsOptionsField(key=option.get(""key"")))

        ai_fields.append(
            CreateAiExtractStructuredFields(
                key=field.get(""key""),
                description=field.get(""description""),
                display_name=field.get(""display_name""),
                prompt=field.get(""prompt""),
                type=field.get(""type""),
                options=options if options is not None and len(options) > 0 else None,
            )
        )
    response: AiExtractResponse = client.ai.create_ai_extract_structured(items=[ai_item], fields=ai_fields)
    return json.dumps(response.to_dict(), indent=2)
",airbyte-integrations/connectors/source-box-data-extract/source_box_data_extract/box_api.py,
survived,"def create_health_agent() -> Agent:
    """"""
    Create a health advisor agent.
    
    Returns:
        An Agent instance specialized in health topics.
    """"""
    instructions = """"""
    You are a health advisor with expertise in fitness, nutrition, and general wellness.
    Provide evidence-based information about health topics, focusing on practical advice.
    Always emphasize that you're not a medical professional and serious concerns should be 
    discussed with a healthcare provider.
    Keep responses concise and actionable.
    """"""
    
    return Agent(
        name=""HealthAdvisor"",
        instructions=instructions,
        model=""gpt-4o-mini"",
    )
",openai-agents-examples/03_sync_agent.py,
survived,"def create_outline_agent() -> Agent:
    """"""
    Create an outline agent that structures content.
    
    Returns:
        An Agent instance specialized in creating outlines.
    """"""
    instructions = """"""
    You are an outline specialist who excels at organizing information into clear, logical structures.
    Your task is to create well-structured outlines for content based on research provided.
    Include main sections, subsections, and key points to cover in each section.
    Ensure the outline has a logical flow and covers the topic comprehensively.
    Focus on creating a structure that will engage readers while effectively communicating information.
    """"""
    
    return Agent(
        name=""OutlineSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent to create a structured outline based on research.""
    )
",openai-agents-examples/11_agent_orchestration.py,
survived,"def test_run_blog_writer_system():
    """"""Test that the blog writer system can run and produce a blog post.""""""
    import pytest
    
    # Skip this test if no API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        pytest.skip(""OPENAI_API_KEY not set"")
    
    # Run a test query for a simple blog post
    blog_post = asyncio.run(run_blog_writer_system(""Write a short blog post about artificial intelligence""))
    
    # Verify we got a non-empty blog post
    assert blog_post
    assert len(blog_post) > 0
    # The blog post should contain relevant terms
    assert any(term in blog_post.lower() for term in [""ai"", ""artificial intelligence"", ""technology""])
",openai-agents-examples/08_agent_with_agent_as_tool.py,
survived,"def create_basic_agent(instructions: str = None) -> Agent:
    """"""
    Create a basic agent with the given instructions.
    
    Args:
        instructions: Custom instructions for the agent. If None, default instructions are used.
        
    Returns:
        An Agent instance configured with the provided instructions.
    """"""
    default_instructions = """"""
    You are a helpful assistant that provides accurate and concise information.
    Always be respectful and provide factual responses based on the latest available information.
    If you don't know something, admit it rather than making up information.
    """"""
    
    # Create and return a basic agent
    return Agent(
        name=""BasicAssistant"",
        instructions=instructions or default_instructions,
        model=""gpt-4o-mini"",  # Using GPT-4o-mini as specified in requirements
    )
",openai-agents-examples/01_basic_agent.py,
survived,"def create_blog_writer_agent(research_agent: Agent) -> Agent:
    """"""
    Create a blog writer agent that can use a research agent as a tool.
    
    Args:
        research_agent: The research agent to use as a tool
        
    Returns:
        An Agent instance specialized in blog writing with research capabilities
    """"""
    instructions = """"""
    You are a professional blog writer who creates engaging, informative content.
    Your writing should be clear, conversational, and tailored to a general audience.
    Structure your blog posts with an introduction, body paragraphs, and conclusion.
    Use the research tool available to you to gather accurate information on topics.
    Incorporate the research seamlessly into your writing while maintaining your voice.
    """"""
    
    # Convert the research agent into a tool
    research_tool = research_agent.as_tool(
        tool_name=""research_topic"",
        tool_description=""Research a specific topic to gather accurate information. Provide a clear, specific topic or question to research.""
    )
    
    return Agent(
        name=""BlogWriter"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[research_tool]
    )
",openai-agents-examples/08_agent_with_agent_as_tool.py,
survived,"def main():
    """"""Main function to parse arguments and run the protected agent.""""""
    parser = argparse.ArgumentParser(description=""Agent with Guardrails Example"")
    parser.add_argument(""--prompt"", ""-p"", type=str, required=True, 
                        help=""The prompt to send to the agent"")
    
    args = parser.parse_args()
    
    # Ensure API key is available
    if not os.environ.get(""OPENAI_API_KEY""):
        console.print(Panel(""[bold red]Error: OPENAI_API_KEY environment variable not set[/bold red]""))
        sys.exit(1)
    
    try:
        # Run the protected agent and get response
        response = asyncio.run(run_protected_agent(args.prompt))
        
        # Display the response
        if ""rejected"" in response.lower():
            console.print(Panel(response, title=""Input Rejected"", border_style=""red""))
        else:
            console.print(Panel(response, title=""Protected Agent Response"", border_style=""green""))
    
    except Exception as e:
        console.print(Panel(f""[bold red]Error: {str(e)}[/bold red]""))
        sys.exit(1)
",openai-agents-examples/10_agent_with_guardrails.py,
survived,"def create_research_agent() -> Agent:
    """"""
    Create a research agent that gathers and analyzes information.
    
    Returns:
        An Agent instance specialized in research.
    """"""
    instructions = """"""
    You are a research specialist who excels at gathering and analyzing information on various topics.
    Your task is to:
    1. Understand the research request
    2. Use the search_for_information tool to gather relevant information
    3. Use the analyze_topic tool to identify key aspects for research
    4. Synthesize the information into a comprehensive, well-organized research report
    5. Include relevant facts, statistics, and context
    6. Ensure the research is accurate, balanced, and thorough
    
    Your research should be detailed enough to serve as the foundation for content creation.
    """"""
    
    # Create the research agent with function tools
    return Agent(
        name=""ResearchSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        tools=[search_for_information, analyze_topic],
        handoff_description=""Use this agent to conduct thorough research on a topic.""
    )
",openai-agents-examples/13_research_blog_system.py,
survived,"def create_anthropic_agent() -> Agent:
    """"""
    Create an agent that uses Anthropic's Claude model.
    
    Returns:
        An Agent instance that uses Anthropic's Claude model.
    """"""
    instructions = """"""
    You are a helpful assistant powered by Anthropic's Claude model.
    You provide accurate, thoughtful responses to user queries.
    You excel at explaining complex concepts in clear, accessible language.
    When appropriate, you break down information into easy-to-understand parts.
    You acknowledge when you don't know something rather than making up information.
    """"""
    
    # Create the Anthropic model provider
    provider = AnthropicModelProvider()
    
    # Create the agent with the Anthropic provider
    return Agent(
        name=""ClaudeAssistant"",
        instructions=instructions,
        model=""gpt-4o-mini"",  # This will be mapped to claude-3-haiku
        model_provider=provider
    )
",openai-agents-examples/12_anthropic_agent.py,
survived,"    def __init__(self, min_length: int = 5, max_length: int = 500):
        """"""
        Initialize the format validation guardrail.
        
        Args:
            min_length: Minimum allowed input length
            max_length: Maximum allowed input length
        """"""
        self.min_length = min_length
        self.max_length = max_length
",openai-agents-examples/10_agent_with_guardrails.py,FormatValidationGuardrail
survived,"def create_tech_agent() -> Agent:
    """"""
    Create a technology specialist agent.
    
    Returns:
        An Agent instance specialized in technology topics.
    """"""
    instructions = """"""
    You are a technology specialist with expertise in computer science, programming, AI, and digital technologies.
    Provide clear, accurate explanations of technical concepts and their practical applications.
    When discussing programming, focus on concepts rather than writing extensive code.
    Explain how technologies work and their real-world impact.
    """"""
    
    return Agent(
        name=""TechSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent for questions about technology, computing, programming, and digital systems.""
    )
",openai-agents-examples/02_multi_agent.py,
survived,"def create_research_agent() -> Agent:
    """"""
    Create a research agent that gathers information.
    
    Returns:
        An Agent instance specialized in research.
    """"""
    instructions = """"""
    You are a research specialist who excels at gathering accurate information on various topics.
    Your task is to collect relevant facts, statistics, and context on the assigned topic.
    Focus on providing comprehensive, well-organized information that covers different aspects of the topic.
    Include both general information and specific details that would be useful for content creation.
    Always prioritize accuracy and cite sources when providing specific facts.
    """"""
    
    return Agent(
        name=""ResearchSpecialist"",
        instructions=instructions,
        model=""gpt-4o-mini"",
        handoff_description=""Use this agent to gather comprehensive information on a topic.""
    )
",openai-agents-examples/11_agent_orchestration.py,
survived,"def test_create_geography_agent():
    """"""Test that the geography agent is created with the correct configuration.""""""
    agent = create_geography_agent()
    assert agent.name == ""GeographySpecialist""
    assert ""geography specialist"" in agent.instructions.lower()
    assert agent.model == ""gpt-4o-mini""
",openai-agents-examples/04_agent_with_tracing.py,
survived,"def test_function_tools():
    """"""Test that the function tools work correctly.""""""
    # Test weather function
    weather_result = get_current_weather(""New York"", ""celsius"")
    assert ""New York"" in weather_result
    assert ""Â°C"" in weather_result
    
    # Test distance function
    distance_result = calculate_distance(""New York"", ""London"", ""kilometers"")
    assert ""New York"" in distance_result
    assert ""London"" in distance_result
    assert ""km"" in distance_result
    
    # Test time function
    time_result = get_current_time()
    assert ""UTC"" in time_result
    assert "":"" in time_result  # Time should contain colons
",openai-agents-examples/05_agent_with_function_tools.py,
survived,"    def _format_messages_for_provider(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """"""Format messages according to provider requirements.""""""
        if not self.is_anthropic:
            return messages
            
        # Anthropic requires messages to start with 'user' role
        if not messages or messages[0][""role""] == ""system"":
            # If first message is system, add a placeholder user message
            return [{""role"": ""user"", ""content"": "".""}, *messages]
        return messages
",src/crewai/llm.py,LLM
survived,"async def test_get_request_not_found():
    request_id = ""nonexistent-id""
    subdomain = ""abcd1234""
    
    mock_redis.get.return_value = None
    
    response = client.get(f""/api/get_request?id={request_id}&subdomain={subdomain}"")
    
    assert response.status_code == 404
    assert response.json() == {""detail"": ""Request not found""}
",backend/tests/test_endpoints.py,
survived,"def test_get_subdomain_from_path_edge_cases():
    assert get_subdomain_from_path("""") is None
    assert get_subdomain_from_path(""/r/"") is None
    assert get_subdomain_from_path(""/r"") is None
    assert get_subdomain_from_path(""/R/abcd1234"") == ""abcd1234""  # Case insensitivity
    assert get_subdomain_from_path(""//r//abcd1234"") == ""abcd1234""  # Extra slashes
",backend/tests/test_utils_extended.py,
survived,"    async def run_async(self):
        """"""è¿è¡Œé€‚é…å™¨""""""
        await self.logger.info('WebChatè°ƒè¯•é€‚é…å™¨å·²å¯åŠ¨')
        
        try:
            while True:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            await self.logger.info('WebChatè°ƒè¯•é€‚é…å™¨å·²åœæ­¢')
            raise
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"    def __init__(self, config: dict, ap: app.Application, logger: logging.Logger):
        self.ap = ap
        self.logger = logger
        self.config = config
        self.debug_messages = {}
        self.debug_sessions = {}
        
        self.debug_sessions['webchatperson'] = {
            'type': 'person',
            'id': 'webchatperson',
            'name': 'è°ƒè¯•ç§èŠ'
        }
        self.debug_sessions['webchatgroup'] = {
            'type': 'group', 
            'id': 'webchatgroup',
            'name': 'è°ƒè¯•ç¾¤èŠ'
        }
        
        self.debug_messages['webchatperson'] = []
        self.debug_messages['webchatgroup'] = []
",pkg/platform/sources/webchat.py,WebChatAdapter
survived,"def toggle_report_public_state(slug: str) -> bool:
    with _lock:
        if slug not in _report_status:
            raise ValueError(f""slug {slug} not found in report status"")
        _report_status[slug][""is_public""] = not _report_status[slug].get(""is_public"", True)
        save_status()
        return _report_status[slug][""is_public""]",server/src/services/report_status.py,
survived,"def search_result_item(result: rx.Var) -> rx.Component:
    """"""Render a single search result item.""""""
    return rx.box(
        rx.vstack(
            rx.text(
                result['title'],
                font_weight=""600"",
                color=""var(--c-slate-12)"",
                font_size=""14px"",
                margin_bottom=""4px""
            ),
            rx.text(
                result['content'],
                color=""var(--c-slate-11)"",
                font_size=""13px"",
                line_height=""1.4"",
                margin_bottom=""4px""
            ),
            rx.text(
                result['path'],
                color=""var(--c-slate-9)"",
                font_size=""12px""
            ),
            align_items=""start"",
            spacing=""1""
        ),
        padding=""12px"",
        border_bottom=""1px solid var(--c-slate-3)"",
        cursor=""pointer"",
        _hover={""background_color"": ""var(--c-slate-2)""},
        on_click=lambda: TypesenseSearchState.navigate_to_result(result['url'])
    )
",pcweb/components/docpage/navbar/typesense.py,
survived,"    def index_documents(self, documents: List[Dict[str, Any]]):
        """"""Index documents to Typesense.""""""
        if not documents:
            logger.warning(""No documents to index"")
            return
        
        try:
            batch_size = 100
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                for j, doc in enumerate(batch):
                    doc['id'] = str(i + j + 1)
                result = self.client.collections['docs'].documents.import_(batch)
                logger.info(f""Indexed batch {i//batch_size + 1}: {len(batch)} documents"")
            
            logger.info(f""Successfully indexed {len(documents)} documents"")
            
        except Exception as e:
            logger.error(f""Error indexing documents: {e}"")
            raise
",scripts/typesense_indexer.py,TypesenseIndexer
survived,"def main():
    """"""Main indexing function.""""""
    repo_root = Path(__file__).parent.parent
    docs_root = repo_root / 'docs'
    
    if not docs_root.exists():
        logger.error(f""Docs directory not found: {docs_root}"")
        sys.exit(1)
    
    logger.info(f""Starting indexing process for docs in: {docs_root}"")
    
    processor = MarkdownProcessor()
    indexer = TypesenseIndexer()
    
    md_files = list(docs_root.rglob('*.md'))
    logger.info(f""Found {len(md_files)} markdown files"")
    
    documents = []
    for md_file in md_files:
        doc = processor.process_file(md_file, docs_root)
        if doc:
            documents.append(doc)
    
    logger.info(f""Processed {len(documents)} documents successfully"")
    
    indexer.recreate_collection()
    indexer.index_documents(documents)
    
    logger.info(""Indexing completed successfully!"")
",scripts/typesense_indexer.py,
survived,"    def __init__(self):
        self.md = Markdown(extensions=['meta', 'toc'])
",scripts/typesense_indexer.py,MarkdownProcessor
survived,"            def embed_documents(self, texts):
                return [[0.1] * OPEN_AI_VECTOR_SIZE for _ in texts]
",airbyte-integrations/connectors/destination-milvus/integration_tests/milvus_integration_test.py,MilvusIntegrationTest.FakeEmbeddings
survived,"    def __init__(self, transaction: str, required_signers: list = None, signer: str = None):
        self.transaction = transaction
        self.requiredSigners = required_signers or []
        self.signer = signer
",python/src/wallets/crossmint/tests/test_solana_smart_wallet.py,SolanaSmartWalletTransactionParams
survived,"def update_user_profile(user_id: str, profile_data: Dict) -> Tuple[bool, Dict]:
    """"""
    Update a user's profile data.
    
    Args:
        user_id: The ID of the user to update
        profile_data: The profile data to update
        
    Returns:
        Tuple of (success, result) where result is either updated user data or error messages
    """"""
    # Get the current user data
    user_data = get_user_by_id(user_id)
    if not user_data:
        return False, {""error"": ""User not found""}
    
    # Validate email if provided
    if ""email"" in profile_data:
        if not validate_email(profile_data[""email""]):
            return False, {""error"": ""Invalid email format""}
    
    # In a real application, this would update the user in the database
    # For this mock, we'll just print the update
    print(f""[UPDATE] User {user_id} profile updated:"")
    for key, value in profile_data.items():
        print(f""  {key}: {value}"")
    
    # Return success with mock updated data
    updated_user = {**user_data, **profile_data}
    return True, {""user"": updated_user}
",codebase-architectures/atomic-composable-architecture/capabilities/user_management.py,
survived,"    def __init__(self):
        """"""Initialize the processing stage.""""""
        self.data = None
        self.metadata = {
            ""stage"": ""processing"",
            ""status"": ""initialized"",
            ""errors"": [],
            ""processing_steps"": []
        }
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage
survived,"    def delete(self, collection_name, id):
        """"""Delete an item from a collection.""""""
        if collection_name not in self.data or id not in self.data[collection_name]:
            return False
        del self.data[collection_name][id]
        return True
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"    def delete_product(product_id):
        """"""Delete a product.""""""
        try:
            # Check if product exists
            product_data = db.get(""products"", product_id)
            if not product_data:
                Logger.warning(app_logger, f""Cannot delete: Product not found: {product_id}"")
                return False
            
            # Delete product
            result = db.delete(""products"", product_id)
            Logger.info(app_logger, f""Deleted product: {product_id}"")
            return result
        except Exception as e:
            Logger.error(app_logger, f""Error deleting product: {str(e)}"", exc_info=True)
            raise",codebase-architectures/layered-architecture/services/product_service.py,ProductService
survived,"    def to_dict(self):
        """"""Convert category to dictionary.""""""
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""description"": self.description,
            ""created_at"": self.created_at,
            ""updated_at"": self.updated_at
        }
",codebase-architectures/layered-architecture/models/category.py,Category
survived,"    def get(self, table_name, item_id):
        """"""Get an item from a table by ID.""""""
        if table_name not in self.data or item_id not in self.data[table_name]:
            Logger.warning(self.logger, f""Item with ID {item_id} not found in '{table_name}'"")
            return None
        
        Logger.debug(self.logger, f""Retrieved item with ID {item_id} from '{table_name}'"")
        return self.data[table_name][item_id]
",codebase-architectures/layered-architecture/data/database.py,InMemoryDatabase
survived,"    def create_user(username, email, name=None):
        """"""Create a new user.""""""
        try:
            user_data = {
                ""username"": username,
                ""email"": email,
                ""name"": name
            }
            return UserService.create_user(user_data)
        except ValueError as e:
            return {""error"": str(e)}
",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI
survived,"    def create_category(name, description=None):
        """"""Create a new category.""""""
        try:
            # Validate category name
            if not name or not isinstance(name, str):
                raise ValueError(""Category name is required and must be a string"")
            
            # Check if category with same name already exists
            existing_categories = db.query(""categories"", lambda c: c[""name""].lower() == name.lower())
            if existing_categories:
                raise ValueError(f""Category with name '{name}' already exists"")
            
            # Create and save category
            category = Category(name=name, description=description)
            saved_category = db.insert(""categories"", category.to_dict())
            Logger.info(app_logger, f""Created category: {name}"")
            return saved_category
        except Exception as e:
            Logger.error(app_logger, f""Error creating category: {str(e)}"", exc_info=True)
            raise
",codebase-architectures/layered-architecture/services/category_service.py,CategoryService
survived,"    def get(self, collection_name, id):
        """"""Get an item from a collection by ID.""""""
        if collection_name not in self.data or id not in self.data[collection_name]:
            return None
        return self.data[collection_name][id]
",codebase-architectures/vertical-slice-architecture/shared/db.py,InMemoryDB
survived,"def main():
    """"""Run the application.""""""
    Logger.info(app_logger, ""Starting Layered Architecture Example"")
    
    display_header(""Layered Architecture Example"")
    
    # Create categories
    display_header(""Creating Categories"")
    
    electronics_result = CategoryAPI.create_category(""Electronics"", ""Electronic devices and gadgets"")
    display_result(electronics_result)
    
    books_result = CategoryAPI.create_category(""Books"", ""Books and e-books"")
    display_result(books_result)
    
    clothing_result = CategoryAPI.create_category(""Clothing"", ""Apparel and accessories"")
    display_result(clothing_result)
    
    # Try to create a duplicate category
    duplicate_result = CategoryAPI.create_category(""Electronics"", ""Duplicate category"")
    display_result(duplicate_result)
    
    # Get all categories
    display_header(""All Categories"")
    categories_result = CategoryAPI.get_all_categories()
    display_result(categories_result)
    
    # Create products
    display_header(""Creating Products"")
    
    # Get category IDs
    categories = categories_result[""data""]
    electronics_id = next((c[""id""] for c in categories if c[""name""] == ""Electronics""), None)
    books_id = next((c[""id""] for c in categories if c[""name""] == ""Books""), None)
    
    # Create products
    laptop_result = ProductAPI.create_product(
        ""Laptop"", 
        999.99, 
        electronics_id, 
        ""High-performance laptop"", 
        ""TECH-001""
    )
    display_result(laptop_result)
    
    phone_result = ProductAPI.create_product(
        ""Smartphone"", 
        499.99, 
        electronics_id, 
        ""Latest smartphone model"", 
        ""TECH-002""
    )
    display_result(phone_result)
    
    book_result = ProductAPI.create_product(
        ""Programming Book"", 
        29.99, 
        books_id, 
        ""Learn programming with this book"", 
        ""BOOK-001""
    )
    display_result(book_result)
    
    # Try to create a product with invalid price
    invalid_result = ProductAPI.create_product(
        ""Invalid Product"", 
        ""not-a-price"", 
        electronics_id
    )
    display_result(invalid_result)
    
    # Get products by category
    display_header(""Electronics Products"")
    electronics_products = ProductAPI.get_products_by_category(electronics_id)
    display_result(electronics_products)
    
    # Update a product
    display_header(""Updating a Product"")
    if laptop_result.get(""success"") and ""data"" in laptop_result:
        laptop_id = laptop_result[""data""][""id""]
        update_result = ProductAPI.update_product(
            laptop_id,
            price=899.99,
            description=""High-performance laptop with discount""
        )
        display_result(update_result)
    
    # Try to delete a category with products
    display_header(""Trying to Delete a Category with Products"")
    delete_result = CategoryAPI.delete_category(electronics_id)
    display_result(delete_result)
    
    # Delete a product
    display_header(""Deleting a Product"")
    if phone_result.get(""success"") and ""data"" in phone_result:
        phone_id = phone_result[""data""][""id""]
        delete_product_result = ProductAPI.delete_product(phone_id)
        display_result(delete_product_result)
    
    # Get all products
    display_header(""All Remaining Products"")
    all_products = ProductAPI.get_all_products()
    display_result(all_products)
    
    Logger.info(app_logger, ""Layered Architecture Example completed"")
",codebase-architectures/layered-architecture/main.py,
survived,"    def filter_data(self, filter_func, description=None):
        """"""
        Filter the data using the provided filter function.
        
        Args:
            filter_func: Function that takes a data item and returns True to keep it
            description: Description of the filter for metadata
        
        Returns:
            dict: Stage result with data and metadata
        """"""
        if self.data is None:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(""No data to filter"")
            return self._create_result()
        
        try:
            original_count = len(self.data) if isinstance(self.data, list) else 1
            
            # Apply filter
            if isinstance(self.data, list):
                self.data = [item for item in self.data if filter_func(item)]
            else:
                self.data = self.data if filter_func(self.data) else None
            
            # Update metadata
            filtered_count = len(self.data) if isinstance(self.data, list) else (1 if self.data else 0)
            filter_info = {
                ""description"": description or ""Custom filter"",
                ""original_count"": original_count,
                ""filtered_count"": filtered_count,
                ""removed_count"": original_count - filtered_count
            }
            
            if not hasattr(self, ""filters_applied""):
                self.filters_applied = []
            self.filters_applied.append(filter_info)
            
            self.metadata[""processing_steps""].append(""filter_data"")
            self.metadata[""filters_applied""] = self.filters_applied
            
            return self._create_result()
        except Exception as e:
            self.metadata[""status""] = ""error""
            self.metadata[""errors""].append(f""Filter error: {str(e)}"")
            return self._create_result()
",codebase-architectures/pipeline-architecture/pipeline/processing_stage.py,ProcessingStage
survived,"    def get_by_username(username):
        """"""Get a user by username.""""""
        user = UserService.get_by_username(username)
        if not user:
            return {""error"": f""User with username '{username}' not found""}
        return user
",codebase-architectures/vertical-slice-architecture/features/users/api.py,UserAPI
survived,"    def __init__(self):
        """"""
        Initialize the file editor pipeline.
        """"""
        # Create pipeline stages
        self.input_stage = InputStage()
        self.processing_stage = ProcessingStage()
        self.output_stage = OutputStage()
        
        # Create and configure pipeline
        self.pipeline = Pipeline(""File Editor Pipeline"")
        
        # Add stages
        self.pipeline.add_stage(""input"", self.input_stage)
        self.pipeline.add_stage(""processing"", self.processing_stage)
        self.pipeline.add_stage(""output"", self.output_stage)
        
        console.log(""[file_editor_pipeline] Initialized file editor pipeline"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/file_editor_pipeline.py,FileEditorPipeline
survived,"    def from_dict(cls, data: Dict[str, Any]) -> 'ToolUseRequest':
        """"""
        Create a tool use request from a dictionary.
        
        Args:
            data: Dictionary containing the tool use request
            
        Returns:
            A ToolUseRequest instance
        """"""
        command = data.get(""command"")
        path = data.get(""path"")
        
        # Extract all other keys as kwargs
        kwargs = {k: v for k, v in data.items() if k not in [""command"", ""path""]}
        
        return cls(command, path, **kwargs)",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,ToolUseRequest
survived,"    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """"""
        Process the input data and return the result.
        
        Args:
            data: The input data to process
            
        Returns:
            The processed data
        """"""
        raise NotImplementedError(""Pipeline stages must implement the process method"")
",example-agent-codebase-arch/pipeline-architecture/pipeline_manager/pipeline_manager.py,PipelineStage
survived,"    def _insert_text(self, path: str, insert_line: int, new_str: str) -> FileOperationResult:
        """"""
        Insert text at a specific location in a file.

        Args:
            path: The path to the file to modify
            insert_line: The line number after which to insert the text
            new_str: The text to insert

        Returns:
            FileOperationResult with result or error message
        """"""
        try:
            if not path or not path.strip():
                error_msg = ""Invalid file path provided: path is empty.""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Normalize the path
            path = normalize_path(path)

            if not os.path.exists(path):
                error_msg = f""File {path} does not exist""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            if insert_line is None:
                error_msg = ""No line number specified: insert_line is missing.""
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            with open(path, ""r"") as f:
                lines = f.readlines()

            # Line is 0-indexed for this function, but Claude provides 1-indexed
            insert_line = min(max(0, insert_line - 1), len(lines))

            # Check that the index is within acceptable bounds
            if insert_line < 0 or insert_line > len(lines):
                error_msg = (
                    f""Insert line number {insert_line} out of range (0-{len(lines)}).""
                )
                console.log(f""[insert_text] Error: {error_msg}"")
                return FileOperationResult(False, error_msg)

            # Ensure new_str ends with newline
            if new_str and not new_str.endswith(""\n""):
                new_str += ""\n""

            lines.insert(insert_line, new_str)

            with open(path, ""w"") as f:
                f.writelines(lines)

            console.print(
                f""[green]Successfully inserted text at line {insert_line + 1} in {path}[/green]""
            )
            console.log(
                f""[insert_text] Successfully inserted text at line {insert_line + 1} in {path}""
            )
            return FileOperationResult(
                True, f""Successfully inserted text at line {insert_line + 1} in {path}""
            )
        except Exception as e:
            error_msg = f""Error inserting text: {str(e)}""
            console.print(f""[red]{error_msg}[/red]"")
            console.log(f""[insert_text] Error: {str(e)}"")
            console.log(traceback.format_exc())
            return FileOperationResult(False, error_msg)
",example-agent-codebase-arch/pipeline-architecture/steps/processing_stage.py,ProcessingStage
survived,"    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the result to a dictionary.
        
        Returns:
            Dictionary representation of the result
        """"""
        return {
            ""success"": self.success,
            ""message"": self.message,
            ""data"": self.data
        }
",example-agent-codebase-arch/vertical-slice-architecture/features/file_operations/model.py,FileOperationResult
survived,"    def run_agent(
        client: Anthropic,
        prompt: str,
        handle_tool_use_func,
        max_thinking_tokens: int = DEFAULT_THINKING_TOKENS,
        max_loops: int = 10,
        use_token_efficiency: bool = False,
    ) -> Tuple[str, int, int]:
        """"""
        Run the Claude agent with file editing capabilities.

        Args:
            client: The Anthropic client
            prompt: The user's prompt
            handle_tool_use_func: Function to handle tool use requests
            max_thinking_tokens: Maximum tokens for thinking
            max_loops: Maximum number of tool use loops
            use_token_efficiency: Whether to use token-efficient tool use beta feature

        Returns:
            Tuple containing:
            - Final response from Claude (str)
            - Total input tokens used (int)
            - Total output tokens used (int)
        """"""
        # Track token usage
        input_tokens_total = 0
        output_tokens_total = 0
        system_prompt = """"""You are a helpful AI assistant with text editing capabilities.
You have access to a text editor tool that can view, edit, and create files.
Always think step by step about what you need to do before taking any action.
Be careful when making edits to files, as they can permanently change the user's files.
Follow these steps when handling file operations:
1. First, view files to understand their content before making changes
2. For edits, ensure you have the correct context and are making the right changes
3. When creating files, make sure they're in the right location with proper formatting
""""""

        # Define text editor tool
        text_editor_tool = {""name"": ""str_replace_editor"", ""type"": ""text_editor_20250124""}

        messages = [
            {
                ""role"": ""user"",
                ""content"": f""""""I need help with editing files. Here's what I want to do:

{prompt}

Please use the text editor tool to help me with this. First, think through what you need to do, then use the appropriate tool.
"""""",
            }
        ]

        loop_count = 0
        tool_use_count = 0
        thinking_start_time = time.time()

        while loop_count < max_loops:
            loop_count += 1

            console.rule(f""[yellow]Agent Loop {loop_count}/{max_loops}[/yellow]"")
            Logger.info(app_logger, f""Starting agent loop {loop_count}/{max_loops}"")

            # Create message with text editor tool
            message_args = {
                ""model"": MODEL,
                ""max_tokens"": 4096,
                ""tools"": [text_editor_tool],
                ""messages"": messages,
                ""system"": system_prompt,
                ""thinking"": {""type"": ""enabled"", ""budget_tokens"": max_thinking_tokens},
            }

            # Use the beta.messages with betas parameter if token efficiency is enabled
            if use_token_efficiency:
                # Using token-efficient tools beta feature
                message_args[""betas""] = [""token-efficient-tools-2025-02-19""]
                response = client.beta.messages.create(**message_args)
            else:
                # Standard approach
                response = client.messages.create(**message_args)

            # Track token usage
            if hasattr(response, ""usage""):
                input_tokens = getattr(response.usage, ""input_tokens"", 0)
                output_tokens = getattr(response.usage, ""output_tokens"", 0)

                input_tokens_total += input_tokens
                output_tokens_total += output_tokens

                console.print(
                    f""[dim]Loop {loop_count} tokens: Input={input_tokens}, Output={output_tokens}[/dim]""
                )
                Logger.info(
                    app_logger, 
                    f""Loop {loop_count} tokens: Input={input_tokens}, Output={output_tokens}""
                )

            # Process response content
            thinking_block = None
            tool_use_block = None
            text_block = None

            for content_block in response.content:
                if content_block.type == ""thinking"":
                    thinking_block = content_block
                    # Access the thinking attribute which contains the actual thinking text
                    if hasattr(thinking_block, ""thinking""):
                        console.print(
                            Panel(
                                thinking_block.thinking,
                                title=f""Claude's Thinking (Loop {loop_count})"",
                                border_style=""blue"",
                            )
                        )
                    else:
                        console.print(
                            Panel(
                                ""Claude is thinking..."",
                                title=f""Claude's Thinking (Loop {loop_count})"",
                                border_style=""blue"",
                            )
                        )
                elif content_block.type == ""tool_use"":
                    tool_use_block = content_block
                    tool_use_count += 1
                elif content_block.type == ""text"":
                    text_block = content_block

            # If we got a final text response with no tool use, we're done
            if text_block and not tool_use_block:
                thinking_end_time = time.time()
                thinking_duration = thinking_end_time - thinking_start_time

                console.print(
                    f""\n[bold green]Completed in {thinking_duration:.2f} seconds after {loop_count} loops and {tool_use_count} tool uses[/bold green]""
                )
                Logger.info(
                    app_logger,
                    f""Completed in {thinking_duration:.2f} seconds after {loop_count} loops and {tool_use_count} tool uses""
                )

                # Add the response to messages
                messages.append(
                    {
                        ""role"": ""assistant"",
                        ""content"": [
                            *([thinking_block] if thinking_block else []),
                            {""type"": ""text"", ""text"": text_block.text},
                        ],
                    }
                )

                return text_block.text, input_tokens_total, output_tokens_total

            # Handle tool use
            if tool_use_block:
                # Add the assistant's response to messages before handling tool calls
                messages.append({""role"": ""assistant"", ""content"": response.content})

                console.print(
                    f""\n[bold blue]Tool Call:[/bold blue] {tool_use_block.name}""
                )
                Logger.info(app_logger, f""Tool Call: {tool_use_block.name}"")

                # Handle the tool use
                tool_result = handle_tool_use_func(tool_use_block.input)

                # Log tool result
                result_text = tool_result.get(""error"") or tool_result.get(""result"", """")
                Logger.info(app_logger, f""Tool Result: {result_text[:100]}..."")

                # Format tool result for Claude
                tool_result_message = {
                    ""role"": ""user"",
                    ""content"": [
                        {
                            ""type"": ""tool_result"",
                            ""tool_use_id"": tool_use_block.id,
                            ""content"": result_text,
                        }
                    ],
                }
                messages.append(tool_result_message)

        # If we reach here, we hit the max loops
        console.print(
            f""\n[bold red]Warning: Reached maximum loops ({max_loops}) without completing the task[/bold red]""
        )
        Logger.warning(
            app_logger,
            f""Reached maximum loops ({max_loops}) without completing the task""
        )
        return (
            ""I wasn't able to complete the task within the allowed number of thinking steps. Please try a more specific prompt or increase the loop limit."",
            input_tokens_total,
            output_tokens_total,
        )",example-agent-codebase-arch/layered-architecture/services/agent_service.py,AgentService
survived,"    def __init__(self, success: bool, message: str, data: Any = None):
        """"""
        Initialize a file operation result.
        
        Args:
            success: Whether the operation was successful
            message: A message describing the result
            data: Optional data returned by the operation
        """"""
        self.success = success
        self.message = message
        self.data = data
",example-agent-codebase-arch/pipeline-architecture/shared/utilities.py,FileOperationResult
survived,"def main():
    """"""Main entry point for the application.""""""
    # Set up argument parser
    parser = argparse.ArgumentParser(description=""Claude 3.7 File Editor Agent"")
    parser.add_argument(
        ""--prompt"",
        ""-p"",
        required=True,
        help=""The prompt for what file operations to perform"",
    )
    parser.add_argument(
        ""--max-loops"",
        ""-l"",
        type=int,
        default=15,
        help=""Maximum number of tool use loops (default: 15)"",
    )
    parser.add_argument(
        ""--thinking"",
        ""-t"",
        type=int,
        default=DEFAULT_THINKING_TOKENS,
        help=f""Maximum thinking tokens (default: {DEFAULT_THINKING_TOKENS})"",
    )
    parser.add_argument(
        ""--efficiency"",
        ""-e"",
        action=""store_true"",
        help=""Enable token-efficient tool use (beta feature)"",
    )
    args = parser.parse_args()

    console.print(Panel.fit(""Claude 3.7 File Editor Agent (Vertical Slice Architecture)""))
    console.print(f""\n[bold]Prompt:[/bold] {args.prompt}\n"")
    console.print(f""[dim]Thinking tokens: {args.thinking}[/dim]"")
    console.print(f""[dim]Max loops: {args.max_loops}[/dim]"")
    
    if args.efficiency:
        console.print(f""[dim]Token-efficient tools: Enabled[/dim]\n"")
    else:
        console.print(f""[dim]Token-efficient tools: Disabled[/dim]\n"")

    # For testing purposes, we'll just print a success message
    console.print(""[green]Successfully loaded the Vertical Slice Architecture implementation![/green]"")
    console.print(""[yellow]This is a mock implementation for testing the architecture structure.[/yellow]"")
    console.print(""[yellow]In a real implementation, this would connect to the Claude API.[/yellow]"")

    # Display mock token usage
    display_token_usage(1000, 500)
",example-agent-codebase-arch/vertical-slice-architecture/main.py,
survived,"def test_create_folder_structure_normal_name_unchanged():
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        folder_path, folder_name, class_name = create_folder_structure(""hello"")
        
        assert folder_name == ""hello""
        assert class_name == ""Hello""
        assert folder_path.name == ""hello""
        assert folder_path.exists()
",tests/cli/test_create_crew.py,
survived,"def test_create_crew_normal_name_still_works(mock_load_env, mock_write_env, mock_copy_template, temp_dir):
    mock_load_env.return_value = {}
    
    with tempfile.TemporaryDirectory() as work_dir:
        os.chdir(work_dir)
        create_crew(""normal-project"", skip_provider=True)
        
        project_path = Path(work_dir) / ""normal_project""
        assert project_path.exists()
        assert (project_path / ""src"" / ""normal_project"").exists()
",tests/cli/test_create_crew.py,
survived,"    def _persist_state(flow_instance: Any, method_name: str) -> None:
        """"""Helper to persist state with error handling.""""""
        try:
            # Get flow UUID from state
            state = getattr(flow_instance, 'state', None)
            if state is None:
                raise ValueError(""Flow instance has no state"")
                
            flow_uuid: Optional[str] = None
            if isinstance(state, dict):
                flow_uuid = state.get('id')
            elif isinstance(state, BaseModel):
                flow_uuid = getattr(state, 'id', None)
                
            if not flow_uuid:
                raise ValueError(
                    ""Flow state must have an 'id' field for persistence""
                )
                
            # Persist the state
            persistence.save_state(
                flow_uuid=flow_uuid,
                method_name=method_name,
                state_data=state,
            )
        except Exception as e:
            logger.error(
                f""Failed to persist state for method {method_name}: {str(e)}""
            )
            raise RuntimeError(f""State persistence failed: {str(e)}"") from e
",src/crewai/flow/persistence/decorators.py,
survived,"    def __init__(self, endpoint: str):
        self.endpoint = endpoint
",python/src/plugins/jsonrpc/goat_plugins/jsonrpc/service.py,JSONRpcService
survived,"def compile_llms_txt():
    """"""Compile all relevant documentation into llms.txt for AI model consumption.""""""
    current_dir = Path(os.getcwd())
    content = ''
    
    # Define names of directories and files to exclude
    excluded_names = {'node_modules', '.git', '__pycache__', '.venv', 'images', '.pytest_cache', 'dist', 'build'}
    
    def should_include_file(file_path):
        """"""Check if a file should be included based on patterns and exclusions.""""""
        path_parts = Path(file_path).parts
        
        if any(part in excluded_names for part in path_parts):
            return False
            
        if file_path.endswith(('.md', '.mdx')):
            return True
            
        return False
    
    for root, dirs, files in os.walk('..'):
        dirs[:] = [d for d in dirs if d not in excluded_names]
        
        for file in files:
            if should_include_file(file):
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, '..')
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                    content += f""## {relative_path}\n\n{file_content}\n\n""
                except (UnicodeDecodeError, PermissionError) as e:
                    print(f""Warning: Could not read {relative_path}: {e}"")
                    continue

    # Write the complete content to llms.txt in the repository root
    output_path = Path('../llms.txt')
    output_path.write_text(content, encoding='utf-8')
    print(f""Successfully compiled documentation to {output_path.absolute()}"")
    print(f""Total content length: {len(content)} characters"")
",docs/compile_llms_txt.py,
survived,"    def supports_function_calling(self) -> bool:
        return True
",tests/custom_llm_test.py,JWTAuthLLM
survived,"    def supports_stop_words(self) -> bool:
        """"""Check if the LLM supports stop words.
        
        Returns:
            True if the LLM supports stop words, False otherwise.
        """"""
        pass
",src/crewai/llm.py,BaseLLM
survived,"    def test_get_content_with_none_delta(self) -> None:
        # Create a mock response with choices but delta is None
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].delta = None
        
        # Ensure text attribute doesn't exist to avoid fallback
        type(mock_response).text = property(lambda self: None)

        # Call get_content with the mock response
        result = get_content(mock_response)

        # Assert that the result is None
        self.assertIsNone(result)
",tests/_server/test_ai.py,TestGetContent
